{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Week 4: Ridge Regression (gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, you will implement ridge regression via gradient descent. You will:\n",
    "* Convert an SFrame into a Numpy array\n",
    "* Write a Numpy function to compute the derivative of the regression weights with respect to a single feature\n",
    "* Write gradient descent function to compute the regression weights given an initial weight vector, step size, tolerance, and L2 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fire up Turi Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have the latest version of Turi Create"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import turicreate\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in house sales data\n",
    "\n",
    "Dataset is from house sales in King County, the region where the city of Seattle, WA is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sales = turicreate.SFrame('~/my-env/1_Regression/data/home_data.sframe/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to do any \"feature engineering\" like creating new features or adjusting existing ones we should do this directly using the SFrames as seen in the first notebook of Week 2. For this notebook, however, we will work with the existing features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import useful functions from previous notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in Week 2, we convert the SFrame into a 2D Numpy array. Copy and paste `get_numpy_data()` from the second notebook of Week 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # note this allows us to refer to numpy as np instead "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_numpy_data(data_sframe, features, output):\n",
    "    data_sframe['constant'] = 1 # this is how you add a constant column to an SFrame\n",
    "    # add the column 'constant' to the front of the features list so that we can extract it along with the others:\n",
    "    features = ['constant'] + features # this is how you combine two lists\n",
    "    # select the columns of data_SFrame given by the features list into the SFrame features_sframe (now including constant):\n",
    "    features_sframe = data_sframe[features]\n",
    "    # the following line will convert the features_SFrame into a numpy matrix:\n",
    "    feature_matrix = features_sframe.to_numpy()\n",
    "    # assign the column of data_sframe associated with the output to the SArray output_sarray\n",
    "    output_sarray = data_sframe[output]\n",
    "    # the following will convert the SArray into a numpy array by first converting it to a list\n",
    "    output_array = output_sarray.to_numpy()\n",
    "    return(feature_matrix, output_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, copy and paste the `predict_output()` function to compute the predictions for an entire matrix of features given the matrix and the weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_output(feature_matrix, weights):\n",
    "    # assume feature_matrix is a numpy matrix containing the features as columns and weights is a corresponding numpy array\n",
    "    # create the predictions vector by using np.dot()\n",
    "    predictions = np.dot(feature_matrix, weights)\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing the Derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to move to computing the derivative of the regression cost function. Recall that the cost function is the sum over the data points of the squared difference between an observed output and a predicted output, plus the L2 penalty term.\n",
    "```\n",
    "Cost(w)\n",
    "= SUM[ (prediction - output)^2 ]\n",
    "+ l2_penalty*(w[0]^2 + w[1]^2 + ... + w[k]^2).\n",
    "```\n",
    "\n",
    "Since the derivative of a sum is the sum of the derivatives, we can take the derivative of the first part (the RSS) as we did in the notebook for the unregularized case in Week 2 and add the derivative of the regularization part.  As we saw, the derivative of the RSS with respect to `w[i]` can be written as: \n",
    "```\n",
    "2*SUM[ error*[feature_i] ].\n",
    "```\n",
    "The derivative of the regularization term with respect to `w[i]` is:\n",
    "```\n",
    "2*l2_penalty*w[i].\n",
    "```\n",
    "Summing both, we get\n",
    "```\n",
    "2*SUM[ error*[feature_i] ] + 2*l2_penalty*w[i].\n",
    "```\n",
    "That is, the derivative for the weight for feature i is the sum (over data points) of 2 times the product of the error and the feature itself, plus `2*l2_penalty*w[i]`. \n",
    "\n",
    "**We will not regularize the constant.**  Thus, in the case of the constant, the derivative is just twice the sum of the errors (without the `2*l2_penalty*w[0]` term).\n",
    "\n",
    "Recall that twice the sum of the product of two vectors is just twice the dot product of the two vectors. Therefore the derivative for the weight for feature_i is just two times the dot product between the values of feature_i and the current errors, plus `2*l2_penalty*w[i]`.\n",
    "\n",
    "With this in mind complete the following derivative function which computes the derivative of the weight given the value of the feature (over all data points) and the errors (over all data points).  To decide when to we are dealing with the constant (so we don't regularize it) we added the extra parameter to the call `feature_is_constant` which you should set to `True` when computing the derivative of the constant and `False` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_derivative_ridge(errors, feature, weight, l2_penalty, feature_is_constant):\n",
    "    # If feature_is_constant is True, derivative is twice the dot product of errors and feature\n",
    "    if feature_is_constant:\n",
    "        derivative = 2 * sum(errors*feature)\n",
    "    # Otherwise, derivative is twice the dot product plus 2*l2_penalty*weight\n",
    "    else:\n",
    "        derivative = (2 * sum(errors*feature)) + (2*l2_penalty*weight)\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To test your feature derivartive run the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-56554166782350.0\n",
      "-56554166782350.0\n",
      "\n",
      "-22446749336.0\n",
      "-22446749336.0\n"
     ]
    }
   ],
   "source": [
    "(example_features, example_output) = get_numpy_data(sales, ['sqft_living'], 'price') \n",
    "my_weights = np.array([1., 10.])\n",
    "test_predictions = predict_output(example_features, my_weights) \n",
    "errors = test_predictions - example_output # prediction errors\n",
    "\n",
    "# next two lines should print the same values\n",
    "print (feature_derivative_ridge(errors, example_features[:,1], my_weights[1], 1, False))\n",
    "print (np.sum(errors*example_features[:,1])*2+20.)\n",
    "print ('')\n",
    "\n",
    "# next two lines should print the same values\n",
    "print (feature_derivative_ridge(errors, example_features[:,0], my_weights[0], 1, True))\n",
    "print (np.sum(errors)*2.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will write a function that performs a gradient descent. The basic premise is simple. Given a starting point we update the current weights by moving in the negative gradient direction. Recall that the gradient is the direction of *increase* and therefore the negative gradient is the direction of *decrease* and we're trying to *minimize* a cost function. \n",
    "\n",
    "The amount by which we move in the negative gradient *direction*  is called the 'step size'. We stop when we are 'sufficiently close' to the optimum. Unlike in Week 2, this time we will set a **maximum number of iterations** and take gradient steps until we reach this maximum number. If no maximum number is supplied, the maximum should be set 100 by default. (Use default parameter values in Python.)\n",
    "\n",
    "With this in mind, complete the following gradient descent function below using your derivative function above. For each step in the gradient descent, we update the weight for each feature before computing our stopping criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ridge_regression_gradient_descent(feature_matrix, output, initial_weights, step_size, l2_penalty, max_iterations=100):\n",
    "    print ('Starting gradient descent with l2_penalty = ' + str(l2_penalty))\n",
    "    \n",
    "    weights = np.array(initial_weights) # make sure it's a numpy array\n",
    "    iteration = 0 # iteration counter\n",
    "    print_frequency = 1  # for adjusting frequency of debugging output\n",
    "    \n",
    "    #while not reached maximum number of iterations:\n",
    "    while iteration < max_iterations:\n",
    "        iteration += 1  # increment iteration counter\n",
    "    ### === code section for adjusting frequency of debugging output. ===\n",
    "        if iteration == 10:\n",
    "            print_frequency = 10\n",
    "        if iteration == 100:\n",
    "            print_frequency = 100\n",
    "        if iteration%print_frequency==0:\n",
    "            print('Iteration = ' + str(iteration))\n",
    "        ### === end code section ===\n",
    "\n",
    "        # compute the predictions based on feature_matrix and weights using your predict_output() function\n",
    "        predictions = predict_output(feature_matrix, weights)\n",
    "        # compute the errors as predictions - output\n",
    "        errors = predictions - output\n",
    "        # from time to time, print the value of the cost function\n",
    "        if iteration%print_frequency==0:\n",
    "            print ('Cost function = ', str(np.dot(errors,errors) + l2_penalty*(np.dot(weights,weights) - weights[0]**2)))\n",
    "\n",
    "        for i in range(len(weights)): # loop over each weight\n",
    "            # Recall that feature_matrix[:,i] is the feature column associated with weights[i]\n",
    "            # compute the derivative for weight[i].\n",
    "            #(Remember: when i=0, you are computing the derivative of the constant!)\n",
    "            if i ==0:\n",
    "                derivative=feature_derivative_ridge(errors, feature_matrix[:,i], weights[i], l2_penalty, True)\n",
    "            else:\n",
    "                derivative=feature_derivative_ridge(errors, feature_matrix[:,i], weights[i], l2_penalty, False)\n",
    "            # subtract the step size times the derivative from the current weight\n",
    "            weights[i] = weights[i] - step_size * derivative\n",
    "            print ('Done with gradient descent at iteration ', iteration)\n",
    "            print ('Learned weights = ', str(weights))\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing effect of L2 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The L2 penalty gets its name because it causes weights to have small L2 norms than otherwise. Let's see how large weights get penalized. Let us consider a simple model with 1 feature:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_features = ['sqft_living']\n",
    "my_output = 'price'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us split the dataset into training set and test set. Make sure to use `seed=0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data = sales.random_split(.8,seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, we will only use `'sqft_living'` to predict `'price'`. Use the `get_numpy_data` function to get a Numpy versions of your data with only this feature, for both the `train_data` and the `test_data`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "(simple_feature_matrix, output) = get_numpy_data(train_data, simple_features, my_output)\n",
    "(simple_test_feature_matrix, test_output) = get_numpy_data(test_data, simple_features, my_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's set the parameters for our optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = np.array([0., 0.])\n",
    "step_size = 1e-12\n",
    "max_iterations=1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's consider no regularization.  Set the `l2_penalty` to `0.0` and run your ridge regression algorithm to learn the weights of your model.  Call your weights:\n",
    "\n",
    "`simple_weights_0_penalty`\n",
    "\n",
    "we'll use them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent with l2_penalty = 0.0\n",
      "Iteration = 1\n",
      "Cost function =  7433051851026171.0\n",
      "Done with gradient descent at iteration  1\n",
      "Learned weights =  [0.0187527 0.       ]\n",
      "Done with gradient descent at iteration  1\n",
      "Learned weights =  [1.87526989e-02 4.73325137e+01]\n",
      "Iteration = 2\n",
      "Cost function =  5394267213135526.0\n",
      "Done with gradient descent at iteration  2\n",
      "Learned weights =  [3.40823824e-02 4.73325137e+01]\n",
      "Done with gradient descent at iteration  2\n",
      "Learned weights =  [3.40823824e-02 8.61473080e+01]\n",
      "Iteration = 3\n",
      "Cost function =  4023237736501162.0\n",
      "Done with gradient descent at iteration  3\n",
      "Learned weights =  [4.6605039e-02 8.6147308e+01]\n",
      "Done with gradient descent at iteration  3\n",
      "Learned weights =  [4.66050390e-02 1.17977189e+02]\n",
      "Iteration = 4\n",
      "Cost function =  3101256183922412.5\n",
      "Done with gradient descent at iteration  4\n",
      "Learned weights =  [5.68258070e-02 1.17977189e+02]\n",
      "Done with gradient descent at iteration  4\n",
      "Learned weights =  [5.68258070e-02 1.44079125e+02]\n",
      "Iteration = 5\n",
      "Cost function =  2481247644505113.0\n",
      "Done with gradient descent at iteration  5\n",
      "Learned weights =  [6.51589227e-02 1.44079125e+02]\n",
      "Done with gradient descent at iteration  5\n",
      "Learned weights =  [6.51589227e-02 1.65483889e+02]\n",
      "Iteration = 6\n",
      "Cost function =  2064308077891938.2\n",
      "Done with gradient descent at iteration  6\n",
      "Learned weights =  [7.19440783e-02 1.65483889e+02]\n",
      "Done with gradient descent at iteration  6\n",
      "Learned weights =  [7.19440783e-02 1.83036761e+02]\n",
      "Iteration = 7\n",
      "Cost function =  1783927097372276.8\n",
      "Done with gradient descent at iteration  7\n",
      "Learned weights =  [7.74598370e-02 1.83036761e+02]\n",
      "Done with gradient descent at iteration  7\n",
      "Learned weights =  [7.74598370e-02 1.97430906e+02]\n",
      "Iteration = 8\n",
      "Cost function =  1595378203154870.0\n",
      "Done with gradient descent at iteration  8\n",
      "Learned weights =  [8.19346329e-02 1.97430906e+02]\n",
      "Done with gradient descent at iteration  8\n",
      "Learned weights =  [8.19346329e-02 2.09234754e+02]\n",
      "Iteration = 9\n",
      "Cost function =  1468583991054994.2\n",
      "Done with gradient descent at iteration  9\n",
      "Learned weights =  [8.55557925e-02 2.09234754e+02]\n",
      "Done with gradient descent at iteration  9\n",
      "Learned weights =  [8.55557925e-02 2.18914442e+02]\n",
      "Iteration = 10\n",
      "Cost function =  1383318191484978.5\n",
      "Done with gradient descent at iteration  10\n",
      "Learned weights =  [8.84769319e-02 2.18914442e+02]\n",
      "Done with gradient descent at iteration  10\n",
      "Learned weights =  [8.84769319e-02 2.26852222e+02]\n",
      "Done with gradient descent at iteration  11\n",
      "Learned weights =  [9.08240230e-02 2.26852222e+02]\n",
      "Done with gradient descent at iteration  11\n",
      "Learned weights =  [9.08240230e-02 2.33361559e+02]\n",
      "Done with gradient descent at iteration  12\n",
      "Learned weights =  [9.27003689e-02 2.33361559e+02]\n",
      "Done with gradient descent at iteration  12\n",
      "Learned weights =  [9.27003689e-02 2.38699509e+02]\n",
      "Done with gradient descent at iteration  13\n",
      "Learned weights =  [9.41906823e-02 2.38699509e+02]\n",
      "Done with gradient descent at iteration  13\n",
      "Learned weights =  [9.41906823e-02 2.43076868e+02]\n",
      "Done with gradient descent at iteration  14\n",
      "Learned weights =  [9.53644317e-02 2.43076868e+02]\n",
      "Done with gradient descent at iteration  14\n",
      "Learned weights =  [9.53644317e-02 2.46666500e+02]\n",
      "Done with gradient descent at iteration  15\n",
      "Learned weights =  [9.62785844e-02 2.46666500e+02]\n",
      "Done with gradient descent at iteration  15\n",
      "Learned weights =  [9.62785844e-02 2.49610160e+02]\n",
      "Done with gradient descent at iteration  16\n",
      "Learned weights =  [9.6979856e-02 2.4961016e+02]\n",
      "Done with gradient descent at iteration  16\n",
      "Learned weights =  [9.69798560e-02 2.52024094e+02]\n",
      "Done with gradient descent at iteration  17\n",
      "Learned weights =  [9.75065557e-02 2.52024094e+02]\n",
      "Done with gradient descent at iteration  17\n",
      "Learned weights =  [9.75065557e-02 2.54003629e+02]\n",
      "Done with gradient descent at iteration  18\n",
      "Learned weights =  [9.78900984e-02 2.54003629e+02]\n",
      "Done with gradient descent at iteration  18\n",
      "Learned weights =  [9.78900984e-02 2.55626937e+02]\n",
      "Done with gradient descent at iteration  19\n",
      "Learned weights =  [9.81562460e-02 2.55626937e+02]\n",
      "Done with gradient descent at iteration  19\n",
      "Learned weights =  [9.81562460e-02 2.56958122e+02]\n",
      "Iteration = 20\n",
      "Cost function =  1211562140496238.8\n",
      "Done with gradient descent at iteration  20\n",
      "Learned weights =  [9.83261243e-02 2.56958122e+02]\n",
      "Done with gradient descent at iteration  20\n",
      "Learned weights =  [9.83261243e-02 2.58049754e+02]\n",
      "Done with gradient descent at iteration  21\n",
      "Learned weights =  [9.84170574e-02 2.58049754e+02]\n",
      "Done with gradient descent at iteration  21\n",
      "Learned weights =  [9.84170574e-02 2.58944942e+02]\n",
      "Done with gradient descent at iteration  22\n",
      "Learned weights =  [9.84432520e-02 2.58944942e+02]\n",
      "Done with gradient descent at iteration  22\n",
      "Learned weights =  [9.84432520e-02 2.59679036e+02]\n",
      "Done with gradient descent at iteration  23\n",
      "Learned weights =  [9.84163580e-02 2.59679036e+02]\n",
      "Done with gradient descent at iteration  23\n",
      "Learned weights =  [9.84163580e-02 2.60281026e+02]\n",
      "Done with gradient descent at iteration  24\n",
      "Learned weights =  [9.83459289e-02 2.60281026e+02]\n",
      "Done with gradient descent at iteration  24\n",
      "Learned weights =  [9.83459289e-02 2.60774685e+02]\n",
      "Done with gradient descent at iteration  25\n",
      "Learned weights =  [9.82397992e-02 2.60774685e+02]\n",
      "Done with gradient descent at iteration  25\n",
      "Learned weights =  [9.82397992e-02 2.61179508e+02]\n",
      "Done with gradient descent at iteration  26\n",
      "Learned weights =  [9.81043934e-02 2.61179508e+02]\n",
      "Done with gradient descent at iteration  26\n",
      "Learned weights =  [9.81043934e-02 2.61511481e+02]\n",
      "Done with gradient descent at iteration  27\n",
      "Learned weights =  [9.79449797e-02 2.61511481e+02]\n",
      "Done with gradient descent at iteration  27\n",
      "Learned weights =  [9.79449797e-02 2.61783714e+02]\n",
      "Done with gradient descent at iteration  28\n",
      "Learned weights =  [9.77658787e-02 2.61783714e+02]\n",
      "Done with gradient descent at iteration  28\n",
      "Learned weights =  [9.77658787e-02 2.62006957e+02]\n",
      "Done with gradient descent at iteration  29\n",
      "Learned weights =  [9.75706330e-02 2.62006957e+02]\n",
      "Done with gradient descent at iteration  29\n",
      "Learned weights =  [9.75706330e-02 2.62190027e+02]\n",
      "Iteration = 30\n",
      "Cost function =  1208313762678823.0\n",
      "Done with gradient descent at iteration  30\n",
      "Learned weights =  [9.73621480e-02 2.62190027e+02]\n",
      "Done with gradient descent at iteration  30\n",
      "Learned weights =  [9.73621480e-02 2.62340152e+02]\n",
      "Done with gradient descent at iteration  31\n",
      "Learned weights =  [9.71428062e-02 2.62340152e+02]\n",
      "Done with gradient descent at iteration  31\n",
      "Learned weights =  [9.71428062e-02 2.62463261e+02]\n",
      "Done with gradient descent at iteration  32\n",
      "Learned weights =  [9.69145613e-02 2.62463261e+02]\n",
      "Done with gradient descent at iteration  32\n",
      "Learned weights =  [9.69145613e-02 2.62564217e+02]\n",
      "Done with gradient descent at iteration  33\n",
      "Learned weights =  [9.66790155e-02 2.62564217e+02]\n",
      "Done with gradient descent at iteration  33\n",
      "Learned weights =  [9.66790155e-02 2.62647005e+02]\n",
      "Done with gradient descent at iteration  34\n",
      "Learned weights =  [9.64374826e-02 2.62647005e+02]\n",
      "Done with gradient descent at iteration  34\n",
      "Learned weights =  [9.64374826e-02 2.62714894e+02]\n",
      "Done with gradient descent at iteration  35\n",
      "Learned weights =  [9.61910400e-02 2.62714894e+02]\n",
      "Done with gradient descent at iteration  35\n",
      "Learned weights =  [9.61910400e-02 2.62770567e+02]\n",
      "Done with gradient descent at iteration  36\n",
      "Learned weights =  [9.59405712e-02 2.62770567e+02]\n",
      "Done with gradient descent at iteration  36\n",
      "Learned weights =  [9.59405712e-02 2.62816221e+02]\n",
      "Done with gradient descent at iteration  37\n",
      "Learned weights =  [9.56868009e-02 2.62816221e+02]\n",
      "Done with gradient descent at iteration  37\n",
      "Learned weights =  [9.56868009e-02 2.62853660e+02]\n",
      "Done with gradient descent at iteration  38\n",
      "Learned weights =  [9.5430323e-02 2.6285366e+02]\n",
      "Done with gradient descent at iteration  38\n",
      "Learned weights =  [9.54303230e-02 2.62884361e+02]\n",
      "Done with gradient descent at iteration  39\n",
      "Learned weights =  [9.51716248e-02 2.62884361e+02]\n",
      "Done with gradient descent at iteration  39\n",
      "Learned weights =  [9.51716248e-02 2.62909538e+02]\n",
      "Iteration = 40\n",
      "Cost function =  1208252326252869.8\n",
      "Done with gradient descent at iteration  40\n",
      "Learned weights =  [9.49111060e-02 2.62909538e+02]\n",
      "Done with gradient descent at iteration  40\n",
      "Learned weights =  [9.49111060e-02 2.62930184e+02]\n",
      "Done with gradient descent at iteration  41\n",
      "Learned weights =  [9.46490941e-02 2.62930184e+02]\n",
      "Done with gradient descent at iteration  41\n",
      "Learned weights =  [9.46490941e-02 2.62947114e+02]\n",
      "Done with gradient descent at iteration  42\n",
      "Learned weights =  [9.43858578e-02 2.62947114e+02]\n",
      "Done with gradient descent at iteration  42\n",
      "Learned weights =  [9.43858578e-02 2.62960998e+02]\n",
      "Done with gradient descent at iteration  43\n",
      "Learned weights =  [9.41216174e-02 2.62960998e+02]\n",
      "Done with gradient descent at iteration  43\n",
      "Learned weights =  [9.41216174e-02 2.62972383e+02]\n",
      "Done with gradient descent at iteration  44\n",
      "Learned weights =  [9.38565537e-02 2.62972383e+02]\n",
      "Done with gradient descent at iteration  44\n",
      "Learned weights =  [9.38565537e-02 2.62981720e+02]\n",
      "Done with gradient descent at iteration  45\n",
      "Learned weights =  [9.35908148e-02 2.62981720e+02]\n",
      "Done with gradient descent at iteration  45\n",
      "Learned weights =  [9.35908148e-02 2.62989376e+02]\n",
      "Done with gradient descent at iteration  46\n",
      "Learned weights =  [9.33245222e-02 2.62989376e+02]\n",
      "Done with gradient descent at iteration  46\n",
      "Learned weights =  [9.33245222e-02 2.62995655e+02]\n",
      "Done with gradient descent at iteration  47\n",
      "Learned weights =  [9.30577756e-02 2.62995655e+02]\n",
      "Done with gradient descent at iteration  47\n",
      "Learned weights =  [9.30577756e-02 2.63000804e+02]\n",
      "Done with gradient descent at iteration  48\n",
      "Learned weights =  [9.27906566e-02 2.63000804e+02]\n",
      "Done with gradient descent at iteration  48\n",
      "Learned weights =  [9.27906566e-02 2.63005026e+02]\n",
      "Done with gradient descent at iteration  49\n",
      "Learned weights =  [9.25232323e-02 2.63005026e+02]\n",
      "Done with gradient descent at iteration  49\n",
      "Learned weights =  [9.25232323e-02 2.63008488e+02]\n",
      "Iteration = 50\n",
      "Cost function =  1208251163612919.5\n",
      "Done with gradient descent at iteration  50\n",
      "Learned weights =  [9.22555576e-02 2.63008488e+02]\n",
      "Done with gradient descent at iteration  50\n",
      "Learned weights =  [9.22555576e-02 2.63011328e+02]\n",
      "Done with gradient descent at iteration  51\n",
      "Learned weights =  [9.19876775e-02 2.63011328e+02]\n",
      "Done with gradient descent at iteration  51\n",
      "Learned weights =  [9.19876775e-02 2.63013656e+02]\n",
      "Done with gradient descent at iteration  52\n",
      "Learned weights =  [9.17196291e-02 2.63013656e+02]\n",
      "Done with gradient descent at iteration  52\n",
      "Learned weights =  [9.17196291e-02 2.63015566e+02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  53\n",
      "Learned weights =  [9.14514426e-02 2.63015566e+02]\n",
      "Done with gradient descent at iteration  53\n",
      "Learned weights =  [9.14514426e-02 2.63017132e+02]\n",
      "Done with gradient descent at iteration  54\n",
      "Learned weights =  [9.11831428e-02 2.63017132e+02]\n",
      "Done with gradient descent at iteration  54\n",
      "Learned weights =  [9.11831428e-02 2.63018416e+02]\n",
      "Done with gradient descent at iteration  55\n",
      "Learned weights =  [9.09147502e-02 2.63018416e+02]\n",
      "Done with gradient descent at iteration  55\n",
      "Learned weights =  [9.09147502e-02 2.63019469e+02]\n",
      "Done with gradient descent at iteration  56\n",
      "Learned weights =  [9.06462815e-02 2.63019469e+02]\n",
      "Done with gradient descent at iteration  56\n",
      "Learned weights =  [9.06462815e-02 2.63020332e+02]\n",
      "Done with gradient descent at iteration  57\n",
      "Learned weights =  [9.03777503e-02 2.63020332e+02]\n",
      "Done with gradient descent at iteration  57\n",
      "Learned weights =  [9.03777503e-02 2.63021040e+02]\n",
      "Done with gradient descent at iteration  58\n",
      "Learned weights =  [9.01091679e-02 2.63021040e+02]\n",
      "Done with gradient descent at iteration  58\n",
      "Learned weights =  [9.01091679e-02 2.63021621e+02]\n",
      "Done with gradient descent at iteration  59\n",
      "Learned weights =  [8.98405436e-02 2.63021621e+02]\n",
      "Done with gradient descent at iteration  59\n",
      "Learned weights =  [8.98405436e-02 2.63022097e+02]\n",
      "Iteration = 60\n",
      "Cost function =  1208251140915263.0\n",
      "Done with gradient descent at iteration  60\n",
      "Learned weights =  [8.95718848e-02 2.63022097e+02]\n",
      "Done with gradient descent at iteration  60\n",
      "Learned weights =  [8.95718848e-02 2.63022488e+02]\n",
      "Done with gradient descent at iteration  61\n",
      "Learned weights =  [8.93031977e-02 2.63022488e+02]\n",
      "Done with gradient descent at iteration  61\n",
      "Learned weights =  [8.93031977e-02 2.63022808e+02]\n",
      "Done with gradient descent at iteration  62\n",
      "Learned weights =  [8.90344875e-02 2.63022808e+02]\n",
      "Done with gradient descent at iteration  62\n",
      "Learned weights =  [8.90344875e-02 2.63023071e+02]\n",
      "Done with gradient descent at iteration  63\n",
      "Learned weights =  [8.87657583e-02 2.63023071e+02]\n",
      "Done with gradient descent at iteration  63\n",
      "Learned weights =  [8.87657583e-02 2.63023286e+02]\n",
      "Done with gradient descent at iteration  64\n",
      "Learned weights =  [8.84970136e-02 2.63023286e+02]\n",
      "Done with gradient descent at iteration  64\n",
      "Learned weights =  [8.84970136e-02 2.63023463e+02]\n",
      "Done with gradient descent at iteration  65\n",
      "Learned weights =  [8.82282561e-02 2.63023463e+02]\n",
      "Done with gradient descent at iteration  65\n",
      "Learned weights =  [8.82282561e-02 2.63023608e+02]\n",
      "Done with gradient descent at iteration  66\n",
      "Learned weights =  [8.79594881e-02 2.63023608e+02]\n",
      "Done with gradient descent at iteration  66\n",
      "Learned weights =  [8.79594881e-02 2.63023727e+02]\n",
      "Done with gradient descent at iteration  67\n",
      "Learned weights =  [8.76907115e-02 2.63023727e+02]\n",
      "Done with gradient descent at iteration  67\n",
      "Learned weights =  [8.76907115e-02 2.63023824e+02]\n",
      "Done with gradient descent at iteration  68\n",
      "Learned weights =  [8.74219279e-02 2.63023824e+02]\n",
      "Done with gradient descent at iteration  68\n",
      "Learned weights =  [8.74219279e-02 2.63023904e+02]\n",
      "Done with gradient descent at iteration  69\n",
      "Learned weights =  [8.71531385e-02 2.63023904e+02]\n",
      "Done with gradient descent at iteration  69\n",
      "Learned weights =  [8.71531385e-02 2.63023970e+02]\n",
      "Iteration = 70\n",
      "Cost function =  1208251139777036.0\n",
      "Done with gradient descent at iteration  70\n",
      "Learned weights =  [8.68843444e-02 2.63023970e+02]\n",
      "Done with gradient descent at iteration  70\n",
      "Learned weights =  [8.68843444e-02 2.63024024e+02]\n",
      "Done with gradient descent at iteration  71\n",
      "Learned weights =  [8.66155463e-02 2.63024024e+02]\n",
      "Done with gradient descent at iteration  71\n",
      "Learned weights =  [8.66155463e-02 2.63024068e+02]\n",
      "Done with gradient descent at iteration  72\n",
      "Learned weights =  [8.63467452e-02 2.63024068e+02]\n",
      "Done with gradient descent at iteration  72\n",
      "Learned weights =  [8.63467452e-02 2.63024104e+02]\n",
      "Done with gradient descent at iteration  73\n",
      "Learned weights =  [8.60779414e-02 2.63024104e+02]\n",
      "Done with gradient descent at iteration  73\n",
      "Learned weights =  [8.60779414e-02 2.63024134e+02]\n",
      "Done with gradient descent at iteration  74\n",
      "Learned weights =  [8.58091354e-02 2.63024134e+02]\n",
      "Done with gradient descent at iteration  74\n",
      "Learned weights =  [8.58091354e-02 2.63024158e+02]\n",
      "Done with gradient descent at iteration  75\n",
      "Learned weights =  [8.55403277e-02 2.63024158e+02]\n",
      "Done with gradient descent at iteration  75\n",
      "Learned weights =  [8.55403277e-02 2.63024178e+02]\n",
      "Done with gradient descent at iteration  76\n",
      "Learned weights =  [8.52715186e-02 2.63024178e+02]\n",
      "Done with gradient descent at iteration  76\n",
      "Learned weights =  [8.52715186e-02 2.63024195e+02]\n",
      "Done with gradient descent at iteration  77\n",
      "Learned weights =  [8.50027083e-02 2.63024195e+02]\n",
      "Done with gradient descent at iteration  77\n",
      "Learned weights =  [8.50027083e-02 2.63024208e+02]\n",
      "Done with gradient descent at iteration  78\n",
      "Learned weights =  [8.47338970e-02 2.63024208e+02]\n",
      "Done with gradient descent at iteration  78\n",
      "Learned weights =  [8.47338970e-02 2.63024219e+02]\n",
      "Done with gradient descent at iteration  79\n",
      "Learned weights =  [8.44650849e-02 2.63024219e+02]\n",
      "Done with gradient descent at iteration  79\n",
      "Learned weights =  [8.44650849e-02 2.63024228e+02]\n",
      "Iteration = 80\n",
      "Cost function =  1208251139046557.0\n",
      "Done with gradient descent at iteration  80\n",
      "Learned weights =  [8.41962722e-02 2.63024228e+02]\n",
      "Done with gradient descent at iteration  80\n",
      "Learned weights =  [8.41962722e-02 2.63024236e+02]\n",
      "Done with gradient descent at iteration  81\n",
      "Learned weights =  [8.39274589e-02 2.63024236e+02]\n",
      "Done with gradient descent at iteration  81\n",
      "Learned weights =  [8.39274589e-02 2.63024242e+02]\n",
      "Done with gradient descent at iteration  82\n",
      "Learned weights =  [8.36586452e-02 2.63024242e+02]\n",
      "Done with gradient descent at iteration  82\n",
      "Learned weights =  [8.36586452e-02 2.63024247e+02]\n",
      "Done with gradient descent at iteration  83\n",
      "Learned weights =  [8.33898312e-02 2.63024247e+02]\n",
      "Done with gradient descent at iteration  83\n",
      "Learned weights =  [8.33898312e-02 2.63024251e+02]\n",
      "Done with gradient descent at iteration  84\n",
      "Learned weights =  [8.31210168e-02 2.63024251e+02]\n",
      "Done with gradient descent at iteration  84\n",
      "Learned weights =  [8.31210168e-02 2.63024255e+02]\n",
      "Done with gradient descent at iteration  85\n",
      "Learned weights =  [8.28522023e-02 2.63024255e+02]\n",
      "Done with gradient descent at iteration  85\n",
      "Learned weights =  [8.28522023e-02 2.63024258e+02]\n",
      "Done with gradient descent at iteration  86\n",
      "Learned weights =  [8.25833875e-02 2.63024258e+02]\n",
      "Done with gradient descent at iteration  86\n",
      "Learned weights =  [8.25833875e-02 2.63024260e+02]\n",
      "Done with gradient descent at iteration  87\n",
      "Learned weights =  [8.23145725e-02 2.63024260e+02]\n",
      "Done with gradient descent at iteration  87\n",
      "Learned weights =  [8.23145725e-02 2.63024262e+02]\n",
      "Done with gradient descent at iteration  88\n",
      "Learned weights =  [8.20457575e-02 2.63024262e+02]\n",
      "Done with gradient descent at iteration  88\n",
      "Learned weights =  [8.20457575e-02 2.63024264e+02]\n",
      "Done with gradient descent at iteration  89\n",
      "Learned weights =  [8.17769423e-02 2.63024264e+02]\n",
      "Done with gradient descent at iteration  89\n",
      "Learned weights =  [8.17769423e-02 2.63024265e+02]\n",
      "Iteration = 90\n",
      "Cost function =  1208251138323789.0\n",
      "Done with gradient descent at iteration  90\n",
      "Learned weights =  [8.15081270e-02 2.63024265e+02]\n",
      "Done with gradient descent at iteration  90\n",
      "Learned weights =  [8.15081270e-02 2.63024266e+02]\n",
      "Done with gradient descent at iteration  91\n",
      "Learned weights =  [8.12393117e-02 2.63024266e+02]\n",
      "Done with gradient descent at iteration  91\n",
      "Learned weights =  [8.12393117e-02 2.63024267e+02]\n",
      "Done with gradient descent at iteration  92\n",
      "Learned weights =  [8.09704963e-02 2.63024267e+02]\n",
      "Done with gradient descent at iteration  92\n",
      "Learned weights =  [8.09704963e-02 2.63024268e+02]\n",
      "Done with gradient descent at iteration  93\n",
      "Learned weights =  [8.07016808e-02 2.63024268e+02]\n",
      "Done with gradient descent at iteration  93\n",
      "Learned weights =  [8.07016808e-02 2.63024268e+02]\n",
      "Done with gradient descent at iteration  94\n",
      "Learned weights =  [8.04328654e-02 2.63024268e+02]\n",
      "Done with gradient descent at iteration  94\n",
      "Learned weights =  [8.04328654e-02 2.63024269e+02]\n",
      "Done with gradient descent at iteration  95\n",
      "Learned weights =  [8.01640498e-02 2.63024269e+02]\n",
      "Done with gradient descent at iteration  95\n",
      "Learned weights =  [8.01640498e-02 2.63024269e+02]\n",
      "Done with gradient descent at iteration  96\n",
      "Learned weights =  [7.98952343e-02 2.63024269e+02]\n",
      "Done with gradient descent at iteration  96\n",
      "Learned weights =  [7.98952343e-02 2.63024270e+02]\n",
      "Done with gradient descent at iteration  97\n",
      "Learned weights =  [7.96264187e-02 2.63024270e+02]\n",
      "Done with gradient descent at iteration  97\n",
      "Learned weights =  [7.96264187e-02 2.63024270e+02]\n",
      "Done with gradient descent at iteration  98\n",
      "Learned weights =  [7.93576031e-02 2.63024270e+02]\n",
      "Done with gradient descent at iteration  98\n",
      "Learned weights =  [7.93576031e-02 2.63024271e+02]\n",
      "Done with gradient descent at iteration  99\n",
      "Learned weights =  [7.90887876e-02 2.63024271e+02]\n",
      "Done with gradient descent at iteration  99\n",
      "Learned weights =  [7.90887876e-02 2.63024271e+02]\n",
      "Iteration = 100\n",
      "Cost function =  1208251137601168.0\n",
      "Done with gradient descent at iteration  100\n",
      "Learned weights =  [7.88199720e-02 2.63024271e+02]\n",
      "Done with gradient descent at iteration  100\n",
      "Learned weights =  [7.88199720e-02 2.63024271e+02]\n",
      "Done with gradient descent at iteration  101\n",
      "Learned weights =  [7.85511563e-02 2.63024271e+02]\n",
      "Done with gradient descent at iteration  101\n",
      "Learned weights =  [7.85511563e-02 2.63024271e+02]\n",
      "Done with gradient descent at iteration  102\n",
      "Learned weights =  [7.82823407e-02 2.63024271e+02]\n",
      "Done with gradient descent at iteration  102\n",
      "Learned weights =  [7.82823407e-02 2.63024271e+02]\n",
      "Done with gradient descent at iteration  103\n",
      "Learned weights =  [7.80135251e-02 2.63024271e+02]\n",
      "Done with gradient descent at iteration  103\n",
      "Learned weights =  [7.80135251e-02 2.63024272e+02]\n",
      "Done with gradient descent at iteration  104\n",
      "Learned weights =  [7.77447095e-02 2.63024272e+02]\n",
      "Done with gradient descent at iteration  104\n",
      "Learned weights =  [7.77447095e-02 2.63024272e+02]\n",
      "Done with gradient descent at iteration  105\n",
      "Learned weights =  [7.74758938e-02 2.63024272e+02]\n",
      "Done with gradient descent at iteration  105\n",
      "Learned weights =  [7.74758938e-02 2.63024272e+02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  106\n",
      "Learned weights =  [7.72070782e-02 2.63024272e+02]\n",
      "Done with gradient descent at iteration  106\n",
      "Learned weights =  [7.72070782e-02 2.63024272e+02]\n",
      "Done with gradient descent at iteration  107\n",
      "Learned weights =  [7.69382626e-02 2.63024272e+02]\n",
      "Done with gradient descent at iteration  107\n",
      "Learned weights =  [7.69382626e-02 2.63024272e+02]\n",
      "Done with gradient descent at iteration  108\n",
      "Learned weights =  [7.66694469e-02 2.63024272e+02]\n",
      "Done with gradient descent at iteration  108\n",
      "Learned weights =  [7.66694469e-02 2.63024272e+02]\n",
      "Done with gradient descent at iteration  109\n",
      "Learned weights =  [7.64006313e-02 2.63024272e+02]\n",
      "Done with gradient descent at iteration  109\n",
      "Learned weights =  [7.64006313e-02 2.63024273e+02]\n",
      "Done with gradient descent at iteration  110\n",
      "Learned weights =  [7.61318156e-02 2.63024273e+02]\n",
      "Done with gradient descent at iteration  110\n",
      "Learned weights =  [7.61318156e-02 2.63024273e+02]\n",
      "Done with gradient descent at iteration  111\n",
      "Learned weights =  [7.58630000e-02 2.63024273e+02]\n",
      "Done with gradient descent at iteration  111\n",
      "Learned weights =  [7.58630000e-02 2.63024273e+02]\n",
      "Done with gradient descent at iteration  112\n",
      "Learned weights =  [7.55941844e-02 2.63024273e+02]\n",
      "Done with gradient descent at iteration  112\n",
      "Learned weights =  [7.55941844e-02 2.63024273e+02]\n",
      "Done with gradient descent at iteration  113\n",
      "Learned weights =  [7.53253687e-02 2.63024273e+02]\n",
      "Done with gradient descent at iteration  113\n",
      "Learned weights =  [7.53253687e-02 2.63024273e+02]\n",
      "Done with gradient descent at iteration  114\n",
      "Learned weights =  [7.50565531e-02 2.63024273e+02]\n",
      "Done with gradient descent at iteration  114\n",
      "Learned weights =  [7.50565531e-02 2.63024273e+02]\n",
      "Done with gradient descent at iteration  115\n",
      "Learned weights =  [7.47877375e-02 2.63024273e+02]\n",
      "Done with gradient descent at iteration  115\n",
      "Learned weights =  [7.47877375e-02 2.63024273e+02]\n",
      "Done with gradient descent at iteration  116\n",
      "Learned weights =  [7.45189218e-02 2.63024273e+02]\n",
      "Done with gradient descent at iteration  116\n",
      "Learned weights =  [7.45189218e-02 2.63024273e+02]\n",
      "Done with gradient descent at iteration  117\n",
      "Learned weights =  [7.42501062e-02 2.63024273e+02]\n",
      "Done with gradient descent at iteration  117\n",
      "Learned weights =  [7.42501062e-02 2.63024273e+02]\n",
      "Done with gradient descent at iteration  118\n",
      "Learned weights =  [7.39812906e-02 2.63024273e+02]\n",
      "Done with gradient descent at iteration  118\n",
      "Learned weights =  [7.39812906e-02 2.63024274e+02]\n",
      "Done with gradient descent at iteration  119\n",
      "Learned weights =  [7.37124749e-02 2.63024274e+02]\n",
      "Done with gradient descent at iteration  119\n",
      "Learned weights =  [7.37124749e-02 2.63024274e+02]\n",
      "Done with gradient descent at iteration  120\n",
      "Learned weights =  [7.34436593e-02 2.63024274e+02]\n",
      "Done with gradient descent at iteration  120\n",
      "Learned weights =  [7.34436593e-02 2.63024274e+02]\n",
      "Done with gradient descent at iteration  121\n",
      "Learned weights =  [7.31748437e-02 2.63024274e+02]\n",
      "Done with gradient descent at iteration  121\n",
      "Learned weights =  [7.31748437e-02 2.63024274e+02]\n",
      "Done with gradient descent at iteration  122\n",
      "Learned weights =  [7.29060281e-02 2.63024274e+02]\n",
      "Done with gradient descent at iteration  122\n",
      "Learned weights =  [7.29060281e-02 2.63024274e+02]\n",
      "Done with gradient descent at iteration  123\n",
      "Learned weights =  [7.26372124e-02 2.63024274e+02]\n",
      "Done with gradient descent at iteration  123\n",
      "Learned weights =  [7.26372124e-02 2.63024274e+02]\n",
      "Done with gradient descent at iteration  124\n",
      "Learned weights =  [7.23683968e-02 2.63024274e+02]\n",
      "Done with gradient descent at iteration  124\n",
      "Learned weights =  [7.23683968e-02 2.63024274e+02]\n",
      "Done with gradient descent at iteration  125\n",
      "Learned weights =  [7.20995812e-02 2.63024274e+02]\n",
      "Done with gradient descent at iteration  125\n",
      "Learned weights =  [7.20995812e-02 2.63024274e+02]\n",
      "Done with gradient descent at iteration  126\n",
      "Learned weights =  [7.18307656e-02 2.63024274e+02]\n",
      "Done with gradient descent at iteration  126\n",
      "Learned weights =  [7.18307656e-02 2.63024274e+02]\n",
      "Done with gradient descent at iteration  127\n",
      "Learned weights =  [7.15619499e-02 2.63024274e+02]\n",
      "Done with gradient descent at iteration  127\n",
      "Learned weights =  [7.15619499e-02 2.63024275e+02]\n",
      "Done with gradient descent at iteration  128\n",
      "Learned weights =  [7.12931343e-02 2.63024275e+02]\n",
      "Done with gradient descent at iteration  128\n",
      "Learned weights =  [7.12931343e-02 2.63024275e+02]\n",
      "Done with gradient descent at iteration  129\n",
      "Learned weights =  [7.10243187e-02 2.63024275e+02]\n",
      "Done with gradient descent at iteration  129\n",
      "Learned weights =  [7.10243187e-02 2.63024275e+02]\n",
      "Done with gradient descent at iteration  130\n",
      "Learned weights =  [7.07555031e-02 2.63024275e+02]\n",
      "Done with gradient descent at iteration  130\n",
      "Learned weights =  [7.07555031e-02 2.63024275e+02]\n",
      "Done with gradient descent at iteration  131\n",
      "Learned weights =  [7.04866875e-02 2.63024275e+02]\n",
      "Done with gradient descent at iteration  131\n",
      "Learned weights =  [7.04866875e-02 2.63024275e+02]\n",
      "Done with gradient descent at iteration  132\n",
      "Learned weights =  [7.02178719e-02 2.63024275e+02]\n",
      "Done with gradient descent at iteration  132\n",
      "Learned weights =  [7.02178719e-02 2.63024275e+02]\n",
      "Done with gradient descent at iteration  133\n",
      "Learned weights =  [6.99490563e-02 2.63024275e+02]\n",
      "Done with gradient descent at iteration  133\n",
      "Learned weights =  [6.99490563e-02 2.63024275e+02]\n",
      "Done with gradient descent at iteration  134\n",
      "Learned weights =  [6.96802407e-02 2.63024275e+02]\n",
      "Done with gradient descent at iteration  134\n",
      "Learned weights =  [6.96802407e-02 2.63024275e+02]\n",
      "Done with gradient descent at iteration  135\n",
      "Learned weights =  [6.94114250e-02 2.63024275e+02]\n",
      "Done with gradient descent at iteration  135\n",
      "Learned weights =  [6.94114250e-02 2.63024275e+02]\n",
      "Done with gradient descent at iteration  136\n",
      "Learned weights =  [6.91426094e-02 2.63024275e+02]\n",
      "Done with gradient descent at iteration  136\n",
      "Learned weights =  [6.91426094e-02 2.63024276e+02]\n",
      "Done with gradient descent at iteration  137\n",
      "Learned weights =  [6.88737938e-02 2.63024276e+02]\n",
      "Done with gradient descent at iteration  137\n",
      "Learned weights =  [6.88737938e-02 2.63024276e+02]\n",
      "Done with gradient descent at iteration  138\n",
      "Learned weights =  [6.86049782e-02 2.63024276e+02]\n",
      "Done with gradient descent at iteration  138\n",
      "Learned weights =  [6.86049782e-02 2.63024276e+02]\n",
      "Done with gradient descent at iteration  139\n",
      "Learned weights =  [6.83361626e-02 2.63024276e+02]\n",
      "Done with gradient descent at iteration  139\n",
      "Learned weights =  [6.83361626e-02 2.63024276e+02]\n",
      "Done with gradient descent at iteration  140\n",
      "Learned weights =  [6.80673470e-02 2.63024276e+02]\n",
      "Done with gradient descent at iteration  140\n",
      "Learned weights =  [6.80673470e-02 2.63024276e+02]\n",
      "Done with gradient descent at iteration  141\n",
      "Learned weights =  [6.77985314e-02 2.63024276e+02]\n",
      "Done with gradient descent at iteration  141\n",
      "Learned weights =  [6.77985314e-02 2.63024276e+02]\n",
      "Done with gradient descent at iteration  142\n",
      "Learned weights =  [6.75297158e-02 2.63024276e+02]\n",
      "Done with gradient descent at iteration  142\n",
      "Learned weights =  [6.75297158e-02 2.63024276e+02]\n",
      "Done with gradient descent at iteration  143\n",
      "Learned weights =  [6.72609002e-02 2.63024276e+02]\n",
      "Done with gradient descent at iteration  143\n",
      "Learned weights =  [6.72609002e-02 2.63024276e+02]\n",
      "Done with gradient descent at iteration  144\n",
      "Learned weights =  [6.69920847e-02 2.63024276e+02]\n",
      "Done with gradient descent at iteration  144\n",
      "Learned weights =  [6.69920847e-02 2.63024276e+02]\n",
      "Done with gradient descent at iteration  145\n",
      "Learned weights =  [6.67232691e-02 2.63024276e+02]\n",
      "Done with gradient descent at iteration  145\n",
      "Learned weights =  [6.67232691e-02 2.63024277e+02]\n",
      "Done with gradient descent at iteration  146\n",
      "Learned weights =  [6.64544535e-02 2.63024277e+02]\n",
      "Done with gradient descent at iteration  146\n",
      "Learned weights =  [6.64544535e-02 2.63024277e+02]\n",
      "Done with gradient descent at iteration  147\n",
      "Learned weights =  [6.61856379e-02 2.63024277e+02]\n",
      "Done with gradient descent at iteration  147\n",
      "Learned weights =  [6.61856379e-02 2.63024277e+02]\n",
      "Done with gradient descent at iteration  148\n",
      "Learned weights =  [6.59168223e-02 2.63024277e+02]\n",
      "Done with gradient descent at iteration  148\n",
      "Learned weights =  [6.59168223e-02 2.63024277e+02]\n",
      "Done with gradient descent at iteration  149\n",
      "Learned weights =  [6.56480067e-02 2.63024277e+02]\n",
      "Done with gradient descent at iteration  149\n",
      "Learned weights =  [6.56480067e-02 2.63024277e+02]\n",
      "Done with gradient descent at iteration  150\n",
      "Learned weights =  [6.53791911e-02 2.63024277e+02]\n",
      "Done with gradient descent at iteration  150\n",
      "Learned weights =  [6.53791911e-02 2.63024277e+02]\n",
      "Done with gradient descent at iteration  151\n",
      "Learned weights =  [6.51103755e-02 2.63024277e+02]\n",
      "Done with gradient descent at iteration  151\n",
      "Learned weights =  [6.51103755e-02 2.63024277e+02]\n",
      "Done with gradient descent at iteration  152\n",
      "Learned weights =  [6.48415600e-02 2.63024277e+02]\n",
      "Done with gradient descent at iteration  152\n",
      "Learned weights =  [6.48415600e-02 2.63024277e+02]\n",
      "Done with gradient descent at iteration  153\n",
      "Learned weights =  [6.45727444e-02 2.63024277e+02]\n",
      "Done with gradient descent at iteration  153\n",
      "Learned weights =  [6.45727444e-02 2.63024277e+02]\n",
      "Done with gradient descent at iteration  154\n",
      "Learned weights =  [6.43039288e-02 2.63024277e+02]\n",
      "Done with gradient descent at iteration  154\n",
      "Learned weights =  [6.43039288e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  155\n",
      "Learned weights =  [6.40351132e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  155\n",
      "Learned weights =  [6.40351132e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  156\n",
      "Learned weights =  [6.37662977e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  156\n",
      "Learned weights =  [6.37662977e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  157\n",
      "Learned weights =  [6.34974821e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  157\n",
      "Learned weights =  [6.34974821e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  158\n",
      "Learned weights =  [6.32286665e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  158\n",
      "Learned weights =  [6.32286665e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  159\n",
      "Learned weights =  [6.29598509e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  159\n",
      "Learned weights =  [6.29598509e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  160\n",
      "Learned weights =  [6.26910354e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  160\n",
      "Learned weights =  [6.26910354e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  161\n",
      "Learned weights =  [6.24222198e-02 2.63024278e+02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  161\n",
      "Learned weights =  [6.24222198e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  162\n",
      "Learned weights =  [6.21534042e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  162\n",
      "Learned weights =  [6.21534042e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  163\n",
      "Learned weights =  [6.18845887e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  163\n",
      "Learned weights =  [6.18845887e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  164\n",
      "Learned weights =  [6.16157731e-02 2.63024278e+02]\n",
      "Done with gradient descent at iteration  164\n",
      "Learned weights =  [6.16157731e-02 2.63024279e+02]\n",
      "Done with gradient descent at iteration  165\n",
      "Learned weights =  [6.13469576e-02 2.63024279e+02]\n",
      "Done with gradient descent at iteration  165\n",
      "Learned weights =  [6.13469576e-02 2.63024279e+02]\n",
      "Done with gradient descent at iteration  166\n",
      "Learned weights =  [6.10781420e-02 2.63024279e+02]\n",
      "Done with gradient descent at iteration  166\n",
      "Learned weights =  [6.10781420e-02 2.63024279e+02]\n",
      "Done with gradient descent at iteration  167\n",
      "Learned weights =  [6.08093264e-02 2.63024279e+02]\n",
      "Done with gradient descent at iteration  167\n",
      "Learned weights =  [6.08093264e-02 2.63024279e+02]\n",
      "Done with gradient descent at iteration  168\n",
      "Learned weights =  [6.05405109e-02 2.63024279e+02]\n",
      "Done with gradient descent at iteration  168\n",
      "Learned weights =  [6.05405109e-02 2.63024279e+02]\n",
      "Done with gradient descent at iteration  169\n",
      "Learned weights =  [6.02716953e-02 2.63024279e+02]\n",
      "Done with gradient descent at iteration  169\n",
      "Learned weights =  [6.02716953e-02 2.63024279e+02]\n",
      "Done with gradient descent at iteration  170\n",
      "Learned weights =  [6.00028798e-02 2.63024279e+02]\n",
      "Done with gradient descent at iteration  170\n",
      "Learned weights =  [6.00028798e-02 2.63024279e+02]\n",
      "Done with gradient descent at iteration  171\n",
      "Learned weights =  [5.97340642e-02 2.63024279e+02]\n",
      "Done with gradient descent at iteration  171\n",
      "Learned weights =  [5.97340642e-02 2.63024279e+02]\n",
      "Done with gradient descent at iteration  172\n",
      "Learned weights =  [5.94652487e-02 2.63024279e+02]\n",
      "Done with gradient descent at iteration  172\n",
      "Learned weights =  [5.94652487e-02 2.63024279e+02]\n",
      "Done with gradient descent at iteration  173\n",
      "Learned weights =  [5.91964331e-02 2.63024279e+02]\n",
      "Done with gradient descent at iteration  173\n",
      "Learned weights =  [5.91964331e-02 2.63024280e+02]\n",
      "Done with gradient descent at iteration  174\n",
      "Learned weights =  [5.89276176e-02 2.63024280e+02]\n",
      "Done with gradient descent at iteration  174\n",
      "Learned weights =  [5.89276176e-02 2.63024280e+02]\n",
      "Done with gradient descent at iteration  175\n",
      "Learned weights =  [5.8658802e-02 2.6302428e+02]\n",
      "Done with gradient descent at iteration  175\n",
      "Learned weights =  [5.8658802e-02 2.6302428e+02]\n",
      "Done with gradient descent at iteration  176\n",
      "Learned weights =  [5.83899865e-02 2.63024280e+02]\n",
      "Done with gradient descent at iteration  176\n",
      "Learned weights =  [5.83899865e-02 2.63024280e+02]\n",
      "Done with gradient descent at iteration  177\n",
      "Learned weights =  [5.81211709e-02 2.63024280e+02]\n",
      "Done with gradient descent at iteration  177\n",
      "Learned weights =  [5.81211709e-02 2.63024280e+02]\n",
      "Done with gradient descent at iteration  178\n",
      "Learned weights =  [5.78523554e-02 2.63024280e+02]\n",
      "Done with gradient descent at iteration  178\n",
      "Learned weights =  [5.78523554e-02 2.63024280e+02]\n",
      "Done with gradient descent at iteration  179\n",
      "Learned weights =  [5.75835399e-02 2.63024280e+02]\n",
      "Done with gradient descent at iteration  179\n",
      "Learned weights =  [5.75835399e-02 2.63024280e+02]\n",
      "Done with gradient descent at iteration  180\n",
      "Learned weights =  [5.73147243e-02 2.63024280e+02]\n",
      "Done with gradient descent at iteration  180\n",
      "Learned weights =  [5.73147243e-02 2.63024280e+02]\n",
      "Done with gradient descent at iteration  181\n",
      "Learned weights =  [5.70459088e-02 2.63024280e+02]\n",
      "Done with gradient descent at iteration  181\n",
      "Learned weights =  [5.70459088e-02 2.63024280e+02]\n",
      "Done with gradient descent at iteration  182\n",
      "Learned weights =  [5.67770933e-02 2.63024280e+02]\n",
      "Done with gradient descent at iteration  182\n",
      "Learned weights =  [5.67770933e-02 2.63024281e+02]\n",
      "Done with gradient descent at iteration  183\n",
      "Learned weights =  [5.65082777e-02 2.63024281e+02]\n",
      "Done with gradient descent at iteration  183\n",
      "Learned weights =  [5.65082777e-02 2.63024281e+02]\n",
      "Done with gradient descent at iteration  184\n",
      "Learned weights =  [5.62394622e-02 2.63024281e+02]\n",
      "Done with gradient descent at iteration  184\n",
      "Learned weights =  [5.62394622e-02 2.63024281e+02]\n",
      "Done with gradient descent at iteration  185\n",
      "Learned weights =  [5.59706467e-02 2.63024281e+02]\n",
      "Done with gradient descent at iteration  185\n",
      "Learned weights =  [5.59706467e-02 2.63024281e+02]\n",
      "Done with gradient descent at iteration  186\n",
      "Learned weights =  [5.57018311e-02 2.63024281e+02]\n",
      "Done with gradient descent at iteration  186\n",
      "Learned weights =  [5.57018311e-02 2.63024281e+02]\n",
      "Done with gradient descent at iteration  187\n",
      "Learned weights =  [5.54330156e-02 2.63024281e+02]\n",
      "Done with gradient descent at iteration  187\n",
      "Learned weights =  [5.54330156e-02 2.63024281e+02]\n",
      "Done with gradient descent at iteration  188\n",
      "Learned weights =  [5.51642001e-02 2.63024281e+02]\n",
      "Done with gradient descent at iteration  188\n",
      "Learned weights =  [5.51642001e-02 2.63024281e+02]\n",
      "Done with gradient descent at iteration  189\n",
      "Learned weights =  [5.48953845e-02 2.63024281e+02]\n",
      "Done with gradient descent at iteration  189\n",
      "Learned weights =  [5.48953845e-02 2.63024281e+02]\n",
      "Done with gradient descent at iteration  190\n",
      "Learned weights =  [5.46265690e-02 2.63024281e+02]\n",
      "Done with gradient descent at iteration  190\n",
      "Learned weights =  [5.46265690e-02 2.63024281e+02]\n",
      "Done with gradient descent at iteration  191\n",
      "Learned weights =  [5.43577535e-02 2.63024281e+02]\n",
      "Done with gradient descent at iteration  191\n",
      "Learned weights =  [5.43577535e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  192\n",
      "Learned weights =  [5.40889380e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  192\n",
      "Learned weights =  [5.40889380e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  193\n",
      "Learned weights =  [5.38201225e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  193\n",
      "Learned weights =  [5.38201225e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  194\n",
      "Learned weights =  [5.35513070e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  194\n",
      "Learned weights =  [5.35513070e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  195\n",
      "Learned weights =  [5.32824914e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  195\n",
      "Learned weights =  [5.32824914e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  196\n",
      "Learned weights =  [5.30136759e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  196\n",
      "Learned weights =  [5.30136759e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  197\n",
      "Learned weights =  [5.27448604e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  197\n",
      "Learned weights =  [5.27448604e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  198\n",
      "Learned weights =  [5.24760449e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  198\n",
      "Learned weights =  [5.24760449e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  199\n",
      "Learned weights =  [5.22072294e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  199\n",
      "Learned weights =  [5.22072294e-02 2.63024282e+02]\n",
      "Iteration = 200\n",
      "Cost function =  1208251130374984.8\n",
      "Done with gradient descent at iteration  200\n",
      "Learned weights =  [5.19384139e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  200\n",
      "Learned weights =  [5.19384139e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  201\n",
      "Learned weights =  [5.16695984e-02 2.63024282e+02]\n",
      "Done with gradient descent at iteration  201\n",
      "Learned weights =  [5.16695984e-02 2.63024283e+02]\n",
      "Done with gradient descent at iteration  202\n",
      "Learned weights =  [5.14007829e-02 2.63024283e+02]\n",
      "Done with gradient descent at iteration  202\n",
      "Learned weights =  [5.14007829e-02 2.63024283e+02]\n",
      "Done with gradient descent at iteration  203\n",
      "Learned weights =  [5.11319674e-02 2.63024283e+02]\n",
      "Done with gradient descent at iteration  203\n",
      "Learned weights =  [5.11319674e-02 2.63024283e+02]\n",
      "Done with gradient descent at iteration  204\n",
      "Learned weights =  [5.08631519e-02 2.63024283e+02]\n",
      "Done with gradient descent at iteration  204\n",
      "Learned weights =  [5.08631519e-02 2.63024283e+02]\n",
      "Done with gradient descent at iteration  205\n",
      "Learned weights =  [5.05943364e-02 2.63024283e+02]\n",
      "Done with gradient descent at iteration  205\n",
      "Learned weights =  [5.05943364e-02 2.63024283e+02]\n",
      "Done with gradient descent at iteration  206\n",
      "Learned weights =  [5.03255209e-02 2.63024283e+02]\n",
      "Done with gradient descent at iteration  206\n",
      "Learned weights =  [5.03255209e-02 2.63024283e+02]\n",
      "Done with gradient descent at iteration  207\n",
      "Learned weights =  [5.00567054e-02 2.63024283e+02]\n",
      "Done with gradient descent at iteration  207\n",
      "Learned weights =  [5.00567054e-02 2.63024283e+02]\n",
      "Done with gradient descent at iteration  208\n",
      "Learned weights =  [4.97878899e-02 2.63024283e+02]\n",
      "Done with gradient descent at iteration  208\n",
      "Learned weights =  [4.97878899e-02 2.63024283e+02]\n",
      "Done with gradient descent at iteration  209\n",
      "Learned weights =  [4.95190744e-02 2.63024283e+02]\n",
      "Done with gradient descent at iteration  209\n",
      "Learned weights =  [4.95190744e-02 2.63024283e+02]\n",
      "Done with gradient descent at iteration  210\n",
      "Learned weights =  [4.92502589e-02 2.63024283e+02]\n",
      "Done with gradient descent at iteration  210\n",
      "Learned weights =  [4.92502589e-02 2.63024284e+02]\n",
      "Done with gradient descent at iteration  211\n",
      "Learned weights =  [4.89814434e-02 2.63024284e+02]\n",
      "Done with gradient descent at iteration  211\n",
      "Learned weights =  [4.89814434e-02 2.63024284e+02]\n",
      "Done with gradient descent at iteration  212\n",
      "Learned weights =  [4.87126279e-02 2.63024284e+02]\n",
      "Done with gradient descent at iteration  212\n",
      "Learned weights =  [4.87126279e-02 2.63024284e+02]\n",
      "Done with gradient descent at iteration  213\n",
      "Learned weights =  [4.84438124e-02 2.63024284e+02]\n",
      "Done with gradient descent at iteration  213\n",
      "Learned weights =  [4.84438124e-02 2.63024284e+02]\n",
      "Done with gradient descent at iteration  214\n",
      "Learned weights =  [4.81749969e-02 2.63024284e+02]\n",
      "Done with gradient descent at iteration  214\n",
      "Learned weights =  [4.81749969e-02 2.63024284e+02]\n",
      "Done with gradient descent at iteration  215\n",
      "Learned weights =  [4.79061815e-02 2.63024284e+02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  215\n",
      "Learned weights =  [4.79061815e-02 2.63024284e+02]\n",
      "Done with gradient descent at iteration  216\n",
      "Learned weights =  [4.76373660e-02 2.63024284e+02]\n",
      "Done with gradient descent at iteration  216\n",
      "Learned weights =  [4.76373660e-02 2.63024284e+02]\n",
      "Done with gradient descent at iteration  217\n",
      "Learned weights =  [4.73685505e-02 2.63024284e+02]\n",
      "Done with gradient descent at iteration  217\n",
      "Learned weights =  [4.73685505e-02 2.63024284e+02]\n",
      "Done with gradient descent at iteration  218\n",
      "Learned weights =  [4.70997350e-02 2.63024284e+02]\n",
      "Done with gradient descent at iteration  218\n",
      "Learned weights =  [4.70997350e-02 2.63024284e+02]\n",
      "Done with gradient descent at iteration  219\n",
      "Learned weights =  [4.68309195e-02 2.63024284e+02]\n",
      "Done with gradient descent at iteration  219\n",
      "Learned weights =  [4.68309195e-02 2.63024285e+02]\n",
      "Done with gradient descent at iteration  220\n",
      "Learned weights =  [4.65621041e-02 2.63024285e+02]\n",
      "Done with gradient descent at iteration  220\n",
      "Learned weights =  [4.65621041e-02 2.63024285e+02]\n",
      "Done with gradient descent at iteration  221\n",
      "Learned weights =  [4.62932886e-02 2.63024285e+02]\n",
      "Done with gradient descent at iteration  221\n",
      "Learned weights =  [4.62932886e-02 2.63024285e+02]\n",
      "Done with gradient descent at iteration  222\n",
      "Learned weights =  [4.60244731e-02 2.63024285e+02]\n",
      "Done with gradient descent at iteration  222\n",
      "Learned weights =  [4.60244731e-02 2.63024285e+02]\n",
      "Done with gradient descent at iteration  223\n",
      "Learned weights =  [4.57556576e-02 2.63024285e+02]\n",
      "Done with gradient descent at iteration  223\n",
      "Learned weights =  [4.57556576e-02 2.63024285e+02]\n",
      "Done with gradient descent at iteration  224\n",
      "Learned weights =  [4.54868422e-02 2.63024285e+02]\n",
      "Done with gradient descent at iteration  224\n",
      "Learned weights =  [4.54868422e-02 2.63024285e+02]\n",
      "Done with gradient descent at iteration  225\n",
      "Learned weights =  [4.52180267e-02 2.63024285e+02]\n",
      "Done with gradient descent at iteration  225\n",
      "Learned weights =  [4.52180267e-02 2.63024285e+02]\n",
      "Done with gradient descent at iteration  226\n",
      "Learned weights =  [4.49492112e-02 2.63024285e+02]\n",
      "Done with gradient descent at iteration  226\n",
      "Learned weights =  [4.49492112e-02 2.63024285e+02]\n",
      "Done with gradient descent at iteration  227\n",
      "Learned weights =  [4.46803958e-02 2.63024285e+02]\n",
      "Done with gradient descent at iteration  227\n",
      "Learned weights =  [4.46803958e-02 2.63024285e+02]\n",
      "Done with gradient descent at iteration  228\n",
      "Learned weights =  [4.44115803e-02 2.63024285e+02]\n",
      "Done with gradient descent at iteration  228\n",
      "Learned weights =  [4.44115803e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  229\n",
      "Learned weights =  [4.41427648e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  229\n",
      "Learned weights =  [4.41427648e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  230\n",
      "Learned weights =  [4.38739494e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  230\n",
      "Learned weights =  [4.38739494e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  231\n",
      "Learned weights =  [4.36051339e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  231\n",
      "Learned weights =  [4.36051339e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  232\n",
      "Learned weights =  [4.33363185e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  232\n",
      "Learned weights =  [4.33363185e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  233\n",
      "Learned weights =  [4.30675030e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  233\n",
      "Learned weights =  [4.30675030e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  234\n",
      "Learned weights =  [4.27986875e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  234\n",
      "Learned weights =  [4.27986875e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  235\n",
      "Learned weights =  [4.25298721e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  235\n",
      "Learned weights =  [4.25298721e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  236\n",
      "Learned weights =  [4.22610566e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  236\n",
      "Learned weights =  [4.22610566e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  237\n",
      "Learned weights =  [4.19922412e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  237\n",
      "Learned weights =  [4.19922412e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  238\n",
      "Learned weights =  [4.17234257e-02 2.63024286e+02]\n",
      "Done with gradient descent at iteration  238\n",
      "Learned weights =  [4.17234257e-02 2.63024287e+02]\n",
      "Done with gradient descent at iteration  239\n",
      "Learned weights =  [4.14546103e-02 2.63024287e+02]\n",
      "Done with gradient descent at iteration  239\n",
      "Learned weights =  [4.14546103e-02 2.63024287e+02]\n",
      "Done with gradient descent at iteration  240\n",
      "Learned weights =  [4.11857949e-02 2.63024287e+02]\n",
      "Done with gradient descent at iteration  240\n",
      "Learned weights =  [4.11857949e-02 2.63024287e+02]\n",
      "Done with gradient descent at iteration  241\n",
      "Learned weights =  [4.09169794e-02 2.63024287e+02]\n",
      "Done with gradient descent at iteration  241\n",
      "Learned weights =  [4.09169794e-02 2.63024287e+02]\n",
      "Done with gradient descent at iteration  242\n",
      "Learned weights =  [4.06481640e-02 2.63024287e+02]\n",
      "Done with gradient descent at iteration  242\n",
      "Learned weights =  [4.06481640e-02 2.63024287e+02]\n",
      "Done with gradient descent at iteration  243\n",
      "Learned weights =  [4.03793485e-02 2.63024287e+02]\n",
      "Done with gradient descent at iteration  243\n",
      "Learned weights =  [4.03793485e-02 2.63024287e+02]\n",
      "Done with gradient descent at iteration  244\n",
      "Learned weights =  [4.01105331e-02 2.63024287e+02]\n",
      "Done with gradient descent at iteration  244\n",
      "Learned weights =  [4.01105331e-02 2.63024287e+02]\n",
      "Done with gradient descent at iteration  245\n",
      "Learned weights =  [3.98417176e-02 2.63024287e+02]\n",
      "Done with gradient descent at iteration  245\n",
      "Learned weights =  [3.98417176e-02 2.63024287e+02]\n",
      "Done with gradient descent at iteration  246\n",
      "Learned weights =  [3.95729022e-02 2.63024287e+02]\n",
      "Done with gradient descent at iteration  246\n",
      "Learned weights =  [3.95729022e-02 2.63024287e+02]\n",
      "Done with gradient descent at iteration  247\n",
      "Learned weights =  [3.93040868e-02 2.63024287e+02]\n",
      "Done with gradient descent at iteration  247\n",
      "Learned weights =  [3.93040868e-02 2.63024288e+02]\n",
      "Done with gradient descent at iteration  248\n",
      "Learned weights =  [3.90352713e-02 2.63024288e+02]\n",
      "Done with gradient descent at iteration  248\n",
      "Learned weights =  [3.90352713e-02 2.63024288e+02]\n",
      "Done with gradient descent at iteration  249\n",
      "Learned weights =  [3.87664559e-02 2.63024288e+02]\n",
      "Done with gradient descent at iteration  249\n",
      "Learned weights =  [3.87664559e-02 2.63024288e+02]\n",
      "Done with gradient descent at iteration  250\n",
      "Learned weights =  [3.84976405e-02 2.63024288e+02]\n",
      "Done with gradient descent at iteration  250\n",
      "Learned weights =  [3.84976405e-02 2.63024288e+02]\n",
      "Done with gradient descent at iteration  251\n",
      "Learned weights =  [3.82288250e-02 2.63024288e+02]\n",
      "Done with gradient descent at iteration  251\n",
      "Learned weights =  [3.82288250e-02 2.63024288e+02]\n",
      "Done with gradient descent at iteration  252\n",
      "Learned weights =  [3.79600096e-02 2.63024288e+02]\n",
      "Done with gradient descent at iteration  252\n",
      "Learned weights =  [3.79600096e-02 2.63024288e+02]\n",
      "Done with gradient descent at iteration  253\n",
      "Learned weights =  [3.76911942e-02 2.63024288e+02]\n",
      "Done with gradient descent at iteration  253\n",
      "Learned weights =  [3.76911942e-02 2.63024288e+02]\n",
      "Done with gradient descent at iteration  254\n",
      "Learned weights =  [3.74223788e-02 2.63024288e+02]\n",
      "Done with gradient descent at iteration  254\n",
      "Learned weights =  [3.74223788e-02 2.63024288e+02]\n",
      "Done with gradient descent at iteration  255\n",
      "Learned weights =  [3.71535633e-02 2.63024288e+02]\n",
      "Done with gradient descent at iteration  255\n",
      "Learned weights =  [3.71535633e-02 2.63024288e+02]\n",
      "Done with gradient descent at iteration  256\n",
      "Learned weights =  [3.68847479e-02 2.63024288e+02]\n",
      "Done with gradient descent at iteration  256\n",
      "Learned weights =  [3.68847479e-02 2.63024289e+02]\n",
      "Done with gradient descent at iteration  257\n",
      "Learned weights =  [3.66159325e-02 2.63024289e+02]\n",
      "Done with gradient descent at iteration  257\n",
      "Learned weights =  [3.66159325e-02 2.63024289e+02]\n",
      "Done with gradient descent at iteration  258\n",
      "Learned weights =  [3.63471171e-02 2.63024289e+02]\n",
      "Done with gradient descent at iteration  258\n",
      "Learned weights =  [3.63471171e-02 2.63024289e+02]\n",
      "Done with gradient descent at iteration  259\n",
      "Learned weights =  [3.60783017e-02 2.63024289e+02]\n",
      "Done with gradient descent at iteration  259\n",
      "Learned weights =  [3.60783017e-02 2.63024289e+02]\n",
      "Done with gradient descent at iteration  260\n",
      "Learned weights =  [3.58094863e-02 2.63024289e+02]\n",
      "Done with gradient descent at iteration  260\n",
      "Learned weights =  [3.58094863e-02 2.63024289e+02]\n",
      "Done with gradient descent at iteration  261\n",
      "Learned weights =  [3.55406708e-02 2.63024289e+02]\n",
      "Done with gradient descent at iteration  261\n",
      "Learned weights =  [3.55406708e-02 2.63024289e+02]\n",
      "Done with gradient descent at iteration  262\n",
      "Learned weights =  [3.52718554e-02 2.63024289e+02]\n",
      "Done with gradient descent at iteration  262\n",
      "Learned weights =  [3.52718554e-02 2.63024289e+02]\n",
      "Done with gradient descent at iteration  263\n",
      "Learned weights =  [3.50030400e-02 2.63024289e+02]\n",
      "Done with gradient descent at iteration  263\n",
      "Learned weights =  [3.50030400e-02 2.63024289e+02]\n",
      "Done with gradient descent at iteration  264\n",
      "Learned weights =  [3.47342246e-02 2.63024289e+02]\n",
      "Done with gradient descent at iteration  264\n",
      "Learned weights =  [3.47342246e-02 2.63024289e+02]\n",
      "Done with gradient descent at iteration  265\n",
      "Learned weights =  [3.44654092e-02 2.63024289e+02]\n",
      "Done with gradient descent at iteration  265\n",
      "Learned weights =  [3.44654092e-02 2.63024290e+02]\n",
      "Done with gradient descent at iteration  266\n",
      "Learned weights =  [3.41965938e-02 2.63024290e+02]\n",
      "Done with gradient descent at iteration  266\n",
      "Learned weights =  [3.41965938e-02 2.63024290e+02]\n",
      "Done with gradient descent at iteration  267\n",
      "Learned weights =  [3.39277784e-02 2.63024290e+02]\n",
      "Done with gradient descent at iteration  267\n",
      "Learned weights =  [3.39277784e-02 2.63024290e+02]\n",
      "Done with gradient descent at iteration  268\n",
      "Learned weights =  [3.3658963e-02 2.6302429e+02]\n",
      "Done with gradient descent at iteration  268\n",
      "Learned weights =  [3.3658963e-02 2.6302429e+02]\n",
      "Done with gradient descent at iteration  269\n",
      "Learned weights =  [3.33901476e-02 2.63024290e+02]\n",
      "Done with gradient descent at iteration  269\n",
      "Learned weights =  [3.33901476e-02 2.63024290e+02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  270\n",
      "Learned weights =  [3.31213322e-02 2.63024290e+02]\n",
      "Done with gradient descent at iteration  270\n",
      "Learned weights =  [3.31213322e-02 2.63024290e+02]\n",
      "Done with gradient descent at iteration  271\n",
      "Learned weights =  [3.28525168e-02 2.63024290e+02]\n",
      "Done with gradient descent at iteration  271\n",
      "Learned weights =  [3.28525168e-02 2.63024290e+02]\n",
      "Done with gradient descent at iteration  272\n",
      "Learned weights =  [3.25837014e-02 2.63024290e+02]\n",
      "Done with gradient descent at iteration  272\n",
      "Learned weights =  [3.25837014e-02 2.63024290e+02]\n",
      "Done with gradient descent at iteration  273\n",
      "Learned weights =  [3.2314886e-02 2.6302429e+02]\n",
      "Done with gradient descent at iteration  273\n",
      "Learned weights =  [3.2314886e-02 2.6302429e+02]\n",
      "Done with gradient descent at iteration  274\n",
      "Learned weights =  [3.20460706e-02 2.63024290e+02]\n",
      "Done with gradient descent at iteration  274\n",
      "Learned weights =  [3.20460706e-02 2.63024290e+02]\n",
      "Done with gradient descent at iteration  275\n",
      "Learned weights =  [3.17772552e-02 2.63024290e+02]\n",
      "Done with gradient descent at iteration  275\n",
      "Learned weights =  [3.17772552e-02 2.63024291e+02]\n",
      "Done with gradient descent at iteration  276\n",
      "Learned weights =  [3.15084398e-02 2.63024291e+02]\n",
      "Done with gradient descent at iteration  276\n",
      "Learned weights =  [3.15084398e-02 2.63024291e+02]\n",
      "Done with gradient descent at iteration  277\n",
      "Learned weights =  [3.12396244e-02 2.63024291e+02]\n",
      "Done with gradient descent at iteration  277\n",
      "Learned weights =  [3.12396244e-02 2.63024291e+02]\n",
      "Done with gradient descent at iteration  278\n",
      "Learned weights =  [3.09708090e-02 2.63024291e+02]\n",
      "Done with gradient descent at iteration  278\n",
      "Learned weights =  [3.09708090e-02 2.63024291e+02]\n",
      "Done with gradient descent at iteration  279\n",
      "Learned weights =  [3.07019937e-02 2.63024291e+02]\n",
      "Done with gradient descent at iteration  279\n",
      "Learned weights =  [3.07019937e-02 2.63024291e+02]\n",
      "Done with gradient descent at iteration  280\n",
      "Learned weights =  [3.04331783e-02 2.63024291e+02]\n",
      "Done with gradient descent at iteration  280\n",
      "Learned weights =  [3.04331783e-02 2.63024291e+02]\n",
      "Done with gradient descent at iteration  281\n",
      "Learned weights =  [3.01643629e-02 2.63024291e+02]\n",
      "Done with gradient descent at iteration  281\n",
      "Learned weights =  [3.01643629e-02 2.63024291e+02]\n",
      "Done with gradient descent at iteration  282\n",
      "Learned weights =  [2.98955475e-02 2.63024291e+02]\n",
      "Done with gradient descent at iteration  282\n",
      "Learned weights =  [2.98955475e-02 2.63024291e+02]\n",
      "Done with gradient descent at iteration  283\n",
      "Learned weights =  [2.96267321e-02 2.63024291e+02]\n",
      "Done with gradient descent at iteration  283\n",
      "Learned weights =  [2.96267321e-02 2.63024291e+02]\n",
      "Done with gradient descent at iteration  284\n",
      "Learned weights =  [2.93579167e-02 2.63024291e+02]\n",
      "Done with gradient descent at iteration  284\n",
      "Learned weights =  [2.93579167e-02 2.63024292e+02]\n",
      "Done with gradient descent at iteration  285\n",
      "Learned weights =  [2.90891014e-02 2.63024292e+02]\n",
      "Done with gradient descent at iteration  285\n",
      "Learned weights =  [2.90891014e-02 2.63024292e+02]\n",
      "Done with gradient descent at iteration  286\n",
      "Learned weights =  [2.88202860e-02 2.63024292e+02]\n",
      "Done with gradient descent at iteration  286\n",
      "Learned weights =  [2.88202860e-02 2.63024292e+02]\n",
      "Done with gradient descent at iteration  287\n",
      "Learned weights =  [2.85514706e-02 2.63024292e+02]\n",
      "Done with gradient descent at iteration  287\n",
      "Learned weights =  [2.85514706e-02 2.63024292e+02]\n",
      "Done with gradient descent at iteration  288\n",
      "Learned weights =  [2.82826552e-02 2.63024292e+02]\n",
      "Done with gradient descent at iteration  288\n",
      "Learned weights =  [2.82826552e-02 2.63024292e+02]\n",
      "Done with gradient descent at iteration  289\n",
      "Learned weights =  [2.80138399e-02 2.63024292e+02]\n",
      "Done with gradient descent at iteration  289\n",
      "Learned weights =  [2.80138399e-02 2.63024292e+02]\n",
      "Done with gradient descent at iteration  290\n",
      "Learned weights =  [2.77450245e-02 2.63024292e+02]\n",
      "Done with gradient descent at iteration  290\n",
      "Learned weights =  [2.77450245e-02 2.63024292e+02]\n",
      "Done with gradient descent at iteration  291\n",
      "Learned weights =  [2.74762091e-02 2.63024292e+02]\n",
      "Done with gradient descent at iteration  291\n",
      "Learned weights =  [2.74762091e-02 2.63024292e+02]\n",
      "Done with gradient descent at iteration  292\n",
      "Learned weights =  [2.72073938e-02 2.63024292e+02]\n",
      "Done with gradient descent at iteration  292\n",
      "Learned weights =  [2.72073938e-02 2.63024292e+02]\n",
      "Done with gradient descent at iteration  293\n",
      "Learned weights =  [2.69385784e-02 2.63024292e+02]\n",
      "Done with gradient descent at iteration  293\n",
      "Learned weights =  [2.69385784e-02 2.63024293e+02]\n",
      "Done with gradient descent at iteration  294\n",
      "Learned weights =  [2.66697630e-02 2.63024293e+02]\n",
      "Done with gradient descent at iteration  294\n",
      "Learned weights =  [2.66697630e-02 2.63024293e+02]\n",
      "Done with gradient descent at iteration  295\n",
      "Learned weights =  [2.64009477e-02 2.63024293e+02]\n",
      "Done with gradient descent at iteration  295\n",
      "Learned weights =  [2.64009477e-02 2.63024293e+02]\n",
      "Done with gradient descent at iteration  296\n",
      "Learned weights =  [2.61321323e-02 2.63024293e+02]\n",
      "Done with gradient descent at iteration  296\n",
      "Learned weights =  [2.61321323e-02 2.63024293e+02]\n",
      "Done with gradient descent at iteration  297\n",
      "Learned weights =  [2.58633170e-02 2.63024293e+02]\n",
      "Done with gradient descent at iteration  297\n",
      "Learned weights =  [2.58633170e-02 2.63024293e+02]\n",
      "Done with gradient descent at iteration  298\n",
      "Learned weights =  [2.55945016e-02 2.63024293e+02]\n",
      "Done with gradient descent at iteration  298\n",
      "Learned weights =  [2.55945016e-02 2.63024293e+02]\n",
      "Done with gradient descent at iteration  299\n",
      "Learned weights =  [2.53256863e-02 2.63024293e+02]\n",
      "Done with gradient descent at iteration  299\n",
      "Learned weights =  [2.53256863e-02 2.63024293e+02]\n",
      "Iteration = 300\n",
      "Cost function =  1208251123148810.2\n",
      "Done with gradient descent at iteration  300\n",
      "Learned weights =  [2.50568709e-02 2.63024293e+02]\n",
      "Done with gradient descent at iteration  300\n",
      "Learned weights =  [2.50568709e-02 2.63024293e+02]\n",
      "Done with gradient descent at iteration  301\n",
      "Learned weights =  [2.47880555e-02 2.63024293e+02]\n",
      "Done with gradient descent at iteration  301\n",
      "Learned weights =  [2.47880555e-02 2.63024293e+02]\n",
      "Done with gradient descent at iteration  302\n",
      "Learned weights =  [2.45192402e-02 2.63024293e+02]\n",
      "Done with gradient descent at iteration  302\n",
      "Learned weights =  [2.45192402e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  303\n",
      "Learned weights =  [2.42504248e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  303\n",
      "Learned weights =  [2.42504248e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  304\n",
      "Learned weights =  [2.39816095e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  304\n",
      "Learned weights =  [2.39816095e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  305\n",
      "Learned weights =  [2.37127942e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  305\n",
      "Learned weights =  [2.37127942e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  306\n",
      "Learned weights =  [2.34439788e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  306\n",
      "Learned weights =  [2.34439788e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  307\n",
      "Learned weights =  [2.31751635e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  307\n",
      "Learned weights =  [2.31751635e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  308\n",
      "Learned weights =  [2.29063481e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  308\n",
      "Learned weights =  [2.29063481e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  309\n",
      "Learned weights =  [2.26375328e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  309\n",
      "Learned weights =  [2.26375328e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  310\n",
      "Learned weights =  [2.23687174e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  310\n",
      "Learned weights =  [2.23687174e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  311\n",
      "Learned weights =  [2.20999021e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  311\n",
      "Learned weights =  [2.20999021e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  312\n",
      "Learned weights =  [2.18310868e-02 2.63024294e+02]\n",
      "Done with gradient descent at iteration  312\n",
      "Learned weights =  [2.18310868e-02 2.63024295e+02]\n",
      "Done with gradient descent at iteration  313\n",
      "Learned weights =  [2.15622714e-02 2.63024295e+02]\n",
      "Done with gradient descent at iteration  313\n",
      "Learned weights =  [2.15622714e-02 2.63024295e+02]\n",
      "Done with gradient descent at iteration  314\n",
      "Learned weights =  [2.12934561e-02 2.63024295e+02]\n",
      "Done with gradient descent at iteration  314\n",
      "Learned weights =  [2.12934561e-02 2.63024295e+02]\n",
      "Done with gradient descent at iteration  315\n",
      "Learned weights =  [2.10246408e-02 2.63024295e+02]\n",
      "Done with gradient descent at iteration  315\n",
      "Learned weights =  [2.10246408e-02 2.63024295e+02]\n",
      "Done with gradient descent at iteration  316\n",
      "Learned weights =  [2.07558254e-02 2.63024295e+02]\n",
      "Done with gradient descent at iteration  316\n",
      "Learned weights =  [2.07558254e-02 2.63024295e+02]\n",
      "Done with gradient descent at iteration  317\n",
      "Learned weights =  [2.04870101e-02 2.63024295e+02]\n",
      "Done with gradient descent at iteration  317\n",
      "Learned weights =  [2.04870101e-02 2.63024295e+02]\n",
      "Done with gradient descent at iteration  318\n",
      "Learned weights =  [2.02181948e-02 2.63024295e+02]\n",
      "Done with gradient descent at iteration  318\n",
      "Learned weights =  [2.02181948e-02 2.63024295e+02]\n",
      "Done with gradient descent at iteration  319\n",
      "Learned weights =  [1.99493795e-02 2.63024295e+02]\n",
      "Done with gradient descent at iteration  319\n",
      "Learned weights =  [1.99493795e-02 2.63024295e+02]\n",
      "Done with gradient descent at iteration  320\n",
      "Learned weights =  [1.96805641e-02 2.63024295e+02]\n",
      "Done with gradient descent at iteration  320\n",
      "Learned weights =  [1.96805641e-02 2.63024295e+02]\n",
      "Done with gradient descent at iteration  321\n",
      "Learned weights =  [1.94117488e-02 2.63024295e+02]\n",
      "Done with gradient descent at iteration  321\n",
      "Learned weights =  [1.94117488e-02 2.63024296e+02]\n",
      "Done with gradient descent at iteration  322\n",
      "Learned weights =  [1.91429335e-02 2.63024296e+02]\n",
      "Done with gradient descent at iteration  322\n",
      "Learned weights =  [1.91429335e-02 2.63024296e+02]\n",
      "Done with gradient descent at iteration  323\n",
      "Learned weights =  [1.88741182e-02 2.63024296e+02]\n",
      "Done with gradient descent at iteration  323\n",
      "Learned weights =  [1.88741182e-02 2.63024296e+02]\n",
      "Done with gradient descent at iteration  324\n",
      "Learned weights =  [1.86053029e-02 2.63024296e+02]\n",
      "Done with gradient descent at iteration  324\n",
      "Learned weights =  [1.86053029e-02 2.63024296e+02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  325\n",
      "Learned weights =  [1.83364876e-02 2.63024296e+02]\n",
      "Done with gradient descent at iteration  325\n",
      "Learned weights =  [1.83364876e-02 2.63024296e+02]\n",
      "Done with gradient descent at iteration  326\n",
      "Learned weights =  [1.80676722e-02 2.63024296e+02]\n",
      "Done with gradient descent at iteration  326\n",
      "Learned weights =  [1.80676722e-02 2.63024296e+02]\n",
      "Done with gradient descent at iteration  327\n",
      "Learned weights =  [1.77988569e-02 2.63024296e+02]\n",
      "Done with gradient descent at iteration  327\n",
      "Learned weights =  [1.77988569e-02 2.63024296e+02]\n",
      "Done with gradient descent at iteration  328\n",
      "Learned weights =  [1.75300416e-02 2.63024296e+02]\n",
      "Done with gradient descent at iteration  328\n",
      "Learned weights =  [1.75300416e-02 2.63024296e+02]\n",
      "Done with gradient descent at iteration  329\n",
      "Learned weights =  [1.72612263e-02 2.63024296e+02]\n",
      "Done with gradient descent at iteration  329\n",
      "Learned weights =  [1.72612263e-02 2.63024296e+02]\n",
      "Done with gradient descent at iteration  330\n",
      "Learned weights =  [1.69924110e-02 2.63024296e+02]\n",
      "Done with gradient descent at iteration  330\n",
      "Learned weights =  [1.69924110e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  331\n",
      "Learned weights =  [1.67235957e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  331\n",
      "Learned weights =  [1.67235957e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  332\n",
      "Learned weights =  [1.64547804e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  332\n",
      "Learned weights =  [1.64547804e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  333\n",
      "Learned weights =  [1.61859651e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  333\n",
      "Learned weights =  [1.61859651e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  334\n",
      "Learned weights =  [1.59171498e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  334\n",
      "Learned weights =  [1.59171498e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  335\n",
      "Learned weights =  [1.56483345e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  335\n",
      "Learned weights =  [1.56483345e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  336\n",
      "Learned weights =  [1.53795192e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  336\n",
      "Learned weights =  [1.53795192e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  337\n",
      "Learned weights =  [1.51107039e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  337\n",
      "Learned weights =  [1.51107039e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  338\n",
      "Learned weights =  [1.48418886e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  338\n",
      "Learned weights =  [1.48418886e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  339\n",
      "Learned weights =  [1.45730733e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  339\n",
      "Learned weights =  [1.45730733e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  340\n",
      "Learned weights =  [1.43042580e-02 2.63024297e+02]\n",
      "Done with gradient descent at iteration  340\n",
      "Learned weights =  [1.43042580e-02 2.63024298e+02]\n",
      "Done with gradient descent at iteration  341\n",
      "Learned weights =  [1.40354427e-02 2.63024298e+02]\n",
      "Done with gradient descent at iteration  341\n",
      "Learned weights =  [1.40354427e-02 2.63024298e+02]\n",
      "Done with gradient descent at iteration  342\n",
      "Learned weights =  [1.37666274e-02 2.63024298e+02]\n",
      "Done with gradient descent at iteration  342\n",
      "Learned weights =  [1.37666274e-02 2.63024298e+02]\n",
      "Done with gradient descent at iteration  343\n",
      "Learned weights =  [1.34978121e-02 2.63024298e+02]\n",
      "Done with gradient descent at iteration  343\n",
      "Learned weights =  [1.34978121e-02 2.63024298e+02]\n",
      "Done with gradient descent at iteration  344\n",
      "Learned weights =  [1.32289968e-02 2.63024298e+02]\n",
      "Done with gradient descent at iteration  344\n",
      "Learned weights =  [1.32289968e-02 2.63024298e+02]\n",
      "Done with gradient descent at iteration  345\n",
      "Learned weights =  [1.29601816e-02 2.63024298e+02]\n",
      "Done with gradient descent at iteration  345\n",
      "Learned weights =  [1.29601816e-02 2.63024298e+02]\n",
      "Done with gradient descent at iteration  346\n",
      "Learned weights =  [1.26913663e-02 2.63024298e+02]\n",
      "Done with gradient descent at iteration  346\n",
      "Learned weights =  [1.26913663e-02 2.63024298e+02]\n",
      "Done with gradient descent at iteration  347\n",
      "Learned weights =  [1.24225510e-02 2.63024298e+02]\n",
      "Done with gradient descent at iteration  347\n",
      "Learned weights =  [1.24225510e-02 2.63024298e+02]\n",
      "Done with gradient descent at iteration  348\n",
      "Learned weights =  [1.21537357e-02 2.63024298e+02]\n",
      "Done with gradient descent at iteration  348\n",
      "Learned weights =  [1.21537357e-02 2.63024298e+02]\n",
      "Done with gradient descent at iteration  349\n",
      "Learned weights =  [1.18849204e-02 2.63024298e+02]\n",
      "Done with gradient descent at iteration  349\n",
      "Learned weights =  [1.18849204e-02 2.63024299e+02]\n",
      "Done with gradient descent at iteration  350\n",
      "Learned weights =  [1.16161052e-02 2.63024299e+02]\n",
      "Done with gradient descent at iteration  350\n",
      "Learned weights =  [1.16161052e-02 2.63024299e+02]\n",
      "Done with gradient descent at iteration  351\n",
      "Learned weights =  [1.13472899e-02 2.63024299e+02]\n",
      "Done with gradient descent at iteration  351\n",
      "Learned weights =  [1.13472899e-02 2.63024299e+02]\n",
      "Done with gradient descent at iteration  352\n",
      "Learned weights =  [1.10784746e-02 2.63024299e+02]\n",
      "Done with gradient descent at iteration  352\n",
      "Learned weights =  [1.10784746e-02 2.63024299e+02]\n",
      "Done with gradient descent at iteration  353\n",
      "Learned weights =  [1.08096593e-02 2.63024299e+02]\n",
      "Done with gradient descent at iteration  353\n",
      "Learned weights =  [1.08096593e-02 2.63024299e+02]\n",
      "Done with gradient descent at iteration  354\n",
      "Learned weights =  [1.05408441e-02 2.63024299e+02]\n",
      "Done with gradient descent at iteration  354\n",
      "Learned weights =  [1.05408441e-02 2.63024299e+02]\n",
      "Done with gradient descent at iteration  355\n",
      "Learned weights =  [1.02720288e-02 2.63024299e+02]\n",
      "Done with gradient descent at iteration  355\n",
      "Learned weights =  [1.02720288e-02 2.63024299e+02]\n",
      "Done with gradient descent at iteration  356\n",
      "Learned weights =  [1.00032135e-02 2.63024299e+02]\n",
      "Done with gradient descent at iteration  356\n",
      "Learned weights =  [1.00032135e-02 2.63024299e+02]\n",
      "Done with gradient descent at iteration  357\n",
      "Learned weights =  [9.73439826e-03 2.63024299e+02]\n",
      "Done with gradient descent at iteration  357\n",
      "Learned weights =  [9.73439826e-03 2.63024299e+02]\n",
      "Done with gradient descent at iteration  358\n",
      "Learned weights =  [9.46558300e-03 2.63024299e+02]\n",
      "Done with gradient descent at iteration  358\n",
      "Learned weights =  [9.465583e-03 2.630243e+02]\n",
      "Done with gradient descent at iteration  359\n",
      "Learned weights =  [9.19676773e-03 2.63024300e+02]\n",
      "Done with gradient descent at iteration  359\n",
      "Learned weights =  [9.19676773e-03 2.63024300e+02]\n",
      "Done with gradient descent at iteration  360\n",
      "Learned weights =  [8.92795247e-03 2.63024300e+02]\n",
      "Done with gradient descent at iteration  360\n",
      "Learned weights =  [8.92795247e-03 2.63024300e+02]\n",
      "Done with gradient descent at iteration  361\n",
      "Learned weights =  [8.65913721e-03 2.63024300e+02]\n",
      "Done with gradient descent at iteration  361\n",
      "Learned weights =  [8.65913721e-03 2.63024300e+02]\n",
      "Done with gradient descent at iteration  362\n",
      "Learned weights =  [8.39032195e-03 2.63024300e+02]\n",
      "Done with gradient descent at iteration  362\n",
      "Learned weights =  [8.39032195e-03 2.63024300e+02]\n",
      "Done with gradient descent at iteration  363\n",
      "Learned weights =  [8.12150669e-03 2.63024300e+02]\n",
      "Done with gradient descent at iteration  363\n",
      "Learned weights =  [8.12150669e-03 2.63024300e+02]\n",
      "Done with gradient descent at iteration  364\n",
      "Learned weights =  [7.85269144e-03 2.63024300e+02]\n",
      "Done with gradient descent at iteration  364\n",
      "Learned weights =  [7.85269144e-03 2.63024300e+02]\n",
      "Done with gradient descent at iteration  365\n",
      "Learned weights =  [7.58387618e-03 2.63024300e+02]\n",
      "Done with gradient descent at iteration  365\n",
      "Learned weights =  [7.58387618e-03 2.63024300e+02]\n",
      "Done with gradient descent at iteration  366\n",
      "Learned weights =  [7.31506093e-03 2.63024300e+02]\n",
      "Done with gradient descent at iteration  366\n",
      "Learned weights =  [7.31506093e-03 2.63024300e+02]\n",
      "Done with gradient descent at iteration  367\n",
      "Learned weights =  [7.04624568e-03 2.63024300e+02]\n",
      "Done with gradient descent at iteration  367\n",
      "Learned weights =  [7.04624568e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  368\n",
      "Learned weights =  [6.77743043e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  368\n",
      "Learned weights =  [6.77743043e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  369\n",
      "Learned weights =  [6.50861518e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  369\n",
      "Learned weights =  [6.50861518e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  370\n",
      "Learned weights =  [6.23979994e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  370\n",
      "Learned weights =  [6.23979994e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  371\n",
      "Learned weights =  [5.97098469e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  371\n",
      "Learned weights =  [5.97098469e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  372\n",
      "Learned weights =  [5.70216945e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  372\n",
      "Learned weights =  [5.70216945e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  373\n",
      "Learned weights =  [5.43335421e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  373\n",
      "Learned weights =  [5.43335421e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  374\n",
      "Learned weights =  [5.16453897e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  374\n",
      "Learned weights =  [5.16453897e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  375\n",
      "Learned weights =  [4.89572373e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  375\n",
      "Learned weights =  [4.89572373e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  376\n",
      "Learned weights =  [4.62690849e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  376\n",
      "Learned weights =  [4.62690849e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  377\n",
      "Learned weights =  [4.35809325e-03 2.63024301e+02]\n",
      "Done with gradient descent at iteration  377\n",
      "Learned weights =  [4.35809325e-03 2.63024302e+02]\n",
      "Done with gradient descent at iteration  378\n",
      "Learned weights =  [4.08927802e-03 2.63024302e+02]\n",
      "Done with gradient descent at iteration  378\n",
      "Learned weights =  [4.08927802e-03 2.63024302e+02]\n",
      "Done with gradient descent at iteration  379\n",
      "Learned weights =  [3.82046279e-03 2.63024302e+02]\n",
      "Done with gradient descent at iteration  379\n",
      "Learned weights =  [3.82046279e-03 2.63024302e+02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  380\n",
      "Learned weights =  [3.55164755e-03 2.63024302e+02]\n",
      "Done with gradient descent at iteration  380\n",
      "Learned weights =  [3.55164755e-03 2.63024302e+02]\n",
      "Done with gradient descent at iteration  381\n",
      "Learned weights =  [3.28283233e-03 2.63024302e+02]\n",
      "Done with gradient descent at iteration  381\n",
      "Learned weights =  [3.28283233e-03 2.63024302e+02]\n",
      "Done with gradient descent at iteration  382\n",
      "Learned weights =  [3.01401710e-03 2.63024302e+02]\n",
      "Done with gradient descent at iteration  382\n",
      "Learned weights =  [3.01401710e-03 2.63024302e+02]\n",
      "Done with gradient descent at iteration  383\n",
      "Learned weights =  [2.74520187e-03 2.63024302e+02]\n",
      "Done with gradient descent at iteration  383\n",
      "Learned weights =  [2.74520187e-03 2.63024302e+02]\n",
      "Done with gradient descent at iteration  384\n",
      "Learned weights =  [2.47638665e-03 2.63024302e+02]\n",
      "Done with gradient descent at iteration  384\n",
      "Learned weights =  [2.47638665e-03 2.63024302e+02]\n",
      "Done with gradient descent at iteration  385\n",
      "Learned weights =  [2.20757142e-03 2.63024302e+02]\n",
      "Done with gradient descent at iteration  385\n",
      "Learned weights =  [2.20757142e-03 2.63024302e+02]\n",
      "Done with gradient descent at iteration  386\n",
      "Learned weights =  [1.93875620e-03 2.63024302e+02]\n",
      "Done with gradient descent at iteration  386\n",
      "Learned weights =  [1.93875620e-03 2.63024303e+02]\n",
      "Done with gradient descent at iteration  387\n",
      "Learned weights =  [1.66994098e-03 2.63024303e+02]\n",
      "Done with gradient descent at iteration  387\n",
      "Learned weights =  [1.66994098e-03 2.63024303e+02]\n",
      "Done with gradient descent at iteration  388\n",
      "Learned weights =  [1.40112576e-03 2.63024303e+02]\n",
      "Done with gradient descent at iteration  388\n",
      "Learned weights =  [1.40112576e-03 2.63024303e+02]\n",
      "Done with gradient descent at iteration  389\n",
      "Learned weights =  [1.13231054e-03 2.63024303e+02]\n",
      "Done with gradient descent at iteration  389\n",
      "Learned weights =  [1.13231054e-03 2.63024303e+02]\n",
      "Done with gradient descent at iteration  390\n",
      "Learned weights =  [8.63495327e-04 2.63024303e+02]\n",
      "Done with gradient descent at iteration  390\n",
      "Learned weights =  [8.63495327e-04 2.63024303e+02]\n",
      "Done with gradient descent at iteration  391\n",
      "Learned weights =  [5.94680113e-04 2.63024303e+02]\n",
      "Done with gradient descent at iteration  391\n",
      "Learned weights =  [5.94680113e-04 2.63024303e+02]\n",
      "Done with gradient descent at iteration  392\n",
      "Learned weights =  [3.25864900e-04 2.63024303e+02]\n",
      "Done with gradient descent at iteration  392\n",
      "Learned weights =  [3.25864900e-04 2.63024303e+02]\n",
      "Done with gradient descent at iteration  393\n",
      "Learned weights =  [5.70496884e-05 2.63024303e+02]\n",
      "Done with gradient descent at iteration  393\n",
      "Learned weights =  [5.70496884e-05 2.63024303e+02]\n",
      "Done with gradient descent at iteration  394\n",
      "Learned weights =  [-2.11765521e-04  2.63024303e+02]\n",
      "Done with gradient descent at iteration  394\n",
      "Learned weights =  [-2.11765521e-04  2.63024303e+02]\n",
      "Done with gradient descent at iteration  395\n",
      "Learned weights =  [-4.80580730e-04  2.63024303e+02]\n",
      "Done with gradient descent at iteration  395\n",
      "Learned weights =  [-4.80580730e-04  2.63024304e+02]\n",
      "Done with gradient descent at iteration  396\n",
      "Learned weights =  [-7.49395936e-04  2.63024304e+02]\n",
      "Done with gradient descent at iteration  396\n",
      "Learned weights =  [-7.49395936e-04  2.63024304e+02]\n",
      "Done with gradient descent at iteration  397\n",
      "Learned weights =  [-1.01821114e-03  2.63024304e+02]\n",
      "Done with gradient descent at iteration  397\n",
      "Learned weights =  [-1.01821114e-03  2.63024304e+02]\n",
      "Done with gradient descent at iteration  398\n",
      "Learned weights =  [-1.28702635e-03  2.63024304e+02]\n",
      "Done with gradient descent at iteration  398\n",
      "Learned weights =  [-1.28702635e-03  2.63024304e+02]\n",
      "Done with gradient descent at iteration  399\n",
      "Learned weights =  [-1.55584155e-03  2.63024304e+02]\n",
      "Done with gradient descent at iteration  399\n",
      "Learned weights =  [-1.55584155e-03  2.63024304e+02]\n",
      "Iteration = 400\n",
      "Cost function =  1208251115922643.5\n",
      "Done with gradient descent at iteration  400\n",
      "Learned weights =  [-1.82465675e-03  2.63024304e+02]\n",
      "Done with gradient descent at iteration  400\n",
      "Learned weights =  [-1.82465675e-03  2.63024304e+02]\n",
      "Done with gradient descent at iteration  401\n",
      "Learned weights =  [-2.09347195e-03  2.63024304e+02]\n",
      "Done with gradient descent at iteration  401\n",
      "Learned weights =  [-2.09347195e-03  2.63024304e+02]\n",
      "Done with gradient descent at iteration  402\n",
      "Learned weights =  [-2.36228714e-03  2.63024304e+02]\n",
      "Done with gradient descent at iteration  402\n",
      "Learned weights =  [-2.36228714e-03  2.63024304e+02]\n",
      "Done with gradient descent at iteration  403\n",
      "Learned weights =  [-2.63110234e-03  2.63024304e+02]\n",
      "Done with gradient descent at iteration  403\n",
      "Learned weights =  [-2.63110234e-03  2.63024304e+02]\n",
      "Done with gradient descent at iteration  404\n",
      "Learned weights =  [-2.89991753e-03  2.63024304e+02]\n",
      "Done with gradient descent at iteration  404\n",
      "Learned weights =  [-2.89991753e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  405\n",
      "Learned weights =  [-3.16873273e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  405\n",
      "Learned weights =  [-3.16873273e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  406\n",
      "Learned weights =  [-3.43754792e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  406\n",
      "Learned weights =  [-3.43754792e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  407\n",
      "Learned weights =  [-3.70636311e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  407\n",
      "Learned weights =  [-3.70636311e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  408\n",
      "Learned weights =  [-3.97517830e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  408\n",
      "Learned weights =  [-3.97517830e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  409\n",
      "Learned weights =  [-4.24399348e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  409\n",
      "Learned weights =  [-4.24399348e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  410\n",
      "Learned weights =  [-4.51280867e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  410\n",
      "Learned weights =  [-4.51280867e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  411\n",
      "Learned weights =  [-4.78162385e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  411\n",
      "Learned weights =  [-4.78162385e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  412\n",
      "Learned weights =  [-5.05043903e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  412\n",
      "Learned weights =  [-5.05043903e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  413\n",
      "Learned weights =  [-5.31925422e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  413\n",
      "Learned weights =  [-5.31925422e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  414\n",
      "Learned weights =  [-5.58806939e-03  2.63024305e+02]\n",
      "Done with gradient descent at iteration  414\n",
      "Learned weights =  [-5.58806939e-03  2.63024306e+02]\n",
      "Done with gradient descent at iteration  415\n",
      "Learned weights =  [-5.85688457e-03  2.63024306e+02]\n",
      "Done with gradient descent at iteration  415\n",
      "Learned weights =  [-5.85688457e-03  2.63024306e+02]\n",
      "Done with gradient descent at iteration  416\n",
      "Learned weights =  [-6.12569975e-03  2.63024306e+02]\n",
      "Done with gradient descent at iteration  416\n",
      "Learned weights =  [-6.12569975e-03  2.63024306e+02]\n",
      "Done with gradient descent at iteration  417\n",
      "Learned weights =  [-6.39451492e-03  2.63024306e+02]\n",
      "Done with gradient descent at iteration  417\n",
      "Learned weights =  [-6.39451492e-03  2.63024306e+02]\n",
      "Done with gradient descent at iteration  418\n",
      "Learned weights =  [-6.66333010e-03  2.63024306e+02]\n",
      "Done with gradient descent at iteration  418\n",
      "Learned weights =  [-6.66333010e-03  2.63024306e+02]\n",
      "Done with gradient descent at iteration  419\n",
      "Learned weights =  [-6.93214527e-03  2.63024306e+02]\n",
      "Done with gradient descent at iteration  419\n",
      "Learned weights =  [-6.93214527e-03  2.63024306e+02]\n",
      "Done with gradient descent at iteration  420\n",
      "Learned weights =  [-7.20096044e-03  2.63024306e+02]\n",
      "Done with gradient descent at iteration  420\n",
      "Learned weights =  [-7.20096044e-03  2.63024306e+02]\n",
      "Done with gradient descent at iteration  421\n",
      "Learned weights =  [-7.46977561e-03  2.63024306e+02]\n",
      "Done with gradient descent at iteration  421\n",
      "Learned weights =  [-7.46977561e-03  2.63024306e+02]\n",
      "Done with gradient descent at iteration  422\n",
      "Learned weights =  [-7.73859077e-03  2.63024306e+02]\n",
      "Done with gradient descent at iteration  422\n",
      "Learned weights =  [-7.73859077e-03  2.63024306e+02]\n",
      "Done with gradient descent at iteration  423\n",
      "Learned weights =  [-8.00740594e-03  2.63024306e+02]\n",
      "Done with gradient descent at iteration  423\n",
      "Learned weights =  [-8.00740594e-03  2.63024307e+02]\n",
      "Done with gradient descent at iteration  424\n",
      "Learned weights =  [-8.27622110e-03  2.63024307e+02]\n",
      "Done with gradient descent at iteration  424\n",
      "Learned weights =  [-8.27622110e-03  2.63024307e+02]\n",
      "Done with gradient descent at iteration  425\n",
      "Learned weights =  [-8.54503626e-03  2.63024307e+02]\n",
      "Done with gradient descent at iteration  425\n",
      "Learned weights =  [-8.54503626e-03  2.63024307e+02]\n",
      "Done with gradient descent at iteration  426\n",
      "Learned weights =  [-8.81385142e-03  2.63024307e+02]\n",
      "Done with gradient descent at iteration  426\n",
      "Learned weights =  [-8.81385142e-03  2.63024307e+02]\n",
      "Done with gradient descent at iteration  427\n",
      "Learned weights =  [-9.08266658e-03  2.63024307e+02]\n",
      "Done with gradient descent at iteration  427\n",
      "Learned weights =  [-9.08266658e-03  2.63024307e+02]\n",
      "Done with gradient descent at iteration  428\n",
      "Learned weights =  [-9.35148174e-03  2.63024307e+02]\n",
      "Done with gradient descent at iteration  428\n",
      "Learned weights =  [-9.35148174e-03  2.63024307e+02]\n",
      "Done with gradient descent at iteration  429\n",
      "Learned weights =  [-9.62029690e-03  2.63024307e+02]\n",
      "Done with gradient descent at iteration  429\n",
      "Learned weights =  [-9.62029690e-03  2.63024307e+02]\n",
      "Done with gradient descent at iteration  430\n",
      "Learned weights =  [-9.88911205e-03  2.63024307e+02]\n",
      "Done with gradient descent at iteration  430\n",
      "Learned weights =  [-9.88911205e-03  2.63024307e+02]\n",
      "Done with gradient descent at iteration  431\n",
      "Learned weights =  [-1.01579272e-02  2.63024307e+02]\n",
      "Done with gradient descent at iteration  431\n",
      "Learned weights =  [-1.01579272e-02  2.63024307e+02]\n",
      "Done with gradient descent at iteration  432\n",
      "Learned weights =  [-1.04267424e-02  2.63024307e+02]\n",
      "Done with gradient descent at iteration  432\n",
      "Learned weights =  [-1.04267424e-02  2.63024308e+02]\n",
      "Done with gradient descent at iteration  433\n",
      "Learned weights =  [-1.06955575e-02  2.63024308e+02]\n",
      "Done with gradient descent at iteration  433\n",
      "Learned weights =  [-1.06955575e-02  2.63024308e+02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  434\n",
      "Learned weights =  [-1.09643727e-02  2.63024308e+02]\n",
      "Done with gradient descent at iteration  434\n",
      "Learned weights =  [-1.09643727e-02  2.63024308e+02]\n",
      "Done with gradient descent at iteration  435\n",
      "Learned weights =  [-1.12331878e-02  2.63024308e+02]\n",
      "Done with gradient descent at iteration  435\n",
      "Learned weights =  [-1.12331878e-02  2.63024308e+02]\n",
      "Done with gradient descent at iteration  436\n",
      "Learned weights =  [-1.15020029e-02  2.63024308e+02]\n",
      "Done with gradient descent at iteration  436\n",
      "Learned weights =  [-1.15020029e-02  2.63024308e+02]\n",
      "Done with gradient descent at iteration  437\n",
      "Learned weights =  [-1.17708181e-02  2.63024308e+02]\n",
      "Done with gradient descent at iteration  437\n",
      "Learned weights =  [-1.17708181e-02  2.63024308e+02]\n",
      "Done with gradient descent at iteration  438\n",
      "Learned weights =  [-1.20396332e-02  2.63024308e+02]\n",
      "Done with gradient descent at iteration  438\n",
      "Learned weights =  [-1.20396332e-02  2.63024308e+02]\n",
      "Done with gradient descent at iteration  439\n",
      "Learned weights =  [-1.23084484e-02  2.63024308e+02]\n",
      "Done with gradient descent at iteration  439\n",
      "Learned weights =  [-1.23084484e-02  2.63024308e+02]\n",
      "Done with gradient descent at iteration  440\n",
      "Learned weights =  [-1.25772635e-02  2.63024308e+02]\n",
      "Done with gradient descent at iteration  440\n",
      "Learned weights =  [-1.25772635e-02  2.63024308e+02]\n",
      "Done with gradient descent at iteration  441\n",
      "Learned weights =  [-1.28460787e-02  2.63024308e+02]\n",
      "Done with gradient descent at iteration  441\n",
      "Learned weights =  [-1.28460787e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  442\n",
      "Learned weights =  [-1.31148938e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  442\n",
      "Learned weights =  [-1.31148938e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  443\n",
      "Learned weights =  [-1.33837089e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  443\n",
      "Learned weights =  [-1.33837089e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  444\n",
      "Learned weights =  [-1.36525241e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  444\n",
      "Learned weights =  [-1.36525241e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  445\n",
      "Learned weights =  [-1.39213392e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  445\n",
      "Learned weights =  [-1.39213392e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  446\n",
      "Learned weights =  [-1.41901543e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  446\n",
      "Learned weights =  [-1.41901543e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  447\n",
      "Learned weights =  [-1.44589694e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  447\n",
      "Learned weights =  [-1.44589694e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  448\n",
      "Learned weights =  [-1.47277846e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  448\n",
      "Learned weights =  [-1.47277846e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  449\n",
      "Learned weights =  [-1.49965997e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  449\n",
      "Learned weights =  [-1.49965997e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  450\n",
      "Learned weights =  [-1.52654148e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  450\n",
      "Learned weights =  [-1.52654148e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  451\n",
      "Learned weights =  [-1.55342299e-02  2.63024309e+02]\n",
      "Done with gradient descent at iteration  451\n",
      "Learned weights =  [-1.55342299e-02  2.63024310e+02]\n",
      "Done with gradient descent at iteration  452\n",
      "Learned weights =  [-1.58030451e-02  2.63024310e+02]\n",
      "Done with gradient descent at iteration  452\n",
      "Learned weights =  [-1.58030451e-02  2.63024310e+02]\n",
      "Done with gradient descent at iteration  453\n",
      "Learned weights =  [-1.60718602e-02  2.63024310e+02]\n",
      "Done with gradient descent at iteration  453\n",
      "Learned weights =  [-1.60718602e-02  2.63024310e+02]\n",
      "Done with gradient descent at iteration  454\n",
      "Learned weights =  [-1.63406753e-02  2.63024310e+02]\n",
      "Done with gradient descent at iteration  454\n",
      "Learned weights =  [-1.63406753e-02  2.63024310e+02]\n",
      "Done with gradient descent at iteration  455\n",
      "Learned weights =  [-1.66094904e-02  2.63024310e+02]\n",
      "Done with gradient descent at iteration  455\n",
      "Learned weights =  [-1.66094904e-02  2.63024310e+02]\n",
      "Done with gradient descent at iteration  456\n",
      "Learned weights =  [-1.68783055e-02  2.63024310e+02]\n",
      "Done with gradient descent at iteration  456\n",
      "Learned weights =  [-1.68783055e-02  2.63024310e+02]\n",
      "Done with gradient descent at iteration  457\n",
      "Learned weights =  [-1.71471206e-02  2.63024310e+02]\n",
      "Done with gradient descent at iteration  457\n",
      "Learned weights =  [-1.71471206e-02  2.63024310e+02]\n",
      "Done with gradient descent at iteration  458\n",
      "Learned weights =  [-1.74159358e-02  2.63024310e+02]\n",
      "Done with gradient descent at iteration  458\n",
      "Learned weights =  [-1.74159358e-02  2.63024310e+02]\n",
      "Done with gradient descent at iteration  459\n",
      "Learned weights =  [-1.76847509e-02  2.63024310e+02]\n",
      "Done with gradient descent at iteration  459\n",
      "Learned weights =  [-1.76847509e-02  2.63024310e+02]\n",
      "Done with gradient descent at iteration  460\n",
      "Learned weights =  [-1.7953566e-02  2.6302431e+02]\n",
      "Done with gradient descent at iteration  460\n",
      "Learned weights =  [-1.79535660e-02  2.63024311e+02]\n",
      "Done with gradient descent at iteration  461\n",
      "Learned weights =  [-1.82223811e-02  2.63024311e+02]\n",
      "Done with gradient descent at iteration  461\n",
      "Learned weights =  [-1.82223811e-02  2.63024311e+02]\n",
      "Done with gradient descent at iteration  462\n",
      "Learned weights =  [-1.84911962e-02  2.63024311e+02]\n",
      "Done with gradient descent at iteration  462\n",
      "Learned weights =  [-1.84911962e-02  2.63024311e+02]\n",
      "Done with gradient descent at iteration  463\n",
      "Learned weights =  [-1.87600113e-02  2.63024311e+02]\n",
      "Done with gradient descent at iteration  463\n",
      "Learned weights =  [-1.87600113e-02  2.63024311e+02]\n",
      "Done with gradient descent at iteration  464\n",
      "Learned weights =  [-1.90288264e-02  2.63024311e+02]\n",
      "Done with gradient descent at iteration  464\n",
      "Learned weights =  [-1.90288264e-02  2.63024311e+02]\n",
      "Done with gradient descent at iteration  465\n",
      "Learned weights =  [-1.92976415e-02  2.63024311e+02]\n",
      "Done with gradient descent at iteration  465\n",
      "Learned weights =  [-1.92976415e-02  2.63024311e+02]\n",
      "Done with gradient descent at iteration  466\n",
      "Learned weights =  [-1.95664566e-02  2.63024311e+02]\n",
      "Done with gradient descent at iteration  466\n",
      "Learned weights =  [-1.95664566e-02  2.63024311e+02]\n",
      "Done with gradient descent at iteration  467\n",
      "Learned weights =  [-1.98352717e-02  2.63024311e+02]\n",
      "Done with gradient descent at iteration  467\n",
      "Learned weights =  [-1.98352717e-02  2.63024311e+02]\n",
      "Done with gradient descent at iteration  468\n",
      "Learned weights =  [-2.01040868e-02  2.63024311e+02]\n",
      "Done with gradient descent at iteration  468\n",
      "Learned weights =  [-2.01040868e-02  2.63024311e+02]\n",
      "Done with gradient descent at iteration  469\n",
      "Learned weights =  [-2.03729019e-02  2.63024311e+02]\n",
      "Done with gradient descent at iteration  469\n",
      "Learned weights =  [-2.03729019e-02  2.63024312e+02]\n",
      "Done with gradient descent at iteration  470\n",
      "Learned weights =  [-2.06417170e-02  2.63024312e+02]\n",
      "Done with gradient descent at iteration  470\n",
      "Learned weights =  [-2.06417170e-02  2.63024312e+02]\n",
      "Done with gradient descent at iteration  471\n",
      "Learned weights =  [-2.09105321e-02  2.63024312e+02]\n",
      "Done with gradient descent at iteration  471\n",
      "Learned weights =  [-2.09105321e-02  2.63024312e+02]\n",
      "Done with gradient descent at iteration  472\n",
      "Learned weights =  [-2.11793472e-02  2.63024312e+02]\n",
      "Done with gradient descent at iteration  472\n",
      "Learned weights =  [-2.11793472e-02  2.63024312e+02]\n",
      "Done with gradient descent at iteration  473\n",
      "Learned weights =  [-2.14481622e-02  2.63024312e+02]\n",
      "Done with gradient descent at iteration  473\n",
      "Learned weights =  [-2.14481622e-02  2.63024312e+02]\n",
      "Done with gradient descent at iteration  474\n",
      "Learned weights =  [-2.17169773e-02  2.63024312e+02]\n",
      "Done with gradient descent at iteration  474\n",
      "Learned weights =  [-2.17169773e-02  2.63024312e+02]\n",
      "Done with gradient descent at iteration  475\n",
      "Learned weights =  [-2.19857924e-02  2.63024312e+02]\n",
      "Done with gradient descent at iteration  475\n",
      "Learned weights =  [-2.19857924e-02  2.63024312e+02]\n",
      "Done with gradient descent at iteration  476\n",
      "Learned weights =  [-2.22546075e-02  2.63024312e+02]\n",
      "Done with gradient descent at iteration  476\n",
      "Learned weights =  [-2.22546075e-02  2.63024312e+02]\n",
      "Done with gradient descent at iteration  477\n",
      "Learned weights =  [-2.25234226e-02  2.63024312e+02]\n",
      "Done with gradient descent at iteration  477\n",
      "Learned weights =  [-2.25234226e-02  2.63024312e+02]\n",
      "Done with gradient descent at iteration  478\n",
      "Learned weights =  [-2.27922377e-02  2.63024312e+02]\n",
      "Done with gradient descent at iteration  478\n",
      "Learned weights =  [-2.27922377e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  479\n",
      "Learned weights =  [-2.30610527e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  479\n",
      "Learned weights =  [-2.30610527e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  480\n",
      "Learned weights =  [-2.33298678e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  480\n",
      "Learned weights =  [-2.33298678e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  481\n",
      "Learned weights =  [-2.35986829e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  481\n",
      "Learned weights =  [-2.35986829e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  482\n",
      "Learned weights =  [-2.38674980e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  482\n",
      "Learned weights =  [-2.38674980e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  483\n",
      "Learned weights =  [-2.41363130e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  483\n",
      "Learned weights =  [-2.41363130e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  484\n",
      "Learned weights =  [-2.44051281e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  484\n",
      "Learned weights =  [-2.44051281e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  485\n",
      "Learned weights =  [-2.46739432e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  485\n",
      "Learned weights =  [-2.46739432e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  486\n",
      "Learned weights =  [-2.49427583e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  486\n",
      "Learned weights =  [-2.49427583e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  487\n",
      "Learned weights =  [-2.52115733e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  487\n",
      "Learned weights =  [-2.52115733e-02  2.63024313e+02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  488\n",
      "Learned weights =  [-2.54803884e-02  2.63024313e+02]\n",
      "Done with gradient descent at iteration  488\n",
      "Learned weights =  [-2.54803884e-02  2.63024314e+02]\n",
      "Done with gradient descent at iteration  489\n",
      "Learned weights =  [-2.57492035e-02  2.63024314e+02]\n",
      "Done with gradient descent at iteration  489\n",
      "Learned weights =  [-2.57492035e-02  2.63024314e+02]\n",
      "Done with gradient descent at iteration  490\n",
      "Learned weights =  [-2.60180185e-02  2.63024314e+02]\n",
      "Done with gradient descent at iteration  490\n",
      "Learned weights =  [-2.60180185e-02  2.63024314e+02]\n",
      "Done with gradient descent at iteration  491\n",
      "Learned weights =  [-2.62868336e-02  2.63024314e+02]\n",
      "Done with gradient descent at iteration  491\n",
      "Learned weights =  [-2.62868336e-02  2.63024314e+02]\n",
      "Done with gradient descent at iteration  492\n",
      "Learned weights =  [-2.65556486e-02  2.63024314e+02]\n",
      "Done with gradient descent at iteration  492\n",
      "Learned weights =  [-2.65556486e-02  2.63024314e+02]\n",
      "Done with gradient descent at iteration  493\n",
      "Learned weights =  [-2.68244637e-02  2.63024314e+02]\n",
      "Done with gradient descent at iteration  493\n",
      "Learned weights =  [-2.68244637e-02  2.63024314e+02]\n",
      "Done with gradient descent at iteration  494\n",
      "Learned weights =  [-2.70932788e-02  2.63024314e+02]\n",
      "Done with gradient descent at iteration  494\n",
      "Learned weights =  [-2.70932788e-02  2.63024314e+02]\n",
      "Done with gradient descent at iteration  495\n",
      "Learned weights =  [-2.73620938e-02  2.63024314e+02]\n",
      "Done with gradient descent at iteration  495\n",
      "Learned weights =  [-2.73620938e-02  2.63024314e+02]\n",
      "Done with gradient descent at iteration  496\n",
      "Learned weights =  [-2.76309089e-02  2.63024314e+02]\n",
      "Done with gradient descent at iteration  496\n",
      "Learned weights =  [-2.76309089e-02  2.63024314e+02]\n",
      "Done with gradient descent at iteration  497\n",
      "Learned weights =  [-2.78997239e-02  2.63024314e+02]\n",
      "Done with gradient descent at iteration  497\n",
      "Learned weights =  [-2.78997239e-02  2.63024315e+02]\n",
      "Done with gradient descent at iteration  498\n",
      "Learned weights =  [-2.81685390e-02  2.63024315e+02]\n",
      "Done with gradient descent at iteration  498\n",
      "Learned weights =  [-2.81685390e-02  2.63024315e+02]\n",
      "Done with gradient descent at iteration  499\n",
      "Learned weights =  [-2.84373540e-02  2.63024315e+02]\n",
      "Done with gradient descent at iteration  499\n",
      "Learned weights =  [-2.84373540e-02  2.63024315e+02]\n",
      "Iteration = 500\n",
      "Cost function =  1208251108696485.0\n",
      "Done with gradient descent at iteration  500\n",
      "Learned weights =  [-2.87061691e-02  2.63024315e+02]\n",
      "Done with gradient descent at iteration  500\n",
      "Learned weights =  [-2.87061691e-02  2.63024315e+02]\n",
      "Done with gradient descent at iteration  501\n",
      "Learned weights =  [-2.89749841e-02  2.63024315e+02]\n",
      "Done with gradient descent at iteration  501\n",
      "Learned weights =  [-2.89749841e-02  2.63024315e+02]\n",
      "Done with gradient descent at iteration  502\n",
      "Learned weights =  [-2.92437991e-02  2.63024315e+02]\n",
      "Done with gradient descent at iteration  502\n",
      "Learned weights =  [-2.92437991e-02  2.63024315e+02]\n",
      "Done with gradient descent at iteration  503\n",
      "Learned weights =  [-2.95126142e-02  2.63024315e+02]\n",
      "Done with gradient descent at iteration  503\n",
      "Learned weights =  [-2.95126142e-02  2.63024315e+02]\n",
      "Done with gradient descent at iteration  504\n",
      "Learned weights =  [-2.97814292e-02  2.63024315e+02]\n",
      "Done with gradient descent at iteration  504\n",
      "Learned weights =  [-2.97814292e-02  2.63024315e+02]\n",
      "Done with gradient descent at iteration  505\n",
      "Learned weights =  [-3.00502443e-02  2.63024315e+02]\n",
      "Done with gradient descent at iteration  505\n",
      "Learned weights =  [-3.00502443e-02  2.63024315e+02]\n",
      "Done with gradient descent at iteration  506\n",
      "Learned weights =  [-3.03190593e-02  2.63024315e+02]\n",
      "Done with gradient descent at iteration  506\n",
      "Learned weights =  [-3.03190593e-02  2.63024316e+02]\n",
      "Done with gradient descent at iteration  507\n",
      "Learned weights =  [-3.05878743e-02  2.63024316e+02]\n",
      "Done with gradient descent at iteration  507\n",
      "Learned weights =  [-3.05878743e-02  2.63024316e+02]\n",
      "Done with gradient descent at iteration  508\n",
      "Learned weights =  [-3.08566894e-02  2.63024316e+02]\n",
      "Done with gradient descent at iteration  508\n",
      "Learned weights =  [-3.08566894e-02  2.63024316e+02]\n",
      "Done with gradient descent at iteration  509\n",
      "Learned weights =  [-3.11255044e-02  2.63024316e+02]\n",
      "Done with gradient descent at iteration  509\n",
      "Learned weights =  [-3.11255044e-02  2.63024316e+02]\n",
      "Done with gradient descent at iteration  510\n",
      "Learned weights =  [-3.13943194e-02  2.63024316e+02]\n",
      "Done with gradient descent at iteration  510\n",
      "Learned weights =  [-3.13943194e-02  2.63024316e+02]\n",
      "Done with gradient descent at iteration  511\n",
      "Learned weights =  [-3.16631345e-02  2.63024316e+02]\n",
      "Done with gradient descent at iteration  511\n",
      "Learned weights =  [-3.16631345e-02  2.63024316e+02]\n",
      "Done with gradient descent at iteration  512\n",
      "Learned weights =  [-3.19319495e-02  2.63024316e+02]\n",
      "Done with gradient descent at iteration  512\n",
      "Learned weights =  [-3.19319495e-02  2.63024316e+02]\n",
      "Done with gradient descent at iteration  513\n",
      "Learned weights =  [-3.22007645e-02  2.63024316e+02]\n",
      "Done with gradient descent at iteration  513\n",
      "Learned weights =  [-3.22007645e-02  2.63024316e+02]\n",
      "Done with gradient descent at iteration  514\n",
      "Learned weights =  [-3.24695796e-02  2.63024316e+02]\n",
      "Done with gradient descent at iteration  514\n",
      "Learned weights =  [-3.24695796e-02  2.63024316e+02]\n",
      "Done with gradient descent at iteration  515\n",
      "Learned weights =  [-3.27383946e-02  2.63024316e+02]\n",
      "Done with gradient descent at iteration  515\n",
      "Learned weights =  [-3.27383946e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  516\n",
      "Learned weights =  [-3.30072096e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  516\n",
      "Learned weights =  [-3.30072096e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  517\n",
      "Learned weights =  [-3.32760246e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  517\n",
      "Learned weights =  [-3.32760246e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  518\n",
      "Learned weights =  [-3.35448396e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  518\n",
      "Learned weights =  [-3.35448396e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  519\n",
      "Learned weights =  [-3.38136547e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  519\n",
      "Learned weights =  [-3.38136547e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  520\n",
      "Learned weights =  [-3.40824697e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  520\n",
      "Learned weights =  [-3.40824697e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  521\n",
      "Learned weights =  [-3.43512847e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  521\n",
      "Learned weights =  [-3.43512847e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  522\n",
      "Learned weights =  [-3.46200997e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  522\n",
      "Learned weights =  [-3.46200997e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  523\n",
      "Learned weights =  [-3.48889147e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  523\n",
      "Learned weights =  [-3.48889147e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  524\n",
      "Learned weights =  [-3.51577297e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  524\n",
      "Learned weights =  [-3.51577297e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  525\n",
      "Learned weights =  [-3.54265447e-02  2.63024317e+02]\n",
      "Done with gradient descent at iteration  525\n",
      "Learned weights =  [-3.54265447e-02  2.63024318e+02]\n",
      "Done with gradient descent at iteration  526\n",
      "Learned weights =  [-3.56953597e-02  2.63024318e+02]\n",
      "Done with gradient descent at iteration  526\n",
      "Learned weights =  [-3.56953597e-02  2.63024318e+02]\n",
      "Done with gradient descent at iteration  527\n",
      "Learned weights =  [-3.59641748e-02  2.63024318e+02]\n",
      "Done with gradient descent at iteration  527\n",
      "Learned weights =  [-3.59641748e-02  2.63024318e+02]\n",
      "Done with gradient descent at iteration  528\n",
      "Learned weights =  [-3.62329898e-02  2.63024318e+02]\n",
      "Done with gradient descent at iteration  528\n",
      "Learned weights =  [-3.62329898e-02  2.63024318e+02]\n",
      "Done with gradient descent at iteration  529\n",
      "Learned weights =  [-3.65018048e-02  2.63024318e+02]\n",
      "Done with gradient descent at iteration  529\n",
      "Learned weights =  [-3.65018048e-02  2.63024318e+02]\n",
      "Done with gradient descent at iteration  530\n",
      "Learned weights =  [-3.67706198e-02  2.63024318e+02]\n",
      "Done with gradient descent at iteration  530\n",
      "Learned weights =  [-3.67706198e-02  2.63024318e+02]\n",
      "Done with gradient descent at iteration  531\n",
      "Learned weights =  [-3.70394348e-02  2.63024318e+02]\n",
      "Done with gradient descent at iteration  531\n",
      "Learned weights =  [-3.70394348e-02  2.63024318e+02]\n",
      "Done with gradient descent at iteration  532\n",
      "Learned weights =  [-3.73082498e-02  2.63024318e+02]\n",
      "Done with gradient descent at iteration  532\n",
      "Learned weights =  [-3.73082498e-02  2.63024318e+02]\n",
      "Done with gradient descent at iteration  533\n",
      "Learned weights =  [-3.75770648e-02  2.63024318e+02]\n",
      "Done with gradient descent at iteration  533\n",
      "Learned weights =  [-3.75770648e-02  2.63024318e+02]\n",
      "Done with gradient descent at iteration  534\n",
      "Learned weights =  [-3.78458798e-02  2.63024318e+02]\n",
      "Done with gradient descent at iteration  534\n",
      "Learned weights =  [-3.78458798e-02  2.63024319e+02]\n",
      "Done with gradient descent at iteration  535\n",
      "Learned weights =  [-3.81146947e-02  2.63024319e+02]\n",
      "Done with gradient descent at iteration  535\n",
      "Learned weights =  [-3.81146947e-02  2.63024319e+02]\n",
      "Done with gradient descent at iteration  536\n",
      "Learned weights =  [-3.83835097e-02  2.63024319e+02]\n",
      "Done with gradient descent at iteration  536\n",
      "Learned weights =  [-3.83835097e-02  2.63024319e+02]\n",
      "Done with gradient descent at iteration  537\n",
      "Learned weights =  [-3.86523247e-02  2.63024319e+02]\n",
      "Done with gradient descent at iteration  537\n",
      "Learned weights =  [-3.86523247e-02  2.63024319e+02]\n",
      "Done with gradient descent at iteration  538\n",
      "Learned weights =  [-3.89211397e-02  2.63024319e+02]\n",
      "Done with gradient descent at iteration  538\n",
      "Learned weights =  [-3.89211397e-02  2.63024319e+02]\n",
      "Done with gradient descent at iteration  539\n",
      "Learned weights =  [-3.91899547e-02  2.63024319e+02]\n",
      "Done with gradient descent at iteration  539\n",
      "Learned weights =  [-3.91899547e-02  2.63024319e+02]\n",
      "Done with gradient descent at iteration  540\n",
      "Learned weights =  [-3.94587697e-02  2.63024319e+02]\n",
      "Done with gradient descent at iteration  540\n",
      "Learned weights =  [-3.94587697e-02  2.63024319e+02]\n",
      "Done with gradient descent at iteration  541\n",
      "Learned weights =  [-3.97275847e-02  2.63024319e+02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  541\n",
      "Learned weights =  [-3.97275847e-02  2.63024319e+02]\n",
      "Done with gradient descent at iteration  542\n",
      "Learned weights =  [-3.99963997e-02  2.63024319e+02]\n",
      "Done with gradient descent at iteration  542\n",
      "Learned weights =  [-3.99963997e-02  2.63024319e+02]\n",
      "Done with gradient descent at iteration  543\n",
      "Learned weights =  [-4.02652146e-02  2.63024319e+02]\n",
      "Done with gradient descent at iteration  543\n",
      "Learned weights =  [-4.02652146e-02  2.63024320e+02]\n",
      "Done with gradient descent at iteration  544\n",
      "Learned weights =  [-4.05340296e-02  2.63024320e+02]\n",
      "Done with gradient descent at iteration  544\n",
      "Learned weights =  [-4.05340296e-02  2.63024320e+02]\n",
      "Done with gradient descent at iteration  545\n",
      "Learned weights =  [-4.08028446e-02  2.63024320e+02]\n",
      "Done with gradient descent at iteration  545\n",
      "Learned weights =  [-4.08028446e-02  2.63024320e+02]\n",
      "Done with gradient descent at iteration  546\n",
      "Learned weights =  [-4.10716596e-02  2.63024320e+02]\n",
      "Done with gradient descent at iteration  546\n",
      "Learned weights =  [-4.10716596e-02  2.63024320e+02]\n",
      "Done with gradient descent at iteration  547\n",
      "Learned weights =  [-4.13404745e-02  2.63024320e+02]\n",
      "Done with gradient descent at iteration  547\n",
      "Learned weights =  [-4.13404745e-02  2.63024320e+02]\n",
      "Done with gradient descent at iteration  548\n",
      "Learned weights =  [-4.16092895e-02  2.63024320e+02]\n",
      "Done with gradient descent at iteration  548\n",
      "Learned weights =  [-4.16092895e-02  2.63024320e+02]\n",
      "Done with gradient descent at iteration  549\n",
      "Learned weights =  [-4.18781045e-02  2.63024320e+02]\n",
      "Done with gradient descent at iteration  549\n",
      "Learned weights =  [-4.18781045e-02  2.63024320e+02]\n",
      "Done with gradient descent at iteration  550\n",
      "Learned weights =  [-4.21469195e-02  2.63024320e+02]\n",
      "Done with gradient descent at iteration  550\n",
      "Learned weights =  [-4.21469195e-02  2.63024320e+02]\n",
      "Done with gradient descent at iteration  551\n",
      "Learned weights =  [-4.24157344e-02  2.63024320e+02]\n",
      "Done with gradient descent at iteration  551\n",
      "Learned weights =  [-4.24157344e-02  2.63024320e+02]\n",
      "Done with gradient descent at iteration  552\n",
      "Learned weights =  [-4.26845494e-02  2.63024320e+02]\n",
      "Done with gradient descent at iteration  552\n",
      "Learned weights =  [-4.26845494e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  553\n",
      "Learned weights =  [-4.29533644e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  553\n",
      "Learned weights =  [-4.29533644e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  554\n",
      "Learned weights =  [-4.32221793e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  554\n",
      "Learned weights =  [-4.32221793e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  555\n",
      "Learned weights =  [-4.34909943e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  555\n",
      "Learned weights =  [-4.34909943e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  556\n",
      "Learned weights =  [-4.37598093e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  556\n",
      "Learned weights =  [-4.37598093e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  557\n",
      "Learned weights =  [-4.40286242e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  557\n",
      "Learned weights =  [-4.40286242e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  558\n",
      "Learned weights =  [-4.42974392e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  558\n",
      "Learned weights =  [-4.42974392e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  559\n",
      "Learned weights =  [-4.45662541e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  559\n",
      "Learned weights =  [-4.45662541e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  560\n",
      "Learned weights =  [-4.48350691e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  560\n",
      "Learned weights =  [-4.48350691e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  561\n",
      "Learned weights =  [-4.51038840e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  561\n",
      "Learned weights =  [-4.51038840e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  562\n",
      "Learned weights =  [-4.53726990e-02  2.63024321e+02]\n",
      "Done with gradient descent at iteration  562\n",
      "Learned weights =  [-4.53726990e-02  2.63024322e+02]\n",
      "Done with gradient descent at iteration  563\n",
      "Learned weights =  [-4.56415139e-02  2.63024322e+02]\n",
      "Done with gradient descent at iteration  563\n",
      "Learned weights =  [-4.56415139e-02  2.63024322e+02]\n",
      "Done with gradient descent at iteration  564\n",
      "Learned weights =  [-4.59103289e-02  2.63024322e+02]\n",
      "Done with gradient descent at iteration  564\n",
      "Learned weights =  [-4.59103289e-02  2.63024322e+02]\n",
      "Done with gradient descent at iteration  565\n",
      "Learned weights =  [-4.61791438e-02  2.63024322e+02]\n",
      "Done with gradient descent at iteration  565\n",
      "Learned weights =  [-4.61791438e-02  2.63024322e+02]\n",
      "Done with gradient descent at iteration  566\n",
      "Learned weights =  [-4.64479588e-02  2.63024322e+02]\n",
      "Done with gradient descent at iteration  566\n",
      "Learned weights =  [-4.64479588e-02  2.63024322e+02]\n",
      "Done with gradient descent at iteration  567\n",
      "Learned weights =  [-4.67167737e-02  2.63024322e+02]\n",
      "Done with gradient descent at iteration  567\n",
      "Learned weights =  [-4.67167737e-02  2.63024322e+02]\n",
      "Done with gradient descent at iteration  568\n",
      "Learned weights =  [-4.69855887e-02  2.63024322e+02]\n",
      "Done with gradient descent at iteration  568\n",
      "Learned weights =  [-4.69855887e-02  2.63024322e+02]\n",
      "Done with gradient descent at iteration  569\n",
      "Learned weights =  [-4.72544036e-02  2.63024322e+02]\n",
      "Done with gradient descent at iteration  569\n",
      "Learned weights =  [-4.72544036e-02  2.63024322e+02]\n",
      "Done with gradient descent at iteration  570\n",
      "Learned weights =  [-4.75232186e-02  2.63024322e+02]\n",
      "Done with gradient descent at iteration  570\n",
      "Learned weights =  [-4.75232186e-02  2.63024322e+02]\n",
      "Done with gradient descent at iteration  571\n",
      "Learned weights =  [-4.77920335e-02  2.63024322e+02]\n",
      "Done with gradient descent at iteration  571\n",
      "Learned weights =  [-4.77920335e-02  2.63024323e+02]\n",
      "Done with gradient descent at iteration  572\n",
      "Learned weights =  [-4.80608484e-02  2.63024323e+02]\n",
      "Done with gradient descent at iteration  572\n",
      "Learned weights =  [-4.80608484e-02  2.63024323e+02]\n",
      "Done with gradient descent at iteration  573\n",
      "Learned weights =  [-4.83296634e-02  2.63024323e+02]\n",
      "Done with gradient descent at iteration  573\n",
      "Learned weights =  [-4.83296634e-02  2.63024323e+02]\n",
      "Done with gradient descent at iteration  574\n",
      "Learned weights =  [-4.85984783e-02  2.63024323e+02]\n",
      "Done with gradient descent at iteration  574\n",
      "Learned weights =  [-4.85984783e-02  2.63024323e+02]\n",
      "Done with gradient descent at iteration  575\n",
      "Learned weights =  [-4.88672932e-02  2.63024323e+02]\n",
      "Done with gradient descent at iteration  575\n",
      "Learned weights =  [-4.88672932e-02  2.63024323e+02]\n",
      "Done with gradient descent at iteration  576\n",
      "Learned weights =  [-4.91361082e-02  2.63024323e+02]\n",
      "Done with gradient descent at iteration  576\n",
      "Learned weights =  [-4.91361082e-02  2.63024323e+02]\n",
      "Done with gradient descent at iteration  577\n",
      "Learned weights =  [-4.94049231e-02  2.63024323e+02]\n",
      "Done with gradient descent at iteration  577\n",
      "Learned weights =  [-4.94049231e-02  2.63024323e+02]\n",
      "Done with gradient descent at iteration  578\n",
      "Learned weights =  [-4.96737380e-02  2.63024323e+02]\n",
      "Done with gradient descent at iteration  578\n",
      "Learned weights =  [-4.96737380e-02  2.63024323e+02]\n",
      "Done with gradient descent at iteration  579\n",
      "Learned weights =  [-4.99425529e-02  2.63024323e+02]\n",
      "Done with gradient descent at iteration  579\n",
      "Learned weights =  [-4.99425529e-02  2.63024323e+02]\n",
      "Done with gradient descent at iteration  580\n",
      "Learned weights =  [-5.02113679e-02  2.63024323e+02]\n",
      "Done with gradient descent at iteration  580\n",
      "Learned weights =  [-5.02113679e-02  2.63024324e+02]\n",
      "Done with gradient descent at iteration  581\n",
      "Learned weights =  [-5.04801828e-02  2.63024324e+02]\n",
      "Done with gradient descent at iteration  581\n",
      "Learned weights =  [-5.04801828e-02  2.63024324e+02]\n",
      "Done with gradient descent at iteration  582\n",
      "Learned weights =  [-5.07489977e-02  2.63024324e+02]\n",
      "Done with gradient descent at iteration  582\n",
      "Learned weights =  [-5.07489977e-02  2.63024324e+02]\n",
      "Done with gradient descent at iteration  583\n",
      "Learned weights =  [-5.10178126e-02  2.63024324e+02]\n",
      "Done with gradient descent at iteration  583\n",
      "Learned weights =  [-5.10178126e-02  2.63024324e+02]\n",
      "Done with gradient descent at iteration  584\n",
      "Learned weights =  [-5.12866275e-02  2.63024324e+02]\n",
      "Done with gradient descent at iteration  584\n",
      "Learned weights =  [-5.12866275e-02  2.63024324e+02]\n",
      "Done with gradient descent at iteration  585\n",
      "Learned weights =  [-5.15554425e-02  2.63024324e+02]\n",
      "Done with gradient descent at iteration  585\n",
      "Learned weights =  [-5.15554425e-02  2.63024324e+02]\n",
      "Done with gradient descent at iteration  586\n",
      "Learned weights =  [-5.18242574e-02  2.63024324e+02]\n",
      "Done with gradient descent at iteration  586\n",
      "Learned weights =  [-5.18242574e-02  2.63024324e+02]\n",
      "Done with gradient descent at iteration  587\n",
      "Learned weights =  [-5.20930723e-02  2.63024324e+02]\n",
      "Done with gradient descent at iteration  587\n",
      "Learned weights =  [-5.20930723e-02  2.63024324e+02]\n",
      "Done with gradient descent at iteration  588\n",
      "Learned weights =  [-5.23618872e-02  2.63024324e+02]\n",
      "Done with gradient descent at iteration  588\n",
      "Learned weights =  [-5.23618872e-02  2.63024324e+02]\n",
      "Done with gradient descent at iteration  589\n",
      "Learned weights =  [-5.26307021e-02  2.63024324e+02]\n",
      "Done with gradient descent at iteration  589\n",
      "Learned weights =  [-5.26307021e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  590\n",
      "Learned weights =  [-5.28995170e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  590\n",
      "Learned weights =  [-5.28995170e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  591\n",
      "Learned weights =  [-5.31683319e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  591\n",
      "Learned weights =  [-5.31683319e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  592\n",
      "Learned weights =  [-5.34371468e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  592\n",
      "Learned weights =  [-5.34371468e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  593\n",
      "Learned weights =  [-5.37059617e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  593\n",
      "Learned weights =  [-5.37059617e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  594\n",
      "Learned weights =  [-5.39747766e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  594\n",
      "Learned weights =  [-5.39747766e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  595\n",
      "Learned weights =  [-5.42435915e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  595\n",
      "Learned weights =  [-5.42435915e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  596\n",
      "Learned weights =  [-5.45124064e-02  2.63024325e+02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  596\n",
      "Learned weights =  [-5.45124064e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  597\n",
      "Learned weights =  [-5.47812213e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  597\n",
      "Learned weights =  [-5.47812213e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  598\n",
      "Learned weights =  [-5.50500362e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  598\n",
      "Learned weights =  [-5.50500362e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  599\n",
      "Learned weights =  [-5.53188511e-02  2.63024325e+02]\n",
      "Done with gradient descent at iteration  599\n",
      "Learned weights =  [-5.53188511e-02  2.63024326e+02]\n",
      "Iteration = 600\n",
      "Cost function =  1208251101470335.0\n",
      "Done with gradient descent at iteration  600\n",
      "Learned weights =  [-5.55876660e-02  2.63024326e+02]\n",
      "Done with gradient descent at iteration  600\n",
      "Learned weights =  [-5.55876660e-02  2.63024326e+02]\n",
      "Done with gradient descent at iteration  601\n",
      "Learned weights =  [-5.58564809e-02  2.63024326e+02]\n",
      "Done with gradient descent at iteration  601\n",
      "Learned weights =  [-5.58564809e-02  2.63024326e+02]\n",
      "Done with gradient descent at iteration  602\n",
      "Learned weights =  [-5.61252958e-02  2.63024326e+02]\n",
      "Done with gradient descent at iteration  602\n",
      "Learned weights =  [-5.61252958e-02  2.63024326e+02]\n",
      "Done with gradient descent at iteration  603\n",
      "Learned weights =  [-5.63941107e-02  2.63024326e+02]\n",
      "Done with gradient descent at iteration  603\n",
      "Learned weights =  [-5.63941107e-02  2.63024326e+02]\n",
      "Done with gradient descent at iteration  604\n",
      "Learned weights =  [-5.66629256e-02  2.63024326e+02]\n",
      "Done with gradient descent at iteration  604\n",
      "Learned weights =  [-5.66629256e-02  2.63024326e+02]\n",
      "Done with gradient descent at iteration  605\n",
      "Learned weights =  [-5.69317405e-02  2.63024326e+02]\n",
      "Done with gradient descent at iteration  605\n",
      "Learned weights =  [-5.69317405e-02  2.63024326e+02]\n",
      "Done with gradient descent at iteration  606\n",
      "Learned weights =  [-5.72005554e-02  2.63024326e+02]\n",
      "Done with gradient descent at iteration  606\n",
      "Learned weights =  [-5.72005554e-02  2.63024326e+02]\n",
      "Done with gradient descent at iteration  607\n",
      "Learned weights =  [-5.74693702e-02  2.63024326e+02]\n",
      "Done with gradient descent at iteration  607\n",
      "Learned weights =  [-5.74693702e-02  2.63024326e+02]\n",
      "Done with gradient descent at iteration  608\n",
      "Learned weights =  [-5.77381851e-02  2.63024326e+02]\n",
      "Done with gradient descent at iteration  608\n",
      "Learned weights =  [-5.77381851e-02  2.63024327e+02]\n",
      "Done with gradient descent at iteration  609\n",
      "Learned weights =  [-5.80070000e-02  2.63024327e+02]\n",
      "Done with gradient descent at iteration  609\n",
      "Learned weights =  [-5.80070000e-02  2.63024327e+02]\n",
      "Done with gradient descent at iteration  610\n",
      "Learned weights =  [-5.82758149e-02  2.63024327e+02]\n",
      "Done with gradient descent at iteration  610\n",
      "Learned weights =  [-5.82758149e-02  2.63024327e+02]\n",
      "Done with gradient descent at iteration  611\n",
      "Learned weights =  [-5.85446298e-02  2.63024327e+02]\n",
      "Done with gradient descent at iteration  611\n",
      "Learned weights =  [-5.85446298e-02  2.63024327e+02]\n",
      "Done with gradient descent at iteration  612\n",
      "Learned weights =  [-5.88134446e-02  2.63024327e+02]\n",
      "Done with gradient descent at iteration  612\n",
      "Learned weights =  [-5.88134446e-02  2.63024327e+02]\n",
      "Done with gradient descent at iteration  613\n",
      "Learned weights =  [-5.90822595e-02  2.63024327e+02]\n",
      "Done with gradient descent at iteration  613\n",
      "Learned weights =  [-5.90822595e-02  2.63024327e+02]\n",
      "Done with gradient descent at iteration  614\n",
      "Learned weights =  [-5.93510744e-02  2.63024327e+02]\n",
      "Done with gradient descent at iteration  614\n",
      "Learned weights =  [-5.93510744e-02  2.63024327e+02]\n",
      "Done with gradient descent at iteration  615\n",
      "Learned weights =  [-5.96198893e-02  2.63024327e+02]\n",
      "Done with gradient descent at iteration  615\n",
      "Learned weights =  [-5.96198893e-02  2.63024327e+02]\n",
      "Done with gradient descent at iteration  616\n",
      "Learned weights =  [-5.98887041e-02  2.63024327e+02]\n",
      "Done with gradient descent at iteration  616\n",
      "Learned weights =  [-5.98887041e-02  2.63024327e+02]\n",
      "Done with gradient descent at iteration  617\n",
      "Learned weights =  [-6.01575190e-02  2.63024327e+02]\n",
      "Done with gradient descent at iteration  617\n",
      "Learned weights =  [-6.01575190e-02  2.63024328e+02]\n",
      "Done with gradient descent at iteration  618\n",
      "Learned weights =  [-6.04263339e-02  2.63024328e+02]\n",
      "Done with gradient descent at iteration  618\n",
      "Learned weights =  [-6.04263339e-02  2.63024328e+02]\n",
      "Done with gradient descent at iteration  619\n",
      "Learned weights =  [-6.06951487e-02  2.63024328e+02]\n",
      "Done with gradient descent at iteration  619\n",
      "Learned weights =  [-6.06951487e-02  2.63024328e+02]\n",
      "Done with gradient descent at iteration  620\n",
      "Learned weights =  [-6.09639636e-02  2.63024328e+02]\n",
      "Done with gradient descent at iteration  620\n",
      "Learned weights =  [-6.09639636e-02  2.63024328e+02]\n",
      "Done with gradient descent at iteration  621\n",
      "Learned weights =  [-6.12327784e-02  2.63024328e+02]\n",
      "Done with gradient descent at iteration  621\n",
      "Learned weights =  [-6.12327784e-02  2.63024328e+02]\n",
      "Done with gradient descent at iteration  622\n",
      "Learned weights =  [-6.15015933e-02  2.63024328e+02]\n",
      "Done with gradient descent at iteration  622\n",
      "Learned weights =  [-6.15015933e-02  2.63024328e+02]\n",
      "Done with gradient descent at iteration  623\n",
      "Learned weights =  [-6.17704082e-02  2.63024328e+02]\n",
      "Done with gradient descent at iteration  623\n",
      "Learned weights =  [-6.17704082e-02  2.63024328e+02]\n",
      "Done with gradient descent at iteration  624\n",
      "Learned weights =  [-6.20392230e-02  2.63024328e+02]\n",
      "Done with gradient descent at iteration  624\n",
      "Learned weights =  [-6.20392230e-02  2.63024328e+02]\n",
      "Done with gradient descent at iteration  625\n",
      "Learned weights =  [-6.23080379e-02  2.63024328e+02]\n",
      "Done with gradient descent at iteration  625\n",
      "Learned weights =  [-6.23080379e-02  2.63024328e+02]\n",
      "Done with gradient descent at iteration  626\n",
      "Learned weights =  [-6.25768527e-02  2.63024328e+02]\n",
      "Done with gradient descent at iteration  626\n",
      "Learned weights =  [-6.25768527e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  627\n",
      "Learned weights =  [-6.28456676e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  627\n",
      "Learned weights =  [-6.28456676e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  628\n",
      "Learned weights =  [-6.31144824e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  628\n",
      "Learned weights =  [-6.31144824e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  629\n",
      "Learned weights =  [-6.33832973e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  629\n",
      "Learned weights =  [-6.33832973e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  630\n",
      "Learned weights =  [-6.36521121e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  630\n",
      "Learned weights =  [-6.36521121e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  631\n",
      "Learned weights =  [-6.39209270e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  631\n",
      "Learned weights =  [-6.39209270e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  632\n",
      "Learned weights =  [-6.41897418e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  632\n",
      "Learned weights =  [-6.41897418e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  633\n",
      "Learned weights =  [-6.44585567e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  633\n",
      "Learned weights =  [-6.44585567e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  634\n",
      "Learned weights =  [-6.47273715e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  634\n",
      "Learned weights =  [-6.47273715e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  635\n",
      "Learned weights =  [-6.49961864e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  635\n",
      "Learned weights =  [-6.49961864e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  636\n",
      "Learned weights =  [-6.52650012e-02  2.63024329e+02]\n",
      "Done with gradient descent at iteration  636\n",
      "Learned weights =  [-6.52650012e-02  2.63024330e+02]\n",
      "Done with gradient descent at iteration  637\n",
      "Learned weights =  [-6.5533816e-02  2.6302433e+02]\n",
      "Done with gradient descent at iteration  637\n",
      "Learned weights =  [-6.5533816e-02  2.6302433e+02]\n",
      "Done with gradient descent at iteration  638\n",
      "Learned weights =  [-6.58026309e-02  2.63024330e+02]\n",
      "Done with gradient descent at iteration  638\n",
      "Learned weights =  [-6.58026309e-02  2.63024330e+02]\n",
      "Done with gradient descent at iteration  639\n",
      "Learned weights =  [-6.60714457e-02  2.63024330e+02]\n",
      "Done with gradient descent at iteration  639\n",
      "Learned weights =  [-6.60714457e-02  2.63024330e+02]\n",
      "Done with gradient descent at iteration  640\n",
      "Learned weights =  [-6.63402605e-02  2.63024330e+02]\n",
      "Done with gradient descent at iteration  640\n",
      "Learned weights =  [-6.63402605e-02  2.63024330e+02]\n",
      "Done with gradient descent at iteration  641\n",
      "Learned weights =  [-6.66090754e-02  2.63024330e+02]\n",
      "Done with gradient descent at iteration  641\n",
      "Learned weights =  [-6.66090754e-02  2.63024330e+02]\n",
      "Done with gradient descent at iteration  642\n",
      "Learned weights =  [-6.68778902e-02  2.63024330e+02]\n",
      "Done with gradient descent at iteration  642\n",
      "Learned weights =  [-6.68778902e-02  2.63024330e+02]\n",
      "Done with gradient descent at iteration  643\n",
      "Learned weights =  [-6.7146705e-02  2.6302433e+02]\n",
      "Done with gradient descent at iteration  643\n",
      "Learned weights =  [-6.7146705e-02  2.6302433e+02]\n",
      "Done with gradient descent at iteration  644\n",
      "Learned weights =  [-6.74155198e-02  2.63024330e+02]\n",
      "Done with gradient descent at iteration  644\n",
      "Learned weights =  [-6.74155198e-02  2.63024330e+02]\n",
      "Done with gradient descent at iteration  645\n",
      "Learned weights =  [-6.76843347e-02  2.63024330e+02]\n",
      "Done with gradient descent at iteration  645\n",
      "Learned weights =  [-6.76843347e-02  2.63024331e+02]\n",
      "Done with gradient descent at iteration  646\n",
      "Learned weights =  [-6.79531495e-02  2.63024331e+02]\n",
      "Done with gradient descent at iteration  646\n",
      "Learned weights =  [-6.79531495e-02  2.63024331e+02]\n",
      "Done with gradient descent at iteration  647\n",
      "Learned weights =  [-6.82219643e-02  2.63024331e+02]\n",
      "Done with gradient descent at iteration  647\n",
      "Learned weights =  [-6.82219643e-02  2.63024331e+02]\n",
      "Done with gradient descent at iteration  648\n",
      "Learned weights =  [-6.84907791e-02  2.63024331e+02]\n",
      "Done with gradient descent at iteration  648\n",
      "Learned weights =  [-6.84907791e-02  2.63024331e+02]\n",
      "Done with gradient descent at iteration  649\n",
      "Learned weights =  [-6.87595940e-02  2.63024331e+02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  649\n",
      "Learned weights =  [-6.87595940e-02  2.63024331e+02]\n",
      "Done with gradient descent at iteration  650\n",
      "Learned weights =  [-6.90284088e-02  2.63024331e+02]\n",
      "Done with gradient descent at iteration  650\n",
      "Learned weights =  [-6.90284088e-02  2.63024331e+02]\n",
      "Done with gradient descent at iteration  651\n",
      "Learned weights =  [-6.92972236e-02  2.63024331e+02]\n",
      "Done with gradient descent at iteration  651\n",
      "Learned weights =  [-6.92972236e-02  2.63024331e+02]\n",
      "Done with gradient descent at iteration  652\n",
      "Learned weights =  [-6.95660384e-02  2.63024331e+02]\n",
      "Done with gradient descent at iteration  652\n",
      "Learned weights =  [-6.95660384e-02  2.63024331e+02]\n",
      "Done with gradient descent at iteration  653\n",
      "Learned weights =  [-6.98348532e-02  2.63024331e+02]\n",
      "Done with gradient descent at iteration  653\n",
      "Learned weights =  [-6.98348532e-02  2.63024331e+02]\n",
      "Done with gradient descent at iteration  654\n",
      "Learned weights =  [-7.01036680e-02  2.63024331e+02]\n",
      "Done with gradient descent at iteration  654\n",
      "Learned weights =  [-7.01036680e-02  2.63024332e+02]\n",
      "Done with gradient descent at iteration  655\n",
      "Learned weights =  [-7.03724828e-02  2.63024332e+02]\n",
      "Done with gradient descent at iteration  655\n",
      "Learned weights =  [-7.03724828e-02  2.63024332e+02]\n",
      "Done with gradient descent at iteration  656\n",
      "Learned weights =  [-7.06412976e-02  2.63024332e+02]\n",
      "Done with gradient descent at iteration  656\n",
      "Learned weights =  [-7.06412976e-02  2.63024332e+02]\n",
      "Done with gradient descent at iteration  657\n",
      "Learned weights =  [-7.09101124e-02  2.63024332e+02]\n",
      "Done with gradient descent at iteration  657\n",
      "Learned weights =  [-7.09101124e-02  2.63024332e+02]\n",
      "Done with gradient descent at iteration  658\n",
      "Learned weights =  [-7.11789273e-02  2.63024332e+02]\n",
      "Done with gradient descent at iteration  658\n",
      "Learned weights =  [-7.11789273e-02  2.63024332e+02]\n",
      "Done with gradient descent at iteration  659\n",
      "Learned weights =  [-7.14477421e-02  2.63024332e+02]\n",
      "Done with gradient descent at iteration  659\n",
      "Learned weights =  [-7.14477421e-02  2.63024332e+02]\n",
      "Done with gradient descent at iteration  660\n",
      "Learned weights =  [-7.17165569e-02  2.63024332e+02]\n",
      "Done with gradient descent at iteration  660\n",
      "Learned weights =  [-7.17165569e-02  2.63024332e+02]\n",
      "Done with gradient descent at iteration  661\n",
      "Learned weights =  [-7.19853717e-02  2.63024332e+02]\n",
      "Done with gradient descent at iteration  661\n",
      "Learned weights =  [-7.19853717e-02  2.63024332e+02]\n",
      "Done with gradient descent at iteration  662\n",
      "Learned weights =  [-7.22541865e-02  2.63024332e+02]\n",
      "Done with gradient descent at iteration  662\n",
      "Learned weights =  [-7.22541865e-02  2.63024332e+02]\n",
      "Done with gradient descent at iteration  663\n",
      "Learned weights =  [-7.25230013e-02  2.63024332e+02]\n",
      "Done with gradient descent at iteration  663\n",
      "Learned weights =  [-7.25230013e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  664\n",
      "Learned weights =  [-7.27918160e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  664\n",
      "Learned weights =  [-7.27918160e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  665\n",
      "Learned weights =  [-7.30606308e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  665\n",
      "Learned weights =  [-7.30606308e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  666\n",
      "Learned weights =  [-7.33294456e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  666\n",
      "Learned weights =  [-7.33294456e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  667\n",
      "Learned weights =  [-7.35982604e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  667\n",
      "Learned weights =  [-7.35982604e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  668\n",
      "Learned weights =  [-7.38670752e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  668\n",
      "Learned weights =  [-7.38670752e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  669\n",
      "Learned weights =  [-7.41358900e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  669\n",
      "Learned weights =  [-7.41358900e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  670\n",
      "Learned weights =  [-7.44047048e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  670\n",
      "Learned weights =  [-7.44047048e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  671\n",
      "Learned weights =  [-7.46735196e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  671\n",
      "Learned weights =  [-7.46735196e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  672\n",
      "Learned weights =  [-7.49423344e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  672\n",
      "Learned weights =  [-7.49423344e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  673\n",
      "Learned weights =  [-7.52111491e-02  2.63024333e+02]\n",
      "Done with gradient descent at iteration  673\n",
      "Learned weights =  [-7.52111491e-02  2.63024334e+02]\n",
      "Done with gradient descent at iteration  674\n",
      "Learned weights =  [-7.54799639e-02  2.63024334e+02]\n",
      "Done with gradient descent at iteration  674\n",
      "Learned weights =  [-7.54799639e-02  2.63024334e+02]\n",
      "Done with gradient descent at iteration  675\n",
      "Learned weights =  [-7.57487787e-02  2.63024334e+02]\n",
      "Done with gradient descent at iteration  675\n",
      "Learned weights =  [-7.57487787e-02  2.63024334e+02]\n",
      "Done with gradient descent at iteration  676\n",
      "Learned weights =  [-7.60175935e-02  2.63024334e+02]\n",
      "Done with gradient descent at iteration  676\n",
      "Learned weights =  [-7.60175935e-02  2.63024334e+02]\n",
      "Done with gradient descent at iteration  677\n",
      "Learned weights =  [-7.62864083e-02  2.63024334e+02]\n",
      "Done with gradient descent at iteration  677\n",
      "Learned weights =  [-7.62864083e-02  2.63024334e+02]\n",
      "Done with gradient descent at iteration  678\n",
      "Learned weights =  [-7.65552230e-02  2.63024334e+02]\n",
      "Done with gradient descent at iteration  678\n",
      "Learned weights =  [-7.65552230e-02  2.63024334e+02]\n",
      "Done with gradient descent at iteration  679\n",
      "Learned weights =  [-7.68240378e-02  2.63024334e+02]\n",
      "Done with gradient descent at iteration  679\n",
      "Learned weights =  [-7.68240378e-02  2.63024334e+02]\n",
      "Done with gradient descent at iteration  680\n",
      "Learned weights =  [-7.70928526e-02  2.63024334e+02]\n",
      "Done with gradient descent at iteration  680\n",
      "Learned weights =  [-7.70928526e-02  2.63024334e+02]\n",
      "Done with gradient descent at iteration  681\n",
      "Learned weights =  [-7.73616673e-02  2.63024334e+02]\n",
      "Done with gradient descent at iteration  681\n",
      "Learned weights =  [-7.73616673e-02  2.63024334e+02]\n",
      "Done with gradient descent at iteration  682\n",
      "Learned weights =  [-7.76304821e-02  2.63024334e+02]\n",
      "Done with gradient descent at iteration  682\n",
      "Learned weights =  [-7.76304821e-02  2.63024335e+02]\n",
      "Done with gradient descent at iteration  683\n",
      "Learned weights =  [-7.78992969e-02  2.63024335e+02]\n",
      "Done with gradient descent at iteration  683\n",
      "Learned weights =  [-7.78992969e-02  2.63024335e+02]\n",
      "Done with gradient descent at iteration  684\n",
      "Learned weights =  [-7.81681116e-02  2.63024335e+02]\n",
      "Done with gradient descent at iteration  684\n",
      "Learned weights =  [-7.81681116e-02  2.63024335e+02]\n",
      "Done with gradient descent at iteration  685\n",
      "Learned weights =  [-7.84369264e-02  2.63024335e+02]\n",
      "Done with gradient descent at iteration  685\n",
      "Learned weights =  [-7.84369264e-02  2.63024335e+02]\n",
      "Done with gradient descent at iteration  686\n",
      "Learned weights =  [-7.87057412e-02  2.63024335e+02]\n",
      "Done with gradient descent at iteration  686\n",
      "Learned weights =  [-7.87057412e-02  2.63024335e+02]\n",
      "Done with gradient descent at iteration  687\n",
      "Learned weights =  [-7.89745559e-02  2.63024335e+02]\n",
      "Done with gradient descent at iteration  687\n",
      "Learned weights =  [-7.89745559e-02  2.63024335e+02]\n",
      "Done with gradient descent at iteration  688\n",
      "Learned weights =  [-7.92433707e-02  2.63024335e+02]\n",
      "Done with gradient descent at iteration  688\n",
      "Learned weights =  [-7.92433707e-02  2.63024335e+02]\n",
      "Done with gradient descent at iteration  689\n",
      "Learned weights =  [-7.95121854e-02  2.63024335e+02]\n",
      "Done with gradient descent at iteration  689\n",
      "Learned weights =  [-7.95121854e-02  2.63024335e+02]\n",
      "Done with gradient descent at iteration  690\n",
      "Learned weights =  [-7.97810002e-02  2.63024335e+02]\n",
      "Done with gradient descent at iteration  690\n",
      "Learned weights =  [-7.97810002e-02  2.63024335e+02]\n",
      "Done with gradient descent at iteration  691\n",
      "Learned weights =  [-8.00498150e-02  2.63024335e+02]\n",
      "Done with gradient descent at iteration  691\n",
      "Learned weights =  [-8.00498150e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  692\n",
      "Learned weights =  [-8.03186297e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  692\n",
      "Learned weights =  [-8.03186297e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  693\n",
      "Learned weights =  [-8.05874445e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  693\n",
      "Learned weights =  [-8.05874445e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  694\n",
      "Learned weights =  [-8.08562592e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  694\n",
      "Learned weights =  [-8.08562592e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  695\n",
      "Learned weights =  [-8.11250740e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  695\n",
      "Learned weights =  [-8.11250740e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  696\n",
      "Learned weights =  [-8.13938887e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  696\n",
      "Learned weights =  [-8.13938887e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  697\n",
      "Learned weights =  [-8.16627034e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  697\n",
      "Learned weights =  [-8.16627034e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  698\n",
      "Learned weights =  [-8.19315182e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  698\n",
      "Learned weights =  [-8.19315182e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  699\n",
      "Learned weights =  [-8.22003329e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  699\n",
      "Learned weights =  [-8.22003329e-02  2.63024336e+02]\n",
      "Iteration = 700\n",
      "Cost function =  1208251094244193.5\n",
      "Done with gradient descent at iteration  700\n",
      "Learned weights =  [-8.24691477e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  700\n",
      "Learned weights =  [-8.24691477e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  701\n",
      "Learned weights =  [-8.27379624e-02  2.63024336e+02]\n",
      "Done with gradient descent at iteration  701\n",
      "Learned weights =  [-8.27379624e-02  2.63024337e+02]\n",
      "Done with gradient descent at iteration  702\n",
      "Learned weights =  [-8.30067771e-02  2.63024337e+02]\n",
      "Done with gradient descent at iteration  702\n",
      "Learned weights =  [-8.30067771e-02  2.63024337e+02]\n",
      "Done with gradient descent at iteration  703\n",
      "Learned weights =  [-8.32755919e-02  2.63024337e+02]\n",
      "Done with gradient descent at iteration  703\n",
      "Learned weights =  [-8.32755919e-02  2.63024337e+02]\n",
      "Done with gradient descent at iteration  704\n",
      "Learned weights =  [-8.35444066e-02  2.63024337e+02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  704\n",
      "Learned weights =  [-8.35444066e-02  2.63024337e+02]\n",
      "Done with gradient descent at iteration  705\n",
      "Learned weights =  [-8.38132213e-02  2.63024337e+02]\n",
      "Done with gradient descent at iteration  705\n",
      "Learned weights =  [-8.38132213e-02  2.63024337e+02]\n",
      "Done with gradient descent at iteration  706\n",
      "Learned weights =  [-8.40820361e-02  2.63024337e+02]\n",
      "Done with gradient descent at iteration  706\n",
      "Learned weights =  [-8.40820361e-02  2.63024337e+02]\n",
      "Done with gradient descent at iteration  707\n",
      "Learned weights =  [-8.43508508e-02  2.63024337e+02]\n",
      "Done with gradient descent at iteration  707\n",
      "Learned weights =  [-8.43508508e-02  2.63024337e+02]\n",
      "Done with gradient descent at iteration  708\n",
      "Learned weights =  [-8.46196655e-02  2.63024337e+02]\n",
      "Done with gradient descent at iteration  708\n",
      "Learned weights =  [-8.46196655e-02  2.63024337e+02]\n",
      "Done with gradient descent at iteration  709\n",
      "Learned weights =  [-8.48884803e-02  2.63024337e+02]\n",
      "Done with gradient descent at iteration  709\n",
      "Learned weights =  [-8.48884803e-02  2.63024337e+02]\n",
      "Done with gradient descent at iteration  710\n",
      "Learned weights =  [-8.51572950e-02  2.63024337e+02]\n",
      "Done with gradient descent at iteration  710\n",
      "Learned weights =  [-8.51572950e-02  2.63024338e+02]\n",
      "Done with gradient descent at iteration  711\n",
      "Learned weights =  [-8.54261097e-02  2.63024338e+02]\n",
      "Done with gradient descent at iteration  711\n",
      "Learned weights =  [-8.54261097e-02  2.63024338e+02]\n",
      "Done with gradient descent at iteration  712\n",
      "Learned weights =  [-8.56949244e-02  2.63024338e+02]\n",
      "Done with gradient descent at iteration  712\n",
      "Learned weights =  [-8.56949244e-02  2.63024338e+02]\n",
      "Done with gradient descent at iteration  713\n",
      "Learned weights =  [-8.59637392e-02  2.63024338e+02]\n",
      "Done with gradient descent at iteration  713\n",
      "Learned weights =  [-8.59637392e-02  2.63024338e+02]\n",
      "Done with gradient descent at iteration  714\n",
      "Learned weights =  [-8.62325539e-02  2.63024338e+02]\n",
      "Done with gradient descent at iteration  714\n",
      "Learned weights =  [-8.62325539e-02  2.63024338e+02]\n",
      "Done with gradient descent at iteration  715\n",
      "Learned weights =  [-8.65013686e-02  2.63024338e+02]\n",
      "Done with gradient descent at iteration  715\n",
      "Learned weights =  [-8.65013686e-02  2.63024338e+02]\n",
      "Done with gradient descent at iteration  716\n",
      "Learned weights =  [-8.67701833e-02  2.63024338e+02]\n",
      "Done with gradient descent at iteration  716\n",
      "Learned weights =  [-8.67701833e-02  2.63024338e+02]\n",
      "Done with gradient descent at iteration  717\n",
      "Learned weights =  [-8.70389980e-02  2.63024338e+02]\n",
      "Done with gradient descent at iteration  717\n",
      "Learned weights =  [-8.70389980e-02  2.63024338e+02]\n",
      "Done with gradient descent at iteration  718\n",
      "Learned weights =  [-8.73078127e-02  2.63024338e+02]\n",
      "Done with gradient descent at iteration  718\n",
      "Learned weights =  [-8.73078127e-02  2.63024338e+02]\n",
      "Done with gradient descent at iteration  719\n",
      "Learned weights =  [-8.75766274e-02  2.63024338e+02]\n",
      "Done with gradient descent at iteration  719\n",
      "Learned weights =  [-8.75766274e-02  2.63024339e+02]\n",
      "Done with gradient descent at iteration  720\n",
      "Learned weights =  [-8.78454422e-02  2.63024339e+02]\n",
      "Done with gradient descent at iteration  720\n",
      "Learned weights =  [-8.78454422e-02  2.63024339e+02]\n",
      "Done with gradient descent at iteration  721\n",
      "Learned weights =  [-8.81142569e-02  2.63024339e+02]\n",
      "Done with gradient descent at iteration  721\n",
      "Learned weights =  [-8.81142569e-02  2.63024339e+02]\n",
      "Done with gradient descent at iteration  722\n",
      "Learned weights =  [-8.83830716e-02  2.63024339e+02]\n",
      "Done with gradient descent at iteration  722\n",
      "Learned weights =  [-8.83830716e-02  2.63024339e+02]\n",
      "Done with gradient descent at iteration  723\n",
      "Learned weights =  [-8.86518863e-02  2.63024339e+02]\n",
      "Done with gradient descent at iteration  723\n",
      "Learned weights =  [-8.86518863e-02  2.63024339e+02]\n",
      "Done with gradient descent at iteration  724\n",
      "Learned weights =  [-8.89207010e-02  2.63024339e+02]\n",
      "Done with gradient descent at iteration  724\n",
      "Learned weights =  [-8.89207010e-02  2.63024339e+02]\n",
      "Done with gradient descent at iteration  725\n",
      "Learned weights =  [-8.91895157e-02  2.63024339e+02]\n",
      "Done with gradient descent at iteration  725\n",
      "Learned weights =  [-8.91895157e-02  2.63024339e+02]\n",
      "Done with gradient descent at iteration  726\n",
      "Learned weights =  [-8.94583304e-02  2.63024339e+02]\n",
      "Done with gradient descent at iteration  726\n",
      "Learned weights =  [-8.94583304e-02  2.63024339e+02]\n",
      "Done with gradient descent at iteration  727\n",
      "Learned weights =  [-8.97271451e-02  2.63024339e+02]\n",
      "Done with gradient descent at iteration  727\n",
      "Learned weights =  [-8.97271451e-02  2.63024339e+02]\n",
      "Done with gradient descent at iteration  728\n",
      "Learned weights =  [-8.99959598e-02  2.63024339e+02]\n",
      "Done with gradient descent at iteration  728\n",
      "Learned weights =  [-8.99959598e-02  2.63024340e+02]\n",
      "Done with gradient descent at iteration  729\n",
      "Learned weights =  [-9.02647745e-02  2.63024340e+02]\n",
      "Done with gradient descent at iteration  729\n",
      "Learned weights =  [-9.02647745e-02  2.63024340e+02]\n",
      "Done with gradient descent at iteration  730\n",
      "Learned weights =  [-9.05335892e-02  2.63024340e+02]\n",
      "Done with gradient descent at iteration  730\n",
      "Learned weights =  [-9.05335892e-02  2.63024340e+02]\n",
      "Done with gradient descent at iteration  731\n",
      "Learned weights =  [-9.08024039e-02  2.63024340e+02]\n",
      "Done with gradient descent at iteration  731\n",
      "Learned weights =  [-9.08024039e-02  2.63024340e+02]\n",
      "Done with gradient descent at iteration  732\n",
      "Learned weights =  [-9.10712186e-02  2.63024340e+02]\n",
      "Done with gradient descent at iteration  732\n",
      "Learned weights =  [-9.10712186e-02  2.63024340e+02]\n",
      "Done with gradient descent at iteration  733\n",
      "Learned weights =  [-9.13400332e-02  2.63024340e+02]\n",
      "Done with gradient descent at iteration  733\n",
      "Learned weights =  [-9.13400332e-02  2.63024340e+02]\n",
      "Done with gradient descent at iteration  734\n",
      "Learned weights =  [-9.16088479e-02  2.63024340e+02]\n",
      "Done with gradient descent at iteration  734\n",
      "Learned weights =  [-9.16088479e-02  2.63024340e+02]\n",
      "Done with gradient descent at iteration  735\n",
      "Learned weights =  [-9.18776626e-02  2.63024340e+02]\n",
      "Done with gradient descent at iteration  735\n",
      "Learned weights =  [-9.18776626e-02  2.63024340e+02]\n",
      "Done with gradient descent at iteration  736\n",
      "Learned weights =  [-9.21464773e-02  2.63024340e+02]\n",
      "Done with gradient descent at iteration  736\n",
      "Learned weights =  [-9.21464773e-02  2.63024340e+02]\n",
      "Done with gradient descent at iteration  737\n",
      "Learned weights =  [-9.2415292e-02  2.6302434e+02]\n",
      "Done with gradient descent at iteration  737\n",
      "Learned weights =  [-9.2415292e-02  2.6302434e+02]\n",
      "Done with gradient descent at iteration  738\n",
      "Learned weights =  [-9.26841067e-02  2.63024340e+02]\n",
      "Done with gradient descent at iteration  738\n",
      "Learned weights =  [-9.26841067e-02  2.63024341e+02]\n",
      "Done with gradient descent at iteration  739\n",
      "Learned weights =  [-9.29529214e-02  2.63024341e+02]\n",
      "Done with gradient descent at iteration  739\n",
      "Learned weights =  [-9.29529214e-02  2.63024341e+02]\n",
      "Done with gradient descent at iteration  740\n",
      "Learned weights =  [-9.32217360e-02  2.63024341e+02]\n",
      "Done with gradient descent at iteration  740\n",
      "Learned weights =  [-9.32217360e-02  2.63024341e+02]\n",
      "Done with gradient descent at iteration  741\n",
      "Learned weights =  [-9.34905507e-02  2.63024341e+02]\n",
      "Done with gradient descent at iteration  741\n",
      "Learned weights =  [-9.34905507e-02  2.63024341e+02]\n",
      "Done with gradient descent at iteration  742\n",
      "Learned weights =  [-9.37593654e-02  2.63024341e+02]\n",
      "Done with gradient descent at iteration  742\n",
      "Learned weights =  [-9.37593654e-02  2.63024341e+02]\n",
      "Done with gradient descent at iteration  743\n",
      "Learned weights =  [-9.40281801e-02  2.63024341e+02]\n",
      "Done with gradient descent at iteration  743\n",
      "Learned weights =  [-9.40281801e-02  2.63024341e+02]\n",
      "Done with gradient descent at iteration  744\n",
      "Learned weights =  [-9.42969947e-02  2.63024341e+02]\n",
      "Done with gradient descent at iteration  744\n",
      "Learned weights =  [-9.42969947e-02  2.63024341e+02]\n",
      "Done with gradient descent at iteration  745\n",
      "Learned weights =  [-9.45658094e-02  2.63024341e+02]\n",
      "Done with gradient descent at iteration  745\n",
      "Learned weights =  [-9.45658094e-02  2.63024341e+02]\n",
      "Done with gradient descent at iteration  746\n",
      "Learned weights =  [-9.48346241e-02  2.63024341e+02]\n",
      "Done with gradient descent at iteration  746\n",
      "Learned weights =  [-9.48346241e-02  2.63024341e+02]\n",
      "Done with gradient descent at iteration  747\n",
      "Learned weights =  [-9.51034387e-02  2.63024341e+02]\n",
      "Done with gradient descent at iteration  747\n",
      "Learned weights =  [-9.51034387e-02  2.63024342e+02]\n",
      "Done with gradient descent at iteration  748\n",
      "Learned weights =  [-9.53722534e-02  2.63024342e+02]\n",
      "Done with gradient descent at iteration  748\n",
      "Learned weights =  [-9.53722534e-02  2.63024342e+02]\n",
      "Done with gradient descent at iteration  749\n",
      "Learned weights =  [-9.56410681e-02  2.63024342e+02]\n",
      "Done with gradient descent at iteration  749\n",
      "Learned weights =  [-9.56410681e-02  2.63024342e+02]\n",
      "Done with gradient descent at iteration  750\n",
      "Learned weights =  [-9.59098827e-02  2.63024342e+02]\n",
      "Done with gradient descent at iteration  750\n",
      "Learned weights =  [-9.59098827e-02  2.63024342e+02]\n",
      "Done with gradient descent at iteration  751\n",
      "Learned weights =  [-9.61786974e-02  2.63024342e+02]\n",
      "Done with gradient descent at iteration  751\n",
      "Learned weights =  [-9.61786974e-02  2.63024342e+02]\n",
      "Done with gradient descent at iteration  752\n",
      "Learned weights =  [-9.64475121e-02  2.63024342e+02]\n",
      "Done with gradient descent at iteration  752\n",
      "Learned weights =  [-9.64475121e-02  2.63024342e+02]\n",
      "Done with gradient descent at iteration  753\n",
      "Learned weights =  [-9.67163267e-02  2.63024342e+02]\n",
      "Done with gradient descent at iteration  753\n",
      "Learned weights =  [-9.67163267e-02  2.63024342e+02]\n",
      "Done with gradient descent at iteration  754\n",
      "Learned weights =  [-9.69851414e-02  2.63024342e+02]\n",
      "Done with gradient descent at iteration  754\n",
      "Learned weights =  [-9.69851414e-02  2.63024342e+02]\n",
      "Done with gradient descent at iteration  755\n",
      "Learned weights =  [-9.72539560e-02  2.63024342e+02]\n",
      "Done with gradient descent at iteration  755\n",
      "Learned weights =  [-9.72539560e-02  2.63024342e+02]\n",
      "Done with gradient descent at iteration  756\n",
      "Learned weights =  [-9.75227707e-02  2.63024342e+02]\n",
      "Done with gradient descent at iteration  756\n",
      "Learned weights =  [-9.75227707e-02  2.63024343e+02]\n",
      "Done with gradient descent at iteration  757\n",
      "Learned weights =  [-9.77915853e-02  2.63024343e+02]\n",
      "Done with gradient descent at iteration  757\n",
      "Learned weights =  [-9.77915853e-02  2.63024343e+02]\n",
      "Done with gradient descent at iteration  758\n",
      "Learned weights =  [-9.80604000e-02  2.63024343e+02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  758\n",
      "Learned weights =  [-9.80604000e-02  2.63024343e+02]\n",
      "Done with gradient descent at iteration  759\n",
      "Learned weights =  [-9.83292146e-02  2.63024343e+02]\n",
      "Done with gradient descent at iteration  759\n",
      "Learned weights =  [-9.83292146e-02  2.63024343e+02]\n",
      "Done with gradient descent at iteration  760\n",
      "Learned weights =  [-9.85980293e-02  2.63024343e+02]\n",
      "Done with gradient descent at iteration  760\n",
      "Learned weights =  [-9.85980293e-02  2.63024343e+02]\n",
      "Done with gradient descent at iteration  761\n",
      "Learned weights =  [-9.88668439e-02  2.63024343e+02]\n",
      "Done with gradient descent at iteration  761\n",
      "Learned weights =  [-9.88668439e-02  2.63024343e+02]\n",
      "Done with gradient descent at iteration  762\n",
      "Learned weights =  [-9.91356586e-02  2.63024343e+02]\n",
      "Done with gradient descent at iteration  762\n",
      "Learned weights =  [-9.91356586e-02  2.63024343e+02]\n",
      "Done with gradient descent at iteration  763\n",
      "Learned weights =  [-9.94044732e-02  2.63024343e+02]\n",
      "Done with gradient descent at iteration  763\n",
      "Learned weights =  [-9.94044732e-02  2.63024343e+02]\n",
      "Done with gradient descent at iteration  764\n",
      "Learned weights =  [-9.96732879e-02  2.63024343e+02]\n",
      "Done with gradient descent at iteration  764\n",
      "Learned weights =  [-9.96732879e-02  2.63024343e+02]\n",
      "Done with gradient descent at iteration  765\n",
      "Learned weights =  [-9.99421025e-02  2.63024343e+02]\n",
      "Done with gradient descent at iteration  765\n",
      "Learned weights =  [-9.99421025e-02  2.63024344e+02]\n",
      "Done with gradient descent at iteration  766\n",
      "Learned weights =  [-1.00210917e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  766\n",
      "Learned weights =  [-1.00210917e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  767\n",
      "Learned weights =  [-1.00479732e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  767\n",
      "Learned weights =  [-1.00479732e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  768\n",
      "Learned weights =  [-1.00748546e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  768\n",
      "Learned weights =  [-1.00748546e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  769\n",
      "Learned weights =  [-1.01017361e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  769\n",
      "Learned weights =  [-1.01017361e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  770\n",
      "Learned weights =  [-1.01286176e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  770\n",
      "Learned weights =  [-1.01286176e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  771\n",
      "Learned weights =  [-1.01554990e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  771\n",
      "Learned weights =  [-1.01554990e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  772\n",
      "Learned weights =  [-1.01823805e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  772\n",
      "Learned weights =  [-1.01823805e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  773\n",
      "Learned weights =  [-1.02092620e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  773\n",
      "Learned weights =  [-1.02092620e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  774\n",
      "Learned weights =  [-1.02361434e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  774\n",
      "Learned weights =  [-1.02361434e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  775\n",
      "Learned weights =  [-1.02630249e-01  2.63024344e+02]\n",
      "Done with gradient descent at iteration  775\n",
      "Learned weights =  [-1.02630249e-01  2.63024345e+02]\n",
      "Done with gradient descent at iteration  776\n",
      "Learned weights =  [-1.02899063e-01  2.63024345e+02]\n",
      "Done with gradient descent at iteration  776\n",
      "Learned weights =  [-1.02899063e-01  2.63024345e+02]\n",
      "Done with gradient descent at iteration  777\n",
      "Learned weights =  [-1.03167878e-01  2.63024345e+02]\n",
      "Done with gradient descent at iteration  777\n",
      "Learned weights =  [-1.03167878e-01  2.63024345e+02]\n",
      "Done with gradient descent at iteration  778\n",
      "Learned weights =  [-1.03436693e-01  2.63024345e+02]\n",
      "Done with gradient descent at iteration  778\n",
      "Learned weights =  [-1.03436693e-01  2.63024345e+02]\n",
      "Done with gradient descent at iteration  779\n",
      "Learned weights =  [-1.03705507e-01  2.63024345e+02]\n",
      "Done with gradient descent at iteration  779\n",
      "Learned weights =  [-1.03705507e-01  2.63024345e+02]\n",
      "Done with gradient descent at iteration  780\n",
      "Learned weights =  [-1.03974322e-01  2.63024345e+02]\n",
      "Done with gradient descent at iteration  780\n",
      "Learned weights =  [-1.03974322e-01  2.63024345e+02]\n",
      "Done with gradient descent at iteration  781\n",
      "Learned weights =  [-1.04243137e-01  2.63024345e+02]\n",
      "Done with gradient descent at iteration  781\n",
      "Learned weights =  [-1.04243137e-01  2.63024345e+02]\n",
      "Done with gradient descent at iteration  782\n",
      "Learned weights =  [-1.04511951e-01  2.63024345e+02]\n",
      "Done with gradient descent at iteration  782\n",
      "Learned weights =  [-1.04511951e-01  2.63024345e+02]\n",
      "Done with gradient descent at iteration  783\n",
      "Learned weights =  [-1.04780766e-01  2.63024345e+02]\n",
      "Done with gradient descent at iteration  783\n",
      "Learned weights =  [-1.04780766e-01  2.63024345e+02]\n",
      "Done with gradient descent at iteration  784\n",
      "Learned weights =  [-1.05049580e-01  2.63024345e+02]\n",
      "Done with gradient descent at iteration  784\n",
      "Learned weights =  [-1.05049580e-01  2.63024346e+02]\n",
      "Done with gradient descent at iteration  785\n",
      "Learned weights =  [-1.05318395e-01  2.63024346e+02]\n",
      "Done with gradient descent at iteration  785\n",
      "Learned weights =  [-1.05318395e-01  2.63024346e+02]\n",
      "Done with gradient descent at iteration  786\n",
      "Learned weights =  [-1.05587210e-01  2.63024346e+02]\n",
      "Done with gradient descent at iteration  786\n",
      "Learned weights =  [-1.05587210e-01  2.63024346e+02]\n",
      "Done with gradient descent at iteration  787\n",
      "Learned weights =  [-1.05856024e-01  2.63024346e+02]\n",
      "Done with gradient descent at iteration  787\n",
      "Learned weights =  [-1.05856024e-01  2.63024346e+02]\n",
      "Done with gradient descent at iteration  788\n",
      "Learned weights =  [-1.06124839e-01  2.63024346e+02]\n",
      "Done with gradient descent at iteration  788\n",
      "Learned weights =  [-1.06124839e-01  2.63024346e+02]\n",
      "Done with gradient descent at iteration  789\n",
      "Learned weights =  [-1.06393653e-01  2.63024346e+02]\n",
      "Done with gradient descent at iteration  789\n",
      "Learned weights =  [-1.06393653e-01  2.63024346e+02]\n",
      "Done with gradient descent at iteration  790\n",
      "Learned weights =  [-1.06662468e-01  2.63024346e+02]\n",
      "Done with gradient descent at iteration  790\n",
      "Learned weights =  [-1.06662468e-01  2.63024346e+02]\n",
      "Done with gradient descent at iteration  791\n",
      "Learned weights =  [-1.06931283e-01  2.63024346e+02]\n",
      "Done with gradient descent at iteration  791\n",
      "Learned weights =  [-1.06931283e-01  2.63024346e+02]\n",
      "Done with gradient descent at iteration  792\n",
      "Learned weights =  [-1.07200097e-01  2.63024346e+02]\n",
      "Done with gradient descent at iteration  792\n",
      "Learned weights =  [-1.07200097e-01  2.63024346e+02]\n",
      "Done with gradient descent at iteration  793\n",
      "Learned weights =  [-1.07468912e-01  2.63024346e+02]\n",
      "Done with gradient descent at iteration  793\n",
      "Learned weights =  [-1.07468912e-01  2.63024347e+02]\n",
      "Done with gradient descent at iteration  794\n",
      "Learned weights =  [-1.07737726e-01  2.63024347e+02]\n",
      "Done with gradient descent at iteration  794\n",
      "Learned weights =  [-1.07737726e-01  2.63024347e+02]\n",
      "Done with gradient descent at iteration  795\n",
      "Learned weights =  [-1.08006541e-01  2.63024347e+02]\n",
      "Done with gradient descent at iteration  795\n",
      "Learned weights =  [-1.08006541e-01  2.63024347e+02]\n",
      "Done with gradient descent at iteration  796\n",
      "Learned weights =  [-1.08275356e-01  2.63024347e+02]\n",
      "Done with gradient descent at iteration  796\n",
      "Learned weights =  [-1.08275356e-01  2.63024347e+02]\n",
      "Done with gradient descent at iteration  797\n",
      "Learned weights =  [-1.08544170e-01  2.63024347e+02]\n",
      "Done with gradient descent at iteration  797\n",
      "Learned weights =  [-1.08544170e-01  2.63024347e+02]\n",
      "Done with gradient descent at iteration  798\n",
      "Learned weights =  [-1.08812985e-01  2.63024347e+02]\n",
      "Done with gradient descent at iteration  798\n",
      "Learned weights =  [-1.08812985e-01  2.63024347e+02]\n",
      "Done with gradient descent at iteration  799\n",
      "Learned weights =  [-1.09081799e-01  2.63024347e+02]\n",
      "Done with gradient descent at iteration  799\n",
      "Learned weights =  [-1.09081799e-01  2.63024347e+02]\n",
      "Iteration = 800\n",
      "Cost function =  1208251087018059.5\n",
      "Done with gradient descent at iteration  800\n",
      "Learned weights =  [-1.09350614e-01  2.63024347e+02]\n",
      "Done with gradient descent at iteration  800\n",
      "Learned weights =  [-1.09350614e-01  2.63024347e+02]\n",
      "Done with gradient descent at iteration  801\n",
      "Learned weights =  [-1.09619429e-01  2.63024347e+02]\n",
      "Done with gradient descent at iteration  801\n",
      "Learned weights =  [-1.09619429e-01  2.63024347e+02]\n",
      "Done with gradient descent at iteration  802\n",
      "Learned weights =  [-1.09888243e-01  2.63024347e+02]\n",
      "Done with gradient descent at iteration  802\n",
      "Learned weights =  [-1.09888243e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  803\n",
      "Learned weights =  [-1.10157058e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  803\n",
      "Learned weights =  [-1.10157058e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  804\n",
      "Learned weights =  [-1.10425872e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  804\n",
      "Learned weights =  [-1.10425872e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  805\n",
      "Learned weights =  [-1.10694687e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  805\n",
      "Learned weights =  [-1.10694687e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  806\n",
      "Learned weights =  [-1.10963501e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  806\n",
      "Learned weights =  [-1.10963501e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  807\n",
      "Learned weights =  [-1.11232316e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  807\n",
      "Learned weights =  [-1.11232316e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  808\n",
      "Learned weights =  [-1.11501131e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  808\n",
      "Learned weights =  [-1.11501131e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  809\n",
      "Learned weights =  [-1.11769945e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  809\n",
      "Learned weights =  [-1.11769945e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  810\n",
      "Learned weights =  [-1.12038760e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  810\n",
      "Learned weights =  [-1.12038760e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  811\n",
      "Learned weights =  [-1.12307574e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  811\n",
      "Learned weights =  [-1.12307574e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  812\n",
      "Learned weights =  [-1.12576389e-01  2.63024348e+02]\n",
      "Done with gradient descent at iteration  812\n",
      "Learned weights =  [-1.12576389e-01  2.63024349e+02]\n",
      "Done with gradient descent at iteration  813\n",
      "Learned weights =  [-1.12845203e-01  2.63024349e+02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  813\n",
      "Learned weights =  [-1.12845203e-01  2.63024349e+02]\n",
      "Done with gradient descent at iteration  814\n",
      "Learned weights =  [-1.13114018e-01  2.63024349e+02]\n",
      "Done with gradient descent at iteration  814\n",
      "Learned weights =  [-1.13114018e-01  2.63024349e+02]\n",
      "Done with gradient descent at iteration  815\n",
      "Learned weights =  [-1.13382833e-01  2.63024349e+02]\n",
      "Done with gradient descent at iteration  815\n",
      "Learned weights =  [-1.13382833e-01  2.63024349e+02]\n",
      "Done with gradient descent at iteration  816\n",
      "Learned weights =  [-1.13651647e-01  2.63024349e+02]\n",
      "Done with gradient descent at iteration  816\n",
      "Learned weights =  [-1.13651647e-01  2.63024349e+02]\n",
      "Done with gradient descent at iteration  817\n",
      "Learned weights =  [-1.13920462e-01  2.63024349e+02]\n",
      "Done with gradient descent at iteration  817\n",
      "Learned weights =  [-1.13920462e-01  2.63024349e+02]\n",
      "Done with gradient descent at iteration  818\n",
      "Learned weights =  [-1.14189276e-01  2.63024349e+02]\n",
      "Done with gradient descent at iteration  818\n",
      "Learned weights =  [-1.14189276e-01  2.63024349e+02]\n",
      "Done with gradient descent at iteration  819\n",
      "Learned weights =  [-1.14458091e-01  2.63024349e+02]\n",
      "Done with gradient descent at iteration  819\n",
      "Learned weights =  [-1.14458091e-01  2.63024349e+02]\n",
      "Done with gradient descent at iteration  820\n",
      "Learned weights =  [-1.14726905e-01  2.63024349e+02]\n",
      "Done with gradient descent at iteration  820\n",
      "Learned weights =  [-1.14726905e-01  2.63024349e+02]\n",
      "Done with gradient descent at iteration  821\n",
      "Learned weights =  [-1.14995720e-01  2.63024349e+02]\n",
      "Done with gradient descent at iteration  821\n",
      "Learned weights =  [-1.1499572e-01  2.6302435e+02]\n",
      "Done with gradient descent at iteration  822\n",
      "Learned weights =  [-1.15264534e-01  2.63024350e+02]\n",
      "Done with gradient descent at iteration  822\n",
      "Learned weights =  [-1.15264534e-01  2.63024350e+02]\n",
      "Done with gradient descent at iteration  823\n",
      "Learned weights =  [-1.15533349e-01  2.63024350e+02]\n",
      "Done with gradient descent at iteration  823\n",
      "Learned weights =  [-1.15533349e-01  2.63024350e+02]\n",
      "Done with gradient descent at iteration  824\n",
      "Learned weights =  [-1.15802164e-01  2.63024350e+02]\n",
      "Done with gradient descent at iteration  824\n",
      "Learned weights =  [-1.15802164e-01  2.63024350e+02]\n",
      "Done with gradient descent at iteration  825\n",
      "Learned weights =  [-1.16070978e-01  2.63024350e+02]\n",
      "Done with gradient descent at iteration  825\n",
      "Learned weights =  [-1.16070978e-01  2.63024350e+02]\n",
      "Done with gradient descent at iteration  826\n",
      "Learned weights =  [-1.16339793e-01  2.63024350e+02]\n",
      "Done with gradient descent at iteration  826\n",
      "Learned weights =  [-1.16339793e-01  2.63024350e+02]\n",
      "Done with gradient descent at iteration  827\n",
      "Learned weights =  [-1.16608607e-01  2.63024350e+02]\n",
      "Done with gradient descent at iteration  827\n",
      "Learned weights =  [-1.16608607e-01  2.63024350e+02]\n",
      "Done with gradient descent at iteration  828\n",
      "Learned weights =  [-1.16877422e-01  2.63024350e+02]\n",
      "Done with gradient descent at iteration  828\n",
      "Learned weights =  [-1.16877422e-01  2.63024350e+02]\n",
      "Done with gradient descent at iteration  829\n",
      "Learned weights =  [-1.17146236e-01  2.63024350e+02]\n",
      "Done with gradient descent at iteration  829\n",
      "Learned weights =  [-1.17146236e-01  2.63024350e+02]\n",
      "Done with gradient descent at iteration  830\n",
      "Learned weights =  [-1.17415051e-01  2.63024350e+02]\n",
      "Done with gradient descent at iteration  830\n",
      "Learned weights =  [-1.17415051e-01  2.63024351e+02]\n",
      "Done with gradient descent at iteration  831\n",
      "Learned weights =  [-1.17683865e-01  2.63024351e+02]\n",
      "Done with gradient descent at iteration  831\n",
      "Learned weights =  [-1.17683865e-01  2.63024351e+02]\n",
      "Done with gradient descent at iteration  832\n",
      "Learned weights =  [-1.17952680e-01  2.63024351e+02]\n",
      "Done with gradient descent at iteration  832\n",
      "Learned weights =  [-1.17952680e-01  2.63024351e+02]\n",
      "Done with gradient descent at iteration  833\n",
      "Learned weights =  [-1.18221494e-01  2.63024351e+02]\n",
      "Done with gradient descent at iteration  833\n",
      "Learned weights =  [-1.18221494e-01  2.63024351e+02]\n",
      "Done with gradient descent at iteration  834\n",
      "Learned weights =  [-1.18490309e-01  2.63024351e+02]\n",
      "Done with gradient descent at iteration  834\n",
      "Learned weights =  [-1.18490309e-01  2.63024351e+02]\n",
      "Done with gradient descent at iteration  835\n",
      "Learned weights =  [-1.18759124e-01  2.63024351e+02]\n",
      "Done with gradient descent at iteration  835\n",
      "Learned weights =  [-1.18759124e-01  2.63024351e+02]\n",
      "Done with gradient descent at iteration  836\n",
      "Learned weights =  [-1.19027938e-01  2.63024351e+02]\n",
      "Done with gradient descent at iteration  836\n",
      "Learned weights =  [-1.19027938e-01  2.63024351e+02]\n",
      "Done with gradient descent at iteration  837\n",
      "Learned weights =  [-1.19296753e-01  2.63024351e+02]\n",
      "Done with gradient descent at iteration  837\n",
      "Learned weights =  [-1.19296753e-01  2.63024351e+02]\n",
      "Done with gradient descent at iteration  838\n",
      "Learned weights =  [-1.19565567e-01  2.63024351e+02]\n",
      "Done with gradient descent at iteration  838\n",
      "Learned weights =  [-1.19565567e-01  2.63024351e+02]\n",
      "Done with gradient descent at iteration  839\n",
      "Learned weights =  [-1.19834382e-01  2.63024351e+02]\n",
      "Done with gradient descent at iteration  839\n",
      "Learned weights =  [-1.19834382e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  840\n",
      "Learned weights =  [-1.20103196e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  840\n",
      "Learned weights =  [-1.20103196e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  841\n",
      "Learned weights =  [-1.20372011e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  841\n",
      "Learned weights =  [-1.20372011e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  842\n",
      "Learned weights =  [-1.20640825e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  842\n",
      "Learned weights =  [-1.20640825e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  843\n",
      "Learned weights =  [-1.20909640e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  843\n",
      "Learned weights =  [-1.20909640e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  844\n",
      "Learned weights =  [-1.21178454e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  844\n",
      "Learned weights =  [-1.21178454e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  845\n",
      "Learned weights =  [-1.21447269e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  845\n",
      "Learned weights =  [-1.21447269e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  846\n",
      "Learned weights =  [-1.21716083e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  846\n",
      "Learned weights =  [-1.21716083e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  847\n",
      "Learned weights =  [-1.21984898e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  847\n",
      "Learned weights =  [-1.21984898e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  848\n",
      "Learned weights =  [-1.22253712e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  848\n",
      "Learned weights =  [-1.22253712e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  849\n",
      "Learned weights =  [-1.22522527e-01  2.63024352e+02]\n",
      "Done with gradient descent at iteration  849\n",
      "Learned weights =  [-1.22522527e-01  2.63024353e+02]\n",
      "Done with gradient descent at iteration  850\n",
      "Learned weights =  [-1.22791341e-01  2.63024353e+02]\n",
      "Done with gradient descent at iteration  850\n",
      "Learned weights =  [-1.22791341e-01  2.63024353e+02]\n",
      "Done with gradient descent at iteration  851\n",
      "Learned weights =  [-1.23060156e-01  2.63024353e+02]\n",
      "Done with gradient descent at iteration  851\n",
      "Learned weights =  [-1.23060156e-01  2.63024353e+02]\n",
      "Done with gradient descent at iteration  852\n",
      "Learned weights =  [-1.23328970e-01  2.63024353e+02]\n",
      "Done with gradient descent at iteration  852\n",
      "Learned weights =  [-1.23328970e-01  2.63024353e+02]\n",
      "Done with gradient descent at iteration  853\n",
      "Learned weights =  [-1.23597785e-01  2.63024353e+02]\n",
      "Done with gradient descent at iteration  853\n",
      "Learned weights =  [-1.23597785e-01  2.63024353e+02]\n",
      "Done with gradient descent at iteration  854\n",
      "Learned weights =  [-1.23866599e-01  2.63024353e+02]\n",
      "Done with gradient descent at iteration  854\n",
      "Learned weights =  [-1.23866599e-01  2.63024353e+02]\n",
      "Done with gradient descent at iteration  855\n",
      "Learned weights =  [-1.24135414e-01  2.63024353e+02]\n",
      "Done with gradient descent at iteration  855\n",
      "Learned weights =  [-1.24135414e-01  2.63024353e+02]\n",
      "Done with gradient descent at iteration  856\n",
      "Learned weights =  [-1.24404228e-01  2.63024353e+02]\n",
      "Done with gradient descent at iteration  856\n",
      "Learned weights =  [-1.24404228e-01  2.63024353e+02]\n",
      "Done with gradient descent at iteration  857\n",
      "Learned weights =  [-1.24673043e-01  2.63024353e+02]\n",
      "Done with gradient descent at iteration  857\n",
      "Learned weights =  [-1.24673043e-01  2.63024353e+02]\n",
      "Done with gradient descent at iteration  858\n",
      "Learned weights =  [-1.24941857e-01  2.63024353e+02]\n",
      "Done with gradient descent at iteration  858\n",
      "Learned weights =  [-1.24941857e-01  2.63024354e+02]\n",
      "Done with gradient descent at iteration  859\n",
      "Learned weights =  [-1.25210672e-01  2.63024354e+02]\n",
      "Done with gradient descent at iteration  859\n",
      "Learned weights =  [-1.25210672e-01  2.63024354e+02]\n",
      "Done with gradient descent at iteration  860\n",
      "Learned weights =  [-1.25479486e-01  2.63024354e+02]\n",
      "Done with gradient descent at iteration  860\n",
      "Learned weights =  [-1.25479486e-01  2.63024354e+02]\n",
      "Done with gradient descent at iteration  861\n",
      "Learned weights =  [-1.25748301e-01  2.63024354e+02]\n",
      "Done with gradient descent at iteration  861\n",
      "Learned weights =  [-1.25748301e-01  2.63024354e+02]\n",
      "Done with gradient descent at iteration  862\n",
      "Learned weights =  [-1.26017115e-01  2.63024354e+02]\n",
      "Done with gradient descent at iteration  862\n",
      "Learned weights =  [-1.26017115e-01  2.63024354e+02]\n",
      "Done with gradient descent at iteration  863\n",
      "Learned weights =  [-1.26285930e-01  2.63024354e+02]\n",
      "Done with gradient descent at iteration  863\n",
      "Learned weights =  [-1.26285930e-01  2.63024354e+02]\n",
      "Done with gradient descent at iteration  864\n",
      "Learned weights =  [-1.26554744e-01  2.63024354e+02]\n",
      "Done with gradient descent at iteration  864\n",
      "Learned weights =  [-1.26554744e-01  2.63024354e+02]\n",
      "Done with gradient descent at iteration  865\n",
      "Learned weights =  [-1.26823559e-01  2.63024354e+02]\n",
      "Done with gradient descent at iteration  865\n",
      "Learned weights =  [-1.26823559e-01  2.63024354e+02]\n",
      "Done with gradient descent at iteration  866\n",
      "Learned weights =  [-1.27092373e-01  2.63024354e+02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  866\n",
      "Learned weights =  [-1.27092373e-01  2.63024354e+02]\n",
      "Done with gradient descent at iteration  867\n",
      "Learned weights =  [-1.27361188e-01  2.63024354e+02]\n",
      "Done with gradient descent at iteration  867\n",
      "Learned weights =  [-1.27361188e-01  2.63024355e+02]\n",
      "Done with gradient descent at iteration  868\n",
      "Learned weights =  [-1.27630002e-01  2.63024355e+02]\n",
      "Done with gradient descent at iteration  868\n",
      "Learned weights =  [-1.27630002e-01  2.63024355e+02]\n",
      "Done with gradient descent at iteration  869\n",
      "Learned weights =  [-1.27898817e-01  2.63024355e+02]\n",
      "Done with gradient descent at iteration  869\n",
      "Learned weights =  [-1.27898817e-01  2.63024355e+02]\n",
      "Done with gradient descent at iteration  870\n",
      "Learned weights =  [-1.28167631e-01  2.63024355e+02]\n",
      "Done with gradient descent at iteration  870\n",
      "Learned weights =  [-1.28167631e-01  2.63024355e+02]\n",
      "Done with gradient descent at iteration  871\n",
      "Learned weights =  [-1.28436446e-01  2.63024355e+02]\n",
      "Done with gradient descent at iteration  871\n",
      "Learned weights =  [-1.28436446e-01  2.63024355e+02]\n",
      "Done with gradient descent at iteration  872\n",
      "Learned weights =  [-1.28705260e-01  2.63024355e+02]\n",
      "Done with gradient descent at iteration  872\n",
      "Learned weights =  [-1.28705260e-01  2.63024355e+02]\n",
      "Done with gradient descent at iteration  873\n",
      "Learned weights =  [-1.28974075e-01  2.63024355e+02]\n",
      "Done with gradient descent at iteration  873\n",
      "Learned weights =  [-1.28974075e-01  2.63024355e+02]\n",
      "Done with gradient descent at iteration  874\n",
      "Learned weights =  [-1.29242889e-01  2.63024355e+02]\n",
      "Done with gradient descent at iteration  874\n",
      "Learned weights =  [-1.29242889e-01  2.63024355e+02]\n",
      "Done with gradient descent at iteration  875\n",
      "Learned weights =  [-1.29511704e-01  2.63024355e+02]\n",
      "Done with gradient descent at iteration  875\n",
      "Learned weights =  [-1.29511704e-01  2.63024355e+02]\n",
      "Done with gradient descent at iteration  876\n",
      "Learned weights =  [-1.29780518e-01  2.63024355e+02]\n",
      "Done with gradient descent at iteration  876\n",
      "Learned weights =  [-1.29780518e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  877\n",
      "Learned weights =  [-1.30049333e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  877\n",
      "Learned weights =  [-1.30049333e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  878\n",
      "Learned weights =  [-1.30318147e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  878\n",
      "Learned weights =  [-1.30318147e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  879\n",
      "Learned weights =  [-1.30586962e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  879\n",
      "Learned weights =  [-1.30586962e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  880\n",
      "Learned weights =  [-1.30855776e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  880\n",
      "Learned weights =  [-1.30855776e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  881\n",
      "Learned weights =  [-1.31124590e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  881\n",
      "Learned weights =  [-1.31124590e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  882\n",
      "Learned weights =  [-1.31393405e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  882\n",
      "Learned weights =  [-1.31393405e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  883\n",
      "Learned weights =  [-1.31662219e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  883\n",
      "Learned weights =  [-1.31662219e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  884\n",
      "Learned weights =  [-1.31931034e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  884\n",
      "Learned weights =  [-1.31931034e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  885\n",
      "Learned weights =  [-1.32199848e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  885\n",
      "Learned weights =  [-1.32199848e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  886\n",
      "Learned weights =  [-1.32468663e-01  2.63024356e+02]\n",
      "Done with gradient descent at iteration  886\n",
      "Learned weights =  [-1.32468663e-01  2.63024357e+02]\n",
      "Done with gradient descent at iteration  887\n",
      "Learned weights =  [-1.32737477e-01  2.63024357e+02]\n",
      "Done with gradient descent at iteration  887\n",
      "Learned weights =  [-1.32737477e-01  2.63024357e+02]\n",
      "Done with gradient descent at iteration  888\n",
      "Learned weights =  [-1.33006292e-01  2.63024357e+02]\n",
      "Done with gradient descent at iteration  888\n",
      "Learned weights =  [-1.33006292e-01  2.63024357e+02]\n",
      "Done with gradient descent at iteration  889\n",
      "Learned weights =  [-1.33275106e-01  2.63024357e+02]\n",
      "Done with gradient descent at iteration  889\n",
      "Learned weights =  [-1.33275106e-01  2.63024357e+02]\n",
      "Done with gradient descent at iteration  890\n",
      "Learned weights =  [-1.33543921e-01  2.63024357e+02]\n",
      "Done with gradient descent at iteration  890\n",
      "Learned weights =  [-1.33543921e-01  2.63024357e+02]\n",
      "Done with gradient descent at iteration  891\n",
      "Learned weights =  [-1.33812735e-01  2.63024357e+02]\n",
      "Done with gradient descent at iteration  891\n",
      "Learned weights =  [-1.33812735e-01  2.63024357e+02]\n",
      "Done with gradient descent at iteration  892\n",
      "Learned weights =  [-1.34081549e-01  2.63024357e+02]\n",
      "Done with gradient descent at iteration  892\n",
      "Learned weights =  [-1.34081549e-01  2.63024357e+02]\n",
      "Done with gradient descent at iteration  893\n",
      "Learned weights =  [-1.34350364e-01  2.63024357e+02]\n",
      "Done with gradient descent at iteration  893\n",
      "Learned weights =  [-1.34350364e-01  2.63024357e+02]\n",
      "Done with gradient descent at iteration  894\n",
      "Learned weights =  [-1.34619178e-01  2.63024357e+02]\n",
      "Done with gradient descent at iteration  894\n",
      "Learned weights =  [-1.34619178e-01  2.63024357e+02]\n",
      "Done with gradient descent at iteration  895\n",
      "Learned weights =  [-1.34887993e-01  2.63024357e+02]\n",
      "Done with gradient descent at iteration  895\n",
      "Learned weights =  [-1.34887993e-01  2.63024358e+02]\n",
      "Done with gradient descent at iteration  896\n",
      "Learned weights =  [-1.35156807e-01  2.63024358e+02]\n",
      "Done with gradient descent at iteration  896\n",
      "Learned weights =  [-1.35156807e-01  2.63024358e+02]\n",
      "Done with gradient descent at iteration  897\n",
      "Learned weights =  [-1.35425622e-01  2.63024358e+02]\n",
      "Done with gradient descent at iteration  897\n",
      "Learned weights =  [-1.35425622e-01  2.63024358e+02]\n",
      "Done with gradient descent at iteration  898\n",
      "Learned weights =  [-1.35694436e-01  2.63024358e+02]\n",
      "Done with gradient descent at iteration  898\n",
      "Learned weights =  [-1.35694436e-01  2.63024358e+02]\n",
      "Done with gradient descent at iteration  899\n",
      "Learned weights =  [-1.35963250e-01  2.63024358e+02]\n",
      "Done with gradient descent at iteration  899\n",
      "Learned weights =  [-1.35963250e-01  2.63024358e+02]\n",
      "Iteration = 900\n",
      "Cost function =  1208251079791934.2\n",
      "Done with gradient descent at iteration  900\n",
      "Learned weights =  [-1.36232065e-01  2.63024358e+02]\n",
      "Done with gradient descent at iteration  900\n",
      "Learned weights =  [-1.36232065e-01  2.63024358e+02]\n",
      "Done with gradient descent at iteration  901\n",
      "Learned weights =  [-1.36500879e-01  2.63024358e+02]\n",
      "Done with gradient descent at iteration  901\n",
      "Learned weights =  [-1.36500879e-01  2.63024358e+02]\n",
      "Done with gradient descent at iteration  902\n",
      "Learned weights =  [-1.36769694e-01  2.63024358e+02]\n",
      "Done with gradient descent at iteration  902\n",
      "Learned weights =  [-1.36769694e-01  2.63024358e+02]\n",
      "Done with gradient descent at iteration  903\n",
      "Learned weights =  [-1.37038508e-01  2.63024358e+02]\n",
      "Done with gradient descent at iteration  903\n",
      "Learned weights =  [-1.37038508e-01  2.63024358e+02]\n",
      "Done with gradient descent at iteration  904\n",
      "Learned weights =  [-1.37307323e-01  2.63024358e+02]\n",
      "Done with gradient descent at iteration  904\n",
      "Learned weights =  [-1.37307323e-01  2.63024359e+02]\n",
      "Done with gradient descent at iteration  905\n",
      "Learned weights =  [-1.37576137e-01  2.63024359e+02]\n",
      "Done with gradient descent at iteration  905\n",
      "Learned weights =  [-1.37576137e-01  2.63024359e+02]\n",
      "Done with gradient descent at iteration  906\n",
      "Learned weights =  [-1.37844952e-01  2.63024359e+02]\n",
      "Done with gradient descent at iteration  906\n",
      "Learned weights =  [-1.37844952e-01  2.63024359e+02]\n",
      "Done with gradient descent at iteration  907\n",
      "Learned weights =  [-1.38113766e-01  2.63024359e+02]\n",
      "Done with gradient descent at iteration  907\n",
      "Learned weights =  [-1.38113766e-01  2.63024359e+02]\n",
      "Done with gradient descent at iteration  908\n",
      "Learned weights =  [-1.38382580e-01  2.63024359e+02]\n",
      "Done with gradient descent at iteration  908\n",
      "Learned weights =  [-1.38382580e-01  2.63024359e+02]\n",
      "Done with gradient descent at iteration  909\n",
      "Learned weights =  [-1.38651395e-01  2.63024359e+02]\n",
      "Done with gradient descent at iteration  909\n",
      "Learned weights =  [-1.38651395e-01  2.63024359e+02]\n",
      "Done with gradient descent at iteration  910\n",
      "Learned weights =  [-1.38920209e-01  2.63024359e+02]\n",
      "Done with gradient descent at iteration  910\n",
      "Learned weights =  [-1.38920209e-01  2.63024359e+02]\n",
      "Done with gradient descent at iteration  911\n",
      "Learned weights =  [-1.39189024e-01  2.63024359e+02]\n",
      "Done with gradient descent at iteration  911\n",
      "Learned weights =  [-1.39189024e-01  2.63024359e+02]\n",
      "Done with gradient descent at iteration  912\n",
      "Learned weights =  [-1.39457838e-01  2.63024359e+02]\n",
      "Done with gradient descent at iteration  912\n",
      "Learned weights =  [-1.39457838e-01  2.63024359e+02]\n",
      "Done with gradient descent at iteration  913\n",
      "Learned weights =  [-1.39726652e-01  2.63024359e+02]\n",
      "Done with gradient descent at iteration  913\n",
      "Learned weights =  [-1.39726652e-01  2.63024360e+02]\n",
      "Done with gradient descent at iteration  914\n",
      "Learned weights =  [-1.39995467e-01  2.63024360e+02]\n",
      "Done with gradient descent at iteration  914\n",
      "Learned weights =  [-1.39995467e-01  2.63024360e+02]\n",
      "Done with gradient descent at iteration  915\n",
      "Learned weights =  [-1.40264281e-01  2.63024360e+02]\n",
      "Done with gradient descent at iteration  915\n",
      "Learned weights =  [-1.40264281e-01  2.63024360e+02]\n",
      "Done with gradient descent at iteration  916\n",
      "Learned weights =  [-1.40533096e-01  2.63024360e+02]\n",
      "Done with gradient descent at iteration  916\n",
      "Learned weights =  [-1.40533096e-01  2.63024360e+02]\n",
      "Done with gradient descent at iteration  917\n",
      "Learned weights =  [-1.4080191e-01  2.6302436e+02]\n",
      "Done with gradient descent at iteration  917\n",
      "Learned weights =  [-1.4080191e-01  2.6302436e+02]\n",
      "Done with gradient descent at iteration  918\n",
      "Learned weights =  [-1.41070724e-01  2.63024360e+02]\n",
      "Done with gradient descent at iteration  918\n",
      "Learned weights =  [-1.41070724e-01  2.63024360e+02]\n",
      "Done with gradient descent at iteration  919\n",
      "Learned weights =  [-1.41339539e-01  2.63024360e+02]\n",
      "Done with gradient descent at iteration  919\n",
      "Learned weights =  [-1.41339539e-01  2.63024360e+02]\n",
      "Done with gradient descent at iteration  920\n",
      "Learned weights =  [-1.41608353e-01  2.63024360e+02]\n",
      "Done with gradient descent at iteration  920\n",
      "Learned weights =  [-1.41608353e-01  2.63024360e+02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  921\n",
      "Learned weights =  [-1.41877168e-01  2.63024360e+02]\n",
      "Done with gradient descent at iteration  921\n",
      "Learned weights =  [-1.41877168e-01  2.63024360e+02]\n",
      "Done with gradient descent at iteration  922\n",
      "Learned weights =  [-1.42145982e-01  2.63024360e+02]\n",
      "Done with gradient descent at iteration  922\n",
      "Learned weights =  [-1.42145982e-01  2.63024360e+02]\n",
      "Done with gradient descent at iteration  923\n",
      "Learned weights =  [-1.42414796e-01  2.63024360e+02]\n",
      "Done with gradient descent at iteration  923\n",
      "Learned weights =  [-1.42414796e-01  2.63024361e+02]\n",
      "Done with gradient descent at iteration  924\n",
      "Learned weights =  [-1.42683611e-01  2.63024361e+02]\n",
      "Done with gradient descent at iteration  924\n",
      "Learned weights =  [-1.42683611e-01  2.63024361e+02]\n",
      "Done with gradient descent at iteration  925\n",
      "Learned weights =  [-1.42952425e-01  2.63024361e+02]\n",
      "Done with gradient descent at iteration  925\n",
      "Learned weights =  [-1.42952425e-01  2.63024361e+02]\n",
      "Done with gradient descent at iteration  926\n",
      "Learned weights =  [-1.43221240e-01  2.63024361e+02]\n",
      "Done with gradient descent at iteration  926\n",
      "Learned weights =  [-1.43221240e-01  2.63024361e+02]\n",
      "Done with gradient descent at iteration  927\n",
      "Learned weights =  [-1.43490054e-01  2.63024361e+02]\n",
      "Done with gradient descent at iteration  927\n",
      "Learned weights =  [-1.43490054e-01  2.63024361e+02]\n",
      "Done with gradient descent at iteration  928\n",
      "Learned weights =  [-1.43758868e-01  2.63024361e+02]\n",
      "Done with gradient descent at iteration  928\n",
      "Learned weights =  [-1.43758868e-01  2.63024361e+02]\n",
      "Done with gradient descent at iteration  929\n",
      "Learned weights =  [-1.44027683e-01  2.63024361e+02]\n",
      "Done with gradient descent at iteration  929\n",
      "Learned weights =  [-1.44027683e-01  2.63024361e+02]\n",
      "Done with gradient descent at iteration  930\n",
      "Learned weights =  [-1.44296497e-01  2.63024361e+02]\n",
      "Done with gradient descent at iteration  930\n",
      "Learned weights =  [-1.44296497e-01  2.63024361e+02]\n",
      "Done with gradient descent at iteration  931\n",
      "Learned weights =  [-1.44565312e-01  2.63024361e+02]\n",
      "Done with gradient descent at iteration  931\n",
      "Learned weights =  [-1.44565312e-01  2.63024361e+02]\n",
      "Done with gradient descent at iteration  932\n",
      "Learned weights =  [-1.44834126e-01  2.63024361e+02]\n",
      "Done with gradient descent at iteration  932\n",
      "Learned weights =  [-1.44834126e-01  2.63024362e+02]\n",
      "Done with gradient descent at iteration  933\n",
      "Learned weights =  [-1.45102940e-01  2.63024362e+02]\n",
      "Done with gradient descent at iteration  933\n",
      "Learned weights =  [-1.45102940e-01  2.63024362e+02]\n",
      "Done with gradient descent at iteration  934\n",
      "Learned weights =  [-1.45371755e-01  2.63024362e+02]\n",
      "Done with gradient descent at iteration  934\n",
      "Learned weights =  [-1.45371755e-01  2.63024362e+02]\n",
      "Done with gradient descent at iteration  935\n",
      "Learned weights =  [-1.45640569e-01  2.63024362e+02]\n",
      "Done with gradient descent at iteration  935\n",
      "Learned weights =  [-1.45640569e-01  2.63024362e+02]\n",
      "Done with gradient descent at iteration  936\n",
      "Learned weights =  [-1.45909384e-01  2.63024362e+02]\n",
      "Done with gradient descent at iteration  936\n",
      "Learned weights =  [-1.45909384e-01  2.63024362e+02]\n",
      "Done with gradient descent at iteration  937\n",
      "Learned weights =  [-1.46178198e-01  2.63024362e+02]\n",
      "Done with gradient descent at iteration  937\n",
      "Learned weights =  [-1.46178198e-01  2.63024362e+02]\n",
      "Done with gradient descent at iteration  938\n",
      "Learned weights =  [-1.46447012e-01  2.63024362e+02]\n",
      "Done with gradient descent at iteration  938\n",
      "Learned weights =  [-1.46447012e-01  2.63024362e+02]\n",
      "Done with gradient descent at iteration  939\n",
      "Learned weights =  [-1.46715827e-01  2.63024362e+02]\n",
      "Done with gradient descent at iteration  939\n",
      "Learned weights =  [-1.46715827e-01  2.63024362e+02]\n",
      "Done with gradient descent at iteration  940\n",
      "Learned weights =  [-1.46984641e-01  2.63024362e+02]\n",
      "Done with gradient descent at iteration  940\n",
      "Learned weights =  [-1.46984641e-01  2.63024362e+02]\n",
      "Done with gradient descent at iteration  941\n",
      "Learned weights =  [-1.47253455e-01  2.63024362e+02]\n",
      "Done with gradient descent at iteration  941\n",
      "Learned weights =  [-1.47253455e-01  2.63024363e+02]\n",
      "Done with gradient descent at iteration  942\n",
      "Learned weights =  [-1.47522270e-01  2.63024363e+02]\n",
      "Done with gradient descent at iteration  942\n",
      "Learned weights =  [-1.47522270e-01  2.63024363e+02]\n",
      "Done with gradient descent at iteration  943\n",
      "Learned weights =  [-1.47791084e-01  2.63024363e+02]\n",
      "Done with gradient descent at iteration  943\n",
      "Learned weights =  [-1.47791084e-01  2.63024363e+02]\n",
      "Done with gradient descent at iteration  944\n",
      "Learned weights =  [-1.48059898e-01  2.63024363e+02]\n",
      "Done with gradient descent at iteration  944\n",
      "Learned weights =  [-1.48059898e-01  2.63024363e+02]\n",
      "Done with gradient descent at iteration  945\n",
      "Learned weights =  [-1.48328713e-01  2.63024363e+02]\n",
      "Done with gradient descent at iteration  945\n",
      "Learned weights =  [-1.48328713e-01  2.63024363e+02]\n",
      "Done with gradient descent at iteration  946\n",
      "Learned weights =  [-1.48597527e-01  2.63024363e+02]\n",
      "Done with gradient descent at iteration  946\n",
      "Learned weights =  [-1.48597527e-01  2.63024363e+02]\n",
      "Done with gradient descent at iteration  947\n",
      "Learned weights =  [-1.48866342e-01  2.63024363e+02]\n",
      "Done with gradient descent at iteration  947\n",
      "Learned weights =  [-1.48866342e-01  2.63024363e+02]\n",
      "Done with gradient descent at iteration  948\n",
      "Learned weights =  [-1.49135156e-01  2.63024363e+02]\n",
      "Done with gradient descent at iteration  948\n",
      "Learned weights =  [-1.49135156e-01  2.63024363e+02]\n",
      "Done with gradient descent at iteration  949\n",
      "Learned weights =  [-1.49403970e-01  2.63024363e+02]\n",
      "Done with gradient descent at iteration  949\n",
      "Learned weights =  [-1.49403970e-01  2.63024363e+02]\n",
      "Done with gradient descent at iteration  950\n",
      "Learned weights =  [-1.49672785e-01  2.63024363e+02]\n",
      "Done with gradient descent at iteration  950\n",
      "Learned weights =  [-1.49672785e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  951\n",
      "Learned weights =  [-1.49941599e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  951\n",
      "Learned weights =  [-1.49941599e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  952\n",
      "Learned weights =  [-1.50210413e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  952\n",
      "Learned weights =  [-1.50210413e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  953\n",
      "Learned weights =  [-1.50479228e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  953\n",
      "Learned weights =  [-1.50479228e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  954\n",
      "Learned weights =  [-1.50748042e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  954\n",
      "Learned weights =  [-1.50748042e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  955\n",
      "Learned weights =  [-1.51016856e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  955\n",
      "Learned weights =  [-1.51016856e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  956\n",
      "Learned weights =  [-1.51285671e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  956\n",
      "Learned weights =  [-1.51285671e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  957\n",
      "Learned weights =  [-1.51554485e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  957\n",
      "Learned weights =  [-1.51554485e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  958\n",
      "Learned weights =  [-1.51823299e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  958\n",
      "Learned weights =  [-1.51823299e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  959\n",
      "Learned weights =  [-1.52092114e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  959\n",
      "Learned weights =  [-1.52092114e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  960\n",
      "Learned weights =  [-1.52360928e-01  2.63024364e+02]\n",
      "Done with gradient descent at iteration  960\n",
      "Learned weights =  [-1.52360928e-01  2.63024365e+02]\n",
      "Done with gradient descent at iteration  961\n",
      "Learned weights =  [-1.52629742e-01  2.63024365e+02]\n",
      "Done with gradient descent at iteration  961\n",
      "Learned weights =  [-1.52629742e-01  2.63024365e+02]\n",
      "Done with gradient descent at iteration  962\n",
      "Learned weights =  [-1.52898557e-01  2.63024365e+02]\n",
      "Done with gradient descent at iteration  962\n",
      "Learned weights =  [-1.52898557e-01  2.63024365e+02]\n",
      "Done with gradient descent at iteration  963\n",
      "Learned weights =  [-1.53167371e-01  2.63024365e+02]\n",
      "Done with gradient descent at iteration  963\n",
      "Learned weights =  [-1.53167371e-01  2.63024365e+02]\n",
      "Done with gradient descent at iteration  964\n",
      "Learned weights =  [-1.53436186e-01  2.63024365e+02]\n",
      "Done with gradient descent at iteration  964\n",
      "Learned weights =  [-1.53436186e-01  2.63024365e+02]\n",
      "Done with gradient descent at iteration  965\n",
      "Learned weights =  [-1.53705000e-01  2.63024365e+02]\n",
      "Done with gradient descent at iteration  965\n",
      "Learned weights =  [-1.53705000e-01  2.63024365e+02]\n",
      "Done with gradient descent at iteration  966\n",
      "Learned weights =  [-1.53973814e-01  2.63024365e+02]\n",
      "Done with gradient descent at iteration  966\n",
      "Learned weights =  [-1.53973814e-01  2.63024365e+02]\n",
      "Done with gradient descent at iteration  967\n",
      "Learned weights =  [-1.54242628e-01  2.63024365e+02]\n",
      "Done with gradient descent at iteration  967\n",
      "Learned weights =  [-1.54242628e-01  2.63024365e+02]\n",
      "Done with gradient descent at iteration  968\n",
      "Learned weights =  [-1.54511443e-01  2.63024365e+02]\n",
      "Done with gradient descent at iteration  968\n",
      "Learned weights =  [-1.54511443e-01  2.63024365e+02]\n",
      "Done with gradient descent at iteration  969\n",
      "Learned weights =  [-1.54780257e-01  2.63024365e+02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  969\n",
      "Learned weights =  [-1.54780257e-01  2.63024366e+02]\n",
      "Done with gradient descent at iteration  970\n",
      "Learned weights =  [-1.55049071e-01  2.63024366e+02]\n",
      "Done with gradient descent at iteration  970\n",
      "Learned weights =  [-1.55049071e-01  2.63024366e+02]\n",
      "Done with gradient descent at iteration  971\n",
      "Learned weights =  [-1.55317886e-01  2.63024366e+02]\n",
      "Done with gradient descent at iteration  971\n",
      "Learned weights =  [-1.55317886e-01  2.63024366e+02]\n",
      "Done with gradient descent at iteration  972\n",
      "Learned weights =  [-1.55586700e-01  2.63024366e+02]\n",
      "Done with gradient descent at iteration  972\n",
      "Learned weights =  [-1.55586700e-01  2.63024366e+02]\n",
      "Done with gradient descent at iteration  973\n",
      "Learned weights =  [-1.55855514e-01  2.63024366e+02]\n",
      "Done with gradient descent at iteration  973\n",
      "Learned weights =  [-1.55855514e-01  2.63024366e+02]\n",
      "Done with gradient descent at iteration  974\n",
      "Learned weights =  [-1.56124329e-01  2.63024366e+02]\n",
      "Done with gradient descent at iteration  974\n",
      "Learned weights =  [-1.56124329e-01  2.63024366e+02]\n",
      "Done with gradient descent at iteration  975\n",
      "Learned weights =  [-1.56393143e-01  2.63024366e+02]\n",
      "Done with gradient descent at iteration  975\n",
      "Learned weights =  [-1.56393143e-01  2.63024366e+02]\n",
      "Done with gradient descent at iteration  976\n",
      "Learned weights =  [-1.56661957e-01  2.63024366e+02]\n",
      "Done with gradient descent at iteration  976\n",
      "Learned weights =  [-1.56661957e-01  2.63024366e+02]\n",
      "Done with gradient descent at iteration  977\n",
      "Learned weights =  [-1.56930772e-01  2.63024366e+02]\n",
      "Done with gradient descent at iteration  977\n",
      "Learned weights =  [-1.56930772e-01  2.63024366e+02]\n",
      "Done with gradient descent at iteration  978\n",
      "Learned weights =  [-1.57199586e-01  2.63024366e+02]\n",
      "Done with gradient descent at iteration  978\n",
      "Learned weights =  [-1.57199586e-01  2.63024367e+02]\n",
      "Done with gradient descent at iteration  979\n",
      "Learned weights =  [-1.57468400e-01  2.63024367e+02]\n",
      "Done with gradient descent at iteration  979\n",
      "Learned weights =  [-1.57468400e-01  2.63024367e+02]\n",
      "Done with gradient descent at iteration  980\n",
      "Learned weights =  [-1.57737215e-01  2.63024367e+02]\n",
      "Done with gradient descent at iteration  980\n",
      "Learned weights =  [-1.57737215e-01  2.63024367e+02]\n",
      "Done with gradient descent at iteration  981\n",
      "Learned weights =  [-1.58006029e-01  2.63024367e+02]\n",
      "Done with gradient descent at iteration  981\n",
      "Learned weights =  [-1.58006029e-01  2.63024367e+02]\n",
      "Done with gradient descent at iteration  982\n",
      "Learned weights =  [-1.58274843e-01  2.63024367e+02]\n",
      "Done with gradient descent at iteration  982\n",
      "Learned weights =  [-1.58274843e-01  2.63024367e+02]\n",
      "Done with gradient descent at iteration  983\n",
      "Learned weights =  [-1.58543658e-01  2.63024367e+02]\n",
      "Done with gradient descent at iteration  983\n",
      "Learned weights =  [-1.58543658e-01  2.63024367e+02]\n",
      "Done with gradient descent at iteration  984\n",
      "Learned weights =  [-1.58812472e-01  2.63024367e+02]\n",
      "Done with gradient descent at iteration  984\n",
      "Learned weights =  [-1.58812472e-01  2.63024367e+02]\n",
      "Done with gradient descent at iteration  985\n",
      "Learned weights =  [-1.59081286e-01  2.63024367e+02]\n",
      "Done with gradient descent at iteration  985\n",
      "Learned weights =  [-1.59081286e-01  2.63024367e+02]\n",
      "Done with gradient descent at iteration  986\n",
      "Learned weights =  [-1.59350100e-01  2.63024367e+02]\n",
      "Done with gradient descent at iteration  986\n",
      "Learned weights =  [-1.59350100e-01  2.63024367e+02]\n",
      "Done with gradient descent at iteration  987\n",
      "Learned weights =  [-1.59618915e-01  2.63024367e+02]\n",
      "Done with gradient descent at iteration  987\n",
      "Learned weights =  [-1.59618915e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  988\n",
      "Learned weights =  [-1.59887729e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  988\n",
      "Learned weights =  [-1.59887729e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  989\n",
      "Learned weights =  [-1.60156543e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  989\n",
      "Learned weights =  [-1.60156543e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  990\n",
      "Learned weights =  [-1.60425358e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  990\n",
      "Learned weights =  [-1.60425358e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  991\n",
      "Learned weights =  [-1.60694172e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  991\n",
      "Learned weights =  [-1.60694172e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  992\n",
      "Learned weights =  [-1.60962986e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  992\n",
      "Learned weights =  [-1.60962986e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  993\n",
      "Learned weights =  [-1.61231801e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  993\n",
      "Learned weights =  [-1.61231801e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  994\n",
      "Learned weights =  [-1.61500615e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  994\n",
      "Learned weights =  [-1.61500615e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  995\n",
      "Learned weights =  [-1.61769429e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  995\n",
      "Learned weights =  [-1.61769429e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  996\n",
      "Learned weights =  [-1.62038243e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  996\n",
      "Learned weights =  [-1.62038243e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  997\n",
      "Learned weights =  [-1.62307058e-01  2.63024368e+02]\n",
      "Done with gradient descent at iteration  997\n",
      "Learned weights =  [-1.62307058e-01  2.63024369e+02]\n",
      "Done with gradient descent at iteration  998\n",
      "Learned weights =  [-1.62575872e-01  2.63024369e+02]\n",
      "Done with gradient descent at iteration  998\n",
      "Learned weights =  [-1.62575872e-01  2.63024369e+02]\n",
      "Done with gradient descent at iteration  999\n",
      "Learned weights =  [-1.62844686e-01  2.63024369e+02]\n",
      "Done with gradient descent at iteration  999\n",
      "Learned weights =  [-1.62844686e-01  2.63024369e+02]\n",
      "Iteration = 1000\n",
      "Cost function =  1208251072565817.5\n",
      "Done with gradient descent at iteration  1000\n",
      "Learned weights =  [-1.63113501e-01  2.63024369e+02]\n",
      "Done with gradient descent at iteration  1000\n",
      "Learned weights =  [-1.63113501e-01  2.63024369e+02]\n"
     ]
    }
   ],
   "source": [
    "simple_weights_0_penalty = ridge_regression_gradient_descent(simple_feature_matrix, output, initial_weights, step_size, 0.0, max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's consider high regularization.  Set the `l2_penalty` to `1e11` and run your ridge regression algorithm to learn the weights of your model.  Call your weights:\n",
    "\n",
    "`simple_weights_high_penalty`\n",
    "\n",
    "we'll use them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent with l2_penalty = 100000000000.0\n",
      "Iteration = 1\n",
      "Cost function =  7433051851026171.0\n",
      "Done with gradient descent at iteration  1\n",
      "Learned weights =  [0.0187527 0.       ]\n",
      "Done with gradient descent at iteration  1\n",
      "Learned weights =  [1.87526989e-02 4.73325137e+01]\n",
      "Iteration = 2\n",
      "Cost function =  5618303898412631.0\n",
      "Done with gradient descent at iteration  2\n",
      "Learned weights =  [3.40823824e-02 4.73325137e+01]\n",
      "Done with gradient descent at iteration  2\n",
      "Learned weights =  [3.40823824e-02 7.66808053e+01]\n",
      "Iteration = 3\n",
      "Cost function =  4920613278115386.0\n",
      "Done with gradient descent at iteration  3\n",
      "Learned weights =  [4.72896420e-02 7.66808053e+01]\n",
      "Done with gradient descent at iteration  3\n",
      "Learned weights =  [4.72896420e-02 9.48780684e+01]\n",
      "Iteration = 4\n",
      "Cost function =  4652381942612288.0\n",
      "Done with gradient descent at iteration  4\n",
      "Learned weights =  [5.91809029e-02 9.48780684e+01]\n",
      "Done with gradient descent at iteration  4\n",
      "Learned weights =  [5.91809029e-02 1.06161191e+02]\n",
      "Iteration = 5\n",
      "Cost function =  4549258764014155.0\n",
      "Done with gradient descent at iteration  5\n",
      "Learned weights =  [7.02561853e-02 1.06161191e+02]\n",
      "Done with gradient descent at iteration  5\n",
      "Learned weights =  [7.02561853e-02 1.13157235e+02]\n",
      "Iteration = 6\n",
      "Cost function =  4509612390882263.0\n",
      "Done with gradient descent at iteration  6\n",
      "Learned weights =  [8.08255241e-02 1.13157235e+02]\n",
      "Done with gradient descent at iteration  6\n",
      "Learned weights =  [8.08255241e-02 1.17495097e+02]\n",
      "Iteration = 7\n",
      "Cost function =  4494370050281117.0\n",
      "Done with gradient descent at iteration  7\n",
      "Learned weights =  [9.10811550e-02 1.17495097e+02]\n",
      "Done with gradient descent at iteration  7\n",
      "Learned weights =  [9.10811550e-02 1.20184767e+02]\n",
      "Iteration = 8\n",
      "Cost function =  4488509984030220.5\n",
      "Done with gradient descent at iteration  8\n",
      "Learned weights =  [1.01142273e-01 1.20184767e+02]\n",
      "Done with gradient descent at iteration  8\n",
      "Learned weights =  [1.01142273e-01 1.21852482e+02]\n",
      "Iteration = 9\n",
      "Cost function =  4486256988531770.0\n",
      "Done with gradient descent at iteration  9\n",
      "Learned weights =  [1.11082784e-01 1.21852482e+02]\n",
      "Done with gradient descent at iteration  9\n",
      "Learned weights =  [1.11082784e-01 1.22886540e+02]\n",
      "Iteration = 10\n",
      "Cost function =  4485390752674688.0\n",
      "Done with gradient descent at iteration  10\n",
      "Learned weights =  [1.20948513e-01 1.22886540e+02]\n",
      "Done with gradient descent at iteration  10\n",
      "Learned weights =  [1.20948513e-01 1.23527702e+02]\n",
      "Done with gradient descent at iteration  11\n",
      "Learned weights =  [  0.13076787 123.52770211]\n",
      "Done with gradient descent at iteration  11\n",
      "Learned weights =  [  0.13076787 123.92525071]\n",
      "Done with gradient descent at iteration  12\n",
      "Learned weights =  [  0.14055848 123.92525071]\n",
      "Done with gradient descent at iteration  12\n",
      "Learned weights =  [  0.14055848 124.17174806]\n",
      "Done with gradient descent at iteration  13\n",
      "Learned weights =  [  0.15033127 124.17174806]\n",
      "Done with gradient descent at iteration  13\n",
      "Learned weights =  [  0.15033127 124.32458682]\n",
      "Done with gradient descent at iteration  14\n",
      "Learned weights =  [  0.160093   124.32458682]\n",
      "Done with gradient descent at iteration  14\n",
      "Learned weights =  [  0.160093   124.41935304]\n",
      "Done with gradient descent at iteration  15\n",
      "Learned weights =  [  0.16984787 124.41935304]\n",
      "Done with gradient descent at iteration  15\n",
      "Learned weights =  [  0.16984787 124.47811166]\n",
      "Done with gradient descent at iteration  16\n",
      "Learned weights =  [  0.1795985  124.47811166]\n",
      "Done with gradient descent at iteration  16\n",
      "Learned weights =  [  0.1795985  124.51454395]\n",
      "Done with gradient descent at iteration  17\n",
      "Learned weights =  [  0.18934649 124.51454395]\n",
      "Done with gradient descent at iteration  17\n",
      "Learned weights =  [  0.18934649 124.53713291]\n",
      "Done with gradient descent at iteration  18\n",
      "Learned weights =  [  0.19909285 124.53713291]\n",
      "Done with gradient descent at iteration  18\n",
      "Learned weights =  [  0.19909285 124.55113838]\n",
      "Done with gradient descent at iteration  19\n",
      "Learned weights =  [  0.2088382  124.55113838]\n",
      "Done with gradient descent at iteration  19\n",
      "Learned weights =  [  0.2088382 124.5598217]\n",
      "Iteration = 20\n",
      "Cost function =  4484848868034299.0\n",
      "Done with gradient descent at iteration  20\n",
      "Learned weights =  [  0.21858291 124.5598217 ]\n",
      "Done with gradient descent at iteration  20\n",
      "Learned weights =  [  0.21858291 124.56520504]\n",
      "Done with gradient descent at iteration  21\n",
      "Learned weights =  [  0.22832724 124.56520504]\n",
      "Done with gradient descent at iteration  21\n",
      "Learned weights =  [  0.22832724 124.56854225]\n",
      "Done with gradient descent at iteration  22\n",
      "Learned weights =  [  0.23807132 124.56854225]\n",
      "Done with gradient descent at iteration  22\n",
      "Learned weights =  [  0.23807132 124.57061077]\n",
      "Done with gradient descent at iteration  23\n",
      "Learned weights =  [  0.24781526 124.57061077]\n",
      "Done with gradient descent at iteration  23\n",
      "Learned weights =  [  0.24781526 124.57189264]\n",
      "Done with gradient descent at iteration  24\n",
      "Learned weights =  [  0.2575591  124.57189264]\n",
      "Done with gradient descent at iteration  24\n",
      "Learned weights =  [  0.2575591  124.57268675]\n",
      "Done with gradient descent at iteration  25\n",
      "Learned weights =  [  0.26730289 124.57268675]\n",
      "Done with gradient descent at iteration  25\n",
      "Learned weights =  [  0.26730289 124.57317843]\n",
      "Done with gradient descent at iteration  26\n",
      "Learned weights =  [  0.27704663 124.57317843]\n",
      "Done with gradient descent at iteration  26\n",
      "Learned weights =  [  0.27704663 124.57348259]\n",
      "Done with gradient descent at iteration  27\n",
      "Learned weights =  [  0.28679036 124.57348259]\n",
      "Done with gradient descent at iteration  27\n",
      "Learned weights =  [  0.28679036 124.57367047]\n",
      "Done with gradient descent at iteration  28\n",
      "Learned weights =  [  0.29653407 124.57367047]\n",
      "Done with gradient descent at iteration  28\n",
      "Learned weights =  [  0.29653407 124.57378627]\n",
      "Done with gradient descent at iteration  29\n",
      "Learned weights =  [  0.30627778 124.57378627]\n",
      "Done with gradient descent at iteration  29\n",
      "Learned weights =  [  0.30627778 124.57385736]\n",
      "Iteration = 30\n",
      "Cost function =  4484847880479027.5\n",
      "Done with gradient descent at iteration  30\n",
      "Learned weights =  [  0.31602147 124.57385736]\n",
      "Done with gradient descent at iteration  30\n",
      "Learned weights =  [  0.31602147 124.57390074]\n",
      "Done with gradient descent at iteration  31\n",
      "Learned weights =  [  0.32576517 124.57390074]\n",
      "Done with gradient descent at iteration  31\n",
      "Learned weights =  [  0.32576517 124.57392693]\n",
      "Done with gradient descent at iteration  32\n",
      "Learned weights =  [  0.33550886 124.57392693]\n",
      "Done with gradient descent at iteration  32\n",
      "Learned weights =  [  0.33550886 124.57394246]\n",
      "Done with gradient descent at iteration  33\n",
      "Learned weights =  [  0.34525255 124.57394246]\n",
      "Done with gradient descent at iteration  33\n",
      "Learned weights =  [  0.34525255 124.57395139]\n",
      "Done with gradient descent at iteration  34\n",
      "Learned weights =  [  0.35499624 124.57395139]\n",
      "Done with gradient descent at iteration  34\n",
      "Learned weights =  [  0.35499624 124.57395622]\n",
      "Done with gradient descent at iteration  35\n",
      "Learned weights =  [  0.36473993 124.57395622]\n",
      "Done with gradient descent at iteration  35\n",
      "Learned weights =  [  0.36473993 124.57395851]\n",
      "Done with gradient descent at iteration  36\n",
      "Learned weights =  [  0.37448362 124.57395851]\n",
      "Done with gradient descent at iteration  36\n",
      "Learned weights =  [  0.37448362 124.57395923]\n",
      "Done with gradient descent at iteration  37\n",
      "Learned weights =  [  0.38422731 124.57395923]\n",
      "Done with gradient descent at iteration  37\n",
      "Learned weights =  [  0.38422731 124.57395897]\n",
      "Done with gradient descent at iteration  38\n",
      "Learned weights =  [  0.393971   124.57395897]\n",
      "Done with gradient descent at iteration  38\n",
      "Learned weights =  [  0.393971  124.5739581]\n",
      "Done with gradient descent at iteration  39\n",
      "Learned weights =  [  0.40371468 124.5739581 ]\n",
      "Done with gradient descent at iteration  39\n",
      "Learned weights =  [  0.40371468 124.57395686]\n",
      "Iteration = 40\n",
      "Cost function =  4484846931081657.0\n",
      "Done with gradient descent at iteration  40\n",
      "Learned weights =  [  0.41345837 124.57395686]\n",
      "Done with gradient descent at iteration  40\n",
      "Learned weights =  [  0.41345837 124.57395538]\n",
      "Done with gradient descent at iteration  41\n",
      "Learned weights =  [  0.42320206 124.57395538]\n",
      "Done with gradient descent at iteration  41\n",
      "Learned weights =  [  0.42320206 124.57395377]\n",
      "Done with gradient descent at iteration  42\n",
      "Learned weights =  [  0.43294575 124.57395377]\n",
      "Done with gradient descent at iteration  42\n",
      "Learned weights =  [  0.43294575 124.57395206]\n",
      "Done with gradient descent at iteration  43\n",
      "Learned weights =  [  0.44268943 124.57395206]\n",
      "Done with gradient descent at iteration  43\n",
      "Learned weights =  [  0.44268943 124.57395029]\n",
      "Done with gradient descent at iteration  44\n",
      "Learned weights =  [  0.45243312 124.57395029]\n",
      "Done with gradient descent at iteration  44\n",
      "Learned weights =  [  0.45243312 124.57394849]\n",
      "Done with gradient descent at iteration  45\n",
      "Learned weights =  [  0.4621768  124.57394849]\n",
      "Done with gradient descent at iteration  45\n",
      "Learned weights =  [  0.4621768  124.57394667]\n",
      "Done with gradient descent at iteration  46\n",
      "Learned weights =  [  0.47192049 124.57394667]\n",
      "Done with gradient descent at iteration  46\n",
      "Learned weights =  [  0.47192049 124.57394484]\n",
      "Done with gradient descent at iteration  47\n",
      "Learned weights =  [  0.48166418 124.57394484]\n",
      "Done with gradient descent at iteration  47\n",
      "Learned weights =  [  0.48166418 124.573943  ]\n",
      "Done with gradient descent at iteration  48\n",
      "Learned weights =  [  0.49140786 124.573943  ]\n",
      "Done with gradient descent at iteration  48\n",
      "Learned weights =  [  0.49140786 124.57394115]\n",
      "Done with gradient descent at iteration  49\n",
      "Learned weights =  [  0.50115155 124.57394115]\n",
      "Done with gradient descent at iteration  49\n",
      "Learned weights =  [  0.50115155 124.5739393 ]\n",
      "Iteration = 50\n",
      "Cost function =  4484845981687379.0\n",
      "Done with gradient descent at iteration  50\n",
      "Learned weights =  [  0.51089523 124.5739393 ]\n",
      "Done with gradient descent at iteration  50\n",
      "Learned weights =  [  0.51089523 124.57393745]\n",
      "Done with gradient descent at iteration  51\n",
      "Learned weights =  [  0.52063892 124.57393745]\n",
      "Done with gradient descent at iteration  51\n",
      "Learned weights =  [  0.52063892 124.5739356 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  52\n",
      "Learned weights =  [  0.5303826 124.5739356]\n",
      "Done with gradient descent at iteration  52\n",
      "Learned weights =  [  0.5303826  124.57393375]\n",
      "Done with gradient descent at iteration  53\n",
      "Learned weights =  [  0.54012629 124.57393375]\n",
      "Done with gradient descent at iteration  53\n",
      "Learned weights =  [  0.54012629 124.57393189]\n",
      "Done with gradient descent at iteration  54\n",
      "Learned weights =  [  0.54986997 124.57393189]\n",
      "Done with gradient descent at iteration  54\n",
      "Learned weights =  [  0.54986997 124.57393004]\n",
      "Done with gradient descent at iteration  55\n",
      "Learned weights =  [  0.55961366 124.57393004]\n",
      "Done with gradient descent at iteration  55\n",
      "Learned weights =  [  0.55961366 124.57392819]\n",
      "Done with gradient descent at iteration  56\n",
      "Learned weights =  [  0.56935734 124.57392819]\n",
      "Done with gradient descent at iteration  56\n",
      "Learned weights =  [  0.56935734 124.57392633]\n",
      "Done with gradient descent at iteration  57\n",
      "Learned weights =  [  0.57910102 124.57392633]\n",
      "Done with gradient descent at iteration  57\n",
      "Learned weights =  [  0.57910102 124.57392448]\n",
      "Done with gradient descent at iteration  58\n",
      "Learned weights =  [  0.58884471 124.57392448]\n",
      "Done with gradient descent at iteration  58\n",
      "Learned weights =  [  0.58884471 124.57392262]\n",
      "Done with gradient descent at iteration  59\n",
      "Learned weights =  [  0.59858839 124.57392262]\n",
      "Done with gradient descent at iteration  59\n",
      "Learned weights =  [  0.59858839 124.57392077]\n",
      "Iteration = 60\n",
      "Cost function =  4484845032293500.0\n",
      "Done with gradient descent at iteration  60\n",
      "Learned weights =  [  0.60833208 124.57392077]\n",
      "Done with gradient descent at iteration  60\n",
      "Learned weights =  [  0.60833208 124.57391891]\n",
      "Done with gradient descent at iteration  61\n",
      "Learned weights =  [  0.61807576 124.57391891]\n",
      "Done with gradient descent at iteration  61\n",
      "Learned weights =  [  0.61807576 124.57391706]\n",
      "Done with gradient descent at iteration  62\n",
      "Learned weights =  [  0.62781944 124.57391706]\n",
      "Done with gradient descent at iteration  62\n",
      "Learned weights =  [  0.62781944 124.5739152 ]\n",
      "Done with gradient descent at iteration  63\n",
      "Learned weights =  [  0.63756312 124.5739152 ]\n",
      "Done with gradient descent at iteration  63\n",
      "Learned weights =  [  0.63756312 124.57391335]\n",
      "Done with gradient descent at iteration  64\n",
      "Learned weights =  [  0.64730681 124.57391335]\n",
      "Done with gradient descent at iteration  64\n",
      "Learned weights =  [  0.64730681 124.57391149]\n",
      "Done with gradient descent at iteration  65\n",
      "Learned weights =  [  0.65705049 124.57391149]\n",
      "Done with gradient descent at iteration  65\n",
      "Learned weights =  [  0.65705049 124.57390964]\n",
      "Done with gradient descent at iteration  66\n",
      "Learned weights =  [  0.66679417 124.57390964]\n",
      "Done with gradient descent at iteration  66\n",
      "Learned weights =  [  0.66679417 124.57390779]\n",
      "Done with gradient descent at iteration  67\n",
      "Learned weights =  [  0.67653785 124.57390779]\n",
      "Done with gradient descent at iteration  67\n",
      "Learned weights =  [  0.67653785 124.57390593]\n",
      "Done with gradient descent at iteration  68\n",
      "Learned weights =  [  0.68628153 124.57390593]\n",
      "Done with gradient descent at iteration  68\n",
      "Learned weights =  [  0.68628153 124.57390408]\n",
      "Done with gradient descent at iteration  69\n",
      "Learned weights =  [  0.69602522 124.57390408]\n",
      "Done with gradient descent at iteration  69\n",
      "Learned weights =  [  0.69602522 124.57390222]\n",
      "Iteration = 70\n",
      "Cost function =  4484844082900019.0\n",
      "Done with gradient descent at iteration  70\n",
      "Learned weights =  [  0.7057689  124.57390222]\n",
      "Done with gradient descent at iteration  70\n",
      "Learned weights =  [  0.7057689  124.57390037]\n",
      "Done with gradient descent at iteration  71\n",
      "Learned weights =  [  0.71551258 124.57390037]\n",
      "Done with gradient descent at iteration  71\n",
      "Learned weights =  [  0.71551258 124.57389851]\n",
      "Done with gradient descent at iteration  72\n",
      "Learned weights =  [  0.72525626 124.57389851]\n",
      "Done with gradient descent at iteration  72\n",
      "Learned weights =  [  0.72525626 124.57389666]\n",
      "Done with gradient descent at iteration  73\n",
      "Learned weights =  [  0.73499994 124.57389666]\n",
      "Done with gradient descent at iteration  73\n",
      "Learned weights =  [  0.73499994 124.5738948 ]\n",
      "Done with gradient descent at iteration  74\n",
      "Learned weights =  [  0.74474362 124.5738948 ]\n",
      "Done with gradient descent at iteration  74\n",
      "Learned weights =  [  0.74474362 124.57389295]\n",
      "Done with gradient descent at iteration  75\n",
      "Learned weights =  [  0.7544873  124.57389295]\n",
      "Done with gradient descent at iteration  75\n",
      "Learned weights =  [  0.7544873  124.57389109]\n",
      "Done with gradient descent at iteration  76\n",
      "Learned weights =  [  0.76423098 124.57389109]\n",
      "Done with gradient descent at iteration  76\n",
      "Learned weights =  [  0.76423098 124.57388924]\n",
      "Done with gradient descent at iteration  77\n",
      "Learned weights =  [  0.77397466 124.57388924]\n",
      "Done with gradient descent at iteration  77\n",
      "Learned weights =  [  0.77397466 124.57388739]\n",
      "Done with gradient descent at iteration  78\n",
      "Learned weights =  [  0.78371834 124.57388739]\n",
      "Done with gradient descent at iteration  78\n",
      "Learned weights =  [  0.78371834 124.57388553]\n",
      "Done with gradient descent at iteration  79\n",
      "Learned weights =  [  0.79346202 124.57388553]\n",
      "Done with gradient descent at iteration  79\n",
      "Learned weights =  [  0.79346202 124.57388368]\n",
      "Iteration = 80\n",
      "Cost function =  4484843133506937.5\n",
      "Done with gradient descent at iteration  80\n",
      "Learned weights =  [  0.8032057  124.57388368]\n",
      "Done with gradient descent at iteration  80\n",
      "Learned weights =  [  0.8032057  124.57388182]\n",
      "Done with gradient descent at iteration  81\n",
      "Learned weights =  [  0.81294938 124.57388182]\n",
      "Done with gradient descent at iteration  81\n",
      "Learned weights =  [  0.81294938 124.57387997]\n",
      "Done with gradient descent at iteration  82\n",
      "Learned weights =  [  0.82269306 124.57387997]\n",
      "Done with gradient descent at iteration  82\n",
      "Learned weights =  [  0.82269306 124.57387811]\n",
      "Done with gradient descent at iteration  83\n",
      "Learned weights =  [  0.83243673 124.57387811]\n",
      "Done with gradient descent at iteration  83\n",
      "Learned weights =  [  0.83243673 124.57387626]\n",
      "Done with gradient descent at iteration  84\n",
      "Learned weights =  [  0.84218041 124.57387626]\n",
      "Done with gradient descent at iteration  84\n",
      "Learned weights =  [  0.84218041 124.5738744 ]\n",
      "Done with gradient descent at iteration  85\n",
      "Learned weights =  [  0.85192409 124.5738744 ]\n",
      "Done with gradient descent at iteration  85\n",
      "Learned weights =  [  0.85192409 124.57387255]\n",
      "Done with gradient descent at iteration  86\n",
      "Learned weights =  [  0.86166777 124.57387255]\n",
      "Done with gradient descent at iteration  86\n",
      "Learned weights =  [  0.86166777 124.57387069]\n",
      "Done with gradient descent at iteration  87\n",
      "Learned weights =  [  0.87141145 124.57387069]\n",
      "Done with gradient descent at iteration  87\n",
      "Learned weights =  [  0.87141145 124.57386884]\n",
      "Done with gradient descent at iteration  88\n",
      "Learned weights =  [  0.88115512 124.57386884]\n",
      "Done with gradient descent at iteration  88\n",
      "Learned weights =  [  0.88115512 124.57386699]\n",
      "Done with gradient descent at iteration  89\n",
      "Learned weights =  [  0.8908988  124.57386699]\n",
      "Done with gradient descent at iteration  89\n",
      "Learned weights =  [  0.8908988  124.57386513]\n",
      "Iteration = 90\n",
      "Cost function =  4484842184114255.0\n",
      "Done with gradient descent at iteration  90\n",
      "Learned weights =  [  0.90064248 124.57386513]\n",
      "Done with gradient descent at iteration  90\n",
      "Learned weights =  [  0.90064248 124.57386328]\n",
      "Done with gradient descent at iteration  91\n",
      "Learned weights =  [  0.91038615 124.57386328]\n",
      "Done with gradient descent at iteration  91\n",
      "Learned weights =  [  0.91038615 124.57386142]\n",
      "Done with gradient descent at iteration  92\n",
      "Learned weights =  [  0.92012983 124.57386142]\n",
      "Done with gradient descent at iteration  92\n",
      "Learned weights =  [  0.92012983 124.57385957]\n",
      "Done with gradient descent at iteration  93\n",
      "Learned weights =  [  0.92987351 124.57385957]\n",
      "Done with gradient descent at iteration  93\n",
      "Learned weights =  [  0.92987351 124.57385771]\n",
      "Done with gradient descent at iteration  94\n",
      "Learned weights =  [  0.93961718 124.57385771]\n",
      "Done with gradient descent at iteration  94\n",
      "Learned weights =  [  0.93961718 124.57385586]\n",
      "Done with gradient descent at iteration  95\n",
      "Learned weights =  [  0.94936086 124.57385586]\n",
      "Done with gradient descent at iteration  95\n",
      "Learned weights =  [  0.94936086 124.573854  ]\n",
      "Done with gradient descent at iteration  96\n",
      "Learned weights =  [  0.95910454 124.573854  ]\n",
      "Done with gradient descent at iteration  96\n",
      "Learned weights =  [  0.95910454 124.57385215]\n",
      "Done with gradient descent at iteration  97\n",
      "Learned weights =  [  0.96884821 124.57385215]\n",
      "Done with gradient descent at iteration  97\n",
      "Learned weights =  [  0.96884821 124.57385029]\n",
      "Done with gradient descent at iteration  98\n",
      "Learned weights =  [  0.97859189 124.57385029]\n",
      "Done with gradient descent at iteration  98\n",
      "Learned weights =  [  0.97859189 124.57384844]\n",
      "Done with gradient descent at iteration  99\n",
      "Learned weights =  [  0.98833556 124.57384844]\n",
      "Done with gradient descent at iteration  99\n",
      "Learned weights =  [  0.98833556 124.57384659]\n",
      "Iteration = 100\n",
      "Cost function =  4484841234721970.0\n",
      "Done with gradient descent at iteration  100\n",
      "Learned weights =  [  0.99807924 124.57384659]\n",
      "Done with gradient descent at iteration  100\n",
      "Learned weights =  [  0.99807924 124.57384473]\n",
      "Done with gradient descent at iteration  101\n",
      "Learned weights =  [  1.00782291 124.57384473]\n",
      "Done with gradient descent at iteration  101\n",
      "Learned weights =  [  1.00782291 124.57384288]\n",
      "Done with gradient descent at iteration  102\n",
      "Learned weights =  [  1.01756659 124.57384288]\n",
      "Done with gradient descent at iteration  102\n",
      "Learned weights =  [  1.01756659 124.57384102]\n",
      "Done with gradient descent at iteration  103\n",
      "Learned weights =  [  1.02731026 124.57384102]\n",
      "Done with gradient descent at iteration  103\n",
      "Learned weights =  [  1.02731026 124.57383917]\n",
      "Done with gradient descent at iteration  104\n",
      "Learned weights =  [  1.03705394 124.57383917]\n",
      "Done with gradient descent at iteration  104\n",
      "Learned weights =  [  1.03705394 124.57383731]\n",
      "Done with gradient descent at iteration  105\n",
      "Learned weights =  [  1.04679761 124.57383731]\n",
      "Done with gradient descent at iteration  105\n",
      "Learned weights =  [  1.04679761 124.57383546]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  106\n",
      "Learned weights =  [  1.05654128 124.57383546]\n",
      "Done with gradient descent at iteration  106\n",
      "Learned weights =  [  1.05654128 124.5738336 ]\n",
      "Done with gradient descent at iteration  107\n",
      "Learned weights =  [  1.06628496 124.5738336 ]\n",
      "Done with gradient descent at iteration  107\n",
      "Learned weights =  [  1.06628496 124.57383175]\n",
      "Done with gradient descent at iteration  108\n",
      "Learned weights =  [  1.07602863 124.57383175]\n",
      "Done with gradient descent at iteration  108\n",
      "Learned weights =  [  1.07602863 124.57382989]\n",
      "Done with gradient descent at iteration  109\n",
      "Learned weights =  [  1.0857723  124.57382989]\n",
      "Done with gradient descent at iteration  109\n",
      "Learned weights =  [  1.0857723  124.57382804]\n",
      "Done with gradient descent at iteration  110\n",
      "Learned weights =  [  1.09551598 124.57382804]\n",
      "Done with gradient descent at iteration  110\n",
      "Learned weights =  [  1.09551598 124.57382619]\n",
      "Done with gradient descent at iteration  111\n",
      "Learned weights =  [  1.10525965 124.57382619]\n",
      "Done with gradient descent at iteration  111\n",
      "Learned weights =  [  1.10525965 124.57382433]\n",
      "Done with gradient descent at iteration  112\n",
      "Learned weights =  [  1.11500332 124.57382433]\n",
      "Done with gradient descent at iteration  112\n",
      "Learned weights =  [  1.11500332 124.57382248]\n",
      "Done with gradient descent at iteration  113\n",
      "Learned weights =  [  1.124747   124.57382248]\n",
      "Done with gradient descent at iteration  113\n",
      "Learned weights =  [  1.124747   124.57382062]\n",
      "Done with gradient descent at iteration  114\n",
      "Learned weights =  [  1.13449067 124.57382062]\n",
      "Done with gradient descent at iteration  114\n",
      "Learned weights =  [  1.13449067 124.57381877]\n",
      "Done with gradient descent at iteration  115\n",
      "Learned weights =  [  1.14423434 124.57381877]\n",
      "Done with gradient descent at iteration  115\n",
      "Learned weights =  [  1.14423434 124.57381691]\n",
      "Done with gradient descent at iteration  116\n",
      "Learned weights =  [  1.15397801 124.57381691]\n",
      "Done with gradient descent at iteration  116\n",
      "Learned weights =  [  1.15397801 124.57381506]\n",
      "Done with gradient descent at iteration  117\n",
      "Learned weights =  [  1.16372168 124.57381506]\n",
      "Done with gradient descent at iteration  117\n",
      "Learned weights =  [  1.16372168 124.5738132 ]\n",
      "Done with gradient descent at iteration  118\n",
      "Learned weights =  [  1.17346535 124.5738132 ]\n",
      "Done with gradient descent at iteration  118\n",
      "Learned weights =  [  1.17346535 124.57381135]\n",
      "Done with gradient descent at iteration  119\n",
      "Learned weights =  [  1.18320903 124.57381135]\n",
      "Done with gradient descent at iteration  119\n",
      "Learned weights =  [  1.18320903 124.57380949]\n",
      "Done with gradient descent at iteration  120\n",
      "Learned weights =  [  1.1929527  124.57380949]\n",
      "Done with gradient descent at iteration  120\n",
      "Learned weights =  [  1.1929527  124.57380764]\n",
      "Done with gradient descent at iteration  121\n",
      "Learned weights =  [  1.20269637 124.57380764]\n",
      "Done with gradient descent at iteration  121\n",
      "Learned weights =  [  1.20269637 124.57380579]\n",
      "Done with gradient descent at iteration  122\n",
      "Learned weights =  [  1.21244004 124.57380579]\n",
      "Done with gradient descent at iteration  122\n",
      "Learned weights =  [  1.21244004 124.57380393]\n",
      "Done with gradient descent at iteration  123\n",
      "Learned weights =  [  1.22218371 124.57380393]\n",
      "Done with gradient descent at iteration  123\n",
      "Learned weights =  [  1.22218371 124.57380208]\n",
      "Done with gradient descent at iteration  124\n",
      "Learned weights =  [  1.23192738 124.57380208]\n",
      "Done with gradient descent at iteration  124\n",
      "Learned weights =  [  1.23192738 124.57380022]\n",
      "Done with gradient descent at iteration  125\n",
      "Learned weights =  [  1.24167105 124.57380022]\n",
      "Done with gradient descent at iteration  125\n",
      "Learned weights =  [  1.24167105 124.57379837]\n",
      "Done with gradient descent at iteration  126\n",
      "Learned weights =  [  1.25141472 124.57379837]\n",
      "Done with gradient descent at iteration  126\n",
      "Learned weights =  [  1.25141472 124.57379651]\n",
      "Done with gradient descent at iteration  127\n",
      "Learned weights =  [  1.26115839 124.57379651]\n",
      "Done with gradient descent at iteration  127\n",
      "Learned weights =  [  1.26115839 124.57379466]\n",
      "Done with gradient descent at iteration  128\n",
      "Learned weights =  [  1.27090206 124.57379466]\n",
      "Done with gradient descent at iteration  128\n",
      "Learned weights =  [  1.27090206 124.5737928 ]\n",
      "Done with gradient descent at iteration  129\n",
      "Learned weights =  [  1.28064573 124.5737928 ]\n",
      "Done with gradient descent at iteration  129\n",
      "Learned weights =  [  1.28064573 124.57379095]\n",
      "Done with gradient descent at iteration  130\n",
      "Learned weights =  [  1.2903894  124.57379095]\n",
      "Done with gradient descent at iteration  130\n",
      "Learned weights =  [  1.2903894  124.57378909]\n",
      "Done with gradient descent at iteration  131\n",
      "Learned weights =  [  1.30013306 124.57378909]\n",
      "Done with gradient descent at iteration  131\n",
      "Learned weights =  [  1.30013306 124.57378724]\n",
      "Done with gradient descent at iteration  132\n",
      "Learned weights =  [  1.30987673 124.57378724]\n",
      "Done with gradient descent at iteration  132\n",
      "Learned weights =  [  1.30987673 124.57378539]\n",
      "Done with gradient descent at iteration  133\n",
      "Learned weights =  [  1.3196204  124.57378539]\n",
      "Done with gradient descent at iteration  133\n",
      "Learned weights =  [  1.3196204  124.57378353]\n",
      "Done with gradient descent at iteration  134\n",
      "Learned weights =  [  1.32936407 124.57378353]\n",
      "Done with gradient descent at iteration  134\n",
      "Learned weights =  [  1.32936407 124.57378168]\n",
      "Done with gradient descent at iteration  135\n",
      "Learned weights =  [  1.33910774 124.57378168]\n",
      "Done with gradient descent at iteration  135\n",
      "Learned weights =  [  1.33910774 124.57377982]\n",
      "Done with gradient descent at iteration  136\n",
      "Learned weights =  [  1.3488514  124.57377982]\n",
      "Done with gradient descent at iteration  136\n",
      "Learned weights =  [  1.3488514  124.57377797]\n",
      "Done with gradient descent at iteration  137\n",
      "Learned weights =  [  1.35859507 124.57377797]\n",
      "Done with gradient descent at iteration  137\n",
      "Learned weights =  [  1.35859507 124.57377611]\n",
      "Done with gradient descent at iteration  138\n",
      "Learned weights =  [  1.36833874 124.57377611]\n",
      "Done with gradient descent at iteration  138\n",
      "Learned weights =  [  1.36833874 124.57377426]\n",
      "Done with gradient descent at iteration  139\n",
      "Learned weights =  [  1.37808241 124.57377426]\n",
      "Done with gradient descent at iteration  139\n",
      "Learned weights =  [  1.37808241 124.5737724 ]\n",
      "Done with gradient descent at iteration  140\n",
      "Learned weights =  [  1.38782607 124.5737724 ]\n",
      "Done with gradient descent at iteration  140\n",
      "Learned weights =  [  1.38782607 124.57377055]\n",
      "Done with gradient descent at iteration  141\n",
      "Learned weights =  [  1.39756974 124.57377055]\n",
      "Done with gradient descent at iteration  141\n",
      "Learned weights =  [  1.39756974 124.57376869]\n",
      "Done with gradient descent at iteration  142\n",
      "Learned weights =  [  1.40731341 124.57376869]\n",
      "Done with gradient descent at iteration  142\n",
      "Learned weights =  [  1.40731341 124.57376684]\n",
      "Done with gradient descent at iteration  143\n",
      "Learned weights =  [  1.41705707 124.57376684]\n",
      "Done with gradient descent at iteration  143\n",
      "Learned weights =  [  1.41705707 124.57376499]\n",
      "Done with gradient descent at iteration  144\n",
      "Learned weights =  [  1.42680074 124.57376499]\n",
      "Done with gradient descent at iteration  144\n",
      "Learned weights =  [  1.42680074 124.57376313]\n",
      "Done with gradient descent at iteration  145\n",
      "Learned weights =  [  1.4365444  124.57376313]\n",
      "Done with gradient descent at iteration  145\n",
      "Learned weights =  [  1.4365444  124.57376128]\n",
      "Done with gradient descent at iteration  146\n",
      "Learned weights =  [  1.44628807 124.57376128]\n",
      "Done with gradient descent at iteration  146\n",
      "Learned weights =  [  1.44628807 124.57375942]\n",
      "Done with gradient descent at iteration  147\n",
      "Learned weights =  [  1.45603174 124.57375942]\n",
      "Done with gradient descent at iteration  147\n",
      "Learned weights =  [  1.45603174 124.57375757]\n",
      "Done with gradient descent at iteration  148\n",
      "Learned weights =  [  1.4657754  124.57375757]\n",
      "Done with gradient descent at iteration  148\n",
      "Learned weights =  [  1.4657754  124.57375571]\n",
      "Done with gradient descent at iteration  149\n",
      "Learned weights =  [  1.47551907 124.57375571]\n",
      "Done with gradient descent at iteration  149\n",
      "Learned weights =  [  1.47551907 124.57375386]\n",
      "Done with gradient descent at iteration  150\n",
      "Learned weights =  [  1.48526273 124.57375386]\n",
      "Done with gradient descent at iteration  150\n",
      "Learned weights =  [  1.48526273 124.573752  ]\n",
      "Done with gradient descent at iteration  151\n",
      "Learned weights =  [  1.4950064 124.573752 ]\n",
      "Done with gradient descent at iteration  151\n",
      "Learned weights =  [  1.4950064  124.57375015]\n",
      "Done with gradient descent at iteration  152\n",
      "Learned weights =  [  1.50475006 124.57375015]\n",
      "Done with gradient descent at iteration  152\n",
      "Learned weights =  [  1.50475006 124.57374829]\n",
      "Done with gradient descent at iteration  153\n",
      "Learned weights =  [  1.51449372 124.57374829]\n",
      "Done with gradient descent at iteration  153\n",
      "Learned weights =  [  1.51449372 124.57374644]\n",
      "Done with gradient descent at iteration  154\n",
      "Learned weights =  [  1.52423739 124.57374644]\n",
      "Done with gradient descent at iteration  154\n",
      "Learned weights =  [  1.52423739 124.57374458]\n",
      "Done with gradient descent at iteration  155\n",
      "Learned weights =  [  1.53398105 124.57374458]\n",
      "Done with gradient descent at iteration  155\n",
      "Learned weights =  [  1.53398105 124.57374273]\n",
      "Done with gradient descent at iteration  156\n",
      "Learned weights =  [  1.54372472 124.57374273]\n",
      "Done with gradient descent at iteration  156\n",
      "Learned weights =  [  1.54372472 124.57374088]\n",
      "Done with gradient descent at iteration  157\n",
      "Learned weights =  [  1.55346838 124.57374088]\n",
      "Done with gradient descent at iteration  157\n",
      "Learned weights =  [  1.55346838 124.57373902]\n",
      "Done with gradient descent at iteration  158\n",
      "Learned weights =  [  1.56321204 124.57373902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  158\n",
      "Learned weights =  [  1.56321204 124.57373717]\n",
      "Done with gradient descent at iteration  159\n",
      "Learned weights =  [  1.57295571 124.57373717]\n",
      "Done with gradient descent at iteration  159\n",
      "Learned weights =  [  1.57295571 124.57373531]\n",
      "Done with gradient descent at iteration  160\n",
      "Learned weights =  [  1.58269937 124.57373531]\n",
      "Done with gradient descent at iteration  160\n",
      "Learned weights =  [  1.58269937 124.57373346]\n",
      "Done with gradient descent at iteration  161\n",
      "Learned weights =  [  1.59244303 124.57373346]\n",
      "Done with gradient descent at iteration  161\n",
      "Learned weights =  [  1.59244303 124.5737316 ]\n",
      "Done with gradient descent at iteration  162\n",
      "Learned weights =  [  1.60218669 124.5737316 ]\n",
      "Done with gradient descent at iteration  162\n",
      "Learned weights =  [  1.60218669 124.57372975]\n",
      "Done with gradient descent at iteration  163\n",
      "Learned weights =  [  1.61193036 124.57372975]\n",
      "Done with gradient descent at iteration  163\n",
      "Learned weights =  [  1.61193036 124.57372789]\n",
      "Done with gradient descent at iteration  164\n",
      "Learned weights =  [  1.62167402 124.57372789]\n",
      "Done with gradient descent at iteration  164\n",
      "Learned weights =  [  1.62167402 124.57372604]\n",
      "Done with gradient descent at iteration  165\n",
      "Learned weights =  [  1.63141768 124.57372604]\n",
      "Done with gradient descent at iteration  165\n",
      "Learned weights =  [  1.63141768 124.57372418]\n",
      "Done with gradient descent at iteration  166\n",
      "Learned weights =  [  1.64116134 124.57372418]\n",
      "Done with gradient descent at iteration  166\n",
      "Learned weights =  [  1.64116134 124.57372233]\n",
      "Done with gradient descent at iteration  167\n",
      "Learned weights =  [  1.650905   124.57372233]\n",
      "Done with gradient descent at iteration  167\n",
      "Learned weights =  [  1.650905   124.57372048]\n",
      "Done with gradient descent at iteration  168\n",
      "Learned weights =  [  1.66064866 124.57372048]\n",
      "Done with gradient descent at iteration  168\n",
      "Learned weights =  [  1.66064866 124.57371862]\n",
      "Done with gradient descent at iteration  169\n",
      "Learned weights =  [  1.67039232 124.57371862]\n",
      "Done with gradient descent at iteration  169\n",
      "Learned weights =  [  1.67039232 124.57371677]\n",
      "Done with gradient descent at iteration  170\n",
      "Learned weights =  [  1.68013598 124.57371677]\n",
      "Done with gradient descent at iteration  170\n",
      "Learned weights =  [  1.68013598 124.57371491]\n",
      "Done with gradient descent at iteration  171\n",
      "Learned weights =  [  1.68987965 124.57371491]\n",
      "Done with gradient descent at iteration  171\n",
      "Learned weights =  [  1.68987965 124.57371306]\n",
      "Done with gradient descent at iteration  172\n",
      "Learned weights =  [  1.69962331 124.57371306]\n",
      "Done with gradient descent at iteration  172\n",
      "Learned weights =  [  1.69962331 124.5737112 ]\n",
      "Done with gradient descent at iteration  173\n",
      "Learned weights =  [  1.70936697 124.5737112 ]\n",
      "Done with gradient descent at iteration  173\n",
      "Learned weights =  [  1.70936697 124.57370935]\n",
      "Done with gradient descent at iteration  174\n",
      "Learned weights =  [  1.71911063 124.57370935]\n",
      "Done with gradient descent at iteration  174\n",
      "Learned weights =  [  1.71911063 124.57370749]\n",
      "Done with gradient descent at iteration  175\n",
      "Learned weights =  [  1.72885429 124.57370749]\n",
      "Done with gradient descent at iteration  175\n",
      "Learned weights =  [  1.72885429 124.57370564]\n",
      "Done with gradient descent at iteration  176\n",
      "Learned weights =  [  1.73859795 124.57370564]\n",
      "Done with gradient descent at iteration  176\n",
      "Learned weights =  [  1.73859795 124.57370378]\n",
      "Done with gradient descent at iteration  177\n",
      "Learned weights =  [  1.7483416  124.57370378]\n",
      "Done with gradient descent at iteration  177\n",
      "Learned weights =  [  1.7483416  124.57370193]\n",
      "Done with gradient descent at iteration  178\n",
      "Learned weights =  [  1.75808526 124.57370193]\n",
      "Done with gradient descent at iteration  178\n",
      "Learned weights =  [  1.75808526 124.57370008]\n",
      "Done with gradient descent at iteration  179\n",
      "Learned weights =  [  1.76782892 124.57370008]\n",
      "Done with gradient descent at iteration  179\n",
      "Learned weights =  [  1.76782892 124.57369822]\n",
      "Done with gradient descent at iteration  180\n",
      "Learned weights =  [  1.77757258 124.57369822]\n",
      "Done with gradient descent at iteration  180\n",
      "Learned weights =  [  1.77757258 124.57369637]\n",
      "Done with gradient descent at iteration  181\n",
      "Learned weights =  [  1.78731624 124.57369637]\n",
      "Done with gradient descent at iteration  181\n",
      "Learned weights =  [  1.78731624 124.57369451]\n",
      "Done with gradient descent at iteration  182\n",
      "Learned weights =  [  1.7970599  124.57369451]\n",
      "Done with gradient descent at iteration  182\n",
      "Learned weights =  [  1.7970599  124.57369266]\n",
      "Done with gradient descent at iteration  183\n",
      "Learned weights =  [  1.80680356 124.57369266]\n",
      "Done with gradient descent at iteration  183\n",
      "Learned weights =  [  1.80680356 124.5736908 ]\n",
      "Done with gradient descent at iteration  184\n",
      "Learned weights =  [  1.81654721 124.5736908 ]\n",
      "Done with gradient descent at iteration  184\n",
      "Learned weights =  [  1.81654721 124.57368895]\n",
      "Done with gradient descent at iteration  185\n",
      "Learned weights =  [  1.82629087 124.57368895]\n",
      "Done with gradient descent at iteration  185\n",
      "Learned weights =  [  1.82629087 124.57368709]\n",
      "Done with gradient descent at iteration  186\n",
      "Learned weights =  [  1.83603453 124.57368709]\n",
      "Done with gradient descent at iteration  186\n",
      "Learned weights =  [  1.83603453 124.57368524]\n",
      "Done with gradient descent at iteration  187\n",
      "Learned weights =  [  1.84577819 124.57368524]\n",
      "Done with gradient descent at iteration  187\n",
      "Learned weights =  [  1.84577819 124.57368338]\n",
      "Done with gradient descent at iteration  188\n",
      "Learned weights =  [  1.85552184 124.57368338]\n",
      "Done with gradient descent at iteration  188\n",
      "Learned weights =  [  1.85552184 124.57368153]\n",
      "Done with gradient descent at iteration  189\n",
      "Learned weights =  [  1.8652655  124.57368153]\n",
      "Done with gradient descent at iteration  189\n",
      "Learned weights =  [  1.8652655  124.57367968]\n",
      "Done with gradient descent at iteration  190\n",
      "Learned weights =  [  1.87500916 124.57367968]\n",
      "Done with gradient descent at iteration  190\n",
      "Learned weights =  [  1.87500916 124.57367782]\n",
      "Done with gradient descent at iteration  191\n",
      "Learned weights =  [  1.88475281 124.57367782]\n",
      "Done with gradient descent at iteration  191\n",
      "Learned weights =  [  1.88475281 124.57367597]\n",
      "Done with gradient descent at iteration  192\n",
      "Learned weights =  [  1.89449647 124.57367597]\n",
      "Done with gradient descent at iteration  192\n",
      "Learned weights =  [  1.89449647 124.57367411]\n",
      "Done with gradient descent at iteration  193\n",
      "Learned weights =  [  1.90424013 124.57367411]\n",
      "Done with gradient descent at iteration  193\n",
      "Learned weights =  [  1.90424013 124.57367226]\n",
      "Done with gradient descent at iteration  194\n",
      "Learned weights =  [  1.91398378 124.57367226]\n",
      "Done with gradient descent at iteration  194\n",
      "Learned weights =  [  1.91398378 124.5736704 ]\n",
      "Done with gradient descent at iteration  195\n",
      "Learned weights =  [  1.92372744 124.5736704 ]\n",
      "Done with gradient descent at iteration  195\n",
      "Learned weights =  [  1.92372744 124.57366855]\n",
      "Done with gradient descent at iteration  196\n",
      "Learned weights =  [  1.93347109 124.57366855]\n",
      "Done with gradient descent at iteration  196\n",
      "Learned weights =  [  1.93347109 124.57366669]\n",
      "Done with gradient descent at iteration  197\n",
      "Learned weights =  [  1.94321475 124.57366669]\n",
      "Done with gradient descent at iteration  197\n",
      "Learned weights =  [  1.94321475 124.57366484]\n",
      "Done with gradient descent at iteration  198\n",
      "Learned weights =  [  1.9529584  124.57366484]\n",
      "Done with gradient descent at iteration  198\n",
      "Learned weights =  [  1.9529584  124.57366298]\n",
      "Done with gradient descent at iteration  199\n",
      "Learned weights =  [  1.96270206 124.57366298]\n",
      "Done with gradient descent at iteration  199\n",
      "Learned weights =  [  1.96270206 124.57366113]\n",
      "Iteration = 200\n",
      "Cost function =  4484831740821062.5\n",
      "Done with gradient descent at iteration  200\n",
      "Learned weights =  [  1.97244571 124.57366113]\n",
      "Done with gradient descent at iteration  200\n",
      "Learned weights =  [  1.97244571 124.57365928]\n",
      "Done with gradient descent at iteration  201\n",
      "Learned weights =  [  1.98218937 124.57365928]\n",
      "Done with gradient descent at iteration  201\n",
      "Learned weights =  [  1.98218937 124.57365742]\n",
      "Done with gradient descent at iteration  202\n",
      "Learned weights =  [  1.99193302 124.57365742]\n",
      "Done with gradient descent at iteration  202\n",
      "Learned weights =  [  1.99193302 124.57365557]\n",
      "Done with gradient descent at iteration  203\n",
      "Learned weights =  [  2.00167667 124.57365557]\n",
      "Done with gradient descent at iteration  203\n",
      "Learned weights =  [  2.00167667 124.57365371]\n",
      "Done with gradient descent at iteration  204\n",
      "Learned weights =  [  2.01142033 124.57365371]\n",
      "Done with gradient descent at iteration  204\n",
      "Learned weights =  [  2.01142033 124.57365186]\n",
      "Done with gradient descent at iteration  205\n",
      "Learned weights =  [  2.02116398 124.57365186]\n",
      "Done with gradient descent at iteration  205\n",
      "Learned weights =  [  2.02116398 124.57365   ]\n",
      "Done with gradient descent at iteration  206\n",
      "Learned weights =  [  2.03090764 124.57365   ]\n",
      "Done with gradient descent at iteration  206\n",
      "Learned weights =  [  2.03090764 124.57364815]\n",
      "Done with gradient descent at iteration  207\n",
      "Learned weights =  [  2.04065129 124.57364815]\n",
      "Done with gradient descent at iteration  207\n",
      "Learned weights =  [  2.04065129 124.57364629]\n",
      "Done with gradient descent at iteration  208\n",
      "Learned weights =  [  2.05039494 124.57364629]\n",
      "Done with gradient descent at iteration  208\n",
      "Learned weights =  [  2.05039494 124.57364444]\n",
      "Done with gradient descent at iteration  209\n",
      "Learned weights =  [  2.06013859 124.57364444]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  209\n",
      "Learned weights =  [  2.06013859 124.57364258]\n",
      "Done with gradient descent at iteration  210\n",
      "Learned weights =  [  2.06988225 124.57364258]\n",
      "Done with gradient descent at iteration  210\n",
      "Learned weights =  [  2.06988225 124.57364073]\n",
      "Done with gradient descent at iteration  211\n",
      "Learned weights =  [  2.0796259  124.57364073]\n",
      "Done with gradient descent at iteration  211\n",
      "Learned weights =  [  2.0796259  124.57363888]\n",
      "Done with gradient descent at iteration  212\n",
      "Learned weights =  [  2.08936955 124.57363888]\n",
      "Done with gradient descent at iteration  212\n",
      "Learned weights =  [  2.08936955 124.57363702]\n",
      "Done with gradient descent at iteration  213\n",
      "Learned weights =  [  2.0991132  124.57363702]\n",
      "Done with gradient descent at iteration  213\n",
      "Learned weights =  [  2.0991132  124.57363517]\n",
      "Done with gradient descent at iteration  214\n",
      "Learned weights =  [  2.10885686 124.57363517]\n",
      "Done with gradient descent at iteration  214\n",
      "Learned weights =  [  2.10885686 124.57363331]\n",
      "Done with gradient descent at iteration  215\n",
      "Learned weights =  [  2.11860051 124.57363331]\n",
      "Done with gradient descent at iteration  215\n",
      "Learned weights =  [  2.11860051 124.57363146]\n",
      "Done with gradient descent at iteration  216\n",
      "Learned weights =  [  2.12834416 124.57363146]\n",
      "Done with gradient descent at iteration  216\n",
      "Learned weights =  [  2.12834416 124.5736296 ]\n",
      "Done with gradient descent at iteration  217\n",
      "Learned weights =  [  2.13808781 124.5736296 ]\n",
      "Done with gradient descent at iteration  217\n",
      "Learned weights =  [  2.13808781 124.57362775]\n",
      "Done with gradient descent at iteration  218\n",
      "Learned weights =  [  2.14783146 124.57362775]\n",
      "Done with gradient descent at iteration  218\n",
      "Learned weights =  [  2.14783146 124.57362589]\n",
      "Done with gradient descent at iteration  219\n",
      "Learned weights =  [  2.15757511 124.57362589]\n",
      "Done with gradient descent at iteration  219\n",
      "Learned weights =  [  2.15757511 124.57362404]\n",
      "Done with gradient descent at iteration  220\n",
      "Learned weights =  [  2.16731876 124.57362404]\n",
      "Done with gradient descent at iteration  220\n",
      "Learned weights =  [  2.16731876 124.57362218]\n",
      "Done with gradient descent at iteration  221\n",
      "Learned weights =  [  2.17706241 124.57362218]\n",
      "Done with gradient descent at iteration  221\n",
      "Learned weights =  [  2.17706241 124.57362033]\n",
      "Done with gradient descent at iteration  222\n",
      "Learned weights =  [  2.18680606 124.57362033]\n",
      "Done with gradient descent at iteration  222\n",
      "Learned weights =  [  2.18680606 124.57361848]\n",
      "Done with gradient descent at iteration  223\n",
      "Learned weights =  [  2.19654971 124.57361848]\n",
      "Done with gradient descent at iteration  223\n",
      "Learned weights =  [  2.19654971 124.57361662]\n",
      "Done with gradient descent at iteration  224\n",
      "Learned weights =  [  2.20629336 124.57361662]\n",
      "Done with gradient descent at iteration  224\n",
      "Learned weights =  [  2.20629336 124.57361477]\n",
      "Done with gradient descent at iteration  225\n",
      "Learned weights =  [  2.21603701 124.57361477]\n",
      "Done with gradient descent at iteration  225\n",
      "Learned weights =  [  2.21603701 124.57361291]\n",
      "Done with gradient descent at iteration  226\n",
      "Learned weights =  [  2.22578066 124.57361291]\n",
      "Done with gradient descent at iteration  226\n",
      "Learned weights =  [  2.22578066 124.57361106]\n",
      "Done with gradient descent at iteration  227\n",
      "Learned weights =  [  2.23552431 124.57361106]\n",
      "Done with gradient descent at iteration  227\n",
      "Learned weights =  [  2.23552431 124.5736092 ]\n",
      "Done with gradient descent at iteration  228\n",
      "Learned weights =  [  2.24526796 124.5736092 ]\n",
      "Done with gradient descent at iteration  228\n",
      "Learned weights =  [  2.24526796 124.57360735]\n",
      "Done with gradient descent at iteration  229\n",
      "Learned weights =  [  2.25501161 124.57360735]\n",
      "Done with gradient descent at iteration  229\n",
      "Learned weights =  [  2.25501161 124.57360549]\n",
      "Done with gradient descent at iteration  230\n",
      "Learned weights =  [  2.26475526 124.57360549]\n",
      "Done with gradient descent at iteration  230\n",
      "Learned weights =  [  2.26475526 124.57360364]\n",
      "Done with gradient descent at iteration  231\n",
      "Learned weights =  [  2.2744989  124.57360364]\n",
      "Done with gradient descent at iteration  231\n",
      "Learned weights =  [  2.2744989  124.57360178]\n",
      "Done with gradient descent at iteration  232\n",
      "Learned weights =  [  2.28424255 124.57360178]\n",
      "Done with gradient descent at iteration  232\n",
      "Learned weights =  [  2.28424255 124.57359993]\n",
      "Done with gradient descent at iteration  233\n",
      "Learned weights =  [  2.2939862  124.57359993]\n",
      "Done with gradient descent at iteration  233\n",
      "Learned weights =  [  2.2939862  124.57359808]\n",
      "Done with gradient descent at iteration  234\n",
      "Learned weights =  [  2.30372985 124.57359808]\n",
      "Done with gradient descent at iteration  234\n",
      "Learned weights =  [  2.30372985 124.57359622]\n",
      "Done with gradient descent at iteration  235\n",
      "Learned weights =  [  2.31347349 124.57359622]\n",
      "Done with gradient descent at iteration  235\n",
      "Learned weights =  [  2.31347349 124.57359437]\n",
      "Done with gradient descent at iteration  236\n",
      "Learned weights =  [  2.32321714 124.57359437]\n",
      "Done with gradient descent at iteration  236\n",
      "Learned weights =  [  2.32321714 124.57359251]\n",
      "Done with gradient descent at iteration  237\n",
      "Learned weights =  [  2.33296079 124.57359251]\n",
      "Done with gradient descent at iteration  237\n",
      "Learned weights =  [  2.33296079 124.57359066]\n",
      "Done with gradient descent at iteration  238\n",
      "Learned weights =  [  2.34270444 124.57359066]\n",
      "Done with gradient descent at iteration  238\n",
      "Learned weights =  [  2.34270444 124.5735888 ]\n",
      "Done with gradient descent at iteration  239\n",
      "Learned weights =  [  2.35244808 124.5735888 ]\n",
      "Done with gradient descent at iteration  239\n",
      "Learned weights =  [  2.35244808 124.57358695]\n",
      "Done with gradient descent at iteration  240\n",
      "Learned weights =  [  2.36219173 124.57358695]\n",
      "Done with gradient descent at iteration  240\n",
      "Learned weights =  [  2.36219173 124.57358509]\n",
      "Done with gradient descent at iteration  241\n",
      "Learned weights =  [  2.37193538 124.57358509]\n",
      "Done with gradient descent at iteration  241\n",
      "Learned weights =  [  2.37193538 124.57358324]\n",
      "Done with gradient descent at iteration  242\n",
      "Learned weights =  [  2.38167902 124.57358324]\n",
      "Done with gradient descent at iteration  242\n",
      "Learned weights =  [  2.38167902 124.57358138]\n",
      "Done with gradient descent at iteration  243\n",
      "Learned weights =  [  2.39142267 124.57358138]\n",
      "Done with gradient descent at iteration  243\n",
      "Learned weights =  [  2.39142267 124.57357953]\n",
      "Done with gradient descent at iteration  244\n",
      "Learned weights =  [  2.40116631 124.57357953]\n",
      "Done with gradient descent at iteration  244\n",
      "Learned weights =  [  2.40116631 124.57357768]\n",
      "Done with gradient descent at iteration  245\n",
      "Learned weights =  [  2.41090996 124.57357768]\n",
      "Done with gradient descent at iteration  245\n",
      "Learned weights =  [  2.41090996 124.57357582]\n",
      "Done with gradient descent at iteration  246\n",
      "Learned weights =  [  2.4206536  124.57357582]\n",
      "Done with gradient descent at iteration  246\n",
      "Learned weights =  [  2.4206536  124.57357397]\n",
      "Done with gradient descent at iteration  247\n",
      "Learned weights =  [  2.43039725 124.57357397]\n",
      "Done with gradient descent at iteration  247\n",
      "Learned weights =  [  2.43039725 124.57357211]\n",
      "Done with gradient descent at iteration  248\n",
      "Learned weights =  [  2.44014089 124.57357211]\n",
      "Done with gradient descent at iteration  248\n",
      "Learned weights =  [  2.44014089 124.57357026]\n",
      "Done with gradient descent at iteration  249\n",
      "Learned weights =  [  2.44988454 124.57357026]\n",
      "Done with gradient descent at iteration  249\n",
      "Learned weights =  [  2.44988454 124.5735684 ]\n",
      "Done with gradient descent at iteration  250\n",
      "Learned weights =  [  2.45962818 124.5735684 ]\n",
      "Done with gradient descent at iteration  250\n",
      "Learned weights =  [  2.45962818 124.57356655]\n",
      "Done with gradient descent at iteration  251\n",
      "Learned weights =  [  2.46937183 124.57356655]\n",
      "Done with gradient descent at iteration  251\n",
      "Learned weights =  [  2.46937183 124.57356469]\n",
      "Done with gradient descent at iteration  252\n",
      "Learned weights =  [  2.47911547 124.57356469]\n",
      "Done with gradient descent at iteration  252\n",
      "Learned weights =  [  2.47911547 124.57356284]\n",
      "Done with gradient descent at iteration  253\n",
      "Learned weights =  [  2.48885911 124.57356284]\n",
      "Done with gradient descent at iteration  253\n",
      "Learned weights =  [  2.48885911 124.57356098]\n",
      "Done with gradient descent at iteration  254\n",
      "Learned weights =  [  2.49860276 124.57356098]\n",
      "Done with gradient descent at iteration  254\n",
      "Learned weights =  [  2.49860276 124.57355913]\n",
      "Done with gradient descent at iteration  255\n",
      "Learned weights =  [  2.5083464  124.57355913]\n",
      "Done with gradient descent at iteration  255\n",
      "Learned weights =  [  2.5083464  124.57355728]\n",
      "Done with gradient descent at iteration  256\n",
      "Learned weights =  [  2.51809004 124.57355728]\n",
      "Done with gradient descent at iteration  256\n",
      "Learned weights =  [  2.51809004 124.57355542]\n",
      "Done with gradient descent at iteration  257\n",
      "Learned weights =  [  2.52783369 124.57355542]\n",
      "Done with gradient descent at iteration  257\n",
      "Learned weights =  [  2.52783369 124.57355357]\n",
      "Done with gradient descent at iteration  258\n",
      "Learned weights =  [  2.53757733 124.57355357]\n",
      "Done with gradient descent at iteration  258\n",
      "Learned weights =  [  2.53757733 124.57355171]\n",
      "Done with gradient descent at iteration  259\n",
      "Learned weights =  [  2.54732097 124.57355171]\n",
      "Done with gradient descent at iteration  259\n",
      "Learned weights =  [  2.54732097 124.57354986]\n",
      "Done with gradient descent at iteration  260\n",
      "Learned weights =  [  2.55706461 124.57354986]\n",
      "Done with gradient descent at iteration  260\n",
      "Learned weights =  [  2.55706461 124.573548  ]\n",
      "Done with gradient descent at iteration  261\n",
      "Learned weights =  [  2.56680826 124.573548  ]\n",
      "Done with gradient descent at iteration  261\n",
      "Learned weights =  [  2.56680826 124.57354615]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  262\n",
      "Learned weights =  [  2.5765519  124.57354615]\n",
      "Done with gradient descent at iteration  262\n",
      "Learned weights =  [  2.5765519  124.57354429]\n",
      "Done with gradient descent at iteration  263\n",
      "Learned weights =  [  2.58629554 124.57354429]\n",
      "Done with gradient descent at iteration  263\n",
      "Learned weights =  [  2.58629554 124.57354244]\n",
      "Done with gradient descent at iteration  264\n",
      "Learned weights =  [  2.59603918 124.57354244]\n",
      "Done with gradient descent at iteration  264\n",
      "Learned weights =  [  2.59603918 124.57354058]\n",
      "Done with gradient descent at iteration  265\n",
      "Learned weights =  [  2.60578282 124.57354058]\n",
      "Done with gradient descent at iteration  265\n",
      "Learned weights =  [  2.60578282 124.57353873]\n",
      "Done with gradient descent at iteration  266\n",
      "Learned weights =  [  2.61552646 124.57353873]\n",
      "Done with gradient descent at iteration  266\n",
      "Learned weights =  [  2.61552646 124.57353688]\n",
      "Done with gradient descent at iteration  267\n",
      "Learned weights =  [  2.62527011 124.57353688]\n",
      "Done with gradient descent at iteration  267\n",
      "Learned weights =  [  2.62527011 124.57353502]\n",
      "Done with gradient descent at iteration  268\n",
      "Learned weights =  [  2.63501375 124.57353502]\n",
      "Done with gradient descent at iteration  268\n",
      "Learned weights =  [  2.63501375 124.57353317]\n",
      "Done with gradient descent at iteration  269\n",
      "Learned weights =  [  2.64475739 124.57353317]\n",
      "Done with gradient descent at iteration  269\n",
      "Learned weights =  [  2.64475739 124.57353131]\n",
      "Done with gradient descent at iteration  270\n",
      "Learned weights =  [  2.65450103 124.57353131]\n",
      "Done with gradient descent at iteration  270\n",
      "Learned weights =  [  2.65450103 124.57352946]\n",
      "Done with gradient descent at iteration  271\n",
      "Learned weights =  [  2.66424467 124.57352946]\n",
      "Done with gradient descent at iteration  271\n",
      "Learned weights =  [  2.66424467 124.5735276 ]\n",
      "Done with gradient descent at iteration  272\n",
      "Learned weights =  [  2.67398831 124.5735276 ]\n",
      "Done with gradient descent at iteration  272\n",
      "Learned weights =  [  2.67398831 124.57352575]\n",
      "Done with gradient descent at iteration  273\n",
      "Learned weights =  [  2.68373195 124.57352575]\n",
      "Done with gradient descent at iteration  273\n",
      "Learned weights =  [  2.68373195 124.57352389]\n",
      "Done with gradient descent at iteration  274\n",
      "Learned weights =  [  2.69347559 124.57352389]\n",
      "Done with gradient descent at iteration  274\n",
      "Learned weights =  [  2.69347559 124.57352204]\n",
      "Done with gradient descent at iteration  275\n",
      "Learned weights =  [  2.70321923 124.57352204]\n",
      "Done with gradient descent at iteration  275\n",
      "Learned weights =  [  2.70321923 124.57352018]\n",
      "Done with gradient descent at iteration  276\n",
      "Learned weights =  [  2.71296286 124.57352018]\n",
      "Done with gradient descent at iteration  276\n",
      "Learned weights =  [  2.71296286 124.57351833]\n",
      "Done with gradient descent at iteration  277\n",
      "Learned weights =  [  2.7227065  124.57351833]\n",
      "Done with gradient descent at iteration  277\n",
      "Learned weights =  [  2.7227065  124.57351648]\n",
      "Done with gradient descent at iteration  278\n",
      "Learned weights =  [  2.73245014 124.57351648]\n",
      "Done with gradient descent at iteration  278\n",
      "Learned weights =  [  2.73245014 124.57351462]\n",
      "Done with gradient descent at iteration  279\n",
      "Learned weights =  [  2.74219378 124.57351462]\n",
      "Done with gradient descent at iteration  279\n",
      "Learned weights =  [  2.74219378 124.57351277]\n",
      "Done with gradient descent at iteration  280\n",
      "Learned weights =  [  2.75193742 124.57351277]\n",
      "Done with gradient descent at iteration  280\n",
      "Learned weights =  [  2.75193742 124.57351091]\n",
      "Done with gradient descent at iteration  281\n",
      "Learned weights =  [  2.76168106 124.57351091]\n",
      "Done with gradient descent at iteration  281\n",
      "Learned weights =  [  2.76168106 124.57350906]\n",
      "Done with gradient descent at iteration  282\n",
      "Learned weights =  [  2.77142469 124.57350906]\n",
      "Done with gradient descent at iteration  282\n",
      "Learned weights =  [  2.77142469 124.5735072 ]\n",
      "Done with gradient descent at iteration  283\n",
      "Learned weights =  [  2.78116833 124.5735072 ]\n",
      "Done with gradient descent at iteration  283\n",
      "Learned weights =  [  2.78116833 124.57350535]\n",
      "Done with gradient descent at iteration  284\n",
      "Learned weights =  [  2.79091197 124.57350535]\n",
      "Done with gradient descent at iteration  284\n",
      "Learned weights =  [  2.79091197 124.57350349]\n",
      "Done with gradient descent at iteration  285\n",
      "Learned weights =  [  2.80065561 124.57350349]\n",
      "Done with gradient descent at iteration  285\n",
      "Learned weights =  [  2.80065561 124.57350164]\n",
      "Done with gradient descent at iteration  286\n",
      "Learned weights =  [  2.81039924 124.57350164]\n",
      "Done with gradient descent at iteration  286\n",
      "Learned weights =  [  2.81039924 124.57349978]\n",
      "Done with gradient descent at iteration  287\n",
      "Learned weights =  [  2.82014288 124.57349978]\n",
      "Done with gradient descent at iteration  287\n",
      "Learned weights =  [  2.82014288 124.57349793]\n",
      "Done with gradient descent at iteration  288\n",
      "Learned weights =  [  2.82988652 124.57349793]\n",
      "Done with gradient descent at iteration  288\n",
      "Learned weights =  [  2.82988652 124.57349608]\n",
      "Done with gradient descent at iteration  289\n",
      "Learned weights =  [  2.83963015 124.57349608]\n",
      "Done with gradient descent at iteration  289\n",
      "Learned weights =  [  2.83963015 124.57349422]\n",
      "Done with gradient descent at iteration  290\n",
      "Learned weights =  [  2.84937379 124.57349422]\n",
      "Done with gradient descent at iteration  290\n",
      "Learned weights =  [  2.84937379 124.57349237]\n",
      "Done with gradient descent at iteration  291\n",
      "Learned weights =  [  2.85911743 124.57349237]\n",
      "Done with gradient descent at iteration  291\n",
      "Learned weights =  [  2.85911743 124.57349051]\n",
      "Done with gradient descent at iteration  292\n",
      "Learned weights =  [  2.86886106 124.57349051]\n",
      "Done with gradient descent at iteration  292\n",
      "Learned weights =  [  2.86886106 124.57348866]\n",
      "Done with gradient descent at iteration  293\n",
      "Learned weights =  [  2.8786047  124.57348866]\n",
      "Done with gradient descent at iteration  293\n",
      "Learned weights =  [  2.8786047 124.5734868]\n",
      "Done with gradient descent at iteration  294\n",
      "Learned weights =  [  2.88834833 124.5734868 ]\n",
      "Done with gradient descent at iteration  294\n",
      "Learned weights =  [  2.88834833 124.57348495]\n",
      "Done with gradient descent at iteration  295\n",
      "Learned weights =  [  2.89809197 124.57348495]\n",
      "Done with gradient descent at iteration  295\n",
      "Learned weights =  [  2.89809197 124.57348309]\n",
      "Done with gradient descent at iteration  296\n",
      "Learned weights =  [  2.9078356  124.57348309]\n",
      "Done with gradient descent at iteration  296\n",
      "Learned weights =  [  2.9078356  124.57348124]\n",
      "Done with gradient descent at iteration  297\n",
      "Learned weights =  [  2.91757924 124.57348124]\n",
      "Done with gradient descent at iteration  297\n",
      "Learned weights =  [  2.91757924 124.57347938]\n",
      "Done with gradient descent at iteration  298\n",
      "Learned weights =  [  2.92732287 124.57347938]\n",
      "Done with gradient descent at iteration  298\n",
      "Learned weights =  [  2.92732287 124.57347753]\n",
      "Done with gradient descent at iteration  299\n",
      "Learned weights =  [  2.93706651 124.57347753]\n",
      "Done with gradient descent at iteration  299\n",
      "Learned weights =  [  2.93706651 124.57347568]\n",
      "Iteration = 300\n",
      "Cost function =  4484822246960036.0\n",
      "Done with gradient descent at iteration  300\n",
      "Learned weights =  [  2.94681014 124.57347568]\n",
      "Done with gradient descent at iteration  300\n",
      "Learned weights =  [  2.94681014 124.57347382]\n",
      "Done with gradient descent at iteration  301\n",
      "Learned weights =  [  2.95655377 124.57347382]\n",
      "Done with gradient descent at iteration  301\n",
      "Learned weights =  [  2.95655377 124.57347197]\n",
      "Done with gradient descent at iteration  302\n",
      "Learned weights =  [  2.96629741 124.57347197]\n",
      "Done with gradient descent at iteration  302\n",
      "Learned weights =  [  2.96629741 124.57347011]\n",
      "Done with gradient descent at iteration  303\n",
      "Learned weights =  [  2.97604104 124.57347011]\n",
      "Done with gradient descent at iteration  303\n",
      "Learned weights =  [  2.97604104 124.57346826]\n",
      "Done with gradient descent at iteration  304\n",
      "Learned weights =  [  2.98578467 124.57346826]\n",
      "Done with gradient descent at iteration  304\n",
      "Learned weights =  [  2.98578467 124.5734664 ]\n",
      "Done with gradient descent at iteration  305\n",
      "Learned weights =  [  2.99552831 124.5734664 ]\n",
      "Done with gradient descent at iteration  305\n",
      "Learned weights =  [  2.99552831 124.57346455]\n",
      "Done with gradient descent at iteration  306\n",
      "Learned weights =  [  3.00527194 124.57346455]\n",
      "Done with gradient descent at iteration  306\n",
      "Learned weights =  [  3.00527194 124.57346269]\n",
      "Done with gradient descent at iteration  307\n",
      "Learned weights =  [  3.01501557 124.57346269]\n",
      "Done with gradient descent at iteration  307\n",
      "Learned weights =  [  3.01501557 124.57346084]\n",
      "Done with gradient descent at iteration  308\n",
      "Learned weights =  [  3.02475921 124.57346084]\n",
      "Done with gradient descent at iteration  308\n",
      "Learned weights =  [  3.02475921 124.57345898]\n",
      "Done with gradient descent at iteration  309\n",
      "Learned weights =  [  3.03450284 124.57345898]\n",
      "Done with gradient descent at iteration  309\n",
      "Learned weights =  [  3.03450284 124.57345713]\n",
      "Done with gradient descent at iteration  310\n",
      "Learned weights =  [  3.04424647 124.57345713]\n",
      "Done with gradient descent at iteration  310\n",
      "Learned weights =  [  3.04424647 124.57345528]\n",
      "Done with gradient descent at iteration  311\n",
      "Learned weights =  [  3.0539901  124.57345528]\n",
      "Done with gradient descent at iteration  311\n",
      "Learned weights =  [  3.0539901  124.57345342]\n",
      "Done with gradient descent at iteration  312\n",
      "Learned weights =  [  3.06373373 124.57345342]\n",
      "Done with gradient descent at iteration  312\n",
      "Learned weights =  [  3.06373373 124.57345157]\n",
      "Done with gradient descent at iteration  313\n",
      "Learned weights =  [  3.07347737 124.57345157]\n",
      "Done with gradient descent at iteration  313\n",
      "Learned weights =  [  3.07347737 124.57344971]\n",
      "Done with gradient descent at iteration  314\n",
      "Learned weights =  [  3.083221   124.57344971]\n",
      "Done with gradient descent at iteration  314\n",
      "Learned weights =  [  3.083221   124.57344786]\n",
      "Done with gradient descent at iteration  315\n",
      "Learned weights =  [  3.09296463 124.57344786]\n",
      "Done with gradient descent at iteration  315\n",
      "Learned weights =  [  3.09296463 124.573446  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  316\n",
      "Learned weights =  [  3.10270826 124.573446  ]\n",
      "Done with gradient descent at iteration  316\n",
      "Learned weights =  [  3.10270826 124.57344415]\n",
      "Done with gradient descent at iteration  317\n",
      "Learned weights =  [  3.11245189 124.57344415]\n",
      "Done with gradient descent at iteration  317\n",
      "Learned weights =  [  3.11245189 124.57344229]\n",
      "Done with gradient descent at iteration  318\n",
      "Learned weights =  [  3.12219552 124.57344229]\n",
      "Done with gradient descent at iteration  318\n",
      "Learned weights =  [  3.12219552 124.57344044]\n",
      "Done with gradient descent at iteration  319\n",
      "Learned weights =  [  3.13193915 124.57344044]\n",
      "Done with gradient descent at iteration  319\n",
      "Learned weights =  [  3.13193915 124.57343858]\n",
      "Done with gradient descent at iteration  320\n",
      "Learned weights =  [  3.14168278 124.57343858]\n",
      "Done with gradient descent at iteration  320\n",
      "Learned weights =  [  3.14168278 124.57343673]\n",
      "Done with gradient descent at iteration  321\n",
      "Learned weights =  [  3.15142641 124.57343673]\n",
      "Done with gradient descent at iteration  321\n",
      "Learned weights =  [  3.15142641 124.57343488]\n",
      "Done with gradient descent at iteration  322\n",
      "Learned weights =  [  3.16117004 124.57343488]\n",
      "Done with gradient descent at iteration  322\n",
      "Learned weights =  [  3.16117004 124.57343302]\n",
      "Done with gradient descent at iteration  323\n",
      "Learned weights =  [  3.17091367 124.57343302]\n",
      "Done with gradient descent at iteration  323\n",
      "Learned weights =  [  3.17091367 124.57343117]\n",
      "Done with gradient descent at iteration  324\n",
      "Learned weights =  [  3.1806573  124.57343117]\n",
      "Done with gradient descent at iteration  324\n",
      "Learned weights =  [  3.1806573  124.57342931]\n",
      "Done with gradient descent at iteration  325\n",
      "Learned weights =  [  3.19040093 124.57342931]\n",
      "Done with gradient descent at iteration  325\n",
      "Learned weights =  [  3.19040093 124.57342746]\n",
      "Done with gradient descent at iteration  326\n",
      "Learned weights =  [  3.20014456 124.57342746]\n",
      "Done with gradient descent at iteration  326\n",
      "Learned weights =  [  3.20014456 124.5734256 ]\n",
      "Done with gradient descent at iteration  327\n",
      "Learned weights =  [  3.20988818 124.5734256 ]\n",
      "Done with gradient descent at iteration  327\n",
      "Learned weights =  [  3.20988818 124.57342375]\n",
      "Done with gradient descent at iteration  328\n",
      "Learned weights =  [  3.21963181 124.57342375]\n",
      "Done with gradient descent at iteration  328\n",
      "Learned weights =  [  3.21963181 124.57342189]\n",
      "Done with gradient descent at iteration  329\n",
      "Learned weights =  [  3.22937544 124.57342189]\n",
      "Done with gradient descent at iteration  329\n",
      "Learned weights =  [  3.22937544 124.57342004]\n",
      "Done with gradient descent at iteration  330\n",
      "Learned weights =  [  3.23911907 124.57342004]\n",
      "Done with gradient descent at iteration  330\n",
      "Learned weights =  [  3.23911907 124.57341818]\n",
      "Done with gradient descent at iteration  331\n",
      "Learned weights =  [  3.2488627  124.57341818]\n",
      "Done with gradient descent at iteration  331\n",
      "Learned weights =  [  3.2488627  124.57341633]\n",
      "Done with gradient descent at iteration  332\n",
      "Learned weights =  [  3.25860632 124.57341633]\n",
      "Done with gradient descent at iteration  332\n",
      "Learned weights =  [  3.25860632 124.57341448]\n",
      "Done with gradient descent at iteration  333\n",
      "Learned weights =  [  3.26834995 124.57341448]\n",
      "Done with gradient descent at iteration  333\n",
      "Learned weights =  [  3.26834995 124.57341262]\n",
      "Done with gradient descent at iteration  334\n",
      "Learned weights =  [  3.27809358 124.57341262]\n",
      "Done with gradient descent at iteration  334\n",
      "Learned weights =  [  3.27809358 124.57341077]\n",
      "Done with gradient descent at iteration  335\n",
      "Learned weights =  [  3.28783721 124.57341077]\n",
      "Done with gradient descent at iteration  335\n",
      "Learned weights =  [  3.28783721 124.57340891]\n",
      "Done with gradient descent at iteration  336\n",
      "Learned weights =  [  3.29758083 124.57340891]\n",
      "Done with gradient descent at iteration  336\n",
      "Learned weights =  [  3.29758083 124.57340706]\n",
      "Done with gradient descent at iteration  337\n",
      "Learned weights =  [  3.30732446 124.57340706]\n",
      "Done with gradient descent at iteration  337\n",
      "Learned weights =  [  3.30732446 124.5734052 ]\n",
      "Done with gradient descent at iteration  338\n",
      "Learned weights =  [  3.31706809 124.5734052 ]\n",
      "Done with gradient descent at iteration  338\n",
      "Learned weights =  [  3.31706809 124.57340335]\n",
      "Done with gradient descent at iteration  339\n",
      "Learned weights =  [  3.32681171 124.57340335]\n",
      "Done with gradient descent at iteration  339\n",
      "Learned weights =  [  3.32681171 124.57340149]\n",
      "Done with gradient descent at iteration  340\n",
      "Learned weights =  [  3.33655534 124.57340149]\n",
      "Done with gradient descent at iteration  340\n",
      "Learned weights =  [  3.33655534 124.57339964]\n",
      "Done with gradient descent at iteration  341\n",
      "Learned weights =  [  3.34629896 124.57339964]\n",
      "Done with gradient descent at iteration  341\n",
      "Learned weights =  [  3.34629896 124.57339778]\n",
      "Done with gradient descent at iteration  342\n",
      "Learned weights =  [  3.35604259 124.57339778]\n",
      "Done with gradient descent at iteration  342\n",
      "Learned weights =  [  3.35604259 124.57339593]\n",
      "Done with gradient descent at iteration  343\n",
      "Learned weights =  [  3.36578622 124.57339593]\n",
      "Done with gradient descent at iteration  343\n",
      "Learned weights =  [  3.36578622 124.57339407]\n",
      "Done with gradient descent at iteration  344\n",
      "Learned weights =  [  3.37552984 124.57339407]\n",
      "Done with gradient descent at iteration  344\n",
      "Learned weights =  [  3.37552984 124.57339222]\n",
      "Done with gradient descent at iteration  345\n",
      "Learned weights =  [  3.38527347 124.57339222]\n",
      "Done with gradient descent at iteration  345\n",
      "Learned weights =  [  3.38527347 124.57339037]\n",
      "Done with gradient descent at iteration  346\n",
      "Learned weights =  [  3.39501709 124.57339037]\n",
      "Done with gradient descent at iteration  346\n",
      "Learned weights =  [  3.39501709 124.57338851]\n",
      "Done with gradient descent at iteration  347\n",
      "Learned weights =  [  3.40476071 124.57338851]\n",
      "Done with gradient descent at iteration  347\n",
      "Learned weights =  [  3.40476071 124.57338666]\n",
      "Done with gradient descent at iteration  348\n",
      "Learned weights =  [  3.41450434 124.57338666]\n",
      "Done with gradient descent at iteration  348\n",
      "Learned weights =  [  3.41450434 124.5733848 ]\n",
      "Done with gradient descent at iteration  349\n",
      "Learned weights =  [  3.42424796 124.5733848 ]\n",
      "Done with gradient descent at iteration  349\n",
      "Learned weights =  [  3.42424796 124.57338295]\n",
      "Done with gradient descent at iteration  350\n",
      "Learned weights =  [  3.43399159 124.57338295]\n",
      "Done with gradient descent at iteration  350\n",
      "Learned weights =  [  3.43399159 124.57338109]\n",
      "Done with gradient descent at iteration  351\n",
      "Learned weights =  [  3.44373521 124.57338109]\n",
      "Done with gradient descent at iteration  351\n",
      "Learned weights =  [  3.44373521 124.57337924]\n",
      "Done with gradient descent at iteration  352\n",
      "Learned weights =  [  3.45347883 124.57337924]\n",
      "Done with gradient descent at iteration  352\n",
      "Learned weights =  [  3.45347883 124.57337738]\n",
      "Done with gradient descent at iteration  353\n",
      "Learned weights =  [  3.46322246 124.57337738]\n",
      "Done with gradient descent at iteration  353\n",
      "Learned weights =  [  3.46322246 124.57337553]\n",
      "Done with gradient descent at iteration  354\n",
      "Learned weights =  [  3.47296608 124.57337553]\n",
      "Done with gradient descent at iteration  354\n",
      "Learned weights =  [  3.47296608 124.57337367]\n",
      "Done with gradient descent at iteration  355\n",
      "Learned weights =  [  3.4827097  124.57337367]\n",
      "Done with gradient descent at iteration  355\n",
      "Learned weights =  [  3.4827097  124.57337182]\n",
      "Done with gradient descent at iteration  356\n",
      "Learned weights =  [  3.49245333 124.57337182]\n",
      "Done with gradient descent at iteration  356\n",
      "Learned weights =  [  3.49245333 124.57336997]\n",
      "Done with gradient descent at iteration  357\n",
      "Learned weights =  [  3.50219695 124.57336997]\n",
      "Done with gradient descent at iteration  357\n",
      "Learned weights =  [  3.50219695 124.57336811]\n",
      "Done with gradient descent at iteration  358\n",
      "Learned weights =  [  3.51194057 124.57336811]\n",
      "Done with gradient descent at iteration  358\n",
      "Learned weights =  [  3.51194057 124.57336626]\n",
      "Done with gradient descent at iteration  359\n",
      "Learned weights =  [  3.52168419 124.57336626]\n",
      "Done with gradient descent at iteration  359\n",
      "Learned weights =  [  3.52168419 124.5733644 ]\n",
      "Done with gradient descent at iteration  360\n",
      "Learned weights =  [  3.53142781 124.5733644 ]\n",
      "Done with gradient descent at iteration  360\n",
      "Learned weights =  [  3.53142781 124.57336255]\n",
      "Done with gradient descent at iteration  361\n",
      "Learned weights =  [  3.54117144 124.57336255]\n",
      "Done with gradient descent at iteration  361\n",
      "Learned weights =  [  3.54117144 124.57336069]\n",
      "Done with gradient descent at iteration  362\n",
      "Learned weights =  [  3.55091506 124.57336069]\n",
      "Done with gradient descent at iteration  362\n",
      "Learned weights =  [  3.55091506 124.57335884]\n",
      "Done with gradient descent at iteration  363\n",
      "Learned weights =  [  3.56065868 124.57335884]\n",
      "Done with gradient descent at iteration  363\n",
      "Learned weights =  [  3.56065868 124.57335698]\n",
      "Done with gradient descent at iteration  364\n",
      "Learned weights =  [  3.5704023  124.57335698]\n",
      "Done with gradient descent at iteration  364\n",
      "Learned weights =  [  3.5704023  124.57335513]\n",
      "Done with gradient descent at iteration  365\n",
      "Learned weights =  [  3.58014592 124.57335513]\n",
      "Done with gradient descent at iteration  365\n",
      "Learned weights =  [  3.58014592 124.57335327]\n",
      "Done with gradient descent at iteration  366\n",
      "Learned weights =  [  3.58988954 124.57335327]\n",
      "Done with gradient descent at iteration  366\n",
      "Learned weights =  [  3.58988954 124.57335142]\n",
      "Done with gradient descent at iteration  367\n",
      "Learned weights =  [  3.59963316 124.57335142]\n",
      "Done with gradient descent at iteration  367\n",
      "Learned weights =  [  3.59963316 124.57334957]\n",
      "Done with gradient descent at iteration  368\n",
      "Learned weights =  [  3.60937678 124.57334957]\n",
      "Done with gradient descent at iteration  368\n",
      "Learned weights =  [  3.60937678 124.57334771]\n",
      "Done with gradient descent at iteration  369\n",
      "Learned weights =  [  3.6191204  124.57334771]\n",
      "Done with gradient descent at iteration  369\n",
      "Learned weights =  [  3.6191204  124.57334586]\n",
      "Done with gradient descent at iteration  370\n",
      "Learned weights =  [  3.62886402 124.57334586]\n",
      "Done with gradient descent at iteration  370\n",
      "Learned weights =  [  3.62886402 124.573344  ]\n",
      "Done with gradient descent at iteration  371\n",
      "Learned weights =  [  3.63860764 124.573344  ]\n",
      "Done with gradient descent at iteration  371\n",
      "Learned weights =  [  3.63860764 124.57334215]\n",
      "Done with gradient descent at iteration  372\n",
      "Learned weights =  [  3.64835126 124.57334215]\n",
      "Done with gradient descent at iteration  372\n",
      "Learned weights =  [  3.64835126 124.57334029]\n",
      "Done with gradient descent at iteration  373\n",
      "Learned weights =  [  3.65809488 124.57334029]\n",
      "Done with gradient descent at iteration  373\n",
      "Learned weights =  [  3.65809488 124.57333844]\n",
      "Done with gradient descent at iteration  374\n",
      "Learned weights =  [  3.6678385  124.57333844]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  374\n",
      "Learned weights =  [  3.6678385  124.57333658]\n",
      "Done with gradient descent at iteration  375\n",
      "Learned weights =  [  3.67758212 124.57333658]\n",
      "Done with gradient descent at iteration  375\n",
      "Learned weights =  [  3.67758212 124.57333473]\n",
      "Done with gradient descent at iteration  376\n",
      "Learned weights =  [  3.68732574 124.57333473]\n",
      "Done with gradient descent at iteration  376\n",
      "Learned weights =  [  3.68732574 124.57333287]\n",
      "Done with gradient descent at iteration  377\n",
      "Learned weights =  [  3.69706936 124.57333287]\n",
      "Done with gradient descent at iteration  377\n",
      "Learned weights =  [  3.69706936 124.57333102]\n",
      "Done with gradient descent at iteration  378\n",
      "Learned weights =  [  3.70681297 124.57333102]\n",
      "Done with gradient descent at iteration  378\n",
      "Learned weights =  [  3.70681297 124.57332917]\n",
      "Done with gradient descent at iteration  379\n",
      "Learned weights =  [  3.71655659 124.57332917]\n",
      "Done with gradient descent at iteration  379\n",
      "Learned weights =  [  3.71655659 124.57332731]\n",
      "Done with gradient descent at iteration  380\n",
      "Learned weights =  [  3.72630021 124.57332731]\n",
      "Done with gradient descent at iteration  380\n",
      "Learned weights =  [  3.72630021 124.57332546]\n",
      "Done with gradient descent at iteration  381\n",
      "Learned weights =  [  3.73604383 124.57332546]\n",
      "Done with gradient descent at iteration  381\n",
      "Learned weights =  [  3.73604383 124.5733236 ]\n",
      "Done with gradient descent at iteration  382\n",
      "Learned weights =  [  3.74578744 124.5733236 ]\n",
      "Done with gradient descent at iteration  382\n",
      "Learned weights =  [  3.74578744 124.57332175]\n",
      "Done with gradient descent at iteration  383\n",
      "Learned weights =  [  3.75553106 124.57332175]\n",
      "Done with gradient descent at iteration  383\n",
      "Learned weights =  [  3.75553106 124.57331989]\n",
      "Done with gradient descent at iteration  384\n",
      "Learned weights =  [  3.76527468 124.57331989]\n",
      "Done with gradient descent at iteration  384\n",
      "Learned weights =  [  3.76527468 124.57331804]\n",
      "Done with gradient descent at iteration  385\n",
      "Learned weights =  [  3.77501829 124.57331804]\n",
      "Done with gradient descent at iteration  385\n",
      "Learned weights =  [  3.77501829 124.57331618]\n",
      "Done with gradient descent at iteration  386\n",
      "Learned weights =  [  3.78476191 124.57331618]\n",
      "Done with gradient descent at iteration  386\n",
      "Learned weights =  [  3.78476191 124.57331433]\n",
      "Done with gradient descent at iteration  387\n",
      "Learned weights =  [  3.79450553 124.57331433]\n",
      "Done with gradient descent at iteration  387\n",
      "Learned weights =  [  3.79450553 124.57331247]\n",
      "Done with gradient descent at iteration  388\n",
      "Learned weights =  [  3.80424914 124.57331247]\n",
      "Done with gradient descent at iteration  388\n",
      "Learned weights =  [  3.80424914 124.57331062]\n",
      "Done with gradient descent at iteration  389\n",
      "Learned weights =  [  3.81399276 124.57331062]\n",
      "Done with gradient descent at iteration  389\n",
      "Learned weights =  [  3.81399276 124.57330877]\n",
      "Done with gradient descent at iteration  390\n",
      "Learned weights =  [  3.82373638 124.57330877]\n",
      "Done with gradient descent at iteration  390\n",
      "Learned weights =  [  3.82373638 124.57330691]\n",
      "Done with gradient descent at iteration  391\n",
      "Learned weights =  [  3.83347999 124.57330691]\n",
      "Done with gradient descent at iteration  391\n",
      "Learned weights =  [  3.83347999 124.57330506]\n",
      "Done with gradient descent at iteration  392\n",
      "Learned weights =  [  3.84322361 124.57330506]\n",
      "Done with gradient descent at iteration  392\n",
      "Learned weights =  [  3.84322361 124.5733032 ]\n",
      "Done with gradient descent at iteration  393\n",
      "Learned weights =  [  3.85296722 124.5733032 ]\n",
      "Done with gradient descent at iteration  393\n",
      "Learned weights =  [  3.85296722 124.57330135]\n",
      "Done with gradient descent at iteration  394\n",
      "Learned weights =  [  3.86271084 124.57330135]\n",
      "Done with gradient descent at iteration  394\n",
      "Learned weights =  [  3.86271084 124.57329949]\n",
      "Done with gradient descent at iteration  395\n",
      "Learned weights =  [  3.87245445 124.57329949]\n",
      "Done with gradient descent at iteration  395\n",
      "Learned weights =  [  3.87245445 124.57329764]\n",
      "Done with gradient descent at iteration  396\n",
      "Learned weights =  [  3.88219807 124.57329764]\n",
      "Done with gradient descent at iteration  396\n",
      "Learned weights =  [  3.88219807 124.57329578]\n",
      "Done with gradient descent at iteration  397\n",
      "Learned weights =  [  3.89194168 124.57329578]\n",
      "Done with gradient descent at iteration  397\n",
      "Learned weights =  [  3.89194168 124.57329393]\n",
      "Done with gradient descent at iteration  398\n",
      "Learned weights =  [  3.90168529 124.57329393]\n",
      "Done with gradient descent at iteration  398\n",
      "Learned weights =  [  3.90168529 124.57329207]\n",
      "Done with gradient descent at iteration  399\n",
      "Learned weights =  [  3.91142891 124.57329207]\n",
      "Done with gradient descent at iteration  399\n",
      "Learned weights =  [  3.91142891 124.57329022]\n",
      "Iteration = 400\n",
      "Cost function =  4484812753138890.0\n",
      "Done with gradient descent at iteration  400\n",
      "Learned weights =  [  3.92117252 124.57329022]\n",
      "Done with gradient descent at iteration  400\n",
      "Learned weights =  [  3.92117252 124.57328837]\n",
      "Done with gradient descent at iteration  401\n",
      "Learned weights =  [  3.93091614 124.57328837]\n",
      "Done with gradient descent at iteration  401\n",
      "Learned weights =  [  3.93091614 124.57328651]\n",
      "Done with gradient descent at iteration  402\n",
      "Learned weights =  [  3.94065975 124.57328651]\n",
      "Done with gradient descent at iteration  402\n",
      "Learned weights =  [  3.94065975 124.57328466]\n",
      "Done with gradient descent at iteration  403\n",
      "Learned weights =  [  3.95040336 124.57328466]\n",
      "Done with gradient descent at iteration  403\n",
      "Learned weights =  [  3.95040336 124.5732828 ]\n",
      "Done with gradient descent at iteration  404\n",
      "Learned weights =  [  3.96014697 124.5732828 ]\n",
      "Done with gradient descent at iteration  404\n",
      "Learned weights =  [  3.96014697 124.57328095]\n",
      "Done with gradient descent at iteration  405\n",
      "Learned weights =  [  3.96989059 124.57328095]\n",
      "Done with gradient descent at iteration  405\n",
      "Learned weights =  [  3.96989059 124.57327909]\n",
      "Done with gradient descent at iteration  406\n",
      "Learned weights =  [  3.9796342  124.57327909]\n",
      "Done with gradient descent at iteration  406\n",
      "Learned weights =  [  3.9796342  124.57327724]\n",
      "Done with gradient descent at iteration  407\n",
      "Learned weights =  [  3.98937781 124.57327724]\n",
      "Done with gradient descent at iteration  407\n",
      "Learned weights =  [  3.98937781 124.57327538]\n",
      "Done with gradient descent at iteration  408\n",
      "Learned weights =  [  3.99912142 124.57327538]\n",
      "Done with gradient descent at iteration  408\n",
      "Learned weights =  [  3.99912142 124.57327353]\n",
      "Done with gradient descent at iteration  409\n",
      "Learned weights =  [  4.00886504 124.57327353]\n",
      "Done with gradient descent at iteration  409\n",
      "Learned weights =  [  4.00886504 124.57327167]\n",
      "Done with gradient descent at iteration  410\n",
      "Learned weights =  [  4.01860865 124.57327167]\n",
      "Done with gradient descent at iteration  410\n",
      "Learned weights =  [  4.01860865 124.57326982]\n",
      "Done with gradient descent at iteration  411\n",
      "Learned weights =  [  4.02835226 124.57326982]\n",
      "Done with gradient descent at iteration  411\n",
      "Learned weights =  [  4.02835226 124.57326797]\n",
      "Done with gradient descent at iteration  412\n",
      "Learned weights =  [  4.03809587 124.57326797]\n",
      "Done with gradient descent at iteration  412\n",
      "Learned weights =  [  4.03809587 124.57326611]\n",
      "Done with gradient descent at iteration  413\n",
      "Learned weights =  [  4.04783948 124.57326611]\n",
      "Done with gradient descent at iteration  413\n",
      "Learned weights =  [  4.04783948 124.57326426]\n",
      "Done with gradient descent at iteration  414\n",
      "Learned weights =  [  4.05758309 124.57326426]\n",
      "Done with gradient descent at iteration  414\n",
      "Learned weights =  [  4.05758309 124.5732624 ]\n",
      "Done with gradient descent at iteration  415\n",
      "Learned weights =  [  4.0673267 124.5732624]\n",
      "Done with gradient descent at iteration  415\n",
      "Learned weights =  [  4.0673267  124.57326055]\n",
      "Done with gradient descent at iteration  416\n",
      "Learned weights =  [  4.07707031 124.57326055]\n",
      "Done with gradient descent at iteration  416\n",
      "Learned weights =  [  4.07707031 124.57325869]\n",
      "Done with gradient descent at iteration  417\n",
      "Learned weights =  [  4.08681392 124.57325869]\n",
      "Done with gradient descent at iteration  417\n",
      "Learned weights =  [  4.08681392 124.57325684]\n",
      "Done with gradient descent at iteration  418\n",
      "Learned weights =  [  4.09655753 124.57325684]\n",
      "Done with gradient descent at iteration  418\n",
      "Learned weights =  [  4.09655753 124.57325498]\n",
      "Done with gradient descent at iteration  419\n",
      "Learned weights =  [  4.10630114 124.57325498]\n",
      "Done with gradient descent at iteration  419\n",
      "Learned weights =  [  4.10630114 124.57325313]\n",
      "Done with gradient descent at iteration  420\n",
      "Learned weights =  [  4.11604475 124.57325313]\n",
      "Done with gradient descent at iteration  420\n",
      "Learned weights =  [  4.11604475 124.57325128]\n",
      "Done with gradient descent at iteration  421\n",
      "Learned weights =  [  4.12578836 124.57325128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  421\n",
      "Learned weights =  [  4.12578836 124.57324942]\n",
      "Done with gradient descent at iteration  422\n",
      "Learned weights =  [  4.13553197 124.57324942]\n",
      "Done with gradient descent at iteration  422\n",
      "Learned weights =  [  4.13553197 124.57324757]\n",
      "Done with gradient descent at iteration  423\n",
      "Learned weights =  [  4.14527558 124.57324757]\n",
      "Done with gradient descent at iteration  423\n",
      "Learned weights =  [  4.14527558 124.57324571]\n",
      "Done with gradient descent at iteration  424\n",
      "Learned weights =  [  4.15501919 124.57324571]\n",
      "Done with gradient descent at iteration  424\n",
      "Learned weights =  [  4.15501919 124.57324386]\n",
      "Done with gradient descent at iteration  425\n",
      "Learned weights =  [  4.1647628  124.57324386]\n",
      "Done with gradient descent at iteration  425\n",
      "Learned weights =  [  4.1647628 124.573242 ]\n",
      "Done with gradient descent at iteration  426\n",
      "Learned weights =  [  4.17450641 124.573242  ]\n",
      "Done with gradient descent at iteration  426\n",
      "Learned weights =  [  4.17450641 124.57324015]\n",
      "Done with gradient descent at iteration  427\n",
      "Learned weights =  [  4.18425001 124.57324015]\n",
      "Done with gradient descent at iteration  427\n",
      "Learned weights =  [  4.18425001 124.57323829]\n",
      "Done with gradient descent at iteration  428\n",
      "Learned weights =  [  4.19399362 124.57323829]\n",
      "Done with gradient descent at iteration  428\n",
      "Learned weights =  [  4.19399362 124.57323644]\n",
      "Done with gradient descent at iteration  429\n",
      "Learned weights =  [  4.20373723 124.57323644]\n",
      "Done with gradient descent at iteration  429\n",
      "Learned weights =  [  4.20373723 124.57323458]\n",
      "Done with gradient descent at iteration  430\n",
      "Learned weights =  [  4.21348084 124.57323458]\n",
      "Done with gradient descent at iteration  430\n",
      "Learned weights =  [  4.21348084 124.57323273]\n",
      "Done with gradient descent at iteration  431\n",
      "Learned weights =  [  4.22322444 124.57323273]\n",
      "Done with gradient descent at iteration  431\n",
      "Learned weights =  [  4.22322444 124.57323088]\n",
      "Done with gradient descent at iteration  432\n",
      "Learned weights =  [  4.23296805 124.57323088]\n",
      "Done with gradient descent at iteration  432\n",
      "Learned weights =  [  4.23296805 124.57322902]\n",
      "Done with gradient descent at iteration  433\n",
      "Learned weights =  [  4.24271166 124.57322902]\n",
      "Done with gradient descent at iteration  433\n",
      "Learned weights =  [  4.24271166 124.57322717]\n",
      "Done with gradient descent at iteration  434\n",
      "Learned weights =  [  4.25245527 124.57322717]\n",
      "Done with gradient descent at iteration  434\n",
      "Learned weights =  [  4.25245527 124.57322531]\n",
      "Done with gradient descent at iteration  435\n",
      "Learned weights =  [  4.26219887 124.57322531]\n",
      "Done with gradient descent at iteration  435\n",
      "Learned weights =  [  4.26219887 124.57322346]\n",
      "Done with gradient descent at iteration  436\n",
      "Learned weights =  [  4.27194248 124.57322346]\n",
      "Done with gradient descent at iteration  436\n",
      "Learned weights =  [  4.27194248 124.5732216 ]\n",
      "Done with gradient descent at iteration  437\n",
      "Learned weights =  [  4.28168608 124.5732216 ]\n",
      "Done with gradient descent at iteration  437\n",
      "Learned weights =  [  4.28168608 124.57321975]\n",
      "Done with gradient descent at iteration  438\n",
      "Learned weights =  [  4.29142969 124.57321975]\n",
      "Done with gradient descent at iteration  438\n",
      "Learned weights =  [  4.29142969 124.57321789]\n",
      "Done with gradient descent at iteration  439\n",
      "Learned weights =  [  4.3011733  124.57321789]\n",
      "Done with gradient descent at iteration  439\n",
      "Learned weights =  [  4.3011733  124.57321604]\n",
      "Done with gradient descent at iteration  440\n",
      "Learned weights =  [  4.3109169  124.57321604]\n",
      "Done with gradient descent at iteration  440\n",
      "Learned weights =  [  4.3109169  124.57321418]\n",
      "Done with gradient descent at iteration  441\n",
      "Learned weights =  [  4.32066051 124.57321418]\n",
      "Done with gradient descent at iteration  441\n",
      "Learned weights =  [  4.32066051 124.57321233]\n",
      "Done with gradient descent at iteration  442\n",
      "Learned weights =  [  4.33040411 124.57321233]\n",
      "Done with gradient descent at iteration  442\n",
      "Learned weights =  [  4.33040411 124.57321048]\n",
      "Done with gradient descent at iteration  443\n",
      "Learned weights =  [  4.34014772 124.57321048]\n",
      "Done with gradient descent at iteration  443\n",
      "Learned weights =  [  4.34014772 124.57320862]\n",
      "Done with gradient descent at iteration  444\n",
      "Learned weights =  [  4.34989132 124.57320862]\n",
      "Done with gradient descent at iteration  444\n",
      "Learned weights =  [  4.34989132 124.57320677]\n",
      "Done with gradient descent at iteration  445\n",
      "Learned weights =  [  4.35963493 124.57320677]\n",
      "Done with gradient descent at iteration  445\n",
      "Learned weights =  [  4.35963493 124.57320491]\n",
      "Done with gradient descent at iteration  446\n",
      "Learned weights =  [  4.36937853 124.57320491]\n",
      "Done with gradient descent at iteration  446\n",
      "Learned weights =  [  4.36937853 124.57320306]\n",
      "Done with gradient descent at iteration  447\n",
      "Learned weights =  [  4.37912213 124.57320306]\n",
      "Done with gradient descent at iteration  447\n",
      "Learned weights =  [  4.37912213 124.5732012 ]\n",
      "Done with gradient descent at iteration  448\n",
      "Learned weights =  [  4.38886574 124.5732012 ]\n",
      "Done with gradient descent at iteration  448\n",
      "Learned weights =  [  4.38886574 124.57319935]\n",
      "Done with gradient descent at iteration  449\n",
      "Learned weights =  [  4.39860934 124.57319935]\n",
      "Done with gradient descent at iteration  449\n",
      "Learned weights =  [  4.39860934 124.57319749]\n",
      "Done with gradient descent at iteration  450\n",
      "Learned weights =  [  4.40835294 124.57319749]\n",
      "Done with gradient descent at iteration  450\n",
      "Learned weights =  [  4.40835294 124.57319564]\n",
      "Done with gradient descent at iteration  451\n",
      "Learned weights =  [  4.41809655 124.57319564]\n",
      "Done with gradient descent at iteration  451\n",
      "Learned weights =  [  4.41809655 124.57319378]\n",
      "Done with gradient descent at iteration  452\n",
      "Learned weights =  [  4.42784015 124.57319378]\n",
      "Done with gradient descent at iteration  452\n",
      "Learned weights =  [  4.42784015 124.57319193]\n",
      "Done with gradient descent at iteration  453\n",
      "Learned weights =  [  4.43758375 124.57319193]\n",
      "Done with gradient descent at iteration  453\n",
      "Learned weights =  [  4.43758375 124.57319008]\n",
      "Done with gradient descent at iteration  454\n",
      "Learned weights =  [  4.44732736 124.57319008]\n",
      "Done with gradient descent at iteration  454\n",
      "Learned weights =  [  4.44732736 124.57318822]\n",
      "Done with gradient descent at iteration  455\n",
      "Learned weights =  [  4.45707096 124.57318822]\n",
      "Done with gradient descent at iteration  455\n",
      "Learned weights =  [  4.45707096 124.57318637]\n",
      "Done with gradient descent at iteration  456\n",
      "Learned weights =  [  4.46681456 124.57318637]\n",
      "Done with gradient descent at iteration  456\n",
      "Learned weights =  [  4.46681456 124.57318451]\n",
      "Done with gradient descent at iteration  457\n",
      "Learned weights =  [  4.47655816 124.57318451]\n",
      "Done with gradient descent at iteration  457\n",
      "Learned weights =  [  4.47655816 124.57318266]\n",
      "Done with gradient descent at iteration  458\n",
      "Learned weights =  [  4.48630177 124.57318266]\n",
      "Done with gradient descent at iteration  458\n",
      "Learned weights =  [  4.48630177 124.5731808 ]\n",
      "Done with gradient descent at iteration  459\n",
      "Learned weights =  [  4.49604537 124.5731808 ]\n",
      "Done with gradient descent at iteration  459\n",
      "Learned weights =  [  4.49604537 124.57317895]\n",
      "Done with gradient descent at iteration  460\n",
      "Learned weights =  [  4.50578897 124.57317895]\n",
      "Done with gradient descent at iteration  460\n",
      "Learned weights =  [  4.50578897 124.57317709]\n",
      "Done with gradient descent at iteration  461\n",
      "Learned weights =  [  4.51553257 124.57317709]\n",
      "Done with gradient descent at iteration  461\n",
      "Learned weights =  [  4.51553257 124.57317524]\n",
      "Done with gradient descent at iteration  462\n",
      "Learned weights =  [  4.52527617 124.57317524]\n",
      "Done with gradient descent at iteration  462\n",
      "Learned weights =  [  4.52527617 124.57317338]\n",
      "Done with gradient descent at iteration  463\n",
      "Learned weights =  [  4.53501977 124.57317338]\n",
      "Done with gradient descent at iteration  463\n",
      "Learned weights =  [  4.53501977 124.57317153]\n",
      "Done with gradient descent at iteration  464\n",
      "Learned weights =  [  4.54476337 124.57317153]\n",
      "Done with gradient descent at iteration  464\n",
      "Learned weights =  [  4.54476337 124.57316968]\n",
      "Done with gradient descent at iteration  465\n",
      "Learned weights =  [  4.55450697 124.57316968]\n",
      "Done with gradient descent at iteration  465\n",
      "Learned weights =  [  4.55450697 124.57316782]\n",
      "Done with gradient descent at iteration  466\n",
      "Learned weights =  [  4.56425057 124.57316782]\n",
      "Done with gradient descent at iteration  466\n",
      "Learned weights =  [  4.56425057 124.57316597]\n",
      "Done with gradient descent at iteration  467\n",
      "Learned weights =  [  4.57399417 124.57316597]\n",
      "Done with gradient descent at iteration  467\n",
      "Learned weights =  [  4.57399417 124.57316411]\n",
      "Done with gradient descent at iteration  468\n",
      "Learned weights =  [  4.58373777 124.57316411]\n",
      "Done with gradient descent at iteration  468\n",
      "Learned weights =  [  4.58373777 124.57316226]\n",
      "Done with gradient descent at iteration  469\n",
      "Learned weights =  [  4.59348137 124.57316226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  469\n",
      "Learned weights =  [  4.59348137 124.5731604 ]\n",
      "Done with gradient descent at iteration  470\n",
      "Learned weights =  [  4.60322497 124.5731604 ]\n",
      "Done with gradient descent at iteration  470\n",
      "Learned weights =  [  4.60322497 124.57315855]\n",
      "Done with gradient descent at iteration  471\n",
      "Learned weights =  [  4.61296857 124.57315855]\n",
      "Done with gradient descent at iteration  471\n",
      "Learned weights =  [  4.61296857 124.57315669]\n",
      "Done with gradient descent at iteration  472\n",
      "Learned weights =  [  4.62271217 124.57315669]\n",
      "Done with gradient descent at iteration  472\n",
      "Learned weights =  [  4.62271217 124.57315484]\n",
      "Done with gradient descent at iteration  473\n",
      "Learned weights =  [  4.63245577 124.57315484]\n",
      "Done with gradient descent at iteration  473\n",
      "Learned weights =  [  4.63245577 124.57315298]\n",
      "Done with gradient descent at iteration  474\n",
      "Learned weights =  [  4.64219937 124.57315298]\n",
      "Done with gradient descent at iteration  474\n",
      "Learned weights =  [  4.64219937 124.57315113]\n",
      "Done with gradient descent at iteration  475\n",
      "Learned weights =  [  4.65194296 124.57315113]\n",
      "Done with gradient descent at iteration  475\n",
      "Learned weights =  [  4.65194296 124.57314928]\n",
      "Done with gradient descent at iteration  476\n",
      "Learned weights =  [  4.66168656 124.57314928]\n",
      "Done with gradient descent at iteration  476\n",
      "Learned weights =  [  4.66168656 124.57314742]\n",
      "Done with gradient descent at iteration  477\n",
      "Learned weights =  [  4.67143016 124.57314742]\n",
      "Done with gradient descent at iteration  477\n",
      "Learned weights =  [  4.67143016 124.57314557]\n",
      "Done with gradient descent at iteration  478\n",
      "Learned weights =  [  4.68117376 124.57314557]\n",
      "Done with gradient descent at iteration  478\n",
      "Learned weights =  [  4.68117376 124.57314371]\n",
      "Done with gradient descent at iteration  479\n",
      "Learned weights =  [  4.69091736 124.57314371]\n",
      "Done with gradient descent at iteration  479\n",
      "Learned weights =  [  4.69091736 124.57314186]\n",
      "Done with gradient descent at iteration  480\n",
      "Learned weights =  [  4.70066095 124.57314186]\n",
      "Done with gradient descent at iteration  480\n",
      "Learned weights =  [  4.70066095 124.57314   ]\n",
      "Done with gradient descent at iteration  481\n",
      "Learned weights =  [  4.71040455 124.57314   ]\n",
      "Done with gradient descent at iteration  481\n",
      "Learned weights =  [  4.71040455 124.57313815]\n",
      "Done with gradient descent at iteration  482\n",
      "Learned weights =  [  4.72014815 124.57313815]\n",
      "Done with gradient descent at iteration  482\n",
      "Learned weights =  [  4.72014815 124.57313629]\n",
      "Done with gradient descent at iteration  483\n",
      "Learned weights =  [  4.72989174 124.57313629]\n",
      "Done with gradient descent at iteration  483\n",
      "Learned weights =  [  4.72989174 124.57313444]\n",
      "Done with gradient descent at iteration  484\n",
      "Learned weights =  [  4.73963534 124.57313444]\n",
      "Done with gradient descent at iteration  484\n",
      "Learned weights =  [  4.73963534 124.57313258]\n",
      "Done with gradient descent at iteration  485\n",
      "Learned weights =  [  4.74937894 124.57313258]\n",
      "Done with gradient descent at iteration  485\n",
      "Learned weights =  [  4.74937894 124.57313073]\n",
      "Done with gradient descent at iteration  486\n",
      "Learned weights =  [  4.75912253 124.57313073]\n",
      "Done with gradient descent at iteration  486\n",
      "Learned weights =  [  4.75912253 124.57312888]\n",
      "Done with gradient descent at iteration  487\n",
      "Learned weights =  [  4.76886613 124.57312888]\n",
      "Done with gradient descent at iteration  487\n",
      "Learned weights =  [  4.76886613 124.57312702]\n",
      "Done with gradient descent at iteration  488\n",
      "Learned weights =  [  4.77860972 124.57312702]\n",
      "Done with gradient descent at iteration  488\n",
      "Learned weights =  [  4.77860972 124.57312517]\n",
      "Done with gradient descent at iteration  489\n",
      "Learned weights =  [  4.78835332 124.57312517]\n",
      "Done with gradient descent at iteration  489\n",
      "Learned weights =  [  4.78835332 124.57312331]\n",
      "Done with gradient descent at iteration  490\n",
      "Learned weights =  [  4.79809692 124.57312331]\n",
      "Done with gradient descent at iteration  490\n",
      "Learned weights =  [  4.79809692 124.57312146]\n",
      "Done with gradient descent at iteration  491\n",
      "Learned weights =  [  4.80784051 124.57312146]\n",
      "Done with gradient descent at iteration  491\n",
      "Learned weights =  [  4.80784051 124.5731196 ]\n",
      "Done with gradient descent at iteration  492\n",
      "Learned weights =  [  4.81758411 124.5731196 ]\n",
      "Done with gradient descent at iteration  492\n",
      "Learned weights =  [  4.81758411 124.57311775]\n",
      "Done with gradient descent at iteration  493\n",
      "Learned weights =  [  4.8273277  124.57311775]\n",
      "Done with gradient descent at iteration  493\n",
      "Learned weights =  [  4.8273277  124.57311589]\n",
      "Done with gradient descent at iteration  494\n",
      "Learned weights =  [  4.83707129 124.57311589]\n",
      "Done with gradient descent at iteration  494\n",
      "Learned weights =  [  4.83707129 124.57311404]\n",
      "Done with gradient descent at iteration  495\n",
      "Learned weights =  [  4.84681489 124.57311404]\n",
      "Done with gradient descent at iteration  495\n",
      "Learned weights =  [  4.84681489 124.57311218]\n",
      "Done with gradient descent at iteration  496\n",
      "Learned weights =  [  4.85655848 124.57311218]\n",
      "Done with gradient descent at iteration  496\n",
      "Learned weights =  [  4.85655848 124.57311033]\n",
      "Done with gradient descent at iteration  497\n",
      "Learned weights =  [  4.86630208 124.57311033]\n",
      "Done with gradient descent at iteration  497\n",
      "Learned weights =  [  4.86630208 124.57310848]\n",
      "Done with gradient descent at iteration  498\n",
      "Learned weights =  [  4.87604567 124.57310848]\n",
      "Done with gradient descent at iteration  498\n",
      "Learned weights =  [  4.87604567 124.57310662]\n",
      "Done with gradient descent at iteration  499\n",
      "Learned weights =  [  4.88578926 124.57310662]\n",
      "Done with gradient descent at iteration  499\n",
      "Learned weights =  [  4.88578926 124.57310477]\n",
      "Iteration = 500\n",
      "Cost function =  4484803259357624.0\n",
      "Done with gradient descent at iteration  500\n",
      "Learned weights =  [  4.89553286 124.57310477]\n",
      "Done with gradient descent at iteration  500\n",
      "Learned weights =  [  4.89553286 124.57310291]\n",
      "Done with gradient descent at iteration  501\n",
      "Learned weights =  [  4.90527645 124.57310291]\n",
      "Done with gradient descent at iteration  501\n",
      "Learned weights =  [  4.90527645 124.57310106]\n",
      "Done with gradient descent at iteration  502\n",
      "Learned weights =  [  4.91502004 124.57310106]\n",
      "Done with gradient descent at iteration  502\n",
      "Learned weights =  [  4.91502004 124.5730992 ]\n",
      "Done with gradient descent at iteration  503\n",
      "Learned weights =  [  4.92476364 124.5730992 ]\n",
      "Done with gradient descent at iteration  503\n",
      "Learned weights =  [  4.92476364 124.57309735]\n",
      "Done with gradient descent at iteration  504\n",
      "Learned weights =  [  4.93450723 124.57309735]\n",
      "Done with gradient descent at iteration  504\n",
      "Learned weights =  [  4.93450723 124.57309549]\n",
      "Done with gradient descent at iteration  505\n",
      "Learned weights =  [  4.94425082 124.57309549]\n",
      "Done with gradient descent at iteration  505\n",
      "Learned weights =  [  4.94425082 124.57309364]\n",
      "Done with gradient descent at iteration  506\n",
      "Learned weights =  [  4.95399441 124.57309364]\n",
      "Done with gradient descent at iteration  506\n",
      "Learned weights =  [  4.95399441 124.57309178]\n",
      "Done with gradient descent at iteration  507\n",
      "Learned weights =  [  4.963738   124.57309178]\n",
      "Done with gradient descent at iteration  507\n",
      "Learned weights =  [  4.963738   124.57308993]\n",
      "Done with gradient descent at iteration  508\n",
      "Learned weights =  [  4.9734816  124.57308993]\n",
      "Done with gradient descent at iteration  508\n",
      "Learned weights =  [  4.9734816  124.57308808]\n",
      "Done with gradient descent at iteration  509\n",
      "Learned weights =  [  4.98322519 124.57308808]\n",
      "Done with gradient descent at iteration  509\n",
      "Learned weights =  [  4.98322519 124.57308622]\n",
      "Done with gradient descent at iteration  510\n",
      "Learned weights =  [  4.99296878 124.57308622]\n",
      "Done with gradient descent at iteration  510\n",
      "Learned weights =  [  4.99296878 124.57308437]\n",
      "Done with gradient descent at iteration  511\n",
      "Learned weights =  [  5.00271237 124.57308437]\n",
      "Done with gradient descent at iteration  511\n",
      "Learned weights =  [  5.00271237 124.57308251]\n",
      "Done with gradient descent at iteration  512\n",
      "Learned weights =  [  5.01245596 124.57308251]\n",
      "Done with gradient descent at iteration  512\n",
      "Learned weights =  [  5.01245596 124.57308066]\n",
      "Done with gradient descent at iteration  513\n",
      "Learned weights =  [  5.02219955 124.57308066]\n",
      "Done with gradient descent at iteration  513\n",
      "Learned weights =  [  5.02219955 124.5730788 ]\n",
      "Done with gradient descent at iteration  514\n",
      "Learned weights =  [  5.03194314 124.5730788 ]\n",
      "Done with gradient descent at iteration  514\n",
      "Learned weights =  [  5.03194314 124.57307695]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  515\n",
      "Learned weights =  [  5.04168673 124.57307695]\n",
      "Done with gradient descent at iteration  515\n",
      "Learned weights =  [  5.04168673 124.57307509]\n",
      "Done with gradient descent at iteration  516\n",
      "Learned weights =  [  5.05143032 124.57307509]\n",
      "Done with gradient descent at iteration  516\n",
      "Learned weights =  [  5.05143032 124.57307324]\n",
      "Done with gradient descent at iteration  517\n",
      "Learned weights =  [  5.06117391 124.57307324]\n",
      "Done with gradient descent at iteration  517\n",
      "Learned weights =  [  5.06117391 124.57307138]\n",
      "Done with gradient descent at iteration  518\n",
      "Learned weights =  [  5.0709175  124.57307138]\n",
      "Done with gradient descent at iteration  518\n",
      "Learned weights =  [  5.0709175  124.57306953]\n",
      "Done with gradient descent at iteration  519\n",
      "Learned weights =  [  5.08066109 124.57306953]\n",
      "Done with gradient descent at iteration  519\n",
      "Learned weights =  [  5.08066109 124.57306768]\n",
      "Done with gradient descent at iteration  520\n",
      "Learned weights =  [  5.09040468 124.57306768]\n",
      "Done with gradient descent at iteration  520\n",
      "Learned weights =  [  5.09040468 124.57306582]\n",
      "Done with gradient descent at iteration  521\n",
      "Learned weights =  [  5.10014827 124.57306582]\n",
      "Done with gradient descent at iteration  521\n",
      "Learned weights =  [  5.10014827 124.57306397]\n",
      "Done with gradient descent at iteration  522\n",
      "Learned weights =  [  5.10989186 124.57306397]\n",
      "Done with gradient descent at iteration  522\n",
      "Learned weights =  [  5.10989186 124.57306211]\n",
      "Done with gradient descent at iteration  523\n",
      "Learned weights =  [  5.11963544 124.57306211]\n",
      "Done with gradient descent at iteration  523\n",
      "Learned weights =  [  5.11963544 124.57306026]\n",
      "Done with gradient descent at iteration  524\n",
      "Learned weights =  [  5.12937903 124.57306026]\n",
      "Done with gradient descent at iteration  524\n",
      "Learned weights =  [  5.12937903 124.5730584 ]\n",
      "Done with gradient descent at iteration  525\n",
      "Learned weights =  [  5.13912262 124.5730584 ]\n",
      "Done with gradient descent at iteration  525\n",
      "Learned weights =  [  5.13912262 124.57305655]\n",
      "Done with gradient descent at iteration  526\n",
      "Learned weights =  [  5.14886621 124.57305655]\n",
      "Done with gradient descent at iteration  526\n",
      "Learned weights =  [  5.14886621 124.57305469]\n",
      "Done with gradient descent at iteration  527\n",
      "Learned weights =  [  5.1586098  124.57305469]\n",
      "Done with gradient descent at iteration  527\n",
      "Learned weights =  [  5.1586098  124.57305284]\n",
      "Done with gradient descent at iteration  528\n",
      "Learned weights =  [  5.16835338 124.57305284]\n",
      "Done with gradient descent at iteration  528\n",
      "Learned weights =  [  5.16835338 124.57305098]\n",
      "Done with gradient descent at iteration  529\n",
      "Learned weights =  [  5.17809697 124.57305098]\n",
      "Done with gradient descent at iteration  529\n",
      "Learned weights =  [  5.17809697 124.57304913]\n",
      "Done with gradient descent at iteration  530\n",
      "Learned weights =  [  5.18784056 124.57304913]\n",
      "Done with gradient descent at iteration  530\n",
      "Learned weights =  [  5.18784056 124.57304728]\n",
      "Done with gradient descent at iteration  531\n",
      "Learned weights =  [  5.19758414 124.57304728]\n",
      "Done with gradient descent at iteration  531\n",
      "Learned weights =  [  5.19758414 124.57304542]\n",
      "Done with gradient descent at iteration  532\n",
      "Learned weights =  [  5.20732773 124.57304542]\n",
      "Done with gradient descent at iteration  532\n",
      "Learned weights =  [  5.20732773 124.57304357]\n",
      "Done with gradient descent at iteration  533\n",
      "Learned weights =  [  5.21707132 124.57304357]\n",
      "Done with gradient descent at iteration  533\n",
      "Learned weights =  [  5.21707132 124.57304171]\n",
      "Done with gradient descent at iteration  534\n",
      "Learned weights =  [  5.2268149  124.57304171]\n",
      "Done with gradient descent at iteration  534\n",
      "Learned weights =  [  5.2268149  124.57303986]\n",
      "Done with gradient descent at iteration  535\n",
      "Learned weights =  [  5.23655849 124.57303986]\n",
      "Done with gradient descent at iteration  535\n",
      "Learned weights =  [  5.23655849 124.573038  ]\n",
      "Done with gradient descent at iteration  536\n",
      "Learned weights =  [  5.24630208 124.573038  ]\n",
      "Done with gradient descent at iteration  536\n",
      "Learned weights =  [  5.24630208 124.57303615]\n",
      "Done with gradient descent at iteration  537\n",
      "Learned weights =  [  5.25604566 124.57303615]\n",
      "Done with gradient descent at iteration  537\n",
      "Learned weights =  [  5.25604566 124.57303429]\n",
      "Done with gradient descent at iteration  538\n",
      "Learned weights =  [  5.26578925 124.57303429]\n",
      "Done with gradient descent at iteration  538\n",
      "Learned weights =  [  5.26578925 124.57303244]\n",
      "Done with gradient descent at iteration  539\n",
      "Learned weights =  [  5.27553283 124.57303244]\n",
      "Done with gradient descent at iteration  539\n",
      "Learned weights =  [  5.27553283 124.57303058]\n",
      "Done with gradient descent at iteration  540\n",
      "Learned weights =  [  5.28527642 124.57303058]\n",
      "Done with gradient descent at iteration  540\n",
      "Learned weights =  [  5.28527642 124.57302873]\n",
      "Done with gradient descent at iteration  541\n",
      "Learned weights =  [  5.29502    124.57302873]\n",
      "Done with gradient descent at iteration  541\n",
      "Learned weights =  [  5.29502    124.57302688]\n",
      "Done with gradient descent at iteration  542\n",
      "Learned weights =  [  5.30476359 124.57302688]\n",
      "Done with gradient descent at iteration  542\n",
      "Learned weights =  [  5.30476359 124.57302502]\n",
      "Done with gradient descent at iteration  543\n",
      "Learned weights =  [  5.31450717 124.57302502]\n",
      "Done with gradient descent at iteration  543\n",
      "Learned weights =  [  5.31450717 124.57302317]\n",
      "Done with gradient descent at iteration  544\n",
      "Learned weights =  [  5.32425076 124.57302317]\n",
      "Done with gradient descent at iteration  544\n",
      "Learned weights =  [  5.32425076 124.57302131]\n",
      "Done with gradient descent at iteration  545\n",
      "Learned weights =  [  5.33399434 124.57302131]\n",
      "Done with gradient descent at iteration  545\n",
      "Learned weights =  [  5.33399434 124.57301946]\n",
      "Done with gradient descent at iteration  546\n",
      "Learned weights =  [  5.34373792 124.57301946]\n",
      "Done with gradient descent at iteration  546\n",
      "Learned weights =  [  5.34373792 124.5730176 ]\n",
      "Done with gradient descent at iteration  547\n",
      "Learned weights =  [  5.35348151 124.5730176 ]\n",
      "Done with gradient descent at iteration  547\n",
      "Learned weights =  [  5.35348151 124.57301575]\n",
      "Done with gradient descent at iteration  548\n",
      "Learned weights =  [  5.36322509 124.57301575]\n",
      "Done with gradient descent at iteration  548\n",
      "Learned weights =  [  5.36322509 124.57301389]\n",
      "Done with gradient descent at iteration  549\n",
      "Learned weights =  [  5.37296867 124.57301389]\n",
      "Done with gradient descent at iteration  549\n",
      "Learned weights =  [  5.37296867 124.57301204]\n",
      "Done with gradient descent at iteration  550\n",
      "Learned weights =  [  5.38271226 124.57301204]\n",
      "Done with gradient descent at iteration  550\n",
      "Learned weights =  [  5.38271226 124.57301018]\n",
      "Done with gradient descent at iteration  551\n",
      "Learned weights =  [  5.39245584 124.57301018]\n",
      "Done with gradient descent at iteration  551\n",
      "Learned weights =  [  5.39245584 124.57300833]\n",
      "Done with gradient descent at iteration  552\n",
      "Learned weights =  [  5.40219942 124.57300833]\n",
      "Done with gradient descent at iteration  552\n",
      "Learned weights =  [  5.40219942 124.57300648]\n",
      "Done with gradient descent at iteration  553\n",
      "Learned weights =  [  5.411943   124.57300648]\n",
      "Done with gradient descent at iteration  553\n",
      "Learned weights =  [  5.411943   124.57300462]\n",
      "Done with gradient descent at iteration  554\n",
      "Learned weights =  [  5.42168659 124.57300462]\n",
      "Done with gradient descent at iteration  554\n",
      "Learned weights =  [  5.42168659 124.57300277]\n",
      "Done with gradient descent at iteration  555\n",
      "Learned weights =  [  5.43143017 124.57300277]\n",
      "Done with gradient descent at iteration  555\n",
      "Learned weights =  [  5.43143017 124.57300091]\n",
      "Done with gradient descent at iteration  556\n",
      "Learned weights =  [  5.44117375 124.57300091]\n",
      "Done with gradient descent at iteration  556\n",
      "Learned weights =  [  5.44117375 124.57299906]\n",
      "Done with gradient descent at iteration  557\n",
      "Learned weights =  [  5.45091733 124.57299906]\n",
      "Done with gradient descent at iteration  557\n",
      "Learned weights =  [  5.45091733 124.5729972 ]\n",
      "Done with gradient descent at iteration  558\n",
      "Learned weights =  [  5.46066091 124.5729972 ]\n",
      "Done with gradient descent at iteration  558\n",
      "Learned weights =  [  5.46066091 124.57299535]\n",
      "Done with gradient descent at iteration  559\n",
      "Learned weights =  [  5.47040449 124.57299535]\n",
      "Done with gradient descent at iteration  559\n",
      "Learned weights =  [  5.47040449 124.57299349]\n",
      "Done with gradient descent at iteration  560\n",
      "Learned weights =  [  5.48014808 124.57299349]\n",
      "Done with gradient descent at iteration  560\n",
      "Learned weights =  [  5.48014808 124.57299164]\n",
      "Done with gradient descent at iteration  561\n",
      "Learned weights =  [  5.48989166 124.57299164]\n",
      "Done with gradient descent at iteration  561\n",
      "Learned weights =  [  5.48989166 124.57298978]\n",
      "Done with gradient descent at iteration  562\n",
      "Learned weights =  [  5.49963524 124.57298978]\n",
      "Done with gradient descent at iteration  562\n",
      "Learned weights =  [  5.49963524 124.57298793]\n",
      "Done with gradient descent at iteration  563\n",
      "Learned weights =  [  5.50937882 124.57298793]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  563\n",
      "Learned weights =  [  5.50937882 124.57298608]\n",
      "Done with gradient descent at iteration  564\n",
      "Learned weights =  [  5.5191224  124.57298608]\n",
      "Done with gradient descent at iteration  564\n",
      "Learned weights =  [  5.5191224  124.57298422]\n",
      "Done with gradient descent at iteration  565\n",
      "Learned weights =  [  5.52886598 124.57298422]\n",
      "Done with gradient descent at iteration  565\n",
      "Learned weights =  [  5.52886598 124.57298237]\n",
      "Done with gradient descent at iteration  566\n",
      "Learned weights =  [  5.53860956 124.57298237]\n",
      "Done with gradient descent at iteration  566\n",
      "Learned weights =  [  5.53860956 124.57298051]\n",
      "Done with gradient descent at iteration  567\n",
      "Learned weights =  [  5.54835314 124.57298051]\n",
      "Done with gradient descent at iteration  567\n",
      "Learned weights =  [  5.54835314 124.57297866]\n",
      "Done with gradient descent at iteration  568\n",
      "Learned weights =  [  5.55809672 124.57297866]\n",
      "Done with gradient descent at iteration  568\n",
      "Learned weights =  [  5.55809672 124.5729768 ]\n",
      "Done with gradient descent at iteration  569\n",
      "Learned weights =  [  5.56784029 124.5729768 ]\n",
      "Done with gradient descent at iteration  569\n",
      "Learned weights =  [  5.56784029 124.57297495]\n",
      "Done with gradient descent at iteration  570\n",
      "Learned weights =  [  5.57758387 124.57297495]\n",
      "Done with gradient descent at iteration  570\n",
      "Learned weights =  [  5.57758387 124.57297309]\n",
      "Done with gradient descent at iteration  571\n",
      "Learned weights =  [  5.58732745 124.57297309]\n",
      "Done with gradient descent at iteration  571\n",
      "Learned weights =  [  5.58732745 124.57297124]\n",
      "Done with gradient descent at iteration  572\n",
      "Learned weights =  [  5.59707103 124.57297124]\n",
      "Done with gradient descent at iteration  572\n",
      "Learned weights =  [  5.59707103 124.57296938]\n",
      "Done with gradient descent at iteration  573\n",
      "Learned weights =  [  5.60681461 124.57296938]\n",
      "Done with gradient descent at iteration  573\n",
      "Learned weights =  [  5.60681461 124.57296753]\n",
      "Done with gradient descent at iteration  574\n",
      "Learned weights =  [  5.61655819 124.57296753]\n",
      "Done with gradient descent at iteration  574\n",
      "Learned weights =  [  5.61655819 124.57296568]\n",
      "Done with gradient descent at iteration  575\n",
      "Learned weights =  [  5.62630176 124.57296568]\n",
      "Done with gradient descent at iteration  575\n",
      "Learned weights =  [  5.62630176 124.57296382]\n",
      "Done with gradient descent at iteration  576\n",
      "Learned weights =  [  5.63604534 124.57296382]\n",
      "Done with gradient descent at iteration  576\n",
      "Learned weights =  [  5.63604534 124.57296197]\n",
      "Done with gradient descent at iteration  577\n",
      "Learned weights =  [  5.64578892 124.57296197]\n",
      "Done with gradient descent at iteration  577\n",
      "Learned weights =  [  5.64578892 124.57296011]\n",
      "Done with gradient descent at iteration  578\n",
      "Learned weights =  [  5.6555325  124.57296011]\n",
      "Done with gradient descent at iteration  578\n",
      "Learned weights =  [  5.6555325  124.57295826]\n",
      "Done with gradient descent at iteration  579\n",
      "Learned weights =  [  5.66527607 124.57295826]\n",
      "Done with gradient descent at iteration  579\n",
      "Learned weights =  [  5.66527607 124.5729564 ]\n",
      "Done with gradient descent at iteration  580\n",
      "Learned weights =  [  5.67501965 124.5729564 ]\n",
      "Done with gradient descent at iteration  580\n",
      "Learned weights =  [  5.67501965 124.57295455]\n",
      "Done with gradient descent at iteration  581\n",
      "Learned weights =  [  5.68476323 124.57295455]\n",
      "Done with gradient descent at iteration  581\n",
      "Learned weights =  [  5.68476323 124.57295269]\n",
      "Done with gradient descent at iteration  582\n",
      "Learned weights =  [  5.6945068  124.57295269]\n",
      "Done with gradient descent at iteration  582\n",
      "Learned weights =  [  5.6945068  124.57295084]\n",
      "Done with gradient descent at iteration  583\n",
      "Learned weights =  [  5.70425038 124.57295084]\n",
      "Done with gradient descent at iteration  583\n",
      "Learned weights =  [  5.70425038 124.57294898]\n",
      "Done with gradient descent at iteration  584\n",
      "Learned weights =  [  5.71399396 124.57294898]\n",
      "Done with gradient descent at iteration  584\n",
      "Learned weights =  [  5.71399396 124.57294713]\n",
      "Done with gradient descent at iteration  585\n",
      "Learned weights =  [  5.72373753 124.57294713]\n",
      "Done with gradient descent at iteration  585\n",
      "Learned weights =  [  5.72373753 124.57294528]\n",
      "Done with gradient descent at iteration  586\n",
      "Learned weights =  [  5.73348111 124.57294528]\n",
      "Done with gradient descent at iteration  586\n",
      "Learned weights =  [  5.73348111 124.57294342]\n",
      "Done with gradient descent at iteration  587\n",
      "Learned weights =  [  5.74322468 124.57294342]\n",
      "Done with gradient descent at iteration  587\n",
      "Learned weights =  [  5.74322468 124.57294157]\n",
      "Done with gradient descent at iteration  588\n",
      "Learned weights =  [  5.75296826 124.57294157]\n",
      "Done with gradient descent at iteration  588\n",
      "Learned weights =  [  5.75296826 124.57293971]\n",
      "Done with gradient descent at iteration  589\n",
      "Learned weights =  [  5.76271183 124.57293971]\n",
      "Done with gradient descent at iteration  589\n",
      "Learned weights =  [  5.76271183 124.57293786]\n",
      "Done with gradient descent at iteration  590\n",
      "Learned weights =  [  5.77245541 124.57293786]\n",
      "Done with gradient descent at iteration  590\n",
      "Learned weights =  [  5.77245541 124.572936  ]\n",
      "Done with gradient descent at iteration  591\n",
      "Learned weights =  [  5.78219898 124.572936  ]\n",
      "Done with gradient descent at iteration  591\n",
      "Learned weights =  [  5.78219898 124.57293415]\n",
      "Done with gradient descent at iteration  592\n",
      "Learned weights =  [  5.79194256 124.57293415]\n",
      "Done with gradient descent at iteration  592\n",
      "Learned weights =  [  5.79194256 124.57293229]\n",
      "Done with gradient descent at iteration  593\n",
      "Learned weights =  [  5.80168613 124.57293229]\n",
      "Done with gradient descent at iteration  593\n",
      "Learned weights =  [  5.80168613 124.57293044]\n",
      "Done with gradient descent at iteration  594\n",
      "Learned weights =  [  5.81142971 124.57293044]\n",
      "Done with gradient descent at iteration  594\n",
      "Learned weights =  [  5.81142971 124.57292859]\n",
      "Done with gradient descent at iteration  595\n",
      "Learned weights =  [  5.82117328 124.57292859]\n",
      "Done with gradient descent at iteration  595\n",
      "Learned weights =  [  5.82117328 124.57292673]\n",
      "Done with gradient descent at iteration  596\n",
      "Learned weights =  [  5.83091685 124.57292673]\n",
      "Done with gradient descent at iteration  596\n",
      "Learned weights =  [  5.83091685 124.57292488]\n",
      "Done with gradient descent at iteration  597\n",
      "Learned weights =  [  5.84066043 124.57292488]\n",
      "Done with gradient descent at iteration  597\n",
      "Learned weights =  [  5.84066043 124.57292302]\n",
      "Done with gradient descent at iteration  598\n",
      "Learned weights =  [  5.850404   124.57292302]\n",
      "Done with gradient descent at iteration  598\n",
      "Learned weights =  [  5.850404   124.57292117]\n",
      "Done with gradient descent at iteration  599\n",
      "Learned weights =  [  5.86014757 124.57292117]\n",
      "Done with gradient descent at iteration  599\n",
      "Learned weights =  [  5.86014757 124.57291931]\n",
      "Iteration = 600\n",
      "Cost function =  4484793765616237.5\n",
      "Done with gradient descent at iteration  600\n",
      "Learned weights =  [  5.86989115 124.57291931]\n",
      "Done with gradient descent at iteration  600\n",
      "Learned weights =  [  5.86989115 124.57291746]\n",
      "Done with gradient descent at iteration  601\n",
      "Learned weights =  [  5.87963472 124.57291746]\n",
      "Done with gradient descent at iteration  601\n",
      "Learned weights =  [  5.87963472 124.5729156 ]\n",
      "Done with gradient descent at iteration  602\n",
      "Learned weights =  [  5.88937829 124.5729156 ]\n",
      "Done with gradient descent at iteration  602\n",
      "Learned weights =  [  5.88937829 124.57291375]\n",
      "Done with gradient descent at iteration  603\n",
      "Learned weights =  [  5.89912186 124.57291375]\n",
      "Done with gradient descent at iteration  603\n",
      "Learned weights =  [  5.89912186 124.57291189]\n",
      "Done with gradient descent at iteration  604\n",
      "Learned weights =  [  5.90886543 124.57291189]\n",
      "Done with gradient descent at iteration  604\n",
      "Learned weights =  [  5.90886543 124.57291004]\n",
      "Done with gradient descent at iteration  605\n",
      "Learned weights =  [  5.91860901 124.57291004]\n",
      "Done with gradient descent at iteration  605\n",
      "Learned weights =  [  5.91860901 124.57290819]\n",
      "Done with gradient descent at iteration  606\n",
      "Learned weights =  [  5.92835258 124.57290819]\n",
      "Done with gradient descent at iteration  606\n",
      "Learned weights =  [  5.92835258 124.57290633]\n",
      "Done with gradient descent at iteration  607\n",
      "Learned weights =  [  5.93809615 124.57290633]\n",
      "Done with gradient descent at iteration  607\n",
      "Learned weights =  [  5.93809615 124.57290448]\n",
      "Done with gradient descent at iteration  608\n",
      "Learned weights =  [  5.94783972 124.57290448]\n",
      "Done with gradient descent at iteration  608\n",
      "Learned weights =  [  5.94783972 124.57290262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  609\n",
      "Learned weights =  [  5.95758329 124.57290262]\n",
      "Done with gradient descent at iteration  609\n",
      "Learned weights =  [  5.95758329 124.57290077]\n",
      "Done with gradient descent at iteration  610\n",
      "Learned weights =  [  5.96732686 124.57290077]\n",
      "Done with gradient descent at iteration  610\n",
      "Learned weights =  [  5.96732686 124.57289891]\n",
      "Done with gradient descent at iteration  611\n",
      "Learned weights =  [  5.97707043 124.57289891]\n",
      "Done with gradient descent at iteration  611\n",
      "Learned weights =  [  5.97707043 124.57289706]\n",
      "Done with gradient descent at iteration  612\n",
      "Learned weights =  [  5.986814   124.57289706]\n",
      "Done with gradient descent at iteration  612\n",
      "Learned weights =  [  5.986814  124.5728952]\n",
      "Done with gradient descent at iteration  613\n",
      "Learned weights =  [  5.99655757 124.5728952 ]\n",
      "Done with gradient descent at iteration  613\n",
      "Learned weights =  [  5.99655757 124.57289335]\n",
      "Done with gradient descent at iteration  614\n",
      "Learned weights =  [  6.00630114 124.57289335]\n",
      "Done with gradient descent at iteration  614\n",
      "Learned weights =  [  6.00630114 124.57289149]\n",
      "Done with gradient descent at iteration  615\n",
      "Learned weights =  [  6.01604471 124.57289149]\n",
      "Done with gradient descent at iteration  615\n",
      "Learned weights =  [  6.01604471 124.57288964]\n",
      "Done with gradient descent at iteration  616\n",
      "Learned weights =  [  6.02578828 124.57288964]\n",
      "Done with gradient descent at iteration  616\n",
      "Learned weights =  [  6.02578828 124.57288779]\n",
      "Done with gradient descent at iteration  617\n",
      "Learned weights =  [  6.03553185 124.57288779]\n",
      "Done with gradient descent at iteration  617\n",
      "Learned weights =  [  6.03553185 124.57288593]\n",
      "Done with gradient descent at iteration  618\n",
      "Learned weights =  [  6.04527542 124.57288593]\n",
      "Done with gradient descent at iteration  618\n",
      "Learned weights =  [  6.04527542 124.57288408]\n",
      "Done with gradient descent at iteration  619\n",
      "Learned weights =  [  6.05501899 124.57288408]\n",
      "Done with gradient descent at iteration  619\n",
      "Learned weights =  [  6.05501899 124.57288222]\n",
      "Done with gradient descent at iteration  620\n",
      "Learned weights =  [  6.06476256 124.57288222]\n",
      "Done with gradient descent at iteration  620\n",
      "Learned weights =  [  6.06476256 124.57288037]\n",
      "Done with gradient descent at iteration  621\n",
      "Learned weights =  [  6.07450613 124.57288037]\n",
      "Done with gradient descent at iteration  621\n",
      "Learned weights =  [  6.07450613 124.57287851]\n",
      "Done with gradient descent at iteration  622\n",
      "Learned weights =  [  6.08424969 124.57287851]\n",
      "Done with gradient descent at iteration  622\n",
      "Learned weights =  [  6.08424969 124.57287666]\n",
      "Done with gradient descent at iteration  623\n",
      "Learned weights =  [  6.09399326 124.57287666]\n",
      "Done with gradient descent at iteration  623\n",
      "Learned weights =  [  6.09399326 124.5728748 ]\n",
      "Done with gradient descent at iteration  624\n",
      "Learned weights =  [  6.10373683 124.5728748 ]\n",
      "Done with gradient descent at iteration  624\n",
      "Learned weights =  [  6.10373683 124.57287295]\n",
      "Done with gradient descent at iteration  625\n",
      "Learned weights =  [  6.1134804  124.57287295]\n",
      "Done with gradient descent at iteration  625\n",
      "Learned weights =  [  6.1134804  124.57287109]\n",
      "Done with gradient descent at iteration  626\n",
      "Learned weights =  [  6.12322396 124.57287109]\n",
      "Done with gradient descent at iteration  626\n",
      "Learned weights =  [  6.12322396 124.57286924]\n",
      "Done with gradient descent at iteration  627\n",
      "Learned weights =  [  6.13296753 124.57286924]\n",
      "Done with gradient descent at iteration  627\n",
      "Learned weights =  [  6.13296753 124.57286739]\n",
      "Done with gradient descent at iteration  628\n",
      "Learned weights =  [  6.1427111  124.57286739]\n",
      "Done with gradient descent at iteration  628\n",
      "Learned weights =  [  6.1427111  124.57286553]\n",
      "Done with gradient descent at iteration  629\n",
      "Learned weights =  [  6.15245467 124.57286553]\n",
      "Done with gradient descent at iteration  629\n",
      "Learned weights =  [  6.15245467 124.57286368]\n",
      "Done with gradient descent at iteration  630\n",
      "Learned weights =  [  6.16219823 124.57286368]\n",
      "Done with gradient descent at iteration  630\n",
      "Learned weights =  [  6.16219823 124.57286182]\n",
      "Done with gradient descent at iteration  631\n",
      "Learned weights =  [  6.1719418  124.57286182]\n",
      "Done with gradient descent at iteration  631\n",
      "Learned weights =  [  6.1719418  124.57285997]\n",
      "Done with gradient descent at iteration  632\n",
      "Learned weights =  [  6.18168537 124.57285997]\n",
      "Done with gradient descent at iteration  632\n",
      "Learned weights =  [  6.18168537 124.57285811]\n",
      "Done with gradient descent at iteration  633\n",
      "Learned weights =  [  6.19142893 124.57285811]\n",
      "Done with gradient descent at iteration  633\n",
      "Learned weights =  [  6.19142893 124.57285626]\n",
      "Done with gradient descent at iteration  634\n",
      "Learned weights =  [  6.2011725  124.57285626]\n",
      "Done with gradient descent at iteration  634\n",
      "Learned weights =  [  6.2011725 124.5728544]\n",
      "Done with gradient descent at iteration  635\n",
      "Learned weights =  [  6.21091606 124.5728544 ]\n",
      "Done with gradient descent at iteration  635\n",
      "Learned weights =  [  6.21091606 124.57285255]\n",
      "Done with gradient descent at iteration  636\n",
      "Learned weights =  [  6.22065963 124.57285255]\n",
      "Done with gradient descent at iteration  636\n",
      "Learned weights =  [  6.22065963 124.57285069]\n",
      "Done with gradient descent at iteration  637\n",
      "Learned weights =  [  6.23040319 124.57285069]\n",
      "Done with gradient descent at iteration  637\n",
      "Learned weights =  [  6.23040319 124.57284884]\n",
      "Done with gradient descent at iteration  638\n",
      "Learned weights =  [  6.24014676 124.57284884]\n",
      "Done with gradient descent at iteration  638\n",
      "Learned weights =  [  6.24014676 124.57284699]\n",
      "Done with gradient descent at iteration  639\n",
      "Learned weights =  [  6.24989032 124.57284699]\n",
      "Done with gradient descent at iteration  639\n",
      "Learned weights =  [  6.24989032 124.57284513]\n",
      "Done with gradient descent at iteration  640\n",
      "Learned weights =  [  6.25963389 124.57284513]\n",
      "Done with gradient descent at iteration  640\n",
      "Learned weights =  [  6.25963389 124.57284328]\n",
      "Done with gradient descent at iteration  641\n",
      "Learned weights =  [  6.26937745 124.57284328]\n",
      "Done with gradient descent at iteration  641\n",
      "Learned weights =  [  6.26937745 124.57284142]\n",
      "Done with gradient descent at iteration  642\n",
      "Learned weights =  [  6.27912102 124.57284142]\n",
      "Done with gradient descent at iteration  642\n",
      "Learned weights =  [  6.27912102 124.57283957]\n",
      "Done with gradient descent at iteration  643\n",
      "Learned weights =  [  6.28886458 124.57283957]\n",
      "Done with gradient descent at iteration  643\n",
      "Learned weights =  [  6.28886458 124.57283771]\n",
      "Done with gradient descent at iteration  644\n",
      "Learned weights =  [  6.29860814 124.57283771]\n",
      "Done with gradient descent at iteration  644\n",
      "Learned weights =  [  6.29860814 124.57283586]\n",
      "Done with gradient descent at iteration  645\n",
      "Learned weights =  [  6.30835171 124.57283586]\n",
      "Done with gradient descent at iteration  645\n",
      "Learned weights =  [  6.30835171 124.572834  ]\n",
      "Done with gradient descent at iteration  646\n",
      "Learned weights =  [  6.31809527 124.572834  ]\n",
      "Done with gradient descent at iteration  646\n",
      "Learned weights =  [  6.31809527 124.57283215]\n",
      "Done with gradient descent at iteration  647\n",
      "Learned weights =  [  6.32783883 124.57283215]\n",
      "Done with gradient descent at iteration  647\n",
      "Learned weights =  [  6.32783883 124.57283029]\n",
      "Done with gradient descent at iteration  648\n",
      "Learned weights =  [  6.3375824  124.57283029]\n",
      "Done with gradient descent at iteration  648\n",
      "Learned weights =  [  6.3375824  124.57282844]\n",
      "Done with gradient descent at iteration  649\n",
      "Learned weights =  [  6.34732596 124.57282844]\n",
      "Done with gradient descent at iteration  649\n",
      "Learned weights =  [  6.34732596 124.57282659]\n",
      "Done with gradient descent at iteration  650\n",
      "Learned weights =  [  6.35706952 124.57282659]\n",
      "Done with gradient descent at iteration  650\n",
      "Learned weights =  [  6.35706952 124.57282473]\n",
      "Done with gradient descent at iteration  651\n",
      "Learned weights =  [  6.36681308 124.57282473]\n",
      "Done with gradient descent at iteration  651\n",
      "Learned weights =  [  6.36681308 124.57282288]\n",
      "Done with gradient descent at iteration  652\n",
      "Learned weights =  [  6.37655665 124.57282288]\n",
      "Done with gradient descent at iteration  652\n",
      "Learned weights =  [  6.37655665 124.57282102]\n",
      "Done with gradient descent at iteration  653\n",
      "Learned weights =  [  6.38630021 124.57282102]\n",
      "Done with gradient descent at iteration  653\n",
      "Learned weights =  [  6.38630021 124.57281917]\n",
      "Done with gradient descent at iteration  654\n",
      "Learned weights =  [  6.39604377 124.57281917]\n",
      "Done with gradient descent at iteration  654\n",
      "Learned weights =  [  6.39604377 124.57281731]\n",
      "Done with gradient descent at iteration  655\n",
      "Learned weights =  [  6.40578733 124.57281731]\n",
      "Done with gradient descent at iteration  655\n",
      "Learned weights =  [  6.40578733 124.57281546]\n",
      "Done with gradient descent at iteration  656\n",
      "Learned weights =  [  6.41553089 124.57281546]\n",
      "Done with gradient descent at iteration  656\n",
      "Learned weights =  [  6.41553089 124.5728136 ]\n",
      "Done with gradient descent at iteration  657\n",
      "Learned weights =  [  6.42527445 124.5728136 ]\n",
      "Done with gradient descent at iteration  657\n",
      "Learned weights =  [  6.42527445 124.57281175]\n",
      "Done with gradient descent at iteration  658\n",
      "Learned weights =  [  6.43501801 124.57281175]\n",
      "Done with gradient descent at iteration  658\n",
      "Learned weights =  [  6.43501801 124.57280989]\n",
      "Done with gradient descent at iteration  659\n",
      "Learned weights =  [  6.44476158 124.57280989]\n",
      "Done with gradient descent at iteration  659\n",
      "Learned weights =  [  6.44476158 124.57280804]\n",
      "Done with gradient descent at iteration  660\n",
      "Learned weights =  [  6.45450514 124.57280804]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  660\n",
      "Learned weights =  [  6.45450514 124.57280619]\n",
      "Done with gradient descent at iteration  661\n",
      "Learned weights =  [  6.4642487  124.57280619]\n",
      "Done with gradient descent at iteration  661\n",
      "Learned weights =  [  6.4642487  124.57280433]\n",
      "Done with gradient descent at iteration  662\n",
      "Learned weights =  [  6.47399226 124.57280433]\n",
      "Done with gradient descent at iteration  662\n",
      "Learned weights =  [  6.47399226 124.57280248]\n",
      "Done with gradient descent at iteration  663\n",
      "Learned weights =  [  6.48373582 124.57280248]\n",
      "Done with gradient descent at iteration  663\n",
      "Learned weights =  [  6.48373582 124.57280062]\n",
      "Done with gradient descent at iteration  664\n",
      "Learned weights =  [  6.49347938 124.57280062]\n",
      "Done with gradient descent at iteration  664\n",
      "Learned weights =  [  6.49347938 124.57279877]\n",
      "Done with gradient descent at iteration  665\n",
      "Learned weights =  [  6.50322294 124.57279877]\n",
      "Done with gradient descent at iteration  665\n",
      "Learned weights =  [  6.50322294 124.57279691]\n",
      "Done with gradient descent at iteration  666\n",
      "Learned weights =  [  6.51296649 124.57279691]\n",
      "Done with gradient descent at iteration  666\n",
      "Learned weights =  [  6.51296649 124.57279506]\n",
      "Done with gradient descent at iteration  667\n",
      "Learned weights =  [  6.52271005 124.57279506]\n",
      "Done with gradient descent at iteration  667\n",
      "Learned weights =  [  6.52271005 124.5727932 ]\n",
      "Done with gradient descent at iteration  668\n",
      "Learned weights =  [  6.53245361 124.5727932 ]\n",
      "Done with gradient descent at iteration  668\n",
      "Learned weights =  [  6.53245361 124.57279135]\n",
      "Done with gradient descent at iteration  669\n",
      "Learned weights =  [  6.54219717 124.57279135]\n",
      "Done with gradient descent at iteration  669\n",
      "Learned weights =  [  6.54219717 124.57278949]\n",
      "Done with gradient descent at iteration  670\n",
      "Learned weights =  [  6.55194073 124.57278949]\n",
      "Done with gradient descent at iteration  670\n",
      "Learned weights =  [  6.55194073 124.57278764]\n",
      "Done with gradient descent at iteration  671\n",
      "Learned weights =  [  6.56168429 124.57278764]\n",
      "Done with gradient descent at iteration  671\n",
      "Learned weights =  [  6.56168429 124.57278579]\n",
      "Done with gradient descent at iteration  672\n",
      "Learned weights =  [  6.57142785 124.57278579]\n",
      "Done with gradient descent at iteration  672\n",
      "Learned weights =  [  6.57142785 124.57278393]\n",
      "Done with gradient descent at iteration  673\n",
      "Learned weights =  [  6.5811714  124.57278393]\n",
      "Done with gradient descent at iteration  673\n",
      "Learned weights =  [  6.5811714  124.57278208]\n",
      "Done with gradient descent at iteration  674\n",
      "Learned weights =  [  6.59091496 124.57278208]\n",
      "Done with gradient descent at iteration  674\n",
      "Learned weights =  [  6.59091496 124.57278022]\n",
      "Done with gradient descent at iteration  675\n",
      "Learned weights =  [  6.60065852 124.57278022]\n",
      "Done with gradient descent at iteration  675\n",
      "Learned weights =  [  6.60065852 124.57277837]\n",
      "Done with gradient descent at iteration  676\n",
      "Learned weights =  [  6.61040208 124.57277837]\n",
      "Done with gradient descent at iteration  676\n",
      "Learned weights =  [  6.61040208 124.57277651]\n",
      "Done with gradient descent at iteration  677\n",
      "Learned weights =  [  6.62014563 124.57277651]\n",
      "Done with gradient descent at iteration  677\n",
      "Learned weights =  [  6.62014563 124.57277466]\n",
      "Done with gradient descent at iteration  678\n",
      "Learned weights =  [  6.62988919 124.57277466]\n",
      "Done with gradient descent at iteration  678\n",
      "Learned weights =  [  6.62988919 124.5727728 ]\n",
      "Done with gradient descent at iteration  679\n",
      "Learned weights =  [  6.63963275 124.5727728 ]\n",
      "Done with gradient descent at iteration  679\n",
      "Learned weights =  [  6.63963275 124.57277095]\n",
      "Done with gradient descent at iteration  680\n",
      "Learned weights =  [  6.6493763  124.57277095]\n",
      "Done with gradient descent at iteration  680\n",
      "Learned weights =  [  6.6493763 124.5727691]\n",
      "Done with gradient descent at iteration  681\n",
      "Learned weights =  [  6.65911986 124.5727691 ]\n",
      "Done with gradient descent at iteration  681\n",
      "Learned weights =  [  6.65911986 124.57276724]\n",
      "Done with gradient descent at iteration  682\n",
      "Learned weights =  [  6.66886341 124.57276724]\n",
      "Done with gradient descent at iteration  682\n",
      "Learned weights =  [  6.66886341 124.57276539]\n",
      "Done with gradient descent at iteration  683\n",
      "Learned weights =  [  6.67860697 124.57276539]\n",
      "Done with gradient descent at iteration  683\n",
      "Learned weights =  [  6.67860697 124.57276353]\n",
      "Done with gradient descent at iteration  684\n",
      "Learned weights =  [  6.68835053 124.57276353]\n",
      "Done with gradient descent at iteration  684\n",
      "Learned weights =  [  6.68835053 124.57276168]\n",
      "Done with gradient descent at iteration  685\n",
      "Learned weights =  [  6.69809408 124.57276168]\n",
      "Done with gradient descent at iteration  685\n",
      "Learned weights =  [  6.69809408 124.57275982]\n",
      "Done with gradient descent at iteration  686\n",
      "Learned weights =  [  6.70783764 124.57275982]\n",
      "Done with gradient descent at iteration  686\n",
      "Learned weights =  [  6.70783764 124.57275797]\n",
      "Done with gradient descent at iteration  687\n",
      "Learned weights =  [  6.71758119 124.57275797]\n",
      "Done with gradient descent at iteration  687\n",
      "Learned weights =  [  6.71758119 124.57275611]\n",
      "Done with gradient descent at iteration  688\n",
      "Learned weights =  [  6.72732475 124.57275611]\n",
      "Done with gradient descent at iteration  688\n",
      "Learned weights =  [  6.72732475 124.57275426]\n",
      "Done with gradient descent at iteration  689\n",
      "Learned weights =  [  6.7370683  124.57275426]\n",
      "Done with gradient descent at iteration  689\n",
      "Learned weights =  [  6.7370683 124.5727524]\n",
      "Done with gradient descent at iteration  690\n",
      "Learned weights =  [  6.74681185 124.5727524 ]\n",
      "Done with gradient descent at iteration  690\n",
      "Learned weights =  [  6.74681185 124.57275055]\n",
      "Done with gradient descent at iteration  691\n",
      "Learned weights =  [  6.75655541 124.57275055]\n",
      "Done with gradient descent at iteration  691\n",
      "Learned weights =  [  6.75655541 124.5727487 ]\n",
      "Done with gradient descent at iteration  692\n",
      "Learned weights =  [  6.76629896 124.5727487 ]\n",
      "Done with gradient descent at iteration  692\n",
      "Learned weights =  [  6.76629896 124.57274684]\n",
      "Done with gradient descent at iteration  693\n",
      "Learned weights =  [  6.77604252 124.57274684]\n",
      "Done with gradient descent at iteration  693\n",
      "Learned weights =  [  6.77604252 124.57274499]\n",
      "Done with gradient descent at iteration  694\n",
      "Learned weights =  [  6.78578607 124.57274499]\n",
      "Done with gradient descent at iteration  694\n",
      "Learned weights =  [  6.78578607 124.57274313]\n",
      "Done with gradient descent at iteration  695\n",
      "Learned weights =  [  6.79552962 124.57274313]\n",
      "Done with gradient descent at iteration  695\n",
      "Learned weights =  [  6.79552962 124.57274128]\n",
      "Done with gradient descent at iteration  696\n",
      "Learned weights =  [  6.80527318 124.57274128]\n",
      "Done with gradient descent at iteration  696\n",
      "Learned weights =  [  6.80527318 124.57273942]\n",
      "Done with gradient descent at iteration  697\n",
      "Learned weights =  [  6.81501673 124.57273942]\n",
      "Done with gradient descent at iteration  697\n",
      "Learned weights =  [  6.81501673 124.57273757]\n",
      "Done with gradient descent at iteration  698\n",
      "Learned weights =  [  6.82476028 124.57273757]\n",
      "Done with gradient descent at iteration  698\n",
      "Learned weights =  [  6.82476028 124.57273571]\n",
      "Done with gradient descent at iteration  699\n",
      "Learned weights =  [  6.83450383 124.57273571]\n",
      "Done with gradient descent at iteration  699\n",
      "Learned weights =  [  6.83450383 124.57273386]\n",
      "Iteration = 700\n",
      "Cost function =  4484784271914732.0\n",
      "Done with gradient descent at iteration  700\n",
      "Learned weights =  [  6.84424739 124.57273386]\n",
      "Done with gradient descent at iteration  700\n",
      "Learned weights =  [  6.84424739 124.572732  ]\n",
      "Done with gradient descent at iteration  701\n",
      "Learned weights =  [  6.85399094 124.572732  ]\n",
      "Done with gradient descent at iteration  701\n",
      "Learned weights =  [  6.85399094 124.57273015]\n",
      "Done with gradient descent at iteration  702\n",
      "Learned weights =  [  6.86373449 124.57273015]\n",
      "Done with gradient descent at iteration  702\n",
      "Learned weights =  [  6.86373449 124.5727283 ]\n",
      "Done with gradient descent at iteration  703\n",
      "Learned weights =  [  6.87347804 124.5727283 ]\n",
      "Done with gradient descent at iteration  703\n",
      "Learned weights =  [  6.87347804 124.57272644]\n",
      "Done with gradient descent at iteration  704\n",
      "Learned weights =  [  6.88322159 124.57272644]\n",
      "Done with gradient descent at iteration  704\n",
      "Learned weights =  [  6.88322159 124.57272459]\n",
      "Done with gradient descent at iteration  705\n",
      "Learned weights =  [  6.89296515 124.57272459]\n",
      "Done with gradient descent at iteration  705\n",
      "Learned weights =  [  6.89296515 124.57272273]\n",
      "Done with gradient descent at iteration  706\n",
      "Learned weights =  [  6.9027087  124.57272273]\n",
      "Done with gradient descent at iteration  706\n",
      "Learned weights =  [  6.9027087  124.57272088]\n",
      "Done with gradient descent at iteration  707\n",
      "Learned weights =  [  6.91245225 124.57272088]\n",
      "Done with gradient descent at iteration  707\n",
      "Learned weights =  [  6.91245225 124.57271902]\n",
      "Done with gradient descent at iteration  708\n",
      "Learned weights =  [  6.9221958  124.57271902]\n",
      "Done with gradient descent at iteration  708\n",
      "Learned weights =  [  6.9221958  124.57271717]\n",
      "Done with gradient descent at iteration  709\n",
      "Learned weights =  [  6.93193935 124.57271717]\n",
      "Done with gradient descent at iteration  709\n",
      "Learned weights =  [  6.93193935 124.57271531]\n",
      "Done with gradient descent at iteration  710\n",
      "Learned weights =  [  6.9416829  124.57271531]\n",
      "Done with gradient descent at iteration  710\n",
      "Learned weights =  [  6.9416829  124.57271346]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  711\n",
      "Learned weights =  [  6.95142645 124.57271346]\n",
      "Done with gradient descent at iteration  711\n",
      "Learned weights =  [  6.95142645 124.5727116 ]\n",
      "Done with gradient descent at iteration  712\n",
      "Learned weights =  [  6.96117   124.5727116]\n",
      "Done with gradient descent at iteration  712\n",
      "Learned weights =  [  6.96117    124.57270975]\n",
      "Done with gradient descent at iteration  713\n",
      "Learned weights =  [  6.97091355 124.57270975]\n",
      "Done with gradient descent at iteration  713\n",
      "Learned weights =  [  6.97091355 124.5727079 ]\n",
      "Done with gradient descent at iteration  714\n",
      "Learned weights =  [  6.9806571 124.5727079]\n",
      "Done with gradient descent at iteration  714\n",
      "Learned weights =  [  6.9806571  124.57270604]\n",
      "Done with gradient descent at iteration  715\n",
      "Learned weights =  [  6.99040065 124.57270604]\n",
      "Done with gradient descent at iteration  715\n",
      "Learned weights =  [  6.99040065 124.57270419]\n",
      "Done with gradient descent at iteration  716\n",
      "Learned weights =  [  7.0001442  124.57270419]\n",
      "Done with gradient descent at iteration  716\n",
      "Learned weights =  [  7.0001442  124.57270233]\n",
      "Done with gradient descent at iteration  717\n",
      "Learned weights =  [  7.00988774 124.57270233]\n",
      "Done with gradient descent at iteration  717\n",
      "Learned weights =  [  7.00988774 124.57270048]\n",
      "Done with gradient descent at iteration  718\n",
      "Learned weights =  [  7.01963129 124.57270048]\n",
      "Done with gradient descent at iteration  718\n",
      "Learned weights =  [  7.01963129 124.57269862]\n",
      "Done with gradient descent at iteration  719\n",
      "Learned weights =  [  7.02937484 124.57269862]\n",
      "Done with gradient descent at iteration  719\n",
      "Learned weights =  [  7.02937484 124.57269677]\n",
      "Done with gradient descent at iteration  720\n",
      "Learned weights =  [  7.03911839 124.57269677]\n",
      "Done with gradient descent at iteration  720\n",
      "Learned weights =  [  7.03911839 124.57269491]\n",
      "Done with gradient descent at iteration  721\n",
      "Learned weights =  [  7.04886194 124.57269491]\n",
      "Done with gradient descent at iteration  721\n",
      "Learned weights =  [  7.04886194 124.57269306]\n",
      "Done with gradient descent at iteration  722\n",
      "Learned weights =  [  7.05860549 124.57269306]\n",
      "Done with gradient descent at iteration  722\n",
      "Learned weights =  [  7.05860549 124.5726912 ]\n",
      "Done with gradient descent at iteration  723\n",
      "Learned weights =  [  7.06834903 124.5726912 ]\n",
      "Done with gradient descent at iteration  723\n",
      "Learned weights =  [  7.06834903 124.57268935]\n",
      "Done with gradient descent at iteration  724\n",
      "Learned weights =  [  7.07809258 124.57268935]\n",
      "Done with gradient descent at iteration  724\n",
      "Learned weights =  [  7.07809258 124.5726875 ]\n",
      "Done with gradient descent at iteration  725\n",
      "Learned weights =  [  7.08783613 124.5726875 ]\n",
      "Done with gradient descent at iteration  725\n",
      "Learned weights =  [  7.08783613 124.57268564]\n",
      "Done with gradient descent at iteration  726\n",
      "Learned weights =  [  7.09757967 124.57268564]\n",
      "Done with gradient descent at iteration  726\n",
      "Learned weights =  [  7.09757967 124.57268379]\n",
      "Done with gradient descent at iteration  727\n",
      "Learned weights =  [  7.10732322 124.57268379]\n",
      "Done with gradient descent at iteration  727\n",
      "Learned weights =  [  7.10732322 124.57268193]\n",
      "Done with gradient descent at iteration  728\n",
      "Learned weights =  [  7.11706677 124.57268193]\n",
      "Done with gradient descent at iteration  728\n",
      "Learned weights =  [  7.11706677 124.57268008]\n",
      "Done with gradient descent at iteration  729\n",
      "Learned weights =  [  7.12681031 124.57268008]\n",
      "Done with gradient descent at iteration  729\n",
      "Learned weights =  [  7.12681031 124.57267822]\n",
      "Done with gradient descent at iteration  730\n",
      "Learned weights =  [  7.13655386 124.57267822]\n",
      "Done with gradient descent at iteration  730\n",
      "Learned weights =  [  7.13655386 124.57267637]\n",
      "Done with gradient descent at iteration  731\n",
      "Learned weights =  [  7.14629741 124.57267637]\n",
      "Done with gradient descent at iteration  731\n",
      "Learned weights =  [  7.14629741 124.57267451]\n",
      "Done with gradient descent at iteration  732\n",
      "Learned weights =  [  7.15604095 124.57267451]\n",
      "Done with gradient descent at iteration  732\n",
      "Learned weights =  [  7.15604095 124.57267266]\n",
      "Done with gradient descent at iteration  733\n",
      "Learned weights =  [  7.1657845  124.57267266]\n",
      "Done with gradient descent at iteration  733\n",
      "Learned weights =  [  7.1657845 124.5726708]\n",
      "Done with gradient descent at iteration  734\n",
      "Learned weights =  [  7.17552804 124.5726708 ]\n",
      "Done with gradient descent at iteration  734\n",
      "Learned weights =  [  7.17552804 124.57266895]\n",
      "Done with gradient descent at iteration  735\n",
      "Learned weights =  [  7.18527159 124.57266895]\n",
      "Done with gradient descent at iteration  735\n",
      "Learned weights =  [  7.18527159 124.5726671 ]\n",
      "Done with gradient descent at iteration  736\n",
      "Learned weights =  [  7.19501513 124.5726671 ]\n",
      "Done with gradient descent at iteration  736\n",
      "Learned weights =  [  7.19501513 124.57266524]\n",
      "Done with gradient descent at iteration  737\n",
      "Learned weights =  [  7.20475868 124.57266524]\n",
      "Done with gradient descent at iteration  737\n",
      "Learned weights =  [  7.20475868 124.57266339]\n",
      "Done with gradient descent at iteration  738\n",
      "Learned weights =  [  7.21450222 124.57266339]\n",
      "Done with gradient descent at iteration  738\n",
      "Learned weights =  [  7.21450222 124.57266153]\n",
      "Done with gradient descent at iteration  739\n",
      "Learned weights =  [  7.22424577 124.57266153]\n",
      "Done with gradient descent at iteration  739\n",
      "Learned weights =  [  7.22424577 124.57265968]\n",
      "Done with gradient descent at iteration  740\n",
      "Learned weights =  [  7.23398931 124.57265968]\n",
      "Done with gradient descent at iteration  740\n",
      "Learned weights =  [  7.23398931 124.57265782]\n",
      "Done with gradient descent at iteration  741\n",
      "Learned weights =  [  7.24373285 124.57265782]\n",
      "Done with gradient descent at iteration  741\n",
      "Learned weights =  [  7.24373285 124.57265597]\n",
      "Done with gradient descent at iteration  742\n",
      "Learned weights =  [  7.2534764  124.57265597]\n",
      "Done with gradient descent at iteration  742\n",
      "Learned weights =  [  7.2534764  124.57265411]\n",
      "Done with gradient descent at iteration  743\n",
      "Learned weights =  [  7.26321994 124.57265411]\n",
      "Done with gradient descent at iteration  743\n",
      "Learned weights =  [  7.26321994 124.57265226]\n",
      "Done with gradient descent at iteration  744\n",
      "Learned weights =  [  7.27296349 124.57265226]\n",
      "Done with gradient descent at iteration  744\n",
      "Learned weights =  [  7.27296349 124.5726504 ]\n",
      "Done with gradient descent at iteration  745\n",
      "Learned weights =  [  7.28270703 124.5726504 ]\n",
      "Done with gradient descent at iteration  745\n",
      "Learned weights =  [  7.28270703 124.57264855]\n",
      "Done with gradient descent at iteration  746\n",
      "Learned weights =  [  7.29245057 124.57264855]\n",
      "Done with gradient descent at iteration  746\n",
      "Learned weights =  [  7.29245057 124.5726467 ]\n",
      "Done with gradient descent at iteration  747\n",
      "Learned weights =  [  7.30219411 124.5726467 ]\n",
      "Done with gradient descent at iteration  747\n",
      "Learned weights =  [  7.30219411 124.57264484]\n",
      "Done with gradient descent at iteration  748\n",
      "Learned weights =  [  7.31193766 124.57264484]\n",
      "Done with gradient descent at iteration  748\n",
      "Learned weights =  [  7.31193766 124.57264299]\n",
      "Done with gradient descent at iteration  749\n",
      "Learned weights =  [  7.3216812  124.57264299]\n",
      "Done with gradient descent at iteration  749\n",
      "Learned weights =  [  7.3216812  124.57264113]\n",
      "Done with gradient descent at iteration  750\n",
      "Learned weights =  [  7.33142474 124.57264113]\n",
      "Done with gradient descent at iteration  750\n",
      "Learned weights =  [  7.33142474 124.57263928]\n",
      "Done with gradient descent at iteration  751\n",
      "Learned weights =  [  7.34116828 124.57263928]\n",
      "Done with gradient descent at iteration  751\n",
      "Learned weights =  [  7.34116828 124.57263742]\n",
      "Done with gradient descent at iteration  752\n",
      "Learned weights =  [  7.35091182 124.57263742]\n",
      "Done with gradient descent at iteration  752\n",
      "Learned weights =  [  7.35091182 124.57263557]\n",
      "Done with gradient descent at iteration  753\n",
      "Learned weights =  [  7.36065537 124.57263557]\n",
      "Done with gradient descent at iteration  753\n",
      "Learned weights =  [  7.36065537 124.57263371]\n",
      "Done with gradient descent at iteration  754\n",
      "Learned weights =  [  7.37039891 124.57263371]\n",
      "Done with gradient descent at iteration  754\n",
      "Learned weights =  [  7.37039891 124.57263186]\n",
      "Done with gradient descent at iteration  755\n",
      "Learned weights =  [  7.38014245 124.57263186]\n",
      "Done with gradient descent at iteration  755\n",
      "Learned weights =  [  7.38014245 124.57263001]\n",
      "Done with gradient descent at iteration  756\n",
      "Learned weights =  [  7.38988599 124.57263001]\n",
      "Done with gradient descent at iteration  756\n",
      "Learned weights =  [  7.38988599 124.57262815]\n",
      "Done with gradient descent at iteration  757\n",
      "Learned weights =  [  7.39962953 124.57262815]\n",
      "Done with gradient descent at iteration  757\n",
      "Learned weights =  [  7.39962953 124.5726263 ]\n",
      "Done with gradient descent at iteration  758\n",
      "Learned weights =  [  7.40937307 124.5726263 ]\n",
      "Done with gradient descent at iteration  758\n",
      "Learned weights =  [  7.40937307 124.57262444]\n",
      "Done with gradient descent at iteration  759\n",
      "Learned weights =  [  7.41911661 124.57262444]\n",
      "Done with gradient descent at iteration  759\n",
      "Learned weights =  [  7.41911661 124.57262259]\n",
      "Done with gradient descent at iteration  760\n",
      "Learned weights =  [  7.42886015 124.57262259]\n",
      "Done with gradient descent at iteration  760\n",
      "Learned weights =  [  7.42886015 124.57262073]\n",
      "Done with gradient descent at iteration  761\n",
      "Learned weights =  [  7.43860369 124.57262073]\n",
      "Done with gradient descent at iteration  761\n",
      "Learned weights =  [  7.43860369 124.57261888]\n",
      "Done with gradient descent at iteration  762\n",
      "Learned weights =  [  7.44834723 124.57261888]\n",
      "Done with gradient descent at iteration  762\n",
      "Learned weights =  [  7.44834723 124.57261702]\n",
      "Done with gradient descent at iteration  763\n",
      "Learned weights =  [  7.45809077 124.57261702]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  763\n",
      "Learned weights =  [  7.45809077 124.57261517]\n",
      "Done with gradient descent at iteration  764\n",
      "Learned weights =  [  7.46783431 124.57261517]\n",
      "Done with gradient descent at iteration  764\n",
      "Learned weights =  [  7.46783431 124.57261331]\n",
      "Done with gradient descent at iteration  765\n",
      "Learned weights =  [  7.47757785 124.57261331]\n",
      "Done with gradient descent at iteration  765\n",
      "Learned weights =  [  7.47757785 124.57261146]\n",
      "Done with gradient descent at iteration  766\n",
      "Learned weights =  [  7.48732139 124.57261146]\n",
      "Done with gradient descent at iteration  766\n",
      "Learned weights =  [  7.48732139 124.57260961]\n",
      "Done with gradient descent at iteration  767\n",
      "Learned weights =  [  7.49706492 124.57260961]\n",
      "Done with gradient descent at iteration  767\n",
      "Learned weights =  [  7.49706492 124.57260775]\n",
      "Done with gradient descent at iteration  768\n",
      "Learned weights =  [  7.50680846 124.57260775]\n",
      "Done with gradient descent at iteration  768\n",
      "Learned weights =  [  7.50680846 124.5726059 ]\n",
      "Done with gradient descent at iteration  769\n",
      "Learned weights =  [  7.516552  124.5726059]\n",
      "Done with gradient descent at iteration  769\n",
      "Learned weights =  [  7.516552   124.57260404]\n",
      "Done with gradient descent at iteration  770\n",
      "Learned weights =  [  7.52629554 124.57260404]\n",
      "Done with gradient descent at iteration  770\n",
      "Learned weights =  [  7.52629554 124.57260219]\n",
      "Done with gradient descent at iteration  771\n",
      "Learned weights =  [  7.53603908 124.57260219]\n",
      "Done with gradient descent at iteration  771\n",
      "Learned weights =  [  7.53603908 124.57260033]\n",
      "Done with gradient descent at iteration  772\n",
      "Learned weights =  [  7.54578261 124.57260033]\n",
      "Done with gradient descent at iteration  772\n",
      "Learned weights =  [  7.54578261 124.57259848]\n",
      "Done with gradient descent at iteration  773\n",
      "Learned weights =  [  7.55552615 124.57259848]\n",
      "Done with gradient descent at iteration  773\n",
      "Learned weights =  [  7.55552615 124.57259662]\n",
      "Done with gradient descent at iteration  774\n",
      "Learned weights =  [  7.56526969 124.57259662]\n",
      "Done with gradient descent at iteration  774\n",
      "Learned weights =  [  7.56526969 124.57259477]\n",
      "Done with gradient descent at iteration  775\n",
      "Learned weights =  [  7.57501323 124.57259477]\n",
      "Done with gradient descent at iteration  775\n",
      "Learned weights =  [  7.57501323 124.57259291]\n",
      "Done with gradient descent at iteration  776\n",
      "Learned weights =  [  7.58475676 124.57259291]\n",
      "Done with gradient descent at iteration  776\n",
      "Learned weights =  [  7.58475676 124.57259106]\n",
      "Done with gradient descent at iteration  777\n",
      "Learned weights =  [  7.5945003  124.57259106]\n",
      "Done with gradient descent at iteration  777\n",
      "Learned weights =  [  7.5945003  124.57258921]\n",
      "Done with gradient descent at iteration  778\n",
      "Learned weights =  [  7.60424384 124.57258921]\n",
      "Done with gradient descent at iteration  778\n",
      "Learned weights =  [  7.60424384 124.57258735]\n",
      "Done with gradient descent at iteration  779\n",
      "Learned weights =  [  7.61398737 124.57258735]\n",
      "Done with gradient descent at iteration  779\n",
      "Learned weights =  [  7.61398737 124.5725855 ]\n",
      "Done with gradient descent at iteration  780\n",
      "Learned weights =  [  7.62373091 124.5725855 ]\n",
      "Done with gradient descent at iteration  780\n",
      "Learned weights =  [  7.62373091 124.57258364]\n",
      "Done with gradient descent at iteration  781\n",
      "Learned weights =  [  7.63347444 124.57258364]\n",
      "Done with gradient descent at iteration  781\n",
      "Learned weights =  [  7.63347444 124.57258179]\n",
      "Done with gradient descent at iteration  782\n",
      "Learned weights =  [  7.64321798 124.57258179]\n",
      "Done with gradient descent at iteration  782\n",
      "Learned weights =  [  7.64321798 124.57257993]\n",
      "Done with gradient descent at iteration  783\n",
      "Learned weights =  [  7.65296151 124.57257993]\n",
      "Done with gradient descent at iteration  783\n",
      "Learned weights =  [  7.65296151 124.57257808]\n",
      "Done with gradient descent at iteration  784\n",
      "Learned weights =  [  7.66270505 124.57257808]\n",
      "Done with gradient descent at iteration  784\n",
      "Learned weights =  [  7.66270505 124.57257622]\n",
      "Done with gradient descent at iteration  785\n",
      "Learned weights =  [  7.67244858 124.57257622]\n",
      "Done with gradient descent at iteration  785\n",
      "Learned weights =  [  7.67244858 124.57257437]\n",
      "Done with gradient descent at iteration  786\n",
      "Learned weights =  [  7.68219212 124.57257437]\n",
      "Done with gradient descent at iteration  786\n",
      "Learned weights =  [  7.68219212 124.57257251]\n",
      "Done with gradient descent at iteration  787\n",
      "Learned weights =  [  7.69193565 124.57257251]\n",
      "Done with gradient descent at iteration  787\n",
      "Learned weights =  [  7.69193565 124.57257066]\n",
      "Done with gradient descent at iteration  788\n",
      "Learned weights =  [  7.70167919 124.57257066]\n",
      "Done with gradient descent at iteration  788\n",
      "Learned weights =  [  7.70167919 124.57256881]\n",
      "Done with gradient descent at iteration  789\n",
      "Learned weights =  [  7.71142272 124.57256881]\n",
      "Done with gradient descent at iteration  789\n",
      "Learned weights =  [  7.71142272 124.57256695]\n",
      "Done with gradient descent at iteration  790\n",
      "Learned weights =  [  7.72116626 124.57256695]\n",
      "Done with gradient descent at iteration  790\n",
      "Learned weights =  [  7.72116626 124.5725651 ]\n",
      "Done with gradient descent at iteration  791\n",
      "Learned weights =  [  7.73090979 124.5725651 ]\n",
      "Done with gradient descent at iteration  791\n",
      "Learned weights =  [  7.73090979 124.57256324]\n",
      "Done with gradient descent at iteration  792\n",
      "Learned weights =  [  7.74065332 124.57256324]\n",
      "Done with gradient descent at iteration  792\n",
      "Learned weights =  [  7.74065332 124.57256139]\n",
      "Done with gradient descent at iteration  793\n",
      "Learned weights =  [  7.75039686 124.57256139]\n",
      "Done with gradient descent at iteration  793\n",
      "Learned weights =  [  7.75039686 124.57255953]\n",
      "Done with gradient descent at iteration  794\n",
      "Learned weights =  [  7.76014039 124.57255953]\n",
      "Done with gradient descent at iteration  794\n",
      "Learned weights =  [  7.76014039 124.57255768]\n",
      "Done with gradient descent at iteration  795\n",
      "Learned weights =  [  7.76988392 124.57255768]\n",
      "Done with gradient descent at iteration  795\n",
      "Learned weights =  [  7.76988392 124.57255582]\n",
      "Done with gradient descent at iteration  796\n",
      "Learned weights =  [  7.77962745 124.57255582]\n",
      "Done with gradient descent at iteration  796\n",
      "Learned weights =  [  7.77962745 124.57255397]\n",
      "Done with gradient descent at iteration  797\n",
      "Learned weights =  [  7.78937099 124.57255397]\n",
      "Done with gradient descent at iteration  797\n",
      "Learned weights =  [  7.78937099 124.57255211]\n",
      "Done with gradient descent at iteration  798\n",
      "Learned weights =  [  7.79911452 124.57255211]\n",
      "Done with gradient descent at iteration  798\n",
      "Learned weights =  [  7.79911452 124.57255026]\n",
      "Done with gradient descent at iteration  799\n",
      "Learned weights =  [  7.80885805 124.57255026]\n",
      "Done with gradient descent at iteration  799\n",
      "Learned weights =  [  7.80885805 124.57254841]\n",
      "Iteration = 800\n",
      "Cost function =  4484774778253105.0\n",
      "Done with gradient descent at iteration  800\n",
      "Learned weights =  [  7.81860158 124.57254841]\n",
      "Done with gradient descent at iteration  800\n",
      "Learned weights =  [  7.81860158 124.57254655]\n",
      "Done with gradient descent at iteration  801\n",
      "Learned weights =  [  7.82834511 124.57254655]\n",
      "Done with gradient descent at iteration  801\n",
      "Learned weights =  [  7.82834511 124.5725447 ]\n",
      "Done with gradient descent at iteration  802\n",
      "Learned weights =  [  7.83808865 124.5725447 ]\n",
      "Done with gradient descent at iteration  802\n",
      "Learned weights =  [  7.83808865 124.57254284]\n",
      "Done with gradient descent at iteration  803\n",
      "Learned weights =  [  7.84783218 124.57254284]\n",
      "Done with gradient descent at iteration  803\n",
      "Learned weights =  [  7.84783218 124.57254099]\n",
      "Done with gradient descent at iteration  804\n",
      "Learned weights =  [  7.85757571 124.57254099]\n",
      "Done with gradient descent at iteration  804\n",
      "Learned weights =  [  7.85757571 124.57253913]\n",
      "Done with gradient descent at iteration  805\n",
      "Learned weights =  [  7.86731924 124.57253913]\n",
      "Done with gradient descent at iteration  805\n",
      "Learned weights =  [  7.86731924 124.57253728]\n",
      "Done with gradient descent at iteration  806\n",
      "Learned weights =  [  7.87706277 124.57253728]\n",
      "Done with gradient descent at iteration  806\n",
      "Learned weights =  [  7.87706277 124.57253542]\n",
      "Done with gradient descent at iteration  807\n",
      "Learned weights =  [  7.8868063  124.57253542]\n",
      "Done with gradient descent at iteration  807\n",
      "Learned weights =  [  7.8868063  124.57253357]\n",
      "Done with gradient descent at iteration  808\n",
      "Learned weights =  [  7.89654983 124.57253357]\n",
      "Done with gradient descent at iteration  808\n",
      "Learned weights =  [  7.89654983 124.57253172]\n",
      "Done with gradient descent at iteration  809\n",
      "Learned weights =  [  7.90629336 124.57253172]\n",
      "Done with gradient descent at iteration  809\n",
      "Learned weights =  [  7.90629336 124.57252986]\n",
      "Done with gradient descent at iteration  810\n",
      "Learned weights =  [  7.91603689 124.57252986]\n",
      "Done with gradient descent at iteration  810\n",
      "Learned weights =  [  7.91603689 124.57252801]\n",
      "Done with gradient descent at iteration  811\n",
      "Learned weights =  [  7.92578042 124.57252801]\n",
      "Done with gradient descent at iteration  811\n",
      "Learned weights =  [  7.92578042 124.57252615]\n",
      "Done with gradient descent at iteration  812\n",
      "Learned weights =  [  7.93552395 124.57252615]\n",
      "Done with gradient descent at iteration  812\n",
      "Learned weights =  [  7.93552395 124.5725243 ]\n",
      "Done with gradient descent at iteration  813\n",
      "Learned weights =  [  7.94526748 124.5725243 ]\n",
      "Done with gradient descent at iteration  813\n",
      "Learned weights =  [  7.94526748 124.57252244]\n",
      "Done with gradient descent at iteration  814\n",
      "Learned weights =  [  7.95501101 124.57252244]\n",
      "Done with gradient descent at iteration  814\n",
      "Learned weights =  [  7.95501101 124.57252059]\n",
      "Done with gradient descent at iteration  815\n",
      "Learned weights =  [  7.96475454 124.57252059]\n",
      "Done with gradient descent at iteration  815\n",
      "Learned weights =  [  7.96475454 124.57251873]\n",
      "Done with gradient descent at iteration  816\n",
      "Learned weights =  [  7.97449806 124.57251873]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  816\n",
      "Learned weights =  [  7.97449806 124.57251688]\n",
      "Done with gradient descent at iteration  817\n",
      "Learned weights =  [  7.98424159 124.57251688]\n",
      "Done with gradient descent at iteration  817\n",
      "Learned weights =  [  7.98424159 124.57251502]\n",
      "Done with gradient descent at iteration  818\n",
      "Learned weights =  [  7.99398512 124.57251502]\n",
      "Done with gradient descent at iteration  818\n",
      "Learned weights =  [  7.99398512 124.57251317]\n",
      "Done with gradient descent at iteration  819\n",
      "Learned weights =  [  8.00372865 124.57251317]\n",
      "Done with gradient descent at iteration  819\n",
      "Learned weights =  [  8.00372865 124.57251132]\n",
      "Done with gradient descent at iteration  820\n",
      "Learned weights =  [  8.01347218 124.57251132]\n",
      "Done with gradient descent at iteration  820\n",
      "Learned weights =  [  8.01347218 124.57250946]\n",
      "Done with gradient descent at iteration  821\n",
      "Learned weights =  [  8.0232157  124.57250946]\n",
      "Done with gradient descent at iteration  821\n",
      "Learned weights =  [  8.0232157  124.57250761]\n",
      "Done with gradient descent at iteration  822\n",
      "Learned weights =  [  8.03295923 124.57250761]\n",
      "Done with gradient descent at iteration  822\n",
      "Learned weights =  [  8.03295923 124.57250575]\n",
      "Done with gradient descent at iteration  823\n",
      "Learned weights =  [  8.04270276 124.57250575]\n",
      "Done with gradient descent at iteration  823\n",
      "Learned weights =  [  8.04270276 124.5725039 ]\n",
      "Done with gradient descent at iteration  824\n",
      "Learned weights =  [  8.05244629 124.5725039 ]\n",
      "Done with gradient descent at iteration  824\n",
      "Learned weights =  [  8.05244629 124.57250204]\n",
      "Done with gradient descent at iteration  825\n",
      "Learned weights =  [  8.06218981 124.57250204]\n",
      "Done with gradient descent at iteration  825\n",
      "Learned weights =  [  8.06218981 124.57250019]\n",
      "Done with gradient descent at iteration  826\n",
      "Learned weights =  [  8.07193334 124.57250019]\n",
      "Done with gradient descent at iteration  826\n",
      "Learned weights =  [  8.07193334 124.57249833]\n",
      "Done with gradient descent at iteration  827\n",
      "Learned weights =  [  8.08167686 124.57249833]\n",
      "Done with gradient descent at iteration  827\n",
      "Learned weights =  [  8.08167686 124.57249648]\n",
      "Done with gradient descent at iteration  828\n",
      "Learned weights =  [  8.09142039 124.57249648]\n",
      "Done with gradient descent at iteration  828\n",
      "Learned weights =  [  8.09142039 124.57249462]\n",
      "Done with gradient descent at iteration  829\n",
      "Learned weights =  [  8.10116392 124.57249462]\n",
      "Done with gradient descent at iteration  829\n",
      "Learned weights =  [  8.10116392 124.57249277]\n",
      "Done with gradient descent at iteration  830\n",
      "Learned weights =  [  8.11090744 124.57249277]\n",
      "Done with gradient descent at iteration  830\n",
      "Learned weights =  [  8.11090744 124.57249092]\n",
      "Done with gradient descent at iteration  831\n",
      "Learned weights =  [  8.12065097 124.57249092]\n",
      "Done with gradient descent at iteration  831\n",
      "Learned weights =  [  8.12065097 124.57248906]\n",
      "Done with gradient descent at iteration  832\n",
      "Learned weights =  [  8.13039449 124.57248906]\n",
      "Done with gradient descent at iteration  832\n",
      "Learned weights =  [  8.13039449 124.57248721]\n",
      "Done with gradient descent at iteration  833\n",
      "Learned weights =  [  8.14013802 124.57248721]\n",
      "Done with gradient descent at iteration  833\n",
      "Learned weights =  [  8.14013802 124.57248535]\n",
      "Done with gradient descent at iteration  834\n",
      "Learned weights =  [  8.14988154 124.57248535]\n",
      "Done with gradient descent at iteration  834\n",
      "Learned weights =  [  8.14988154 124.5724835 ]\n",
      "Done with gradient descent at iteration  835\n",
      "Learned weights =  [  8.15962507 124.5724835 ]\n",
      "Done with gradient descent at iteration  835\n",
      "Learned weights =  [  8.15962507 124.57248164]\n",
      "Done with gradient descent at iteration  836\n",
      "Learned weights =  [  8.16936859 124.57248164]\n",
      "Done with gradient descent at iteration  836\n",
      "Learned weights =  [  8.16936859 124.57247979]\n",
      "Done with gradient descent at iteration  837\n",
      "Learned weights =  [  8.17911212 124.57247979]\n",
      "Done with gradient descent at iteration  837\n",
      "Learned weights =  [  8.17911212 124.57247793]\n",
      "Done with gradient descent at iteration  838\n",
      "Learned weights =  [  8.18885564 124.57247793]\n",
      "Done with gradient descent at iteration  838\n",
      "Learned weights =  [  8.18885564 124.57247608]\n",
      "Done with gradient descent at iteration  839\n",
      "Learned weights =  [  8.19859916 124.57247608]\n",
      "Done with gradient descent at iteration  839\n",
      "Learned weights =  [  8.19859916 124.57247422]\n",
      "Done with gradient descent at iteration  840\n",
      "Learned weights =  [  8.20834269 124.57247422]\n",
      "Done with gradient descent at iteration  840\n",
      "Learned weights =  [  8.20834269 124.57247237]\n",
      "Done with gradient descent at iteration  841\n",
      "Learned weights =  [  8.21808621 124.57247237]\n",
      "Done with gradient descent at iteration  841\n",
      "Learned weights =  [  8.21808621 124.57247052]\n",
      "Done with gradient descent at iteration  842\n",
      "Learned weights =  [  8.22782973 124.57247052]\n",
      "Done with gradient descent at iteration  842\n",
      "Learned weights =  [  8.22782973 124.57246866]\n",
      "Done with gradient descent at iteration  843\n",
      "Learned weights =  [  8.23757326 124.57246866]\n",
      "Done with gradient descent at iteration  843\n",
      "Learned weights =  [  8.23757326 124.57246681]\n",
      "Done with gradient descent at iteration  844\n",
      "Learned weights =  [  8.24731678 124.57246681]\n",
      "Done with gradient descent at iteration  844\n",
      "Learned weights =  [  8.24731678 124.57246495]\n",
      "Done with gradient descent at iteration  845\n",
      "Learned weights =  [  8.2570603  124.57246495]\n",
      "Done with gradient descent at iteration  845\n",
      "Learned weights =  [  8.2570603 124.5724631]\n",
      "Done with gradient descent at iteration  846\n",
      "Learned weights =  [  8.26680383 124.5724631 ]\n",
      "Done with gradient descent at iteration  846\n",
      "Learned weights =  [  8.26680383 124.57246124]\n",
      "Done with gradient descent at iteration  847\n",
      "Learned weights =  [  8.27654735 124.57246124]\n",
      "Done with gradient descent at iteration  847\n",
      "Learned weights =  [  8.27654735 124.57245939]\n",
      "Done with gradient descent at iteration  848\n",
      "Learned weights =  [  8.28629087 124.57245939]\n",
      "Done with gradient descent at iteration  848\n",
      "Learned weights =  [  8.28629087 124.57245753]\n",
      "Done with gradient descent at iteration  849\n",
      "Learned weights =  [  8.29603439 124.57245753]\n",
      "Done with gradient descent at iteration  849\n",
      "Learned weights =  [  8.29603439 124.57245568]\n",
      "Done with gradient descent at iteration  850\n",
      "Learned weights =  [  8.30577791 124.57245568]\n",
      "Done with gradient descent at iteration  850\n",
      "Learned weights =  [  8.30577791 124.57245382]\n",
      "Done with gradient descent at iteration  851\n",
      "Learned weights =  [  8.31552143 124.57245382]\n",
      "Done with gradient descent at iteration  851\n",
      "Learned weights =  [  8.31552143 124.57245197]\n",
      "Done with gradient descent at iteration  852\n",
      "Learned weights =  [  8.32526496 124.57245197]\n",
      "Done with gradient descent at iteration  852\n",
      "Learned weights =  [  8.32526496 124.57245012]\n",
      "Done with gradient descent at iteration  853\n",
      "Learned weights =  [  8.33500848 124.57245012]\n",
      "Done with gradient descent at iteration  853\n",
      "Learned weights =  [  8.33500848 124.57244826]\n",
      "Done with gradient descent at iteration  854\n",
      "Learned weights =  [  8.344752   124.57244826]\n",
      "Done with gradient descent at iteration  854\n",
      "Learned weights =  [  8.344752   124.57244641]\n",
      "Done with gradient descent at iteration  855\n",
      "Learned weights =  [  8.35449552 124.57244641]\n",
      "Done with gradient descent at iteration  855\n",
      "Learned weights =  [  8.35449552 124.57244455]\n",
      "Done with gradient descent at iteration  856\n",
      "Learned weights =  [  8.36423904 124.57244455]\n",
      "Done with gradient descent at iteration  856\n",
      "Learned weights =  [  8.36423904 124.5724427 ]\n",
      "Done with gradient descent at iteration  857\n",
      "Learned weights =  [  8.37398256 124.5724427 ]\n",
      "Done with gradient descent at iteration  857\n",
      "Learned weights =  [  8.37398256 124.57244084]\n",
      "Done with gradient descent at iteration  858\n",
      "Learned weights =  [  8.38372608 124.57244084]\n",
      "Done with gradient descent at iteration  858\n",
      "Learned weights =  [  8.38372608 124.57243899]\n",
      "Done with gradient descent at iteration  859\n",
      "Learned weights =  [  8.3934696  124.57243899]\n",
      "Done with gradient descent at iteration  859\n",
      "Learned weights =  [  8.3934696  124.57243713]\n",
      "Done with gradient descent at iteration  860\n",
      "Learned weights =  [  8.40321312 124.57243713]\n",
      "Done with gradient descent at iteration  860\n",
      "Learned weights =  [  8.40321312 124.57243528]\n",
      "Done with gradient descent at iteration  861\n",
      "Learned weights =  [  8.41295664 124.57243528]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  861\n",
      "Learned weights =  [  8.41295664 124.57243343]\n",
      "Done with gradient descent at iteration  862\n",
      "Learned weights =  [  8.42270016 124.57243343]\n",
      "Done with gradient descent at iteration  862\n",
      "Learned weights =  [  8.42270016 124.57243157]\n",
      "Done with gradient descent at iteration  863\n",
      "Learned weights =  [  8.43244368 124.57243157]\n",
      "Done with gradient descent at iteration  863\n",
      "Learned weights =  [  8.43244368 124.57242972]\n",
      "Done with gradient descent at iteration  864\n",
      "Learned weights =  [  8.44218719 124.57242972]\n",
      "Done with gradient descent at iteration  864\n",
      "Learned weights =  [  8.44218719 124.57242786]\n",
      "Done with gradient descent at iteration  865\n",
      "Learned weights =  [  8.45193071 124.57242786]\n",
      "Done with gradient descent at iteration  865\n",
      "Learned weights =  [  8.45193071 124.57242601]\n",
      "Done with gradient descent at iteration  866\n",
      "Learned weights =  [  8.46167423 124.57242601]\n",
      "Done with gradient descent at iteration  866\n",
      "Learned weights =  [  8.46167423 124.57242415]\n",
      "Done with gradient descent at iteration  867\n",
      "Learned weights =  [  8.47141775 124.57242415]\n",
      "Done with gradient descent at iteration  867\n",
      "Learned weights =  [  8.47141775 124.5724223 ]\n",
      "Done with gradient descent at iteration  868\n",
      "Learned weights =  [  8.48116127 124.5724223 ]\n",
      "Done with gradient descent at iteration  868\n",
      "Learned weights =  [  8.48116127 124.57242044]\n",
      "Done with gradient descent at iteration  869\n",
      "Learned weights =  [  8.49090478 124.57242044]\n",
      "Done with gradient descent at iteration  869\n",
      "Learned weights =  [  8.49090478 124.57241859]\n",
      "Done with gradient descent at iteration  870\n",
      "Learned weights =  [  8.5006483  124.57241859]\n",
      "Done with gradient descent at iteration  870\n",
      "Learned weights =  [  8.5006483  124.57241673]\n",
      "Done with gradient descent at iteration  871\n",
      "Learned weights =  [  8.51039182 124.57241673]\n",
      "Done with gradient descent at iteration  871\n",
      "Learned weights =  [  8.51039182 124.57241488]\n",
      "Done with gradient descent at iteration  872\n",
      "Learned weights =  [  8.52013534 124.57241488]\n",
      "Done with gradient descent at iteration  872\n",
      "Learned weights =  [  8.52013534 124.57241303]\n",
      "Done with gradient descent at iteration  873\n",
      "Learned weights =  [  8.52987885 124.57241303]\n",
      "Done with gradient descent at iteration  873\n",
      "Learned weights =  [  8.52987885 124.57241117]\n",
      "Done with gradient descent at iteration  874\n",
      "Learned weights =  [  8.53962237 124.57241117]\n",
      "Done with gradient descent at iteration  874\n",
      "Learned weights =  [  8.53962237 124.57240932]\n",
      "Done with gradient descent at iteration  875\n",
      "Learned weights =  [  8.54936589 124.57240932]\n",
      "Done with gradient descent at iteration  875\n",
      "Learned weights =  [  8.54936589 124.57240746]\n",
      "Done with gradient descent at iteration  876\n",
      "Learned weights =  [  8.5591094  124.57240746]\n",
      "Done with gradient descent at iteration  876\n",
      "Learned weights =  [  8.5591094  124.57240561]\n",
      "Done with gradient descent at iteration  877\n",
      "Learned weights =  [  8.56885292 124.57240561]\n",
      "Done with gradient descent at iteration  877\n",
      "Learned weights =  [  8.56885292 124.57240375]\n",
      "Done with gradient descent at iteration  878\n",
      "Learned weights =  [  8.57859643 124.57240375]\n",
      "Done with gradient descent at iteration  878\n",
      "Learned weights =  [  8.57859643 124.5724019 ]\n",
      "Done with gradient descent at iteration  879\n",
      "Learned weights =  [  8.58833995 124.5724019 ]\n",
      "Done with gradient descent at iteration  879\n",
      "Learned weights =  [  8.58833995 124.57240004]\n",
      "Done with gradient descent at iteration  880\n",
      "Learned weights =  [  8.59808347 124.57240004]\n",
      "Done with gradient descent at iteration  880\n",
      "Learned weights =  [  8.59808347 124.57239819]\n",
      "Done with gradient descent at iteration  881\n",
      "Learned weights =  [  8.60782698 124.57239819]\n",
      "Done with gradient descent at iteration  881\n",
      "Learned weights =  [  8.60782698 124.57239633]\n",
      "Done with gradient descent at iteration  882\n",
      "Learned weights =  [  8.6175705  124.57239633]\n",
      "Done with gradient descent at iteration  882\n",
      "Learned weights =  [  8.6175705  124.57239448]\n",
      "Done with gradient descent at iteration  883\n",
      "Learned weights =  [  8.62731401 124.57239448]\n",
      "Done with gradient descent at iteration  883\n",
      "Learned weights =  [  8.62731401 124.57239263]\n",
      "Done with gradient descent at iteration  884\n",
      "Learned weights =  [  8.63705753 124.57239263]\n",
      "Done with gradient descent at iteration  884\n",
      "Learned weights =  [  8.63705753 124.57239077]\n",
      "Done with gradient descent at iteration  885\n",
      "Learned weights =  [  8.64680104 124.57239077]\n",
      "Done with gradient descent at iteration  885\n",
      "Learned weights =  [  8.64680104 124.57238892]\n",
      "Done with gradient descent at iteration  886\n",
      "Learned weights =  [  8.65654455 124.57238892]\n",
      "Done with gradient descent at iteration  886\n",
      "Learned weights =  [  8.65654455 124.57238706]\n",
      "Done with gradient descent at iteration  887\n",
      "Learned weights =  [  8.66628807 124.57238706]\n",
      "Done with gradient descent at iteration  887\n",
      "Learned weights =  [  8.66628807 124.57238521]\n",
      "Done with gradient descent at iteration  888\n",
      "Learned weights =  [  8.67603158 124.57238521]\n",
      "Done with gradient descent at iteration  888\n",
      "Learned weights =  [  8.67603158 124.57238335]\n",
      "Done with gradient descent at iteration  889\n",
      "Learned weights =  [  8.6857751  124.57238335]\n",
      "Done with gradient descent at iteration  889\n",
      "Learned weights =  [  8.6857751 124.5723815]\n",
      "Done with gradient descent at iteration  890\n",
      "Learned weights =  [  8.69551861 124.5723815 ]\n",
      "Done with gradient descent at iteration  890\n",
      "Learned weights =  [  8.69551861 124.57237964]\n",
      "Done with gradient descent at iteration  891\n",
      "Learned weights =  [  8.70526212 124.57237964]\n",
      "Done with gradient descent at iteration  891\n",
      "Learned weights =  [  8.70526212 124.57237779]\n",
      "Done with gradient descent at iteration  892\n",
      "Learned weights =  [  8.71500564 124.57237779]\n",
      "Done with gradient descent at iteration  892\n",
      "Learned weights =  [  8.71500564 124.57237593]\n",
      "Done with gradient descent at iteration  893\n",
      "Learned weights =  [  8.72474915 124.57237593]\n",
      "Done with gradient descent at iteration  893\n",
      "Learned weights =  [  8.72474915 124.57237408]\n",
      "Done with gradient descent at iteration  894\n",
      "Learned weights =  [  8.73449266 124.57237408]\n",
      "Done with gradient descent at iteration  894\n",
      "Learned weights =  [  8.73449266 124.57237223]\n",
      "Done with gradient descent at iteration  895\n",
      "Learned weights =  [  8.74423617 124.57237223]\n",
      "Done with gradient descent at iteration  895\n",
      "Learned weights =  [  8.74423617 124.57237037]\n",
      "Done with gradient descent at iteration  896\n",
      "Learned weights =  [  8.75397969 124.57237037]\n",
      "Done with gradient descent at iteration  896\n",
      "Learned weights =  [  8.75397969 124.57236852]\n",
      "Done with gradient descent at iteration  897\n",
      "Learned weights =  [  8.7637232  124.57236852]\n",
      "Done with gradient descent at iteration  897\n",
      "Learned weights =  [  8.7637232  124.57236666]\n",
      "Done with gradient descent at iteration  898\n",
      "Learned weights =  [  8.77346671 124.57236666]\n",
      "Done with gradient descent at iteration  898\n",
      "Learned weights =  [  8.77346671 124.57236481]\n",
      "Done with gradient descent at iteration  899\n",
      "Learned weights =  [  8.78321022 124.57236481]\n",
      "Done with gradient descent at iteration  899\n",
      "Learned weights =  [  8.78321022 124.57236295]\n",
      "Iteration = 900\n",
      "Cost function =  4484765284631358.0\n",
      "Done with gradient descent at iteration  900\n",
      "Learned weights =  [  8.79295373 124.57236295]\n",
      "Done with gradient descent at iteration  900\n",
      "Learned weights =  [  8.79295373 124.5723611 ]\n",
      "Done with gradient descent at iteration  901\n",
      "Learned weights =  [  8.80269724 124.5723611 ]\n",
      "Done with gradient descent at iteration  901\n",
      "Learned weights =  [  8.80269724 124.57235924]\n",
      "Done with gradient descent at iteration  902\n",
      "Learned weights =  [  8.81244075 124.57235924]\n",
      "Done with gradient descent at iteration  902\n",
      "Learned weights =  [  8.81244075 124.57235739]\n",
      "Done with gradient descent at iteration  903\n",
      "Learned weights =  [  8.82218426 124.57235739]\n",
      "Done with gradient descent at iteration  903\n",
      "Learned weights =  [  8.82218426 124.57235554]\n",
      "Done with gradient descent at iteration  904\n",
      "Learned weights =  [  8.83192778 124.57235554]\n",
      "Done with gradient descent at iteration  904\n",
      "Learned weights =  [  8.83192778 124.57235368]\n",
      "Done with gradient descent at iteration  905\n",
      "Learned weights =  [  8.84167129 124.57235368]\n",
      "Done with gradient descent at iteration  905\n",
      "Learned weights =  [  8.84167129 124.57235183]\n",
      "Done with gradient descent at iteration  906\n",
      "Learned weights =  [  8.8514148  124.57235183]\n",
      "Done with gradient descent at iteration  906\n",
      "Learned weights =  [  8.8514148  124.57234997]\n",
      "Done with gradient descent at iteration  907\n",
      "Learned weights =  [  8.86115831 124.57234997]\n",
      "Done with gradient descent at iteration  907\n",
      "Learned weights =  [  8.86115831 124.57234812]\n",
      "Done with gradient descent at iteration  908\n",
      "Learned weights =  [  8.87090182 124.57234812]\n",
      "Done with gradient descent at iteration  908\n",
      "Learned weights =  [  8.87090182 124.57234626]\n",
      "Done with gradient descent at iteration  909\n",
      "Learned weights =  [  8.88064532 124.57234626]\n",
      "Done with gradient descent at iteration  909\n",
      "Learned weights =  [  8.88064532 124.57234441]\n",
      "Done with gradient descent at iteration  910\n",
      "Learned weights =  [  8.89038883 124.57234441]\n",
      "Done with gradient descent at iteration  910\n",
      "Learned weights =  [  8.89038883 124.57234255]\n",
      "Done with gradient descent at iteration  911\n",
      "Learned weights =  [  8.90013234 124.57234255]\n",
      "Done with gradient descent at iteration  911\n",
      "Learned weights =  [  8.90013234 124.5723407 ]\n",
      "Done with gradient descent at iteration  912\n",
      "Learned weights =  [  8.90987585 124.5723407 ]\n",
      "Done with gradient descent at iteration  912\n",
      "Learned weights =  [  8.90987585 124.57233884]\n",
      "Done with gradient descent at iteration  913\n",
      "Learned weights =  [  8.91961936 124.57233884]\n",
      "Done with gradient descent at iteration  913\n",
      "Learned weights =  [  8.91961936 124.57233699]\n",
      "Done with gradient descent at iteration  914\n",
      "Learned weights =  [  8.92936287 124.57233699]\n",
      "Done with gradient descent at iteration  914\n",
      "Learned weights =  [  8.92936287 124.57233514]\n",
      "Done with gradient descent at iteration  915\n",
      "Learned weights =  [  8.93910638 124.57233514]\n",
      "Done with gradient descent at iteration  915\n",
      "Learned weights =  [  8.93910638 124.57233328]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  916\n",
      "Learned weights =  [  8.94884989 124.57233328]\n",
      "Done with gradient descent at iteration  916\n",
      "Learned weights =  [  8.94884989 124.57233143]\n",
      "Done with gradient descent at iteration  917\n",
      "Learned weights =  [  8.95859339 124.57233143]\n",
      "Done with gradient descent at iteration  917\n",
      "Learned weights =  [  8.95859339 124.57232957]\n",
      "Done with gradient descent at iteration  918\n",
      "Learned weights =  [  8.9683369  124.57232957]\n",
      "Done with gradient descent at iteration  918\n",
      "Learned weights =  [  8.9683369  124.57232772]\n",
      "Done with gradient descent at iteration  919\n",
      "Learned weights =  [  8.97808041 124.57232772]\n",
      "Done with gradient descent at iteration  919\n",
      "Learned weights =  [  8.97808041 124.57232586]\n",
      "Done with gradient descent at iteration  920\n",
      "Learned weights =  [  8.98782392 124.57232586]\n",
      "Done with gradient descent at iteration  920\n",
      "Learned weights =  [  8.98782392 124.57232401]\n",
      "Done with gradient descent at iteration  921\n",
      "Learned weights =  [  8.99756742 124.57232401]\n",
      "Done with gradient descent at iteration  921\n",
      "Learned weights =  [  8.99756742 124.57232215]\n",
      "Done with gradient descent at iteration  922\n",
      "Learned weights =  [  9.00731093 124.57232215]\n",
      "Done with gradient descent at iteration  922\n",
      "Learned weights =  [  9.00731093 124.5723203 ]\n",
      "Done with gradient descent at iteration  923\n",
      "Learned weights =  [  9.01705444 124.5723203 ]\n",
      "Done with gradient descent at iteration  923\n",
      "Learned weights =  [  9.01705444 124.57231844]\n",
      "Done with gradient descent at iteration  924\n",
      "Learned weights =  [  9.02679794 124.57231844]\n",
      "Done with gradient descent at iteration  924\n",
      "Learned weights =  [  9.02679794 124.57231659]\n",
      "Done with gradient descent at iteration  925\n",
      "Learned weights =  [  9.03654145 124.57231659]\n",
      "Done with gradient descent at iteration  925\n",
      "Learned weights =  [  9.03654145 124.57231474]\n",
      "Done with gradient descent at iteration  926\n",
      "Learned weights =  [  9.04628496 124.57231474]\n",
      "Done with gradient descent at iteration  926\n",
      "Learned weights =  [  9.04628496 124.57231288]\n",
      "Done with gradient descent at iteration  927\n",
      "Learned weights =  [  9.05602846 124.57231288]\n",
      "Done with gradient descent at iteration  927\n",
      "Learned weights =  [  9.05602846 124.57231103]\n",
      "Done with gradient descent at iteration  928\n",
      "Learned weights =  [  9.06577197 124.57231103]\n",
      "Done with gradient descent at iteration  928\n",
      "Learned weights =  [  9.06577197 124.57230917]\n",
      "Done with gradient descent at iteration  929\n",
      "Learned weights =  [  9.07551547 124.57230917]\n",
      "Done with gradient descent at iteration  929\n",
      "Learned weights =  [  9.07551547 124.57230732]\n",
      "Done with gradient descent at iteration  930\n",
      "Learned weights =  [  9.08525898 124.57230732]\n",
      "Done with gradient descent at iteration  930\n",
      "Learned weights =  [  9.08525898 124.57230546]\n",
      "Done with gradient descent at iteration  931\n",
      "Learned weights =  [  9.09500248 124.57230546]\n",
      "Done with gradient descent at iteration  931\n",
      "Learned weights =  [  9.09500248 124.57230361]\n",
      "Done with gradient descent at iteration  932\n",
      "Learned weights =  [  9.10474599 124.57230361]\n",
      "Done with gradient descent at iteration  932\n",
      "Learned weights =  [  9.10474599 124.57230175]\n",
      "Done with gradient descent at iteration  933\n",
      "Learned weights =  [  9.11448949 124.57230175]\n",
      "Done with gradient descent at iteration  933\n",
      "Learned weights =  [  9.11448949 124.5722999 ]\n",
      "Done with gradient descent at iteration  934\n",
      "Learned weights =  [  9.124233  124.5722999]\n",
      "Done with gradient descent at iteration  934\n",
      "Learned weights =  [  9.124233   124.57229804]\n",
      "Done with gradient descent at iteration  935\n",
      "Learned weights =  [  9.1339765  124.57229804]\n",
      "Done with gradient descent at iteration  935\n",
      "Learned weights =  [  9.1339765  124.57229619]\n",
      "Done with gradient descent at iteration  936\n",
      "Learned weights =  [  9.14372    124.57229619]\n",
      "Done with gradient descent at iteration  936\n",
      "Learned weights =  [  9.14372    124.57229434]\n",
      "Done with gradient descent at iteration  937\n",
      "Learned weights =  [  9.15346351 124.57229434]\n",
      "Done with gradient descent at iteration  937\n",
      "Learned weights =  [  9.15346351 124.57229248]\n",
      "Done with gradient descent at iteration  938\n",
      "Learned weights =  [  9.16320701 124.57229248]\n",
      "Done with gradient descent at iteration  938\n",
      "Learned weights =  [  9.16320701 124.57229063]\n",
      "Done with gradient descent at iteration  939\n",
      "Learned weights =  [  9.17295052 124.57229063]\n",
      "Done with gradient descent at iteration  939\n",
      "Learned weights =  [  9.17295052 124.57228877]\n",
      "Done with gradient descent at iteration  940\n",
      "Learned weights =  [  9.18269402 124.57228877]\n",
      "Done with gradient descent at iteration  940\n",
      "Learned weights =  [  9.18269402 124.57228692]\n",
      "Done with gradient descent at iteration  941\n",
      "Learned weights =  [  9.19243752 124.57228692]\n",
      "Done with gradient descent at iteration  941\n",
      "Learned weights =  [  9.19243752 124.57228506]\n",
      "Done with gradient descent at iteration  942\n",
      "Learned weights =  [  9.20218102 124.57228506]\n",
      "Done with gradient descent at iteration  942\n",
      "Learned weights =  [  9.20218102 124.57228321]\n",
      "Done with gradient descent at iteration  943\n",
      "Learned weights =  [  9.21192453 124.57228321]\n",
      "Done with gradient descent at iteration  943\n",
      "Learned weights =  [  9.21192453 124.57228135]\n",
      "Done with gradient descent at iteration  944\n",
      "Learned weights =  [  9.22166803 124.57228135]\n",
      "Done with gradient descent at iteration  944\n",
      "Learned weights =  [  9.22166803 124.5722795 ]\n",
      "Done with gradient descent at iteration  945\n",
      "Learned weights =  [  9.23141153 124.5722795 ]\n",
      "Done with gradient descent at iteration  945\n",
      "Learned weights =  [  9.23141153 124.57227765]\n",
      "Done with gradient descent at iteration  946\n",
      "Learned weights =  [  9.24115503 124.57227765]\n",
      "Done with gradient descent at iteration  946\n",
      "Learned weights =  [  9.24115503 124.57227579]\n",
      "Done with gradient descent at iteration  947\n",
      "Learned weights =  [  9.25089853 124.57227579]\n",
      "Done with gradient descent at iteration  947\n",
      "Learned weights =  [  9.25089853 124.57227394]\n",
      "Done with gradient descent at iteration  948\n",
      "Learned weights =  [  9.26064204 124.57227394]\n",
      "Done with gradient descent at iteration  948\n",
      "Learned weights =  [  9.26064204 124.57227208]\n",
      "Done with gradient descent at iteration  949\n",
      "Learned weights =  [  9.27038554 124.57227208]\n",
      "Done with gradient descent at iteration  949\n",
      "Learned weights =  [  9.27038554 124.57227023]\n",
      "Done with gradient descent at iteration  950\n",
      "Learned weights =  [  9.28012904 124.57227023]\n",
      "Done with gradient descent at iteration  950\n",
      "Learned weights =  [  9.28012904 124.57226837]\n",
      "Done with gradient descent at iteration  951\n",
      "Learned weights =  [  9.28987254 124.57226837]\n",
      "Done with gradient descent at iteration  951\n",
      "Learned weights =  [  9.28987254 124.57226652]\n",
      "Done with gradient descent at iteration  952\n",
      "Learned weights =  [  9.29961604 124.57226652]\n",
      "Done with gradient descent at iteration  952\n",
      "Learned weights =  [  9.29961604 124.57226466]\n",
      "Done with gradient descent at iteration  953\n",
      "Learned weights =  [  9.30935954 124.57226466]\n",
      "Done with gradient descent at iteration  953\n",
      "Learned weights =  [  9.30935954 124.57226281]\n",
      "Done with gradient descent at iteration  954\n",
      "Learned weights =  [  9.31910304 124.57226281]\n",
      "Done with gradient descent at iteration  954\n",
      "Learned weights =  [  9.31910304 124.57226095]\n",
      "Done with gradient descent at iteration  955\n",
      "Learned weights =  [  9.32884654 124.57226095]\n",
      "Done with gradient descent at iteration  955\n",
      "Learned weights =  [  9.32884654 124.5722591 ]\n",
      "Done with gradient descent at iteration  956\n",
      "Learned weights =  [  9.33859004 124.5722591 ]\n",
      "Done with gradient descent at iteration  956\n",
      "Learned weights =  [  9.33859004 124.57225725]\n",
      "Done with gradient descent at iteration  957\n",
      "Learned weights =  [  9.34833354 124.57225725]\n",
      "Done with gradient descent at iteration  957\n",
      "Learned weights =  [  9.34833354 124.57225539]\n",
      "Done with gradient descent at iteration  958\n",
      "Learned weights =  [  9.35807704 124.57225539]\n",
      "Done with gradient descent at iteration  958\n",
      "Learned weights =  [  9.35807704 124.57225354]\n",
      "Done with gradient descent at iteration  959\n",
      "Learned weights =  [  9.36782054 124.57225354]\n",
      "Done with gradient descent at iteration  959\n",
      "Learned weights =  [  9.36782054 124.57225168]\n",
      "Done with gradient descent at iteration  960\n",
      "Learned weights =  [  9.37756404 124.57225168]\n",
      "Done with gradient descent at iteration  960\n",
      "Learned weights =  [  9.37756404 124.57224983]\n",
      "Done with gradient descent at iteration  961\n",
      "Learned weights =  [  9.38730754 124.57224983]\n",
      "Done with gradient descent at iteration  961\n",
      "Learned weights =  [  9.38730754 124.57224797]\n",
      "Done with gradient descent at iteration  962\n",
      "Learned weights =  [  9.39705104 124.57224797]\n",
      "Done with gradient descent at iteration  962\n",
      "Learned weights =  [  9.39705104 124.57224612]\n",
      "Done with gradient descent at iteration  963\n",
      "Learned weights =  [  9.40679453 124.57224612]\n",
      "Done with gradient descent at iteration  963\n",
      "Learned weights =  [  9.40679453 124.57224426]\n",
      "Done with gradient descent at iteration  964\n",
      "Learned weights =  [  9.41653803 124.57224426]\n",
      "Done with gradient descent at iteration  964\n",
      "Learned weights =  [  9.41653803 124.57224241]\n",
      "Done with gradient descent at iteration  965\n",
      "Learned weights =  [  9.42628153 124.57224241]\n",
      "Done with gradient descent at iteration  965\n",
      "Learned weights =  [  9.42628153 124.57224055]\n",
      "Done with gradient descent at iteration  966\n",
      "Learned weights =  [  9.43602503 124.57224055]\n",
      "Done with gradient descent at iteration  966\n",
      "Learned weights =  [  9.43602503 124.5722387 ]\n",
      "Done with gradient descent at iteration  967\n",
      "Learned weights =  [  9.44576853 124.5722387 ]\n",
      "Done with gradient descent at iteration  967\n",
      "Learned weights =  [  9.44576853 124.57223685]\n",
      "Done with gradient descent at iteration  968\n",
      "Learned weights =  [  9.45551202 124.57223685]\n",
      "Done with gradient descent at iteration  968\n",
      "Learned weights =  [  9.45551202 124.57223499]\n",
      "Done with gradient descent at iteration  969\n",
      "Learned weights =  [  9.46525552 124.57223499]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  969\n",
      "Learned weights =  [  9.46525552 124.57223314]\n",
      "Done with gradient descent at iteration  970\n",
      "Learned weights =  [  9.47499902 124.57223314]\n",
      "Done with gradient descent at iteration  970\n",
      "Learned weights =  [  9.47499902 124.57223128]\n",
      "Done with gradient descent at iteration  971\n",
      "Learned weights =  [  9.48474252 124.57223128]\n",
      "Done with gradient descent at iteration  971\n",
      "Learned weights =  [  9.48474252 124.57222943]\n",
      "Done with gradient descent at iteration  972\n",
      "Learned weights =  [  9.49448601 124.57222943]\n",
      "Done with gradient descent at iteration  972\n",
      "Learned weights =  [  9.49448601 124.57222757]\n",
      "Done with gradient descent at iteration  973\n",
      "Learned weights =  [  9.50422951 124.57222757]\n",
      "Done with gradient descent at iteration  973\n",
      "Learned weights =  [  9.50422951 124.57222572]\n",
      "Done with gradient descent at iteration  974\n",
      "Learned weights =  [  9.513973   124.57222572]\n",
      "Done with gradient descent at iteration  974\n",
      "Learned weights =  [  9.513973   124.57222386]\n",
      "Done with gradient descent at iteration  975\n",
      "Learned weights =  [  9.5237165  124.57222386]\n",
      "Done with gradient descent at iteration  975\n",
      "Learned weights =  [  9.5237165  124.57222201]\n",
      "Done with gradient descent at iteration  976\n",
      "Learned weights =  [  9.53346    124.57222201]\n",
      "Done with gradient descent at iteration  976\n",
      "Learned weights =  [  9.53346    124.57222015]\n",
      "Done with gradient descent at iteration  977\n",
      "Learned weights =  [  9.54320349 124.57222015]\n",
      "Done with gradient descent at iteration  977\n",
      "Learned weights =  [  9.54320349 124.5722183 ]\n",
      "Done with gradient descent at iteration  978\n",
      "Learned weights =  [  9.55294699 124.5722183 ]\n",
      "Done with gradient descent at iteration  978\n",
      "Learned weights =  [  9.55294699 124.57221645]\n",
      "Done with gradient descent at iteration  979\n",
      "Learned weights =  [  9.56269048 124.57221645]\n",
      "Done with gradient descent at iteration  979\n",
      "Learned weights =  [  9.56269048 124.57221459]\n",
      "Done with gradient descent at iteration  980\n",
      "Learned weights =  [  9.57243398 124.57221459]\n",
      "Done with gradient descent at iteration  980\n",
      "Learned weights =  [  9.57243398 124.57221274]\n",
      "Done with gradient descent at iteration  981\n",
      "Learned weights =  [  9.58217747 124.57221274]\n",
      "Done with gradient descent at iteration  981\n",
      "Learned weights =  [  9.58217747 124.57221088]\n",
      "Done with gradient descent at iteration  982\n",
      "Learned weights =  [  9.59192097 124.57221088]\n",
      "Done with gradient descent at iteration  982\n",
      "Learned weights =  [  9.59192097 124.57220903]\n",
      "Done with gradient descent at iteration  983\n",
      "Learned weights =  [  9.60166446 124.57220903]\n",
      "Done with gradient descent at iteration  983\n",
      "Learned weights =  [  9.60166446 124.57220717]\n",
      "Done with gradient descent at iteration  984\n",
      "Learned weights =  [  9.61140796 124.57220717]\n",
      "Done with gradient descent at iteration  984\n",
      "Learned weights =  [  9.61140796 124.57220532]\n",
      "Done with gradient descent at iteration  985\n",
      "Learned weights =  [  9.62115145 124.57220532]\n",
      "Done with gradient descent at iteration  985\n",
      "Learned weights =  [  9.62115145 124.57220346]\n",
      "Done with gradient descent at iteration  986\n",
      "Learned weights =  [  9.63089494 124.57220346]\n",
      "Done with gradient descent at iteration  986\n",
      "Learned weights =  [  9.63089494 124.57220161]\n",
      "Done with gradient descent at iteration  987\n",
      "Learned weights =  [  9.64063844 124.57220161]\n",
      "Done with gradient descent at iteration  987\n",
      "Learned weights =  [  9.64063844 124.57219976]\n",
      "Done with gradient descent at iteration  988\n",
      "Learned weights =  [  9.65038193 124.57219976]\n",
      "Done with gradient descent at iteration  988\n",
      "Learned weights =  [  9.65038193 124.5721979 ]\n",
      "Done with gradient descent at iteration  989\n",
      "Learned weights =  [  9.66012542 124.5721979 ]\n",
      "Done with gradient descent at iteration  989\n",
      "Learned weights =  [  9.66012542 124.57219605]\n",
      "Done with gradient descent at iteration  990\n",
      "Learned weights =  [  9.66986892 124.57219605]\n",
      "Done with gradient descent at iteration  990\n",
      "Learned weights =  [  9.66986892 124.57219419]\n",
      "Done with gradient descent at iteration  991\n",
      "Learned weights =  [  9.67961241 124.57219419]\n",
      "Done with gradient descent at iteration  991\n",
      "Learned weights =  [  9.67961241 124.57219234]\n",
      "Done with gradient descent at iteration  992\n",
      "Learned weights =  [  9.6893559  124.57219234]\n",
      "Done with gradient descent at iteration  992\n",
      "Learned weights =  [  9.6893559  124.57219048]\n",
      "Done with gradient descent at iteration  993\n",
      "Learned weights =  [  9.69909939 124.57219048]\n",
      "Done with gradient descent at iteration  993\n",
      "Learned weights =  [  9.69909939 124.57218863]\n",
      "Done with gradient descent at iteration  994\n",
      "Learned weights =  [  9.70884289 124.57218863]\n",
      "Done with gradient descent at iteration  994\n",
      "Learned weights =  [  9.70884289 124.57218677]\n",
      "Done with gradient descent at iteration  995\n",
      "Learned weights =  [  9.71858638 124.57218677]\n",
      "Done with gradient descent at iteration  995\n",
      "Learned weights =  [  9.71858638 124.57218492]\n",
      "Done with gradient descent at iteration  996\n",
      "Learned weights =  [  9.72832987 124.57218492]\n",
      "Done with gradient descent at iteration  996\n",
      "Learned weights =  [  9.72832987 124.57218306]\n",
      "Done with gradient descent at iteration  997\n",
      "Learned weights =  [  9.73807336 124.57218306]\n",
      "Done with gradient descent at iteration  997\n",
      "Learned weights =  [  9.73807336 124.57218121]\n",
      "Done with gradient descent at iteration  998\n",
      "Learned weights =  [  9.74781685 124.57218121]\n",
      "Done with gradient descent at iteration  998\n",
      "Learned weights =  [  9.74781685 124.57217936]\n",
      "Done with gradient descent at iteration  999\n",
      "Learned weights =  [  9.75756034 124.57217936]\n",
      "Done with gradient descent at iteration  999\n",
      "Learned weights =  [  9.75756034 124.5721775 ]\n",
      "Iteration = 1000\n",
      "Cost function =  4484755791049491.0\n",
      "Done with gradient descent at iteration  1000\n",
      "Learned weights =  [  9.76730383 124.5721775 ]\n",
      "Done with gradient descent at iteration  1000\n",
      "Learned weights =  [  9.76730383 124.57217565]\n"
     ]
    }
   ],
   "source": [
    "simple_weights_high_penalty = ridge_regression_gradient_descent(simple_feature_matrix, output, initial_weights, step_size, 1e11, max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code will plot the two learned models.  (The blue line is for the model with no regularization and the red line is for the one with high regularization.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1180.]\n",
      " [2570.]\n",
      " [ 770.]\n",
      " ...\n",
      " [1530.]\n",
      " [1600.]\n",
      " [1020.]]\n"
     ]
    }
   ],
   "source": [
    "print (simple_feature_matrix[:,1:2])\n",
    "#print (output.shape)\n",
    "#print(simple_feature_matrix[:1],output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1222de7b8>,\n",
       " <matplotlib.lines.Line2D at 0x1222decc0>,\n",
       " <matplotlib.lines.Line2D at 0x1222deb38>]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(simple_feature_matrix[:,1:2],output,'k.',\n",
    "         simple_feature_matrix[:,1:2],predict_output(simple_feature_matrix, simple_weights_0_penalty),'b-',\n",
    "        simple_feature_matrix[:,1:2],predict_output(simple_feature_matrix, simple_weights_high_penalty),'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the RSS on the TEST data for the following three sets of weights:\n",
    "1. The initial weights (all zeros)\n",
    "2. The weights learned with no regularization\n",
    "3. The weights learned with high regularization\n",
    "\n",
    "Which weights perform best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2082510724935533"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((predict_output(simple_feature_matrix, simple_weights_0_penalty)-output)**2)/1e15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.932933001588834"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((predict_output(simple_feature_matrix, simple_weights_high_penalty)-output)**2)/1e15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.433051851026171"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((predict_output(simple_feature_matrix, [0,0])-output)**2)/1e15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.757236345975463"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((predict_output(simple_test_feature_matrix,simple_weights_0_penalty )-test_output)**2)/1e14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***QUIZ QUESTIONS***\n",
    "1. What is the value of the coefficient for `sqft_living` that you learned with no regularization, rounded to 1 decimal place?  What about the one with high regularization?\n",
    ": no reg 263.0\n",
    ": high reg 124.6\n",
    "\n",
    "2. Comparing the lines you fit with the with no regularization versus high regularization, which one is steeper?\n",
    ": no reg is steeper\n",
    "3. What are the RSS on the test data for each of the set of weights above (initial, no regularization, high regularization)? \n",
    ": initial 7.43 * 1e15\n",
    ": no 1.208 * 1e15\n",
    ": high 2.93 * 1e15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.63113501e-01  2.63024369e+02]\n",
      "[  9.76730383 124.57217565]\n"
     ]
    }
   ],
   "source": [
    "print (simple_weights_0_penalty)\n",
    "print (simple_weights_high_penalty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running a multiple regression with L2 penalty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now consider a model with 2 features: `['sqft_living', 'sqft_living15']`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, create Numpy versions of your training and test data with these two features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_features = ['sqft_living', 'sqft_living15'] # sqft_living15 is the average squarefeet for the nearest 15 neighbors. \n",
    "my_output = 'price'\n",
    "(feature_matrix, output) = get_numpy_data(train_data, model_features, my_output)\n",
    "(test_feature_matrix, test_output) = get_numpy_data(test_data, model_features, my_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to re-inialize the weights, since we have one extra parameter. Let us also set the step size and maximum number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = np.array([0.0,0.0,0.0])\n",
    "step_size = 1e-12\n",
    "max_iterations = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's consider no regularization.  Set the `l2_penalty` to `0.0` and run your ridge regression algorithm to learn the weights of your model.  Call your weights:\n",
    "\n",
    "`multiple_weights_0_penalty`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent with l2_penalty = 0\n",
      "Iteration = 1\n",
      "Cost function =  7433051851026171.0\n",
      "Done with gradient descent at iteration  1\n",
      "Learned weights =  [0.0187527 0.        0.       ]\n",
      "Done with gradient descent at iteration  1\n",
      "Learned weights =  [1.87526989e-02 4.73325137e+01 0.00000000e+00]\n",
      "Done with gradient descent at iteration  1\n",
      "Learned weights =  [1.87526989e-02 4.73325137e+01 4.23911280e+01]\n",
      "Iteration = 2\n",
      "Cost function =  4056752331500973.0\n",
      "Done with gradient descent at iteration  2\n",
      "Learned weights =  [3.11553143e-02 4.73325137e+01 4.23911280e+01]\n",
      "Done with gradient descent at iteration  2\n",
      "Learned weights =  [3.11553143e-02 7.93537160e+01 4.23911280e+01]\n",
      "Done with gradient descent at iteration  2\n",
      "Learned weights =  [3.11553143e-02 7.93537160e+01 7.06890272e+01]\n",
      "Iteration = 3\n",
      "Cost function =  2529565114333587.0\n",
      "Done with gradient descent at iteration  3\n",
      "Learned weights =  [3.92882612e-02 7.93537160e+01 7.06890272e+01]\n",
      "Done with gradient descent at iteration  3\n",
      "Learned weights =  [3.92882612e-02 1.01077531e+02 7.06890272e+01]\n",
      "Done with gradient descent at iteration  3\n",
      "Learned weights =  [3.92882612e-02 1.01077531e+02 8.95110259e+01]\n",
      "Iteration = 4\n",
      "Cost function =  1838556694275924.5\n",
      "Done with gradient descent at iteration  4\n",
      "Learned weights =  [4.45505334e-02 1.01077531e+02 8.95110259e+01]\n",
      "Done with gradient descent at iteration  4\n",
      "Learned weights =  [4.45505334e-02 1.15875629e+02 8.95110259e+01]\n",
      "Done with gradient descent at iteration  4\n",
      "Learned weights =  [4.45505334e-02 1.15875629e+02 1.01962087e+02]\n",
      "Iteration = 5\n",
      "Cost function =  1525675575208602.5\n",
      "Done with gradient descent at iteration  5\n",
      "Learned weights =  [4.78828954e-02 1.15875629e+02 1.01962087e+02]\n",
      "Done with gradient descent at iteration  5\n",
      "Learned weights =  [4.78828954e-02 1.26015332e+02 1.01962087e+02]\n",
      "Done with gradient descent at iteration  5\n",
      "Learned weights =  [4.78828954e-02 1.26015332e+02 1.10130166e+02]\n",
      "Iteration = 6\n",
      "Cost function =  1383789498674793.0\n",
      "Done with gradient descent at iteration  6\n",
      "Learned weights =  [4.99179712e-02 1.26015332e+02 1.10130166e+02]\n",
      "Done with gradient descent at iteration  6\n",
      "Learned weights =  [4.99179712e-02 1.33021331e+02 1.10130166e+02]\n",
      "Done with gradient descent at iteration  6\n",
      "Learned weights =  [4.99179712e-02 1.33021331e+02 1.15419326e+02]\n",
      "Iteration = 7\n",
      "Cost function =  1319232606276634.0\n",
      "Done with gradient descent at iteration  7\n",
      "Learned weights =  [5.10811723e-02 1.33021331e+02 1.15419326e+02]\n",
      "Done with gradient descent at iteration  7\n",
      "Learned weights =  [5.10811723e-02 1.37918927e+02 1.15419326e+02]\n",
      "Done with gradient descent at iteration  7\n",
      "Learned weights =  [5.10811723e-02 1.37918927e+02 1.18773733e+02]\n",
      "Iteration = 8\n",
      "Cost function =  1289648872028920.8\n",
      "Done with gradient descent at iteration  8\n",
      "Learned weights =  [5.16585679e-02 1.37918927e+02 1.18773733e+02]\n",
      "Done with gradient descent at iteration  8\n",
      "Learned weights =  [5.16585679e-02 1.41397600e+02 1.18773733e+02]\n",
      "Done with gradient descent at iteration  8\n",
      "Learned weights =  [5.16585679e-02 1.41397600e+02 1.20828296e+02]\n",
      "Iteration = 9\n",
      "Cost function =  1275884724079266.8\n",
      "Done with gradient descent at iteration  9\n",
      "Learned weights =  [5.18425256e-02 1.41397600e+02 1.20828296e+02]\n",
      "Done with gradient descent at iteration  9\n",
      "Learned weights =  [5.18425256e-02 1.43921004e+02 1.20828296e+02]\n",
      "Done with gradient descent at iteration  9\n",
      "Learned weights =  [5.18425256e-02 1.43921004e+02 1.22009959e+02]\n",
      "Iteration = 10\n",
      "Cost function =  1269278807577156.8\n",
      "Done with gradient descent at iteration  10\n",
      "Learned weights =  [5.17624018e-02 1.43921004e+02 1.22009959e+02]\n",
      "Done with gradient descent at iteration  10\n",
      "Learned weights =  [5.17624018e-02 1.45800937e+02 1.22009959e+02]\n",
      "Done with gradient descent at iteration  10\n",
      "Learned weights =  [5.17624018e-02 1.45800937e+02 1.22605817e+02]\n",
      "Done with gradient descent at iteration  11\n",
      "Learned weights =  [5.15051807e-02 1.45800937e+02 1.22605817e+02]\n",
      "Done with gradient descent at iteration  11\n",
      "Learned weights =  [5.15051807e-02 1.47247074e+02 1.22605817e+02]\n",
      "Done with gradient descent at iteration  11\n",
      "Learned weights =  [5.15051807e-02 1.47247074e+02 1.22808924e+02]\n",
      "Done with gradient descent at iteration  12\n",
      "Learned weights =  [5.11293528e-02 1.47247074e+02 1.22808924e+02]\n",
      "Done with gradient descent at iteration  12\n",
      "Learned weights =  [5.11293528e-02 1.48400422e+02 1.22808924e+02]\n",
      "Done with gradient descent at iteration  12\n",
      "Learned weights =  [5.11293528e-02 1.48400422e+02 1.22749093e+02]\n",
      "Done with gradient descent at iteration  13\n",
      "Learned weights =  [5.06742479e-02 1.48400422e+02 1.22749093e+02]\n",
      "Done with gradient descent at iteration  13\n",
      "Learned weights =  [5.06742479e-02 1.49355808e+02 1.22749093e+02]\n",
      "Done with gradient descent at iteration  13\n",
      "Learned weights =  [5.06742479e-02 1.49355808e+02 1.22513612e+02]\n",
      "Done with gradient descent at iteration  14\n",
      "Learned weights =  [5.01663107e-02 1.49355808e+02 1.22513612e+02]\n",
      "Done with gradient descent at iteration  14\n",
      "Learned weights =  [5.01663107e-02 1.50177006e+02 1.22513612e+02]\n",
      "Done with gradient descent at iteration  14\n",
      "Learned weights =  [5.01663107e-02 1.50177006e+02 1.22161171e+02]\n",
      "Done with gradient descent at iteration  15\n",
      "Learned weights =  [4.96233215e-02 1.50177006e+02 1.22161171e+02]\n",
      "Done with gradient descent at iteration  15\n",
      "Learned weights =  [4.96233215e-02 1.50906907e+02 1.22161171e+02]\n",
      "Done with gradient descent at iteration  15\n",
      "Learned weights =  [4.96233215e-02 1.50906907e+02 1.21731230e+02]\n",
      "Done with gradient descent at iteration  16\n",
      "Learned weights =  [4.90572339e-02 1.50906907e+02 1.21731230e+02]\n",
      "Done with gradient descent at iteration  16\n",
      "Learned weights =  [4.90572339e-02 1.51574362e+02 1.21731230e+02]\n",
      "Done with gradient descent at iteration  16\n",
      "Learned weights =  [4.90572339e-02 1.51574362e+02 1.21250319e+02]\n",
      "Done with gradient descent at iteration  17\n",
      "Learned weights =  [4.84760836e-02 1.51574362e+02 1.21250319e+02]\n",
      "Done with gradient descent at iteration  17\n",
      "Learned weights =  [4.84760836e-02 1.52198775e+02 1.21250319e+02]\n",
      "Done with gradient descent at iteration  17\n",
      "Learned weights =  [4.84760836e-02 1.52198775e+02 1.20736269e+02]\n",
      "Done with gradient descent at iteration  18\n",
      "Learned weights =  [4.78852713e-02 1.52198775e+02 1.20736269e+02]\n",
      "Done with gradient descent at iteration  18\n",
      "Learned weights =  [4.78852713e-02 1.52793204e+02 1.20736269e+02]\n",
      "Done with gradient descent at iteration  18\n",
      "Learned weights =  [4.78852713e-02 1.52793204e+02 1.20201066e+02]\n",
      "Done with gradient descent at iteration  19\n",
      "Learned weights =  [4.72884261e-02 1.52793204e+02 1.20201066e+02]\n",
      "Done with gradient descent at iteration  19\n",
      "Learned weights =  [4.72884261e-02 1.53366433e+02 1.20201066e+02]\n",
      "Done with gradient descent at iteration  19\n",
      "Learned weights =  [4.72884261e-02 1.53366433e+02 1.19652763e+02]\n",
      "Iteration = 20\n",
      "Cost function =  1257812386316614.8\n",
      "Done with gradient descent at iteration  20\n",
      "Learned weights =  [4.66879858e-02 1.53366433e+02 1.19652763e+02]\n",
      "Done with gradient descent at iteration  20\n",
      "Learned weights =  [4.66879858e-02 1.53924379e+02 1.19652763e+02]\n",
      "Done with gradient descent at iteration  20\n",
      "Learned weights =  [4.66879858e-02 1.53924379e+02 1.19096767e+02]\n",
      "Done with gradient descent at iteration  21\n",
      "Learned weights =  [4.60855866e-02 1.53924379e+02 1.19096767e+02]\n",
      "Done with gradient descent at iteration  21\n",
      "Learned weights =  [4.60855866e-02 1.54471023e+02 1.19096767e+02]\n",
      "Done with gradient descent at iteration  21\n",
      "Learned weights =  [4.60855866e-02 1.54471023e+02 1.18536710e+02]\n",
      "Done with gradient descent at iteration  22\n",
      "Learned weights =  [4.54823265e-02 1.54471023e+02 1.18536710e+02]\n",
      "Done with gradient descent at iteration  22\n",
      "Learned weights =  [4.54823265e-02 1.55009050e+02 1.18536710e+02]\n",
      "Done with gradient descent at iteration  22\n",
      "Learned weights =  [4.54823265e-02 1.55009050e+02 1.17975025e+02]\n",
      "Done with gradient descent at iteration  23\n",
      "Learned weights =  [4.48789408e-02 1.55009050e+02 1.17975025e+02]\n",
      "Done with gradient descent at iteration  23\n",
      "Learned weights =  [4.48789408e-02 1.55540273e+02 1.17975025e+02]\n",
      "Done with gradient descent at iteration  23\n",
      "Learned weights =  [4.48789408e-02 1.55540273e+02 1.17413345e+02]\n",
      "Done with gradient descent at iteration  24\n",
      "Learned weights =  [4.42759216e-02 1.55540273e+02 1.17413345e+02]\n",
      "Done with gradient descent at iteration  24\n",
      "Learned weights =  [4.42759216e-02 1.56065914e+02 1.17413345e+02]\n",
      "Done with gradient descent at iteration  24\n",
      "Learned weights =  [4.42759216e-02 1.56065914e+02 1.16852757e+02]\n",
      "Done with gradient descent at iteration  25\n",
      "Learned weights =  [4.36735968e-02 1.56065914e+02 1.16852757e+02]\n",
      "Done with gradient descent at iteration  25\n",
      "Learned weights =  [4.36735968e-02 1.56586803e+02 1.16852757e+02]\n",
      "Done with gradient descent at iteration  25\n",
      "Learned weights =  [4.36735968e-02 1.56586803e+02 1.16293991e+02]\n",
      "Done with gradient descent at iteration  26\n",
      "Learned weights =  [4.30721845e-02 1.56586803e+02 1.16293991e+02]\n",
      "Done with gradient descent at iteration  26\n",
      "Learned weights =  [4.30721845e-02 1.57103503e+02 1.16293991e+02]\n",
      "Done with gradient descent at iteration  26\n",
      "Learned weights =  [4.30721845e-02 1.57103503e+02 1.15737526e+02]\n",
      "Done with gradient descent at iteration  27\n",
      "Learned weights =  [4.24718286e-02 1.57103503e+02 1.15737526e+02]\n",
      "Done with gradient descent at iteration  27\n",
      "Learned weights =  [4.24718286e-02 1.57616400e+02 1.15737526e+02]\n",
      "Done with gradient descent at iteration  27\n",
      "Learned weights =  [4.24718286e-02 1.57616400e+02 1.15183682e+02]\n",
      "Done with gradient descent at iteration  28\n",
      "Learned weights =  [4.18726232e-02 1.57616400e+02 1.15183682e+02]\n",
      "Done with gradient descent at iteration  28\n",
      "Learned weights =  [4.18726232e-02 1.58125757e+02 1.15183682e+02]\n",
      "Done with gradient descent at iteration  28\n",
      "Learned weights =  [4.18726232e-02 1.58125757e+02 1.14632665e+02]\n",
      "Done with gradient descent at iteration  29\n",
      "Learned weights =  [4.12746292e-02 1.58125757e+02 1.14632665e+02]\n",
      "Done with gradient descent at iteration  29\n",
      "Learned weights =  [4.12746292e-02 1.58631759e+02 1.14632665e+02]\n",
      "Done with gradient descent at iteration  29\n",
      "Learned weights =  [4.12746292e-02 1.58631759e+02 1.14084609e+02]\n",
      "Iteration = 30\n",
      "Cost function =  1251954571266786.0\n",
      "Done with gradient descent at iteration  30\n",
      "Learned weights =  [4.06778847e-02 1.58631759e+02 1.14084609e+02]\n",
      "Done with gradient descent at iteration  30\n",
      "Learned weights =  [4.06778847e-02 1.59134534e+02 1.14084609e+02]\n",
      "Done with gradient descent at iteration  30\n",
      "Learned weights =  [4.06778847e-02 1.59134534e+02 1.13539596e+02]\n",
      "Done with gradient descent at iteration  31\n",
      "Learned weights =  [4.00824128e-02 1.59134534e+02 1.13539596e+02]\n",
      "Done with gradient descent at iteration  31\n",
      "Learned weights =  [4.00824128e-02 1.59634177e+02 1.13539596e+02]\n",
      "Done with gradient descent at iteration  31\n",
      "Learned weights =  [4.00824128e-02 1.59634177e+02 1.12997677e+02]\n",
      "Done with gradient descent at iteration  32\n",
      "Learned weights =  [3.94882265e-02 1.59634177e+02 1.12997677e+02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  32\n",
      "Learned weights =  [3.94882265e-02 1.60130754e+02 1.12997677e+02]\n",
      "Done with gradient descent at iteration  32\n",
      "Learned weights =  [3.94882265e-02 1.60130754e+02 1.12458880e+02]\n",
      "Done with gradient descent at iteration  33\n",
      "Learned weights =  [3.88953321e-02 1.60130754e+02 1.12458880e+02]\n",
      "Done with gradient descent at iteration  33\n",
      "Learned weights =  [3.88953321e-02 1.60624317e+02 1.12458880e+02]\n",
      "Done with gradient descent at iteration  33\n",
      "Learned weights =  [3.88953321e-02 1.60624317e+02 1.11923215e+02]\n",
      "Done with gradient descent at iteration  34\n",
      "Learned weights =  [3.83037312e-02 1.60624317e+02 1.11923215e+02]\n",
      "Done with gradient descent at iteration  34\n",
      "Learned weights =  [3.83037312e-02 1.61114907e+02 1.11923215e+02]\n",
      "Done with gradient descent at iteration  34\n",
      "Learned weights =  [3.83037312e-02 1.61114907e+02 1.11390685e+02]\n",
      "Done with gradient descent at iteration  35\n",
      "Learned weights =  [3.77134222e-02 1.61114907e+02 1.11390685e+02]\n",
      "Done with gradient descent at iteration  35\n",
      "Learned weights =  [3.77134222e-02 1.61602555e+02 1.11390685e+02]\n",
      "Done with gradient descent at iteration  35\n",
      "Learned weights =  [3.77134222e-02 1.61602555e+02 1.10861285e+02]\n",
      "Done with gradient descent at iteration  36\n",
      "Learned weights =  [3.71244018e-02 1.61602555e+02 1.10861285e+02]\n",
      "Done with gradient descent at iteration  36\n",
      "Learned weights =  [3.71244018e-02 1.62087291e+02 1.10861285e+02]\n",
      "Done with gradient descent at iteration  36\n",
      "Learned weights =  [3.71244018e-02 1.62087291e+02 1.10335007e+02]\n",
      "Done with gradient descent at iteration  37\n",
      "Learned weights =  [3.65366652e-02 1.62087291e+02 1.10335007e+02]\n",
      "Done with gradient descent at iteration  37\n",
      "Learned weights =  [3.65366652e-02 1.62569137e+02 1.10335007e+02]\n",
      "Done with gradient descent at iteration  37\n",
      "Learned weights =  [3.65366652e-02 1.62569137e+02 1.09811837e+02]\n",
      "Done with gradient descent at iteration  38\n",
      "Learned weights =  [3.59502066e-02 1.62569137e+02 1.09811837e+02]\n",
      "Done with gradient descent at iteration  38\n",
      "Learned weights =  [3.59502066e-02 1.63048116e+02 1.09811837e+02]\n",
      "Done with gradient descent at iteration  38\n",
      "Learned weights =  [3.59502066e-02 1.63048116e+02 1.09291762e+02]\n",
      "Done with gradient descent at iteration  39\n",
      "Learned weights =  [3.53650196e-02 1.63048116e+02 1.09291762e+02]\n",
      "Done with gradient descent at iteration  39\n",
      "Learned weights =  [3.53650196e-02 1.63524247e+02 1.09291762e+02]\n",
      "Done with gradient descent at iteration  39\n",
      "Learned weights =  [3.53650196e-02 1.63524247e+02 1.08774766e+02]\n",
      "Iteration = 40\n",
      "Cost function =  1246755423155437.5\n",
      "Done with gradient descent at iteration  40\n",
      "Learned weights =  [3.47810978e-02 1.63524247e+02 1.08774766e+02]\n",
      "Done with gradient descent at iteration  40\n",
      "Learned weights =  [3.47810978e-02 1.63997550e+02 1.08774766e+02]\n",
      "Done with gradient descent at iteration  40\n",
      "Learned weights =  [3.47810978e-02 1.63997550e+02 1.08260832e+02]\n",
      "Done with gradient descent at iteration  41\n",
      "Learned weights =  [3.41984340e-02 1.63997550e+02 1.08260832e+02]\n",
      "Done with gradient descent at iteration  41\n",
      "Learned weights =  [3.41984340e-02 1.64468043e+02 1.08260832e+02]\n",
      "Done with gradient descent at iteration  41\n",
      "Learned weights =  [3.41984340e-02 1.64468043e+02 1.07749945e+02]\n",
      "Done with gradient descent at iteration  42\n",
      "Learned weights =  [3.36170213e-02 1.64468043e+02 1.07749945e+02]\n",
      "Done with gradient descent at iteration  42\n",
      "Learned weights =  [3.36170213e-02 1.64935743e+02 1.07749945e+02]\n",
      "Done with gradient descent at iteration  42\n",
      "Learned weights =  [3.36170213e-02 1.64935743e+02 1.07242086e+02]\n",
      "Done with gradient descent at iteration  43\n",
      "Learned weights =  [3.30368524e-02 1.64935743e+02 1.07242086e+02]\n",
      "Done with gradient descent at iteration  43\n",
      "Learned weights =  [3.30368524e-02 1.65400667e+02 1.07242086e+02]\n",
      "Done with gradient descent at iteration  43\n",
      "Learned weights =  [3.30368524e-02 1.65400667e+02 1.06737238e+02]\n",
      "Done with gradient descent at iteration  44\n",
      "Learned weights =  [3.24579202e-02 1.65400667e+02 1.06737238e+02]\n",
      "Done with gradient descent at iteration  44\n",
      "Learned weights =  [3.24579202e-02 1.65862833e+02 1.06737238e+02]\n",
      "Done with gradient descent at iteration  44\n",
      "Learned weights =  [3.24579202e-02 1.65862833e+02 1.06235384e+02]\n",
      "Done with gradient descent at iteration  45\n",
      "Learned weights =  [3.18802175e-02 1.65862833e+02 1.06235384e+02]\n",
      "Done with gradient descent at iteration  45\n",
      "Learned weights =  [3.18802175e-02 1.66322257e+02 1.06235384e+02]\n",
      "Done with gradient descent at iteration  45\n",
      "Learned weights =  [3.18802175e-02 1.66322257e+02 1.05736506e+02]\n",
      "Done with gradient descent at iteration  46\n",
      "Learned weights =  [3.13037370e-02 1.66322257e+02 1.05736506e+02]\n",
      "Done with gradient descent at iteration  46\n",
      "Learned weights =  [3.13037370e-02 1.66778955e+02 1.05736506e+02]\n",
      "Done with gradient descent at iteration  46\n",
      "Learned weights =  [3.13037370e-02 1.66778955e+02 1.05240587e+02]\n",
      "Done with gradient descent at iteration  47\n",
      "Learned weights =  [3.07284716e-02 1.66778955e+02 1.05240587e+02]\n",
      "Done with gradient descent at iteration  47\n",
      "Learned weights =  [3.07284716e-02 1.67232945e+02 1.05240587e+02]\n",
      "Done with gradient descent at iteration  47\n",
      "Learned weights =  [3.07284716e-02 1.67232945e+02 1.04747609e+02]\n",
      "Done with gradient descent at iteration  48\n",
      "Learned weights =  [3.01544140e-02 1.67232945e+02 1.04747609e+02]\n",
      "Done with gradient descent at iteration  48\n",
      "Learned weights =  [3.01544140e-02 1.67684241e+02 1.04747609e+02]\n",
      "Done with gradient descent at iteration  48\n",
      "Learned weights =  [3.01544140e-02 1.67684241e+02 1.04257555e+02]\n",
      "Done with gradient descent at iteration  49\n",
      "Learned weights =  [2.95815571e-02 1.67684241e+02 1.04257555e+02]\n",
      "Done with gradient descent at iteration  49\n",
      "Learned weights =  [2.95815571e-02 1.68132860e+02 1.04257555e+02]\n",
      "Done with gradient descent at iteration  49\n",
      "Learned weights =  [2.95815571e-02 1.68132860e+02 1.03770408e+02]\n",
      "Iteration = 50\n",
      "Cost function =  1242139508748821.0\n",
      "Done with gradient descent at iteration  50\n",
      "Learned weights =  [2.90098939e-02 1.68132860e+02 1.03770408e+02]\n",
      "Done with gradient descent at iteration  50\n",
      "Learned weights =  [2.90098939e-02 1.68578818e+02 1.03770408e+02]\n",
      "Done with gradient descent at iteration  50\n",
      "Learned weights =  [2.90098939e-02 1.68578818e+02 1.03286151e+02]\n",
      "Done with gradient descent at iteration  51\n",
      "Learned weights =  [2.84394173e-02 1.68578818e+02 1.03286151e+02]\n",
      "Done with gradient descent at iteration  51\n",
      "Learned weights =  [2.84394173e-02 1.69022131e+02 1.03286151e+02]\n",
      "Done with gradient descent at iteration  51\n",
      "Learned weights =  [2.84394173e-02 1.69022131e+02 1.02804765e+02]\n",
      "Done with gradient descent at iteration  52\n",
      "Learned weights =  [2.78701201e-02 1.69022131e+02 1.02804765e+02]\n",
      "Done with gradient descent at iteration  52\n",
      "Learned weights =  [2.78701201e-02 1.69462814e+02 1.02804765e+02]\n",
      "Done with gradient descent at iteration  52\n",
      "Learned weights =  [2.78701201e-02 1.69462814e+02 1.02326236e+02]\n",
      "Done with gradient descent at iteration  53\n",
      "Learned weights =  [2.73019956e-02 1.69462814e+02 1.02326236e+02]\n",
      "Done with gradient descent at iteration  53\n",
      "Learned weights =  [2.73019956e-02 1.69900883e+02 1.02326236e+02]\n",
      "Done with gradient descent at iteration  53\n",
      "Learned weights =  [2.73019956e-02 1.69900883e+02 1.01850544e+02]\n",
      "Done with gradient descent at iteration  54\n",
      "Learned weights =  [2.67350365e-02 1.69900883e+02 1.01850544e+02]\n",
      "Done with gradient descent at iteration  54\n",
      "Learned weights =  [2.67350365e-02 1.70336354e+02 1.01850544e+02]\n",
      "Done with gradient descent at iteration  54\n",
      "Learned weights =  [2.67350365e-02 1.70336354e+02 1.01377674e+02]\n",
      "Done with gradient descent at iteration  55\n",
      "Learned weights =  [2.61692362e-02 1.70336354e+02 1.01377674e+02]\n",
      "Done with gradient descent at iteration  55\n",
      "Learned weights =  [2.61692362e-02 1.70769241e+02 1.01377674e+02]\n",
      "Done with gradient descent at iteration  55\n",
      "Learned weights =  [2.61692362e-02 1.70769241e+02 1.00907609e+02]\n",
      "Done with gradient descent at iteration  56\n",
      "Learned weights =  [2.56045877e-02 1.70769241e+02 1.00907609e+02]\n",
      "Done with gradient descent at iteration  56\n",
      "Learned weights =  [2.56045877e-02 1.71199561e+02 1.00907609e+02]\n",
      "Done with gradient descent at iteration  56\n",
      "Learned weights =  [2.56045877e-02 1.71199561e+02 1.00440332e+02]\n",
      "Done with gradient descent at iteration  57\n",
      "Learned weights =  [2.50410841e-02 1.71199561e+02 1.00440332e+02]\n",
      "Done with gradient descent at iteration  57\n",
      "Learned weights =  [2.50410841e-02 1.71627329e+02 1.00440332e+02]\n",
      "Done with gradient descent at iteration  57\n",
      "Learned weights =  [2.50410841e-02 1.71627329e+02 9.99758272e+01]\n",
      "Done with gradient descent at iteration  58\n",
      "Learned weights =  [2.44787187e-02 1.71627329e+02 9.99758272e+01]\n",
      "Done with gradient descent at iteration  58\n",
      "Learned weights =  [2.44787187e-02 1.72052559e+02 9.99758272e+01]\n",
      "Done with gradient descent at iteration  58\n",
      "Learned weights =  [2.44787187e-02 1.72052559e+02 9.95140773e+01]\n",
      "Done with gradient descent at iteration  59\n",
      "Learned weights =  [2.39174848e-02 1.72052559e+02 9.95140773e+01]\n",
      "Done with gradient descent at iteration  59\n",
      "Learned weights =  [2.39174848e-02 1.72475267e+02 9.95140773e+01]\n",
      "Done with gradient descent at iteration  59\n",
      "Learned weights =  [2.39174848e-02 1.72475267e+02 9.90550663e+01]\n",
      "Iteration = 60\n",
      "Cost function =  1238041401137187.8\n",
      "Done with gradient descent at iteration  60\n",
      "Learned weights =  [2.33573756e-02 1.72475267e+02 9.90550663e+01]\n",
      "Done with gradient descent at iteration  60\n",
      "Learned weights =  [2.33573756e-02 1.72895468e+02 9.90550663e+01]\n",
      "Done with gradient descent at iteration  60\n",
      "Learned weights =  [2.33573756e-02 1.72895468e+02 9.85987780e+01]\n",
      "Done with gradient descent at iteration  61\n",
      "Learned weights =  [2.27983844e-02 1.72895468e+02 9.85987780e+01]\n",
      "Done with gradient descent at iteration  61\n",
      "Learned weights =  [2.27983844e-02 1.73313176e+02 9.85987780e+01]\n",
      "Done with gradient descent at iteration  61\n",
      "Learned weights =  [2.27983844e-02 1.73313176e+02 9.81451961e+01]\n",
      "Done with gradient descent at iteration  62\n",
      "Learned weights =  [2.22405046e-02 1.73313176e+02 9.81451961e+01]\n",
      "Done with gradient descent at iteration  62\n",
      "Learned weights =  [2.22405046e-02 1.73728406e+02 9.81451961e+01]\n",
      "Done with gradient descent at iteration  62\n",
      "Learned weights =  [2.22405046e-02 1.73728406e+02 9.76943048e+01]\n",
      "Done with gradient descent at iteration  63\n",
      "Learned weights =  [2.16837296e-02 1.73728406e+02 9.76943048e+01]\n",
      "Done with gradient descent at iteration  63\n",
      "Learned weights =  [2.16837296e-02 1.74141174e+02 9.76943048e+01]\n",
      "Done with gradient descent at iteration  63\n",
      "Learned weights =  [2.16837296e-02 1.74141174e+02 9.72460879e+01]\n",
      "Done with gradient descent at iteration  64\n",
      "Learned weights =  [2.11280530e-02 1.74141174e+02 9.72460879e+01]\n",
      "Done with gradient descent at iteration  64\n",
      "Learned weights =  [2.11280530e-02 1.74551493e+02 9.72460879e+01]\n",
      "Done with gradient descent at iteration  64\n",
      "Learned weights =  [2.11280530e-02 1.74551493e+02 9.68005297e+01]\n",
      "Done with gradient descent at iteration  65\n",
      "Learned weights =  [2.05734680e-02 1.74551493e+02 9.68005297e+01]\n",
      "Done with gradient descent at iteration  65\n",
      "Learned weights =  [2.05734680e-02 1.74959378e+02 9.68005297e+01]\n",
      "Done with gradient descent at iteration  65\n",
      "Learned weights =  [2.05734680e-02 1.74959378e+02 9.63576143e+01]\n",
      "Done with gradient descent at iteration  66\n",
      "Learned weights =  [2.00199684e-02 1.74959378e+02 9.63576143e+01]\n",
      "Done with gradient descent at iteration  66\n",
      "Learned weights =  [2.00199684e-02 1.75364844e+02 9.63576143e+01]\n",
      "Done with gradient descent at iteration  66\n",
      "Learned weights =  [2.00199684e-02 1.75364844e+02 9.59173261e+01]\n",
      "Done with gradient descent at iteration  67\n",
      "Learned weights =  [1.94675476e-02 1.75364844e+02 9.59173261e+01]\n",
      "Done with gradient descent at iteration  67\n",
      "Learned weights =  [1.94675476e-02 1.75767905e+02 9.59173261e+01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  67\n",
      "Learned weights =  [1.94675476e-02 1.75767905e+02 9.54796495e+01]\n",
      "Done with gradient descent at iteration  68\n",
      "Learned weights =  [1.89161992e-02 1.75767905e+02 9.54796495e+01]\n",
      "Done with gradient descent at iteration  68\n",
      "Learned weights =  [1.89161992e-02 1.76168575e+02 9.54796495e+01]\n",
      "Done with gradient descent at iteration  68\n",
      "Learned weights =  [1.89161992e-02 1.76168575e+02 9.50445690e+01]\n",
      "Done with gradient descent at iteration  69\n",
      "Learned weights =  [1.83659169e-02 1.76168575e+02 9.50445690e+01]\n",
      "Done with gradient descent at iteration  69\n",
      "Learned weights =  [1.83659169e-02 1.76566868e+02 9.50445690e+01]\n",
      "Done with gradient descent at iteration  69\n",
      "Learned weights =  [1.83659169e-02 1.76566868e+02 9.46120692e+01]\n",
      "Iteration = 70\n",
      "Cost function =  1234403013463993.5\n",
      "Done with gradient descent at iteration  70\n",
      "Learned weights =  [1.78166944e-02 1.76566868e+02 9.46120692e+01]\n",
      "Done with gradient descent at iteration  70\n",
      "Learned weights =  [1.78166944e-02 1.76962799e+02 9.46120692e+01]\n",
      "Done with gradient descent at iteration  70\n",
      "Learned weights =  [1.78166944e-02 1.76962799e+02 9.41821349e+01]\n",
      "Done with gradient descent at iteration  71\n",
      "Learned weights =  [1.72685254e-02 1.76962799e+02 9.41821349e+01]\n",
      "Done with gradient descent at iteration  71\n",
      "Learned weights =  [1.72685254e-02 1.77356381e+02 9.41821349e+01]\n",
      "Done with gradient descent at iteration  71\n",
      "Learned weights =  [1.72685254e-02 1.77356381e+02 9.37547507e+01]\n",
      "Done with gradient descent at iteration  72\n",
      "Learned weights =  [1.67214036e-02 1.77356381e+02 9.37547507e+01]\n",
      "Done with gradient descent at iteration  72\n",
      "Learned weights =  [1.67214036e-02 1.77747629e+02 9.37547507e+01]\n",
      "Done with gradient descent at iteration  72\n",
      "Learned weights =  [1.67214036e-02 1.77747629e+02 9.33299016e+01]\n",
      "Done with gradient descent at iteration  73\n",
      "Learned weights =  [1.61753227e-02 1.77747629e+02 9.33299016e+01]\n",
      "Done with gradient descent at iteration  73\n",
      "Learned weights =  [1.61753227e-02 1.78136556e+02 9.33299016e+01]\n",
      "Done with gradient descent at iteration  73\n",
      "Learned weights =  [1.61753227e-02 1.78136556e+02 9.29075725e+01]\n",
      "Done with gradient descent at iteration  74\n",
      "Learned weights =  [1.56302768e-02 1.78136556e+02 9.29075725e+01]\n",
      "Done with gradient descent at iteration  74\n",
      "Learned weights =  [1.56302768e-02 1.78523177e+02 9.29075725e+01]\n",
      "Done with gradient descent at iteration  74\n",
      "Learned weights =  [1.56302768e-02 1.78523177e+02 9.24877485e+01]\n",
      "Done with gradient descent at iteration  75\n",
      "Learned weights =  [1.50862595e-02 1.78523177e+02 9.24877485e+01]\n",
      "Done with gradient descent at iteration  75\n",
      "Learned weights =  [1.50862595e-02 1.78907503e+02 9.24877485e+01]\n",
      "Done with gradient descent at iteration  75\n",
      "Learned weights =  [1.50862595e-02 1.78907503e+02 9.20704147e+01]\n",
      "Done with gradient descent at iteration  76\n",
      "Learned weights =  [1.45432648e-02 1.78907503e+02 9.20704147e+01]\n",
      "Done with gradient descent at iteration  76\n",
      "Learned weights =  [1.45432648e-02 1.79289551e+02 9.20704147e+01]\n",
      "Done with gradient descent at iteration  76\n",
      "Learned weights =  [1.45432648e-02 1.79289551e+02 9.16555564e+01]\n",
      "Done with gradient descent at iteration  77\n",
      "Learned weights =  [1.40012867e-02 1.79289551e+02 9.16555564e+01]\n",
      "Done with gradient descent at iteration  77\n",
      "Learned weights =  [1.40012867e-02 1.79669332e+02 9.16555564e+01]\n",
      "Done with gradient descent at iteration  77\n",
      "Learned weights =  [1.40012867e-02 1.79669332e+02 9.12431588e+01]\n",
      "Done with gradient descent at iteration  78\n",
      "Learned weights =  [1.34603190e-02 1.79669332e+02 9.12431588e+01]\n",
      "Done with gradient descent at iteration  78\n",
      "Learned weights =  [1.34603190e-02 1.80046860e+02 9.12431588e+01]\n",
      "Done with gradient descent at iteration  78\n",
      "Learned weights =  [1.34603190e-02 1.80046860e+02 9.08332074e+01]\n",
      "Done with gradient descent at iteration  79\n",
      "Learned weights =  [1.29203559e-02 1.80046860e+02 9.08332074e+01]\n",
      "Done with gradient descent at iteration  79\n",
      "Learned weights =  [1.29203559e-02 1.80422149e+02 9.08332074e+01]\n",
      "Done with gradient descent at iteration  79\n",
      "Learned weights =  [1.29203559e-02 1.80422149e+02 9.04256876e+01]\n",
      "Iteration = 80\n",
      "Cost function =  1231172774976820.2\n",
      "Done with gradient descent at iteration  80\n",
      "Learned weights =  [1.23813913e-02 1.80422149e+02 9.04256876e+01]\n",
      "Done with gradient descent at iteration  80\n",
      "Learned weights =  [1.23813913e-02 1.80795212e+02 9.04256876e+01]\n",
      "Done with gradient descent at iteration  80\n",
      "Learned weights =  [1.23813913e-02 1.80795212e+02 9.00205851e+01]\n",
      "Done with gradient descent at iteration  81\n",
      "Learned weights =  [1.18434193e-02 1.80795212e+02 9.00205851e+01]\n",
      "Done with gradient descent at iteration  81\n",
      "Learned weights =  [1.18434193e-02 1.81166063e+02 9.00205851e+01]\n",
      "Done with gradient descent at iteration  81\n",
      "Learned weights =  [1.18434193e-02 1.81166063e+02 8.96178855e+01]\n",
      "Done with gradient descent at iteration  82\n",
      "Learned weights =  [1.13064341e-02 1.81166063e+02 8.96178855e+01]\n",
      "Done with gradient descent at iteration  82\n",
      "Learned weights =  [1.13064341e-02 1.81534713e+02 8.96178855e+01]\n",
      "Done with gradient descent at iteration  82\n",
      "Learned weights =  [1.13064341e-02 1.81534713e+02 8.92175745e+01]\n",
      "Done with gradient descent at iteration  83\n",
      "Learned weights =  [1.07704297e-02 1.81534713e+02 8.92175745e+01]\n",
      "Done with gradient descent at iteration  83\n",
      "Learned weights =  [1.07704297e-02 1.81901177e+02 8.92175745e+01]\n",
      "Done with gradient descent at iteration  83\n",
      "Learned weights =  [1.07704297e-02 1.81901177e+02 8.88196380e+01]\n",
      "Done with gradient descent at iteration  84\n",
      "Learned weights =  [1.02354005e-02 1.81901177e+02 8.88196380e+01]\n",
      "Done with gradient descent at iteration  84\n",
      "Learned weights =  [1.02354005e-02 1.82265467e+02 8.88196380e+01]\n",
      "Done with gradient descent at iteration  84\n",
      "Learned weights =  [1.02354005e-02 1.82265467e+02 8.84240619e+01]\n",
      "Done with gradient descent at iteration  85\n",
      "Learned weights =  [9.70134048e-03 1.82265467e+02 8.84240619e+01]\n",
      "Done with gradient descent at iteration  85\n",
      "Learned weights =  [9.70134048e-03 1.82627596e+02 8.84240619e+01]\n",
      "Done with gradient descent at iteration  85\n",
      "Learned weights =  [9.70134048e-03 1.82627596e+02 8.80308322e+01]\n",
      "Done with gradient descent at iteration  86\n",
      "Learned weights =  [9.16824403e-03 1.82627596e+02 8.80308322e+01]\n",
      "Done with gradient descent at iteration  86\n",
      "Learned weights =  [9.16824403e-03 1.82987577e+02 8.80308322e+01]\n",
      "Done with gradient descent at iteration  86\n",
      "Learned weights =  [9.16824403e-03 1.82987577e+02 8.76399349e+01]\n",
      "Done with gradient descent at iteration  87\n",
      "Learned weights =  [8.63610540e-03 1.82987577e+02 8.76399349e+01]\n",
      "Done with gradient descent at iteration  87\n",
      "Learned weights =  [8.63610540e-03 1.83345423e+02 8.76399349e+01]\n",
      "Done with gradient descent at iteration  87\n",
      "Learned weights =  [8.63610540e-03 1.83345423e+02 8.72513563e+01]\n",
      "Done with gradient descent at iteration  88\n",
      "Learned weights =  [8.10491890e-03 1.83345423e+02 8.72513563e+01]\n",
      "Done with gradient descent at iteration  88\n",
      "Learned weights =  [8.10491890e-03 1.83701147e+02 8.72513563e+01]\n",
      "Done with gradient descent at iteration  88\n",
      "Learned weights =  [8.10491890e-03 1.83701147e+02 8.68650826e+01]\n",
      "Done with gradient descent at iteration  89\n",
      "Learned weights =  [7.57467889e-03 1.83701147e+02 8.68650826e+01]\n",
      "Done with gradient descent at iteration  89\n",
      "Learned weights =  [7.57467889e-03 1.84054760e+02 8.68650826e+01]\n",
      "Done with gradient descent at iteration  89\n",
      "Learned weights =  [7.57467889e-03 1.84054760e+02 8.64811001e+01]\n",
      "Iteration = 90\n",
      "Cost function =  1228304900059555.0\n",
      "Done with gradient descent at iteration  90\n",
      "Learned weights =  [7.04537976e-03 1.84054760e+02 8.64811001e+01]\n",
      "Done with gradient descent at iteration  90\n",
      "Learned weights =  [7.04537976e-03 1.84406276e+02 8.64811001e+01]\n",
      "Done with gradient descent at iteration  90\n",
      "Learned weights =  [7.04537976e-03 1.84406276e+02 8.60993952e+01]\n",
      "Done with gradient descent at iteration  91\n",
      "Learned weights =  [6.51701593e-03 1.84406276e+02 8.60993952e+01]\n",
      "Done with gradient descent at iteration  91\n",
      "Learned weights =  [6.51701593e-03 1.84755707e+02 8.60993952e+01]\n",
      "Done with gradient descent at iteration  91\n",
      "Learned weights =  [6.51701593e-03 1.84755707e+02 8.57199544e+01]\n",
      "Done with gradient descent at iteration  92\n",
      "Learned weights =  [5.98958184e-03 1.84755707e+02 8.57199544e+01]\n",
      "Done with gradient descent at iteration  92\n",
      "Learned weights =  [5.98958184e-03 1.85103065e+02 8.57199544e+01]\n",
      "Done with gradient descent at iteration  92\n",
      "Learned weights =  [5.98958184e-03 1.85103065e+02 8.53427643e+01]\n",
      "Done with gradient descent at iteration  93\n",
      "Learned weights =  [5.46307199e-03 1.85103065e+02 8.53427643e+01]\n",
      "Done with gradient descent at iteration  93\n",
      "Learned weights =  [5.46307199e-03 1.85448363e+02 8.53427643e+01]\n",
      "Done with gradient descent at iteration  93\n",
      "Learned weights =  [5.46307199e-03 1.85448363e+02 8.49678115e+01]\n",
      "Done with gradient descent at iteration  94\n",
      "Learned weights =  [4.93748088e-03 1.85448363e+02 8.49678115e+01]\n",
      "Done with gradient descent at iteration  94\n",
      "Learned weights =  [4.93748088e-03 1.85791613e+02 8.49678115e+01]\n",
      "Done with gradient descent at iteration  94\n",
      "Learned weights =  [4.93748088e-03 1.85791613e+02 8.45950828e+01]\n",
      "Done with gradient descent at iteration  95\n",
      "Learned weights =  [4.41280308e-03 1.85791613e+02 8.45950828e+01]\n",
      "Done with gradient descent at iteration  95\n",
      "Learned weights =  [4.41280308e-03 1.86132826e+02 8.45950828e+01]\n",
      "Done with gradient descent at iteration  95\n",
      "Learned weights =  [4.41280308e-03 1.86132826e+02 8.42245650e+01]\n",
      "Done with gradient descent at iteration  96\n",
      "Learned weights =  [3.88903316e-03 1.86132826e+02 8.42245650e+01]\n",
      "Done with gradient descent at iteration  96\n",
      "Learned weights =  [3.88903316e-03 1.86472016e+02 8.42245650e+01]\n",
      "Done with gradient descent at iteration  96\n",
      "Learned weights =  [3.88903316e-03 1.86472016e+02 8.38562449e+01]\n",
      "Done with gradient descent at iteration  97\n",
      "Learned weights =  [3.36616574e-03 1.86472016e+02 8.38562449e+01]\n",
      "Done with gradient descent at iteration  97\n",
      "Learned weights =  [3.36616574e-03 1.86809194e+02 8.38562449e+01]\n",
      "Done with gradient descent at iteration  97\n",
      "Learned weights =  [3.36616574e-03 1.86809194e+02 8.34901095e+01]\n",
      "Done with gradient descent at iteration  98\n",
      "Learned weights =  [2.84419546e-03 1.86809194e+02 8.34901095e+01]\n",
      "Done with gradient descent at iteration  98\n",
      "Learned weights =  [2.84419546e-03 1.87144372e+02 8.34901095e+01]\n",
      "Done with gradient descent at iteration  98\n",
      "Learned weights =  [2.84419546e-03 1.87144372e+02 8.31261459e+01]\n",
      "Done with gradient descent at iteration  99\n",
      "Learned weights =  [2.32311701e-03 1.87144372e+02 8.31261459e+01]\n",
      "Done with gradient descent at iteration  99\n",
      "Learned weights =  [2.32311701e-03 1.87477561e+02 8.31261459e+01]\n",
      "Done with gradient descent at iteration  99\n",
      "Learned weights =  [2.32311701e-03 1.87477561e+02 8.27643412e+01]\n",
      "Iteration = 100\n",
      "Cost function =  1225758739263725.8\n",
      "Done with gradient descent at iteration  100\n",
      "Learned weights =  [1.80292509e-03 1.87477561e+02 8.27643412e+01]\n",
      "Done with gradient descent at iteration  100\n",
      "Learned weights =  [1.80292509e-03 1.87808775e+02 8.27643412e+01]\n",
      "Done with gradient descent at iteration  100\n",
      "Learned weights =  [1.80292509e-03 1.87808775e+02 8.24046825e+01]\n",
      "Done with gradient descent at iteration  101\n",
      "Learned weights =  [1.28361444e-03 1.87808775e+02 8.24046825e+01]\n",
      "Done with gradient descent at iteration  101\n",
      "Learned weights =  [1.28361444e-03 1.88138023e+02 8.24046825e+01]\n",
      "Done with gradient descent at iteration  101\n",
      "Learned weights =  [1.28361444e-03 1.88138023e+02 8.20471572e+01]\n",
      "Done with gradient descent at iteration  102\n",
      "Learned weights =  [7.65179847e-04 1.88138023e+02 8.20471572e+01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  102\n",
      "Learned weights =  [7.65179847e-04 1.88465319e+02 8.20471572e+01]\n",
      "Done with gradient descent at iteration  102\n",
      "Learned weights =  [7.65179847e-04 1.88465319e+02 8.16917526e+01]\n",
      "Done with gradient descent at iteration  103\n",
      "Learned weights =  [2.47616102e-04 1.88465319e+02 8.16917526e+01]\n",
      "Done with gradient descent at iteration  103\n",
      "Learned weights =  [2.47616102e-04 1.88790674e+02 8.16917526e+01]\n",
      "Done with gradient descent at iteration  103\n",
      "Learned weights =  [2.47616102e-04 1.88790674e+02 8.13384561e+01]\n",
      "Done with gradient descent at iteration  104\n",
      "Learned weights =  [-2.69081957e-04  1.88790674e+02  8.13384561e+01]\n",
      "Done with gradient descent at iteration  104\n",
      "Learned weights =  [-2.69081957e-04  1.89114098e+02  8.13384561e+01]\n",
      "Done with gradient descent at iteration  104\n",
      "Learned weights =  [-2.69081957e-04  1.89114098e+02  8.09872552e+01]\n",
      "Done with gradient descent at iteration  105\n",
      "Learned weights =  [-7.84919464e-04  1.89114098e+02  8.09872552e+01]\n",
      "Done with gradient descent at iteration  105\n",
      "Learned weights =  [-7.84919464e-04  1.89435604e+02  8.09872552e+01]\n",
      "Done with gradient descent at iteration  105\n",
      "Learned weights =  [-7.84919464e-04  1.89435604e+02  8.06381374e+01]\n",
      "Done with gradient descent at iteration  106\n",
      "Learned weights =  [-1.29990153e-03  1.89435604e+02  8.06381374e+01]\n",
      "Done with gradient descent at iteration  106\n",
      "Learned weights =  [-1.29990153e-03  1.89755203e+02  8.06381374e+01]\n",
      "Done with gradient descent at iteration  106\n",
      "Learned weights =  [-1.29990153e-03  1.89755203e+02  8.02910905e+01]\n",
      "Done with gradient descent at iteration  107\n",
      "Learned weights =  [-1.81403321e-03  1.89755203e+02  8.02910905e+01]\n",
      "Done with gradient descent at iteration  107\n",
      "Learned weights =  [-1.81403321e-03  1.90072907e+02  8.02910905e+01]\n",
      "Done with gradient descent at iteration  107\n",
      "Learned weights =  [-1.81403321e-03  1.90072907e+02  7.99461021e+01]\n",
      "Done with gradient descent at iteration  108\n",
      "Learned weights =  [-2.32731957e-03  1.90072907e+02  7.99461021e+01]\n",
      "Done with gradient descent at iteration  108\n",
      "Learned weights =  [-2.32731957e-03  1.90388725e+02  7.99461021e+01]\n",
      "Done with gradient descent at iteration  108\n",
      "Learned weights =  [-2.32731957e-03  1.90388725e+02  7.96031601e+01]\n",
      "Done with gradient descent at iteration  109\n",
      "Learned weights =  [-2.83976562e-03  1.90388725e+02  7.96031601e+01]\n",
      "Done with gradient descent at iteration  109\n",
      "Learned weights =  [-2.83976562e-03  1.90702671e+02  7.96031601e+01]\n",
      "Done with gradient descent at iteration  109\n",
      "Learned weights =  [-2.83976562e-03  1.90702671e+02  7.92622522e+01]\n",
      "Done with gradient descent at iteration  110\n",
      "Learned weights =  [-3.35137633e-03  1.90702671e+02  7.92622522e+01]\n",
      "Done with gradient descent at iteration  110\n",
      "Learned weights =  [-3.35137633e-03  1.91014754e+02  7.92622522e+01]\n",
      "Done with gradient descent at iteration  110\n",
      "Learned weights =  [-3.35137633e-03  1.91014754e+02  7.89233665e+01]\n",
      "Done with gradient descent at iteration  111\n",
      "Learned weights =  [-3.86215667e-03  1.91014754e+02  7.89233665e+01]\n",
      "Done with gradient descent at iteration  111\n",
      "Learned weights =  [-3.86215667e-03  1.91324987e+02  7.89233665e+01]\n",
      "Done with gradient descent at iteration  111\n",
      "Learned weights =  [-3.86215667e-03  1.91324987e+02  7.85864909e+01]\n",
      "Done with gradient descent at iteration  112\n",
      "Learned weights =  [-4.37211156e-03  1.91324987e+02  7.85864909e+01]\n",
      "Done with gradient descent at iteration  112\n",
      "Learned weights =  [-4.37211156e-03  1.91633379e+02  7.85864909e+01]\n",
      "Done with gradient descent at iteration  112\n",
      "Learned weights =  [-4.37211156e-03  1.91633379e+02  7.82516134e+01]\n",
      "Done with gradient descent at iteration  113\n",
      "Learned weights =  [-4.88124590e-03  1.91633379e+02  7.82516134e+01]\n",
      "Done with gradient descent at iteration  113\n",
      "Learned weights =  [-4.88124590e-03  1.91939941e+02  7.82516134e+01]\n",
      "Done with gradient descent at iteration  113\n",
      "Learned weights =  [-4.88124590e-03  1.91939941e+02  7.79187224e+01]\n",
      "Done with gradient descent at iteration  114\n",
      "Learned weights =  [-5.38956455e-03  1.91939941e+02  7.79187224e+01]\n",
      "Done with gradient descent at iteration  114\n",
      "Learned weights =  [-5.38956455e-03  1.92244686e+02  7.79187224e+01]\n",
      "Done with gradient descent at iteration  114\n",
      "Learned weights =  [-5.38956455e-03  1.92244686e+02  7.75878059e+01]\n",
      "Done with gradient descent at iteration  115\n",
      "Learned weights =  [-5.89707235e-03  1.92244686e+02  7.75878059e+01]\n",
      "Done with gradient descent at iteration  115\n",
      "Learned weights =  [-5.89707235e-03  1.92547623e+02  7.75878059e+01]\n",
      "Done with gradient descent at iteration  115\n",
      "Learned weights =  [-5.89707235e-03  1.92547623e+02  7.72588522e+01]\n",
      "Done with gradient descent at iteration  116\n",
      "Learned weights =  [-6.40377412e-03  1.92547623e+02  7.72588522e+01]\n",
      "Done with gradient descent at iteration  116\n",
      "Learned weights =  [-6.40377412e-03  1.92848762e+02  7.72588522e+01]\n",
      "Done with gradient descent at iteration  116\n",
      "Learned weights =  [-6.40377412e-03  1.92848762e+02  7.69318498e+01]\n",
      "Done with gradient descent at iteration  117\n",
      "Learned weights =  [-6.90967462e-03  1.92848762e+02  7.69318498e+01]\n",
      "Done with gradient descent at iteration  117\n",
      "Learned weights =  [-6.90967462e-03  1.93148116e+02  7.69318498e+01]\n",
      "Done with gradient descent at iteration  117\n",
      "Learned weights =  [-6.90967462e-03  1.93148116e+02  7.66067870e+01]\n",
      "Done with gradient descent at iteration  118\n",
      "Learned weights =  [-7.41477862e-03  1.93148116e+02  7.66067870e+01]\n",
      "Done with gradient descent at iteration  118\n",
      "Learned weights =  [-7.41477862e-03  1.93445694e+02  7.66067870e+01]\n",
      "Done with gradient descent at iteration  118\n",
      "Learned weights =  [-7.41477862e-03  1.93445694e+02  7.62836524e+01]\n",
      "Done with gradient descent at iteration  119\n",
      "Learned weights =  [-7.91909084e-03  1.93445694e+02  7.62836524e+01]\n",
      "Done with gradient descent at iteration  119\n",
      "Learned weights =  [-7.91909084e-03  1.93741507e+02  7.62836524e+01]\n",
      "Done with gradient descent at iteration  119\n",
      "Learned weights =  [-7.91909084e-03  1.93741507e+02  7.59624344e+01]\n",
      "Done with gradient descent at iteration  120\n",
      "Learned weights =  [-8.42261598e-03  1.93741507e+02  7.59624344e+01]\n",
      "Done with gradient descent at iteration  120\n",
      "Learned weights =  [-8.42261598e-03  1.94035565e+02  7.59624344e+01]\n",
      "Done with gradient descent at iteration  120\n",
      "Learned weights =  [-8.42261598e-03  1.94035565e+02  7.56431218e+01]\n",
      "Done with gradient descent at iteration  121\n",
      "Learned weights =  [-8.92535870e-03  1.94035565e+02  7.56431218e+01]\n",
      "Done with gradient descent at iteration  121\n",
      "Learned weights =  [-8.92535870e-03  1.94327879e+02  7.56431218e+01]\n",
      "Done with gradient descent at iteration  121\n",
      "Learned weights =  [-8.92535870e-03  1.94327879e+02  7.53257032e+01]\n",
      "Done with gradient descent at iteration  122\n",
      "Learned weights =  [-9.42732365e-03  1.94327879e+02  7.53257032e+01]\n",
      "Done with gradient descent at iteration  122\n",
      "Learned weights =  [-9.42732365e-03  1.94618459e+02  7.53257032e+01]\n",
      "Done with gradient descent at iteration  122\n",
      "Learned weights =  [-9.42732365e-03  1.94618459e+02  7.50101674e+01]\n",
      "Done with gradient descent at iteration  123\n",
      "Learned weights =  [-9.92851544e-03  1.94618459e+02  7.50101674e+01]\n",
      "Done with gradient descent at iteration  123\n",
      "Learned weights =  [-9.92851544e-03  1.94907316e+02  7.50101674e+01]\n",
      "Done with gradient descent at iteration  123\n",
      "Learned weights =  [-9.92851544e-03  1.94907316e+02  7.46965032e+01]\n",
      "Done with gradient descent at iteration  124\n",
      "Learned weights =  [-1.04289387e-02  1.94907316e+02  7.46965032e+01]\n",
      "Done with gradient descent at iteration  124\n",
      "Learned weights =  [-1.04289387e-02  1.95194459e+02  7.46965032e+01]\n",
      "Done with gradient descent at iteration  124\n",
      "Learned weights =  [-1.04289387e-02  1.95194459e+02  7.43846996e+01]\n",
      "Done with gradient descent at iteration  125\n",
      "Learned weights =  [-1.09285978e-02  1.95194459e+02  7.43846996e+01]\n",
      "Done with gradient descent at iteration  125\n",
      "Learned weights =  [-1.09285978e-02  1.95479899e+02  7.43846996e+01]\n",
      "Done with gradient descent at iteration  125\n",
      "Learned weights =  [-1.09285978e-02  1.95479899e+02  7.40747454e+01]\n",
      "Done with gradient descent at iteration  126\n",
      "Learned weights =  [-1.14274976e-02  1.95479899e+02  7.40747454e+01]\n",
      "Done with gradient descent at iteration  126\n",
      "Learned weights =  [-1.14274976e-02  1.95763646e+02  7.40747454e+01]\n",
      "Done with gradient descent at iteration  126\n",
      "Learned weights =  [-1.14274976e-02  1.95763646e+02  7.37666298e+01]\n",
      "Done with gradient descent at iteration  127\n",
      "Learned weights =  [-1.19256423e-02  1.95763646e+02  7.37666298e+01]\n",
      "Done with gradient descent at iteration  127\n",
      "Learned weights =  [-1.19256423e-02  1.96045710e+02  7.37666298e+01]\n",
      "Done with gradient descent at iteration  127\n",
      "Learned weights =  [-1.19256423e-02  1.96045710e+02  7.34603417e+01]\n",
      "Done with gradient descent at iteration  128\n",
      "Learned weights =  [-1.24230365e-02  1.96045710e+02  7.34603417e+01]\n",
      "Done with gradient descent at iteration  128\n",
      "Learned weights =  [-1.24230365e-02  1.96326101e+02  7.34603417e+01]\n",
      "Done with gradient descent at iteration  128\n",
      "Learned weights =  [-1.24230365e-02  1.96326101e+02  7.31558705e+01]\n",
      "Done with gradient descent at iteration  129\n",
      "Learned weights =  [-1.29196847e-02  1.96326101e+02  7.31558705e+01]\n",
      "Done with gradient descent at iteration  129\n",
      "Learned weights =  [-1.29196847e-02  1.96604828e+02  7.31558705e+01]\n",
      "Done with gradient descent at iteration  129\n",
      "Learned weights =  [-1.29196847e-02  1.96604828e+02  7.28532052e+01]\n",
      "Done with gradient descent at iteration  130\n",
      "Learned weights =  [-1.34155913e-02  1.96604828e+02  7.28532052e+01]\n",
      "Done with gradient descent at iteration  130\n",
      "Learned weights =  [-1.34155913e-02  1.96881902e+02  7.28532052e+01]\n",
      "Done with gradient descent at iteration  130\n",
      "Learned weights =  [-1.34155913e-02  1.96881902e+02  7.25523352e+01]\n",
      "Done with gradient descent at iteration  131\n",
      "Learned weights =  [-1.39107606e-02  1.96881902e+02  7.25523352e+01]\n",
      "Done with gradient descent at iteration  131\n",
      "Learned weights =  [-1.39107606e-02  1.97157333e+02  7.25523352e+01]\n",
      "Done with gradient descent at iteration  131\n",
      "Learned weights =  [-1.39107606e-02  1.97157333e+02  7.22532499e+01]\n",
      "Done with gradient descent at iteration  132\n",
      "Learned weights =  [-1.44051971e-02  1.97157333e+02  7.22532499e+01]\n",
      "Done with gradient descent at iteration  132\n",
      "Learned weights =  [-1.44051971e-02  1.97431130e+02  7.22532499e+01]\n",
      "Done with gradient descent at iteration  132\n",
      "Learned weights =  [-1.44051971e-02  1.97431130e+02  7.19559385e+01]\n",
      "Done with gradient descent at iteration  133\n",
      "Learned weights =  [-1.48989051e-02  1.97431130e+02  7.19559385e+01]\n",
      "Done with gradient descent at iteration  133\n",
      "Learned weights =  [-1.48989051e-02  1.97703303e+02  7.19559385e+01]\n",
      "Done with gradient descent at iteration  133\n",
      "Learned weights =  [-1.48989051e-02  1.97703303e+02  7.16603908e+01]\n",
      "Done with gradient descent at iteration  134\n",
      "Learned weights =  [-1.53918889e-02  1.97703303e+02  7.16603908e+01]\n",
      "Done with gradient descent at iteration  134\n",
      "Learned weights =  [-1.53918889e-02  1.97973862e+02  7.16603908e+01]\n",
      "Done with gradient descent at iteration  134\n",
      "Learned weights =  [-1.53918889e-02  1.97973862e+02  7.13665960e+01]\n",
      "Done with gradient descent at iteration  135\n",
      "Learned weights =  [-1.58841528e-02  1.97973862e+02  7.13665960e+01]\n",
      "Done with gradient descent at iteration  135\n",
      "Learned weights =  [-1.58841528e-02  1.98242816e+02  7.13665960e+01]\n",
      "Done with gradient descent at iteration  135\n",
      "Learned weights =  [-1.58841528e-02  1.98242816e+02  7.10745440e+01]\n",
      "Done with gradient descent at iteration  136\n",
      "Learned weights =  [-1.63757010e-02  1.98242816e+02  7.10745440e+01]\n",
      "Done with gradient descent at iteration  136\n",
      "Learned weights =  [-1.63757010e-02  1.98510174e+02  7.10745440e+01]\n",
      "Done with gradient descent at iteration  136\n",
      "Learned weights =  [-1.63757010e-02  1.98510174e+02  7.07842243e+01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  137\n",
      "Learned weights =  [-1.68665380e-02  1.98510174e+02  7.07842243e+01]\n",
      "Done with gradient descent at iteration  137\n",
      "Learned weights =  [-1.68665380e-02  1.98775947e+02  7.07842243e+01]\n",
      "Done with gradient descent at iteration  137\n",
      "Learned weights =  [-1.68665380e-02  1.98775947e+02  7.04956266e+01]\n",
      "Done with gradient descent at iteration  138\n",
      "Learned weights =  [-1.73566677e-02  1.98775947e+02  7.04956266e+01]\n",
      "Done with gradient descent at iteration  138\n",
      "Learned weights =  [-1.73566677e-02  1.99040143e+02  7.04956266e+01]\n",
      "Done with gradient descent at iteration  138\n",
      "Learned weights =  [-1.73566677e-02  1.99040143e+02  7.02087408e+01]\n",
      "Done with gradient descent at iteration  139\n",
      "Learned weights =  [-1.78460945e-02  1.99040143e+02  7.02087408e+01]\n",
      "Done with gradient descent at iteration  139\n",
      "Learned weights =  [-1.78460945e-02  1.99302772e+02  7.02087408e+01]\n",
      "Done with gradient descent at iteration  139\n",
      "Learned weights =  [-1.78460945e-02  1.99302772e+02  6.99235566e+01]\n",
      "Done with gradient descent at iteration  140\n",
      "Learned weights =  [-1.83348225e-02  1.99302772e+02  6.99235566e+01]\n",
      "Done with gradient descent at iteration  140\n",
      "Learned weights =  [-1.83348225e-02  1.99563843e+02  6.99235566e+01]\n",
      "Done with gradient descent at iteration  140\n",
      "Learned weights =  [-1.83348225e-02  1.99563843e+02  6.96400641e+01]\n",
      "Done with gradient descent at iteration  141\n",
      "Learned weights =  [-1.88228559e-02  1.99563843e+02  6.96400641e+01]\n",
      "Done with gradient descent at iteration  141\n",
      "Learned weights =  [-1.88228559e-02  1.99823366e+02  6.96400641e+01]\n",
      "Done with gradient descent at iteration  141\n",
      "Learned weights =  [-1.88228559e-02  1.99823366e+02  6.93582531e+01]\n",
      "Done with gradient descent at iteration  142\n",
      "Learned weights =  [-1.93101987e-02  1.99823366e+02  6.93582531e+01]\n",
      "Done with gradient descent at iteration  142\n",
      "Learned weights =  [-1.93101987e-02  2.00081349e+02  6.93582531e+01]\n",
      "Done with gradient descent at iteration  142\n",
      "Learned weights =  [-1.93101987e-02  2.00081349e+02  6.90781137e+01]\n",
      "Done with gradient descent at iteration  143\n",
      "Learned weights =  [-1.97968551e-02  2.00081349e+02  6.90781137e+01]\n",
      "Done with gradient descent at iteration  143\n",
      "Learned weights =  [-1.97968551e-02  2.00337802e+02  6.90781137e+01]\n",
      "Done with gradient descent at iteration  143\n",
      "Learned weights =  [-1.97968551e-02  2.00337802e+02  6.87996359e+01]\n",
      "Done with gradient descent at iteration  144\n",
      "Learned weights =  [-2.02828292e-02  2.00337802e+02  6.87996359e+01]\n",
      "Done with gradient descent at iteration  144\n",
      "Learned weights =  [-2.02828292e-02  2.00592734e+02  6.87996359e+01]\n",
      "Done with gradient descent at iteration  144\n",
      "Learned weights =  [-2.02828292e-02  2.00592734e+02  6.85228100e+01]\n",
      "Done with gradient descent at iteration  145\n",
      "Learned weights =  [-2.07681249e-02  2.00592734e+02  6.85228100e+01]\n",
      "Done with gradient descent at iteration  145\n",
      "Learned weights =  [-2.07681249e-02  2.00846154e+02  6.85228100e+01]\n",
      "Done with gradient descent at iteration  145\n",
      "Learned weights =  [-2.07681249e-02  2.00846154e+02  6.82476261e+01]\n",
      "Done with gradient descent at iteration  146\n",
      "Learned weights =  [-2.12527464e-02  2.00846154e+02  6.82476261e+01]\n",
      "Done with gradient descent at iteration  146\n",
      "Learned weights =  [-2.12527464e-02  2.01098070e+02  6.82476261e+01]\n",
      "Done with gradient descent at iteration  146\n",
      "Learned weights =  [-2.12527464e-02  2.01098070e+02  6.79740744e+01]\n",
      "Done with gradient descent at iteration  147\n",
      "Learned weights =  [-2.17366976e-02  2.01098070e+02  6.79740744e+01]\n",
      "Done with gradient descent at iteration  147\n",
      "Learned weights =  [-2.17366976e-02  2.01348492e+02  6.79740744e+01]\n",
      "Done with gradient descent at iteration  147\n",
      "Learned weights =  [-2.17366976e-02  2.01348492e+02  6.77021454e+01]\n",
      "Done with gradient descent at iteration  148\n",
      "Learned weights =  [-2.22199824e-02  2.01348492e+02  6.77021454e+01]\n",
      "Done with gradient descent at iteration  148\n",
      "Learned weights =  [-2.22199824e-02  2.01597429e+02  6.77021454e+01]\n",
      "Done with gradient descent at iteration  148\n",
      "Learned weights =  [-2.22199824e-02  2.01597429e+02  6.74318293e+01]\n",
      "Done with gradient descent at iteration  149\n",
      "Learned weights =  [-2.27026049e-02  2.01597429e+02  6.74318293e+01]\n",
      "Done with gradient descent at iteration  149\n",
      "Learned weights =  [-2.27026049e-02  2.01844890e+02  6.74318293e+01]\n",
      "Done with gradient descent at iteration  149\n",
      "Learned weights =  [-2.27026049e-02  2.01844890e+02  6.71631166e+01]\n",
      "Done with gradient descent at iteration  150\n",
      "Learned weights =  [-2.31845690e-02  2.01844890e+02  6.71631166e+01]\n",
      "Done with gradient descent at iteration  150\n",
      "Learned weights =  [-2.31845690e-02  2.02090882e+02  6.71631166e+01]\n",
      "Done with gradient descent at iteration  150\n",
      "Learned weights =  [-2.31845690e-02  2.02090882e+02  6.68959978e+01]\n",
      "Done with gradient descent at iteration  151\n",
      "Learned weights =  [-2.36658785e-02  2.02090882e+02  6.68959978e+01]\n",
      "Done with gradient descent at iteration  151\n",
      "Learned weights =  [-2.36658785e-02  2.02335415e+02  6.68959978e+01]\n",
      "Done with gradient descent at iteration  151\n",
      "Learned weights =  [-2.36658785e-02  2.02335415e+02  6.66304635e+01]\n",
      "Done with gradient descent at iteration  152\n",
      "Learned weights =  [-2.41465374e-02  2.02335415e+02  6.66304635e+01]\n",
      "Done with gradient descent at iteration  152\n",
      "Learned weights =  [-2.41465374e-02  2.02578498e+02  6.66304635e+01]\n",
      "Done with gradient descent at iteration  152\n",
      "Learned weights =  [-2.41465374e-02  2.02578498e+02  6.63665042e+01]\n",
      "Done with gradient descent at iteration  153\n",
      "Learned weights =  [-2.46265496e-02  2.02578498e+02  6.63665042e+01]\n",
      "Done with gradient descent at iteration  153\n",
      "Learned weights =  [-2.46265496e-02  2.02820139e+02  6.63665042e+01]\n",
      "Done with gradient descent at iteration  153\n",
      "Learned weights =  [-2.46265496e-02  2.02820139e+02  6.61041106e+01]\n",
      "Done with gradient descent at iteration  154\n",
      "Learned weights =  [-2.51059187e-02  2.02820139e+02  6.61041106e+01]\n",
      "Done with gradient descent at iteration  154\n",
      "Learned weights =  [-2.51059187e-02  2.03060347e+02  6.61041106e+01]\n",
      "Done with gradient descent at iteration  154\n",
      "Learned weights =  [-2.51059187e-02  2.03060347e+02  6.58432733e+01]\n",
      "Done with gradient descent at iteration  155\n",
      "Learned weights =  [-2.55846488e-02  2.03060347e+02  6.58432733e+01]\n",
      "Done with gradient descent at iteration  155\n",
      "Learned weights =  [-2.55846488e-02  2.03299130e+02  6.58432733e+01]\n",
      "Done with gradient descent at iteration  155\n",
      "Learned weights =  [-2.55846488e-02  2.03299130e+02  6.55839833e+01]\n",
      "Done with gradient descent at iteration  156\n",
      "Learned weights =  [-2.60627435e-02  2.03299130e+02  6.55839833e+01]\n",
      "Done with gradient descent at iteration  156\n",
      "Learned weights =  [-2.60627435e-02  2.03536496e+02  6.55839833e+01]\n",
      "Done with gradient descent at iteration  156\n",
      "Learned weights =  [-2.60627435e-02  2.03536496e+02  6.53262313e+01]\n",
      "Done with gradient descent at iteration  157\n",
      "Learned weights =  [-2.65402066e-02  2.03536496e+02  6.53262313e+01]\n",
      "Done with gradient descent at iteration  157\n",
      "Learned weights =  [-2.65402066e-02  2.03772455e+02  6.53262313e+01]\n",
      "Done with gradient descent at iteration  157\n",
      "Learned weights =  [-2.65402066e-02  2.03772455e+02  6.50700081e+01]\n",
      "Done with gradient descent at iteration  158\n",
      "Learned weights =  [-2.70170419e-02  2.03772455e+02  6.50700081e+01]\n",
      "Done with gradient descent at iteration  158\n",
      "Learned weights =  [-2.70170419e-02  2.04007014e+02  6.50700081e+01]\n",
      "Done with gradient descent at iteration  158\n",
      "Learned weights =  [-2.70170419e-02  2.04007014e+02  6.48153048e+01]\n",
      "Done with gradient descent at iteration  159\n",
      "Learned weights =  [-2.74932531e-02  2.04007014e+02  6.48153048e+01]\n",
      "Done with gradient descent at iteration  159\n",
      "Learned weights =  [-2.74932531e-02  2.04240182e+02  6.48153048e+01]\n",
      "Done with gradient descent at iteration  159\n",
      "Learned weights =  [-2.74932531e-02  2.04240182e+02  6.45621122e+01]\n",
      "Done with gradient descent at iteration  160\n",
      "Learned weights =  [-2.79688439e-02  2.04240182e+02  6.45621122e+01]\n",
      "Done with gradient descent at iteration  160\n",
      "Learned weights =  [-2.79688439e-02  2.04471966e+02  6.45621122e+01]\n",
      "Done with gradient descent at iteration  160\n",
      "Learned weights =  [-2.79688439e-02  2.04471966e+02  6.43104215e+01]\n",
      "Done with gradient descent at iteration  161\n",
      "Learned weights =  [-2.84438180e-02  2.04471966e+02  6.43104215e+01]\n",
      "Done with gradient descent at iteration  161\n",
      "Learned weights =  [-2.84438180e-02  2.04702376e+02  6.43104215e+01]\n",
      "Done with gradient descent at iteration  161\n",
      "Learned weights =  [-2.84438180e-02  2.04702376e+02  6.40602237e+01]\n",
      "Done with gradient descent at iteration  162\n",
      "Learned weights =  [-2.89181790e-02  2.04702376e+02  6.40602237e+01]\n",
      "Done with gradient descent at iteration  162\n",
      "Learned weights =  [-2.89181790e-02  2.04931419e+02  6.40602237e+01]\n",
      "Done with gradient descent at iteration  162\n",
      "Learned weights =  [-2.89181790e-02  2.04931419e+02  6.38115099e+01]\n",
      "Done with gradient descent at iteration  163\n",
      "Learned weights =  [-2.93919306e-02  2.04931419e+02  6.38115099e+01]\n",
      "Done with gradient descent at iteration  163\n",
      "Learned weights =  [-2.93919306e-02  2.05159103e+02  6.38115099e+01]\n",
      "Done with gradient descent at iteration  163\n",
      "Learned weights =  [-2.93919306e-02  2.05159103e+02  6.35642715e+01]\n",
      "Done with gradient descent at iteration  164\n",
      "Learned weights =  [-2.98650764e-02  2.05159103e+02  6.35642715e+01]\n",
      "Done with gradient descent at iteration  164\n",
      "Learned weights =  [-2.98650764e-02  2.05385437e+02  6.35642715e+01]\n",
      "Done with gradient descent at iteration  164\n",
      "Learned weights =  [-2.98650764e-02  2.05385437e+02  6.33184995e+01]\n",
      "Done with gradient descent at iteration  165\n",
      "Learned weights =  [-3.03376199e-02  2.05385437e+02  6.33184995e+01]\n",
      "Done with gradient descent at iteration  165\n",
      "Learned weights =  [-3.03376199e-02  2.05610429e+02  6.33184995e+01]\n",
      "Done with gradient descent at iteration  165\n",
      "Learned weights =  [-3.03376199e-02  2.05610429e+02  6.30741854e+01]\n",
      "Done with gradient descent at iteration  166\n",
      "Learned weights =  [-3.08095649e-02  2.05610429e+02  6.30741854e+01]\n",
      "Done with gradient descent at iteration  166\n",
      "Learned weights =  [-3.08095649e-02  2.05834086e+02  6.30741854e+01]\n",
      "Done with gradient descent at iteration  166\n",
      "Learned weights =  [-3.08095649e-02  2.05834086e+02  6.28313204e+01]\n",
      "Done with gradient descent at iteration  167\n",
      "Learned weights =  [-3.12809147e-02  2.05834086e+02  6.28313204e+01]\n",
      "Done with gradient descent at iteration  167\n",
      "Learned weights =  [-3.12809147e-02  2.06056416e+02  6.28313204e+01]\n",
      "Done with gradient descent at iteration  167\n",
      "Learned weights =  [-3.12809147e-02  2.06056416e+02  6.25898960e+01]\n",
      "Done with gradient descent at iteration  168\n",
      "Learned weights =  [-3.17516729e-02  2.06056416e+02  6.25898960e+01]\n",
      "Done with gradient descent at iteration  168\n",
      "Learned weights =  [-3.17516729e-02  2.06277427e+02  6.25898960e+01]\n",
      "Done with gradient descent at iteration  168\n",
      "Learned weights =  [-3.17516729e-02  2.06277427e+02  6.23499036e+01]\n",
      "Done with gradient descent at iteration  169\n",
      "Learned weights =  [-3.22218431e-02  2.06277427e+02  6.23499036e+01]\n",
      "Done with gradient descent at iteration  169\n",
      "Learned weights =  [-3.22218431e-02  2.06497128e+02  6.23499036e+01]\n",
      "Done with gradient descent at iteration  169\n",
      "Learned weights =  [-3.22218431e-02  2.06497128e+02  6.21113348e+01]\n",
      "Done with gradient descent at iteration  170\n",
      "Learned weights =  [-3.26914288e-02  2.06497128e+02  6.21113348e+01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  170\n",
      "Learned weights =  [-3.26914288e-02  2.06715525e+02  6.21113348e+01]\n",
      "Done with gradient descent at iteration  170\n",
      "Learned weights =  [-3.26914288e-02  2.06715525e+02  6.18741810e+01]\n",
      "Done with gradient descent at iteration  171\n",
      "Learned weights =  [-3.31604333e-02  2.06715525e+02  6.18741810e+01]\n",
      "Done with gradient descent at iteration  171\n",
      "Learned weights =  [-3.31604333e-02  2.06932627e+02  6.18741810e+01]\n",
      "Done with gradient descent at iteration  171\n",
      "Learned weights =  [-3.31604333e-02  2.06932627e+02  6.16384340e+01]\n",
      "Done with gradient descent at iteration  172\n",
      "Learned weights =  [-3.36288602e-02  2.06932627e+02  6.16384340e+01]\n",
      "Done with gradient descent at iteration  172\n",
      "Learned weights =  [-3.36288602e-02  2.07148441e+02  6.16384340e+01]\n",
      "Done with gradient descent at iteration  172\n",
      "Learned weights =  [-3.36288602e-02  2.07148441e+02  6.14040853e+01]\n",
      "Done with gradient descent at iteration  173\n",
      "Learned weights =  [-3.40967128e-02  2.07148441e+02  6.14040853e+01]\n",
      "Done with gradient descent at iteration  173\n",
      "Learned weights =  [-3.40967128e-02  2.07362975e+02  6.14040853e+01]\n",
      "Done with gradient descent at iteration  173\n",
      "Learned weights =  [-3.40967128e-02  2.07362975e+02  6.11711266e+01]\n",
      "Done with gradient descent at iteration  174\n",
      "Learned weights =  [-3.45639946e-02  2.07362975e+02  6.11711266e+01]\n",
      "Done with gradient descent at iteration  174\n",
      "Learned weights =  [-3.45639946e-02  2.07576237e+02  6.11711266e+01]\n",
      "Done with gradient descent at iteration  174\n",
      "Learned weights =  [-3.45639946e-02  2.07576237e+02  6.09395498e+01]\n",
      "Done with gradient descent at iteration  175\n",
      "Learned weights =  [-3.50307090e-02  2.07576237e+02  6.09395498e+01]\n",
      "Done with gradient descent at iteration  175\n",
      "Learned weights =  [-3.50307090e-02  2.07788234e+02  6.09395498e+01]\n",
      "Done with gradient descent at iteration  175\n",
      "Learned weights =  [-3.50307090e-02  2.07788234e+02  6.07093466e+01]\n",
      "Done with gradient descent at iteration  176\n",
      "Learned weights =  [-3.54968593e-02  2.07788234e+02  6.07093466e+01]\n",
      "Done with gradient descent at iteration  176\n",
      "Learned weights =  [-3.54968593e-02  2.07998973e+02  6.07093466e+01]\n",
      "Done with gradient descent at iteration  176\n",
      "Learned weights =  [-3.54968593e-02  2.07998973e+02  6.04805088e+01]\n",
      "Done with gradient descent at iteration  177\n",
      "Learned weights =  [-3.59624489e-02  2.07998973e+02  6.04805088e+01]\n",
      "Done with gradient descent at iteration  177\n",
      "Learned weights =  [-3.59624489e-02  2.08208462e+02  6.04805088e+01]\n",
      "Done with gradient descent at iteration  177\n",
      "Learned weights =  [-3.59624489e-02  2.08208462e+02  6.02530284e+01]\n",
      "Done with gradient descent at iteration  178\n",
      "Learned weights =  [-3.64274811e-02  2.08208462e+02  6.02530284e+01]\n",
      "Done with gradient descent at iteration  178\n",
      "Learned weights =  [-3.64274811e-02  2.08416708e+02  6.02530284e+01]\n",
      "Done with gradient descent at iteration  178\n",
      "Learned weights =  [-3.64274811e-02  2.08416708e+02  6.00268974e+01]\n",
      "Done with gradient descent at iteration  179\n",
      "Learned weights =  [-3.68919592e-02  2.08416708e+02  6.00268974e+01]\n",
      "Done with gradient descent at iteration  179\n",
      "Learned weights =  [-3.68919592e-02  2.08623719e+02  6.00268974e+01]\n",
      "Done with gradient descent at iteration  179\n",
      "Learned weights =  [-3.68919592e-02  2.08623719e+02  5.98021076e+01]\n",
      "Done with gradient descent at iteration  180\n",
      "Learned weights =  [-3.73558865e-02  2.08623719e+02  5.98021076e+01]\n",
      "Done with gradient descent at iteration  180\n",
      "Learned weights =  [-3.73558865e-02  2.08829503e+02  5.98021076e+01]\n",
      "Done with gradient descent at iteration  180\n",
      "Learned weights =  [-3.73558865e-02  2.08829503e+02  5.95786512e+01]\n",
      "Done with gradient descent at iteration  181\n",
      "Learned weights =  [-3.78192663e-02  2.08829503e+02  5.95786512e+01]\n",
      "Done with gradient descent at iteration  181\n",
      "Learned weights =  [-3.78192663e-02  2.09034065e+02  5.95786512e+01]\n",
      "Done with gradient descent at iteration  181\n",
      "Learned weights =  [-3.78192663e-02  2.09034065e+02  5.93565203e+01]\n",
      "Done with gradient descent at iteration  182\n",
      "Learned weights =  [-3.82821018e-02  2.09034065e+02  5.93565203e+01]\n",
      "Done with gradient descent at iteration  182\n",
      "Learned weights =  [-3.82821018e-02  2.09237415e+02  5.93565203e+01]\n",
      "Done with gradient descent at iteration  182\n",
      "Learned weights =  [-3.82821018e-02  2.09237415e+02  5.91357069e+01]\n",
      "Done with gradient descent at iteration  183\n",
      "Learned weights =  [-3.87443962e-02  2.09237415e+02  5.91357069e+01]\n",
      "Done with gradient descent at iteration  183\n",
      "Learned weights =  [-3.87443962e-02  2.09439558e+02  5.91357069e+01]\n",
      "Done with gradient descent at iteration  183\n",
      "Learned weights =  [-3.87443962e-02  2.09439558e+02  5.89162034e+01]\n",
      "Done with gradient descent at iteration  184\n",
      "Learned weights =  [-3.92061527e-02  2.09439558e+02  5.89162034e+01]\n",
      "Done with gradient descent at iteration  184\n",
      "Learned weights =  [-3.92061527e-02  2.09640502e+02  5.89162034e+01]\n",
      "Done with gradient descent at iteration  184\n",
      "Learned weights =  [-3.92061527e-02  2.09640502e+02  5.86980018e+01]\n",
      "Done with gradient descent at iteration  185\n",
      "Learned weights =  [-3.96673746e-02  2.09640502e+02  5.86980018e+01]\n",
      "Done with gradient descent at iteration  185\n",
      "Learned weights =  [-3.96673746e-02  2.09840254e+02  5.86980018e+01]\n",
      "Done with gradient descent at iteration  185\n",
      "Learned weights =  [-3.96673746e-02  2.09840254e+02  5.84810945e+01]\n",
      "Done with gradient descent at iteration  186\n",
      "Learned weights =  [-4.01280650e-02  2.09840254e+02  5.84810945e+01]\n",
      "Done with gradient descent at iteration  186\n",
      "Learned weights =  [-4.01280650e-02  2.10038822e+02  5.84810945e+01]\n",
      "Done with gradient descent at iteration  186\n",
      "Learned weights =  [-4.01280650e-02  2.10038822e+02  5.82654738e+01]\n",
      "Done with gradient descent at iteration  187\n",
      "Learned weights =  [-4.05882271e-02  2.10038822e+02  5.82654738e+01]\n",
      "Done with gradient descent at iteration  187\n",
      "Learned weights =  [-4.05882271e-02  2.10236211e+02  5.82654738e+01]\n",
      "Done with gradient descent at iteration  187\n",
      "Learned weights =  [-4.05882271e-02  2.10236211e+02  5.80511321e+01]\n",
      "Done with gradient descent at iteration  188\n",
      "Learned weights =  [-4.10478640e-02  2.10236211e+02  5.80511321e+01]\n",
      "Done with gradient descent at iteration  188\n",
      "Learned weights =  [-4.10478640e-02  2.10432430e+02  5.80511321e+01]\n",
      "Done with gradient descent at iteration  188\n",
      "Learned weights =  [-4.10478640e-02  2.10432430e+02  5.78380617e+01]\n",
      "Done with gradient descent at iteration  189\n",
      "Learned weights =  [-4.15069787e-02  2.10432430e+02  5.78380617e+01]\n",
      "Done with gradient descent at iteration  189\n",
      "Learned weights =  [-4.15069787e-02  2.10627485e+02  5.78380617e+01]\n",
      "Done with gradient descent at iteration  189\n",
      "Learned weights =  [-4.15069787e-02  2.10627485e+02  5.76262552e+01]\n",
      "Done with gradient descent at iteration  190\n",
      "Learned weights =  [-4.19655745e-02  2.10627485e+02  5.76262552e+01]\n",
      "Done with gradient descent at iteration  190\n",
      "Learned weights =  [-4.19655745e-02  2.10821383e+02  5.76262552e+01]\n",
      "Done with gradient descent at iteration  190\n",
      "Learned weights =  [-4.19655745e-02  2.10821383e+02  5.74157051e+01]\n",
      "Done with gradient descent at iteration  191\n",
      "Learned weights =  [-4.24236544e-02  2.10821383e+02  5.74157051e+01]\n",
      "Done with gradient descent at iteration  191\n",
      "Learned weights =  [-4.24236544e-02  2.11014130e+02  5.74157051e+01]\n",
      "Done with gradient descent at iteration  191\n",
      "Learned weights =  [-4.24236544e-02  2.11014130e+02  5.72064038e+01]\n",
      "Done with gradient descent at iteration  192\n",
      "Learned weights =  [-4.28812214e-02  2.11014130e+02  5.72064038e+01]\n",
      "Done with gradient descent at iteration  192\n",
      "Learned weights =  [-4.28812214e-02  2.11205735e+02  5.72064038e+01]\n",
      "Done with gradient descent at iteration  192\n",
      "Learned weights =  [-4.28812214e-02  2.11205735e+02  5.69983440e+01]\n",
      "Done with gradient descent at iteration  193\n",
      "Learned weights =  [-4.33382786e-02  2.11205735e+02  5.69983440e+01]\n",
      "Done with gradient descent at iteration  193\n",
      "Learned weights =  [-4.33382786e-02  2.11396203e+02  5.69983440e+01]\n",
      "Done with gradient descent at iteration  193\n",
      "Learned weights =  [-4.33382786e-02  2.11396203e+02  5.67915184e+01]\n",
      "Done with gradient descent at iteration  194\n",
      "Learned weights =  [-4.37948290e-02  2.11396203e+02  5.67915184e+01]\n",
      "Done with gradient descent at iteration  194\n",
      "Learned weights =  [-4.37948290e-02  2.11585541e+02  5.67915184e+01]\n",
      "Done with gradient descent at iteration  194\n",
      "Learned weights =  [-4.37948290e-02  2.11585541e+02  5.65859195e+01]\n",
      "Done with gradient descent at iteration  195\n",
      "Learned weights =  [-4.42508756e-02  2.11585541e+02  5.65859195e+01]\n",
      "Done with gradient descent at iteration  195\n",
      "Learned weights =  [-4.42508756e-02  2.11773756e+02  5.65859195e+01]\n",
      "Done with gradient descent at iteration  195\n",
      "Learned weights =  [-4.42508756e-02  2.11773756e+02  5.63815402e+01]\n",
      "Done with gradient descent at iteration  196\n",
      "Learned weights =  [-4.47064215e-02  2.11773756e+02  5.63815402e+01]\n",
      "Done with gradient descent at iteration  196\n",
      "Learned weights =  [-4.47064215e-02  2.11960855e+02  5.63815402e+01]\n",
      "Done with gradient descent at iteration  196\n",
      "Learned weights =  [-4.47064215e-02  2.11960855e+02  5.61783732e+01]\n",
      "Done with gradient descent at iteration  197\n",
      "Learned weights =  [-4.51614695e-02  2.11960855e+02  5.61783732e+01]\n",
      "Done with gradient descent at iteration  197\n",
      "Learned weights =  [-4.51614695e-02  2.12146843e+02  5.61783732e+01]\n",
      "Done with gradient descent at iteration  197\n",
      "Learned weights =  [-4.51614695e-02  2.12146843e+02  5.59764113e+01]\n",
      "Done with gradient descent at iteration  198\n",
      "Learned weights =  [-4.56160226e-02  2.12146843e+02  5.59764113e+01]\n",
      "Done with gradient descent at iteration  198\n",
      "Learned weights =  [-4.56160226e-02  2.12331729e+02  5.59764113e+01]\n",
      "Done with gradient descent at iteration  198\n",
      "Learned weights =  [-4.56160226e-02  2.12331729e+02  5.57756473e+01]\n",
      "Done with gradient descent at iteration  199\n",
      "Learned weights =  [-4.60700838e-02  2.12331729e+02  5.57756473e+01]\n",
      "Done with gradient descent at iteration  199\n",
      "Learned weights =  [-4.60700838e-02  2.12515518e+02  5.57756473e+01]\n",
      "Done with gradient descent at iteration  199\n",
      "Learned weights =  [-4.60700838e-02  2.12515518e+02  5.55760742e+01]\n",
      "Iteration = 200\n",
      "Cost function =  1211738881421532.5\n",
      "Done with gradient descent at iteration  200\n",
      "Learned weights =  [-4.65236560e-02  2.12515518e+02  5.55760742e+01]\n",
      "Done with gradient descent at iteration  200\n",
      "Learned weights =  [-4.65236560e-02  2.12698217e+02  5.55760742e+01]\n",
      "Done with gradient descent at iteration  200\n",
      "Learned weights =  [-4.65236560e-02  2.12698217e+02  5.53776848e+01]\n",
      "Done with gradient descent at iteration  201\n",
      "Learned weights =  [-4.69767421e-02  2.12698217e+02  5.53776848e+01]\n",
      "Done with gradient descent at iteration  201\n",
      "Learned weights =  [-4.69767421e-02  2.12879832e+02  5.53776848e+01]\n",
      "Done with gradient descent at iteration  201\n",
      "Learned weights =  [-4.69767421e-02  2.12879832e+02  5.51804723e+01]\n",
      "Done with gradient descent at iteration  202\n",
      "Learned weights =  [-4.74293450e-02  2.12879832e+02  5.51804723e+01]\n",
      "Done with gradient descent at iteration  202\n",
      "Learned weights =  [-4.74293450e-02  2.13060370e+02  5.51804723e+01]\n",
      "Done with gradient descent at iteration  202\n",
      "Learned weights =  [-4.74293450e-02  2.13060370e+02  5.49844295e+01]\n",
      "Done with gradient descent at iteration  203\n",
      "Learned weights =  [-4.78814674e-02  2.13060370e+02  5.49844295e+01]\n",
      "Done with gradient descent at iteration  203\n",
      "Learned weights =  [-4.78814674e-02  2.13239837e+02  5.49844295e+01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  203\n",
      "Learned weights =  [-4.78814674e-02  2.13239837e+02  5.47895495e+01]\n",
      "Done with gradient descent at iteration  204\n",
      "Learned weights =  [-4.83331124e-02  2.13239837e+02  5.47895495e+01]\n",
      "Done with gradient descent at iteration  204\n",
      "Learned weights =  [-4.83331124e-02  2.13418239e+02  5.47895495e+01]\n",
      "Done with gradient descent at iteration  204\n",
      "Learned weights =  [-4.83331124e-02  2.13418239e+02  5.45958255e+01]\n",
      "Done with gradient descent at iteration  205\n",
      "Learned weights =  [-4.87842827e-02  2.13418239e+02  5.45958255e+01]\n",
      "Done with gradient descent at iteration  205\n",
      "Learned weights =  [-4.87842827e-02  2.13595584e+02  5.45958255e+01]\n",
      "Done with gradient descent at iteration  205\n",
      "Learned weights =  [-4.87842827e-02  2.13595584e+02  5.44032506e+01]\n",
      "Done with gradient descent at iteration  206\n",
      "Learned weights =  [-4.92349811e-02  2.13595584e+02  5.44032506e+01]\n",
      "Done with gradient descent at iteration  206\n",
      "Learned weights =  [-4.92349811e-02  2.13771876e+02  5.44032506e+01]\n",
      "Done with gradient descent at iteration  206\n",
      "Learned weights =  [-4.92349811e-02  2.13771876e+02  5.42118179e+01]\n",
      "Done with gradient descent at iteration  207\n",
      "Learned weights =  [-4.96852104e-02  2.13771876e+02  5.42118179e+01]\n",
      "Done with gradient descent at iteration  207\n",
      "Learned weights =  [-4.96852104e-02  2.13947123e+02  5.42118179e+01]\n",
      "Done with gradient descent at iteration  207\n",
      "Learned weights =  [-4.96852104e-02  2.13947123e+02  5.40215208e+01]\n",
      "Done with gradient descent at iteration  208\n",
      "Learned weights =  [-5.01349735e-02  2.13947123e+02  5.40215208e+01]\n",
      "Done with gradient descent at iteration  208\n",
      "Learned weights =  [-5.01349735e-02  2.14121330e+02  5.40215208e+01]\n",
      "Done with gradient descent at iteration  208\n",
      "Learned weights =  [-5.01349735e-02  2.14121330e+02  5.38323524e+01]\n",
      "Done with gradient descent at iteration  209\n",
      "Learned weights =  [-5.05842730e-02  2.14121330e+02  5.38323524e+01]\n",
      "Done with gradient descent at iteration  209\n",
      "Learned weights =  [-5.05842730e-02  2.14294504e+02  5.38323524e+01]\n",
      "Done with gradient descent at iteration  209\n",
      "Learned weights =  [-5.05842730e-02  2.14294504e+02  5.36443061e+01]\n",
      "Done with gradient descent at iteration  210\n",
      "Learned weights =  [-5.10331118e-02  2.14294504e+02  5.36443061e+01]\n",
      "Done with gradient descent at iteration  210\n",
      "Learned weights =  [-5.10331118e-02  2.14466650e+02  5.36443061e+01]\n",
      "Done with gradient descent at iteration  210\n",
      "Learned weights =  [-5.10331118e-02  2.14466650e+02  5.34573752e+01]\n",
      "Done with gradient descent at iteration  211\n",
      "Learned weights =  [-5.14814925e-02  2.14466650e+02  5.34573752e+01]\n",
      "Done with gradient descent at iteration  211\n",
      "Learned weights =  [-5.14814925e-02  2.14637776e+02  5.34573752e+01]\n",
      "Done with gradient descent at iteration  211\n",
      "Learned weights =  [-5.14814925e-02  2.14637776e+02  5.32715531e+01]\n",
      "Done with gradient descent at iteration  212\n",
      "Learned weights =  [-5.19294179e-02  2.14637776e+02  5.32715531e+01]\n",
      "Done with gradient descent at iteration  212\n",
      "Learned weights =  [-5.19294179e-02  2.14807886e+02  5.32715531e+01]\n",
      "Done with gradient descent at iteration  212\n",
      "Learned weights =  [-5.19294179e-02  2.14807886e+02  5.30868332e+01]\n",
      "Done with gradient descent at iteration  213\n",
      "Learned weights =  [-5.23768906e-02  2.14807886e+02  5.30868332e+01]\n",
      "Done with gradient descent at iteration  213\n",
      "Learned weights =  [-5.23768906e-02  2.14976988e+02  5.30868332e+01]\n",
      "Done with gradient descent at iteration  213\n",
      "Learned weights =  [-5.23768906e-02  2.14976988e+02  5.29032090e+01]\n",
      "Done with gradient descent at iteration  214\n",
      "Learned weights =  [-5.28239135e-02  2.14976988e+02  5.29032090e+01]\n",
      "Done with gradient descent at iteration  214\n",
      "Learned weights =  [-5.28239135e-02  2.15145086e+02  5.29032090e+01]\n",
      "Done with gradient descent at iteration  214\n",
      "Learned weights =  [-5.28239135e-02  2.15145086e+02  5.27206740e+01]\n",
      "Done with gradient descent at iteration  215\n",
      "Learned weights =  [-5.32704890e-02  2.15145086e+02  5.27206740e+01]\n",
      "Done with gradient descent at iteration  215\n",
      "Learned weights =  [-5.32704890e-02  2.15312188e+02  5.27206740e+01]\n",
      "Done with gradient descent at iteration  215\n",
      "Learned weights =  [-5.32704890e-02  2.15312188e+02  5.25392216e+01]\n",
      "Done with gradient descent at iteration  216\n",
      "Learned weights =  [-5.37166200e-02  2.15312188e+02  5.25392216e+01]\n",
      "Done with gradient descent at iteration  216\n",
      "Learned weights =  [-5.37166200e-02  2.15478298e+02  5.25392216e+01]\n",
      "Done with gradient descent at iteration  216\n",
      "Learned weights =  [-5.37166200e-02  2.15478298e+02  5.23588456e+01]\n",
      "Done with gradient descent at iteration  217\n",
      "Learned weights =  [-5.41623090e-02  2.15478298e+02  5.23588456e+01]\n",
      "Done with gradient descent at iteration  217\n",
      "Learned weights =  [-5.41623090e-02  2.15643423e+02  5.23588456e+01]\n",
      "Done with gradient descent at iteration  217\n",
      "Learned weights =  [-5.41623090e-02  2.15643423e+02  5.21795395e+01]\n",
      "Done with gradient descent at iteration  218\n",
      "Learned weights =  [-5.46075586e-02  2.15643423e+02  5.21795395e+01]\n",
      "Done with gradient descent at iteration  218\n",
      "Learned weights =  [-5.46075586e-02  2.15807568e+02  5.21795395e+01]\n",
      "Done with gradient descent at iteration  218\n",
      "Learned weights =  [-5.46075586e-02  2.15807568e+02  5.20012970e+01]\n",
      "Done with gradient descent at iteration  219\n",
      "Learned weights =  [-5.50523714e-02  2.15807568e+02  5.20012970e+01]\n",
      "Done with gradient descent at iteration  219\n",
      "Learned weights =  [-5.50523714e-02  2.15970740e+02  5.20012970e+01]\n",
      "Done with gradient descent at iteration  219\n",
      "Learned weights =  [-5.50523714e-02  2.15970740e+02  5.18241117e+01]\n",
      "Done with gradient descent at iteration  220\n",
      "Learned weights =  [-5.54967501e-02  2.15970740e+02  5.18241117e+01]\n",
      "Done with gradient descent at iteration  220\n",
      "Learned weights =  [-5.54967501e-02  2.16132944e+02  5.18241117e+01]\n",
      "Done with gradient descent at iteration  220\n",
      "Learned weights =  [-5.54967501e-02  2.16132944e+02  5.16479775e+01]\n",
      "Done with gradient descent at iteration  221\n",
      "Learned weights =  [-5.59406973e-02  2.16132944e+02  5.16479775e+01]\n",
      "Done with gradient descent at iteration  221\n",
      "Learned weights =  [-5.59406973e-02  2.16294186e+02  5.16479775e+01]\n",
      "Done with gradient descent at iteration  221\n",
      "Learned weights =  [-5.59406973e-02  2.16294186e+02  5.14728879e+01]\n",
      "Done with gradient descent at iteration  222\n",
      "Learned weights =  [-5.63842154e-02  2.16294186e+02  5.14728879e+01]\n",
      "Done with gradient descent at iteration  222\n",
      "Learned weights =  [-5.63842154e-02  2.16454471e+02  5.14728879e+01]\n",
      "Done with gradient descent at iteration  222\n",
      "Learned weights =  [-5.63842154e-02  2.16454471e+02  5.12988370e+01]\n",
      "Done with gradient descent at iteration  223\n",
      "Learned weights =  [-5.68273070e-02  2.16454471e+02  5.12988370e+01]\n",
      "Done with gradient descent at iteration  223\n",
      "Learned weights =  [-5.68273070e-02  2.16613806e+02  5.12988370e+01]\n",
      "Done with gradient descent at iteration  223\n",
      "Learned weights =  [-5.68273070e-02  2.16613806e+02  5.11258184e+01]\n",
      "Done with gradient descent at iteration  224\n",
      "Learned weights =  [-5.72699746e-02  2.16613806e+02  5.11258184e+01]\n",
      "Done with gradient descent at iteration  224\n",
      "Learned weights =  [-5.72699746e-02  2.16772195e+02  5.11258184e+01]\n",
      "Done with gradient descent at iteration  224\n",
      "Learned weights =  [-5.72699746e-02  2.16772195e+02  5.09538261e+01]\n",
      "Done with gradient descent at iteration  225\n",
      "Learned weights =  [-5.77122209e-02  2.16772195e+02  5.09538261e+01]\n",
      "Done with gradient descent at iteration  225\n",
      "Learned weights =  [-5.77122209e-02  2.16929645e+02  5.09538261e+01]\n",
      "Done with gradient descent at iteration  225\n",
      "Learned weights =  [-5.77122209e-02  2.16929645e+02  5.07828540e+01]\n",
      "Done with gradient descent at iteration  226\n",
      "Learned weights =  [-5.81540482e-02  2.16929645e+02  5.07828540e+01]\n",
      "Done with gradient descent at iteration  226\n",
      "Learned weights =  [-5.81540482e-02  2.17086162e+02  5.07828540e+01]\n",
      "Done with gradient descent at iteration  226\n",
      "Learned weights =  [-5.81540482e-02  2.17086162e+02  5.06128960e+01]\n",
      "Done with gradient descent at iteration  227\n",
      "Learned weights =  [-5.85954590e-02  2.17086162e+02  5.06128960e+01]\n",
      "Done with gradient descent at iteration  227\n",
      "Learned weights =  [-5.85954590e-02  2.17241749e+02  5.06128960e+01]\n",
      "Done with gradient descent at iteration  227\n",
      "Learned weights =  [-5.85954590e-02  2.17241749e+02  5.04439462e+01]\n",
      "Done with gradient descent at iteration  228\n",
      "Learned weights =  [-5.90364559e-02  2.17241749e+02  5.04439462e+01]\n",
      "Done with gradient descent at iteration  228\n",
      "Learned weights =  [-5.90364559e-02  2.17396414e+02  5.04439462e+01]\n",
      "Done with gradient descent at iteration  228\n",
      "Learned weights =  [-5.90364559e-02  2.17396414e+02  5.02759985e+01]\n",
      "Done with gradient descent at iteration  229\n",
      "Learned weights =  [-5.94770412e-02  2.17396414e+02  5.02759985e+01]\n",
      "Done with gradient descent at iteration  229\n",
      "Learned weights =  [-5.94770412e-02  2.17550162e+02  5.02759985e+01]\n",
      "Done with gradient descent at iteration  229\n",
      "Learned weights =  [-5.94770412e-02  2.17550162e+02  5.01090469e+01]\n",
      "Done with gradient descent at iteration  230\n",
      "Learned weights =  [-5.99172175e-02  2.17550162e+02  5.01090469e+01]\n",
      "Done with gradient descent at iteration  230\n",
      "Learned weights =  [-5.99172175e-02  2.17702997e+02  5.01090469e+01]\n",
      "Done with gradient descent at iteration  230\n",
      "Learned weights =  [-5.99172175e-02  2.17702997e+02  4.99430857e+01]\n",
      "Done with gradient descent at iteration  231\n",
      "Learned weights =  [-6.03569871e-02  2.17702997e+02  4.99430857e+01]\n",
      "Done with gradient descent at iteration  231\n",
      "Learned weights =  [-6.03569871e-02  2.17854926e+02  4.99430857e+01]\n",
      "Done with gradient descent at iteration  231\n",
      "Learned weights =  [-6.03569871e-02  2.17854926e+02  4.97781089e+01]\n",
      "Done with gradient descent at iteration  232\n",
      "Learned weights =  [-6.07963524e-02  2.17854926e+02  4.97781089e+01]\n",
      "Done with gradient descent at iteration  232\n",
      "Learned weights =  [-6.07963524e-02  2.18005954e+02  4.97781089e+01]\n",
      "Done with gradient descent at iteration  232\n",
      "Learned weights =  [-6.07963524e-02  2.18005954e+02  4.96141106e+01]\n",
      "Done with gradient descent at iteration  233\n",
      "Learned weights =  [-6.12353159e-02  2.18005954e+02  4.96141106e+01]\n",
      "Done with gradient descent at iteration  233\n",
      "Learned weights =  [-6.12353159e-02  2.18156086e+02  4.96141106e+01]\n",
      "Done with gradient descent at iteration  233\n",
      "Learned weights =  [-6.12353159e-02  2.18156086e+02  4.94510851e+01]\n",
      "Done with gradient descent at iteration  234\n",
      "Learned weights =  [-6.16738800e-02  2.18156086e+02  4.94510851e+01]\n",
      "Done with gradient descent at iteration  234\n",
      "Learned weights =  [-6.16738800e-02  2.18305327e+02  4.94510851e+01]\n",
      "Done with gradient descent at iteration  234\n",
      "Learned weights =  [-6.16738800e-02  2.18305327e+02  4.92890267e+01]\n",
      "Done with gradient descent at iteration  235\n",
      "Learned weights =  [-6.21120469e-02  2.18305327e+02  4.92890267e+01]\n",
      "Done with gradient descent at iteration  235\n",
      "Learned weights =  [-6.21120469e-02  2.18453683e+02  4.92890267e+01]\n",
      "Done with gradient descent at iteration  235\n",
      "Learned weights =  [-6.21120469e-02  2.18453683e+02  4.91279294e+01]\n",
      "Done with gradient descent at iteration  236\n",
      "Learned weights =  [-6.25498191e-02  2.18453683e+02  4.91279294e+01]\n",
      "Done with gradient descent at iteration  236\n",
      "Learned weights =  [-6.25498191e-02  2.18601159e+02  4.91279294e+01]\n",
      "Done with gradient descent at iteration  236\n",
      "Learned weights =  [-6.25498191e-02  2.18601159e+02  4.89677878e+01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  237\n",
      "Learned weights =  [-6.29871989e-02  2.18601159e+02  4.89677878e+01]\n",
      "Done with gradient descent at iteration  237\n",
      "Learned weights =  [-6.29871989e-02  2.18747761e+02  4.89677878e+01]\n",
      "Done with gradient descent at iteration  237\n",
      "Learned weights =  [-6.29871989e-02  2.18747761e+02  4.88085960e+01]\n",
      "Done with gradient descent at iteration  238\n",
      "Learned weights =  [-6.34241887e-02  2.18747761e+02  4.88085960e+01]\n",
      "Done with gradient descent at iteration  238\n",
      "Learned weights =  [-6.34241887e-02  2.18893493e+02  4.88085960e+01]\n",
      "Done with gradient descent at iteration  238\n",
      "Learned weights =  [-6.34241887e-02  2.18893493e+02  4.86503485e+01]\n",
      "Done with gradient descent at iteration  239\n",
      "Learned weights =  [-6.38607907e-02  2.18893493e+02  4.86503485e+01]\n",
      "Done with gradient descent at iteration  239\n",
      "Learned weights =  [-6.38607907e-02  2.19038360e+02  4.86503485e+01]\n",
      "Done with gradient descent at iteration  239\n",
      "Learned weights =  [-6.38607907e-02  2.19038360e+02  4.84930397e+01]\n",
      "Done with gradient descent at iteration  240\n",
      "Learned weights =  [-6.42970072e-02  2.19038360e+02  4.84930397e+01]\n",
      "Done with gradient descent at iteration  240\n",
      "Learned weights =  [-6.42970072e-02  2.19182368e+02  4.84930397e+01]\n",
      "Done with gradient descent at iteration  240\n",
      "Learned weights =  [-6.42970072e-02  2.19182368e+02  4.83366639e+01]\n",
      "Done with gradient descent at iteration  241\n",
      "Learned weights =  [-6.47328406e-02  2.19182368e+02  4.83366639e+01]\n",
      "Done with gradient descent at iteration  241\n",
      "Learned weights =  [-6.47328406e-02  2.19325522e+02  4.83366639e+01]\n",
      "Done with gradient descent at iteration  241\n",
      "Learned weights =  [-6.47328406e-02  2.19325522e+02  4.81812157e+01]\n",
      "Done with gradient descent at iteration  242\n",
      "Learned weights =  [-6.51682930e-02  2.19325522e+02  4.81812157e+01]\n",
      "Done with gradient descent at iteration  242\n",
      "Learned weights =  [-6.51682930e-02  2.19467827e+02  4.81812157e+01]\n",
      "Done with gradient descent at iteration  242\n",
      "Learned weights =  [-6.51682930e-02  2.19467827e+02  4.80266896e+01]\n",
      "Done with gradient descent at iteration  243\n",
      "Learned weights =  [-6.56033668e-02  2.19467827e+02  4.80266896e+01]\n",
      "Done with gradient descent at iteration  243\n",
      "Learned weights =  [-6.56033668e-02  2.19609288e+02  4.80266896e+01]\n",
      "Done with gradient descent at iteration  243\n",
      "Learned weights =  [-6.56033668e-02  2.19609288e+02  4.78730800e+01]\n",
      "Done with gradient descent at iteration  244\n",
      "Learned weights =  [-6.60380643e-02  2.19609288e+02  4.78730800e+01]\n",
      "Done with gradient descent at iteration  244\n",
      "Learned weights =  [-6.60380643e-02  2.19749909e+02  4.78730800e+01]\n",
      "Done with gradient descent at iteration  244\n",
      "Learned weights =  [-6.60380643e-02  2.19749909e+02  4.77203816e+01]\n",
      "Done with gradient descent at iteration  245\n",
      "Learned weights =  [-6.64723875e-02  2.19749909e+02  4.77203816e+01]\n",
      "Done with gradient descent at iteration  245\n",
      "Learned weights =  [-6.64723875e-02  2.19889697e+02  4.77203816e+01]\n",
      "Done with gradient descent at iteration  245\n",
      "Learned weights =  [-6.64723875e-02  2.19889697e+02  4.75685889e+01]\n",
      "Done with gradient descent at iteration  246\n",
      "Learned weights =  [-6.69063389e-02  2.19889697e+02  4.75685889e+01]\n",
      "Done with gradient descent at iteration  246\n",
      "Learned weights =  [-6.69063389e-02  2.20028655e+02  4.75685889e+01]\n",
      "Done with gradient descent at iteration  246\n",
      "Learned weights =  [-6.69063389e-02  2.20028655e+02  4.74176966e+01]\n",
      "Done with gradient descent at iteration  247\n",
      "Learned weights =  [-6.73399205e-02  2.20028655e+02  4.74176966e+01]\n",
      "Done with gradient descent at iteration  247\n",
      "Learned weights =  [-6.73399205e-02  2.20166789e+02  4.74176966e+01]\n",
      "Done with gradient descent at iteration  247\n",
      "Learned weights =  [-6.73399205e-02  2.20166789e+02  4.72676993e+01]\n",
      "Done with gradient descent at iteration  248\n",
      "Learned weights =  [-6.77731345e-02  2.20166789e+02  4.72676993e+01]\n",
      "Done with gradient descent at iteration  248\n",
      "Learned weights =  [-6.77731345e-02  2.20304104e+02  4.72676993e+01]\n",
      "Done with gradient descent at iteration  248\n",
      "Learned weights =  [-6.77731345e-02  2.20304104e+02  4.71185918e+01]\n",
      "Done with gradient descent at iteration  249\n",
      "Learned weights =  [-6.82059832e-02  2.20304104e+02  4.71185918e+01]\n",
      "Done with gradient descent at iteration  249\n",
      "Learned weights =  [-6.82059832e-02  2.20440604e+02  4.71185918e+01]\n",
      "Done with gradient descent at iteration  249\n",
      "Learned weights =  [-6.82059832e-02  2.20440604e+02  4.69703687e+01]\n",
      "Done with gradient descent at iteration  250\n",
      "Learned weights =  [-6.86384687e-02  2.20440604e+02  4.69703687e+01]\n",
      "Done with gradient descent at iteration  250\n",
      "Learned weights =  [-6.86384687e-02  2.20576295e+02  4.69703687e+01]\n",
      "Done with gradient descent at iteration  250\n",
      "Learned weights =  [-6.86384687e-02  2.20576295e+02  4.68230247e+01]\n",
      "Done with gradient descent at iteration  251\n",
      "Learned weights =  [-6.90705932e-02  2.20576295e+02  4.68230247e+01]\n",
      "Done with gradient descent at iteration  251\n",
      "Learned weights =  [-6.90705932e-02  2.20711181e+02  4.68230247e+01]\n",
      "Done with gradient descent at iteration  251\n",
      "Learned weights =  [-6.90705932e-02  2.20711181e+02  4.66765548e+01]\n",
      "Done with gradient descent at iteration  252\n",
      "Learned weights =  [-6.95023587e-02  2.20711181e+02  4.66765548e+01]\n",
      "Done with gradient descent at iteration  252\n",
      "Learned weights =  [-6.95023587e-02  2.20845266e+02  4.66765548e+01]\n",
      "Done with gradient descent at iteration  252\n",
      "Learned weights =  [-6.95023587e-02  2.20845266e+02  4.65309537e+01]\n",
      "Done with gradient descent at iteration  253\n",
      "Learned weights =  [-6.99337675e-02  2.20845266e+02  4.65309537e+01]\n",
      "Done with gradient descent at iteration  253\n",
      "Learned weights =  [-6.99337675e-02  2.20978557e+02  4.65309537e+01]\n",
      "Done with gradient descent at iteration  253\n",
      "Learned weights =  [-6.99337675e-02  2.20978557e+02  4.63862162e+01]\n",
      "Done with gradient descent at iteration  254\n",
      "Learned weights =  [-7.03648217e-02  2.20978557e+02  4.63862162e+01]\n",
      "Done with gradient descent at iteration  254\n",
      "Learned weights =  [-7.03648217e-02  2.21111056e+02  4.63862162e+01]\n",
      "Done with gradient descent at iteration  254\n",
      "Learned weights =  [-7.03648217e-02  2.21111056e+02  4.62423372e+01]\n",
      "Done with gradient descent at iteration  255\n",
      "Learned weights =  [-7.07955233e-02  2.21111056e+02  4.62423372e+01]\n",
      "Done with gradient descent at iteration  255\n",
      "Learned weights =  [-7.07955233e-02  2.21242770e+02  4.62423372e+01]\n",
      "Done with gradient descent at iteration  255\n",
      "Learned weights =  [-7.07955233e-02  2.21242770e+02  4.60993117e+01]\n",
      "Done with gradient descent at iteration  256\n",
      "Learned weights =  [-7.12258744e-02  2.21242770e+02  4.60993117e+01]\n",
      "Done with gradient descent at iteration  256\n",
      "Learned weights =  [-7.12258744e-02  2.21373703e+02  4.60993117e+01]\n",
      "Done with gradient descent at iteration  256\n",
      "Learned weights =  [-7.12258744e-02  2.21373703e+02  4.59571345e+01]\n",
      "Done with gradient descent at iteration  257\n",
      "Learned weights =  [-7.16558772e-02  2.21373703e+02  4.59571345e+01]\n",
      "Done with gradient descent at iteration  257\n",
      "Learned weights =  [-7.16558772e-02  2.21503858e+02  4.59571345e+01]\n",
      "Done with gradient descent at iteration  257\n",
      "Learned weights =  [-7.16558772e-02  2.21503858e+02  4.58158007e+01]\n",
      "Done with gradient descent at iteration  258\n",
      "Learned weights =  [-7.20855336e-02  2.21503858e+02  4.58158007e+01]\n",
      "Done with gradient descent at iteration  258\n",
      "Learned weights =  [-7.20855336e-02  2.21633242e+02  4.58158007e+01]\n",
      "Done with gradient descent at iteration  258\n",
      "Learned weights =  [-7.20855336e-02  2.21633242e+02  4.56753052e+01]\n",
      "Done with gradient descent at iteration  259\n",
      "Learned weights =  [-7.25148458e-02  2.21633242e+02  4.56753052e+01]\n",
      "Done with gradient descent at iteration  259\n",
      "Learned weights =  [-7.25148458e-02  2.21761859e+02  4.56753052e+01]\n",
      "Done with gradient descent at iteration  259\n",
      "Learned weights =  [-7.25148458e-02  2.21761859e+02  4.55356430e+01]\n",
      "Done with gradient descent at iteration  260\n",
      "Learned weights =  [-7.29438158e-02  2.21761859e+02  4.55356430e+01]\n",
      "Done with gradient descent at iteration  260\n",
      "Learned weights =  [-7.29438158e-02  2.21889712e+02  4.55356430e+01]\n",
      "Done with gradient descent at iteration  260\n",
      "Learned weights =  [-7.29438158e-02  2.21889712e+02  4.53968093e+01]\n",
      "Done with gradient descent at iteration  261\n",
      "Learned weights =  [-7.33724455e-02  2.21889712e+02  4.53968093e+01]\n",
      "Done with gradient descent at iteration  261\n",
      "Learned weights =  [-7.33724455e-02  2.22016807e+02  4.53968093e+01]\n",
      "Done with gradient descent at iteration  261\n",
      "Learned weights =  [-7.33724455e-02  2.22016807e+02  4.52587991e+01]\n",
      "Done with gradient descent at iteration  262\n",
      "Learned weights =  [-7.38007371e-02  2.22016807e+02  4.52587991e+01]\n",
      "Done with gradient descent at iteration  262\n",
      "Learned weights =  [-7.38007371e-02  2.22143148e+02  4.52587991e+01]\n",
      "Done with gradient descent at iteration  262\n",
      "Learned weights =  [-7.38007371e-02  2.22143148e+02  4.51216075e+01]\n",
      "Done with gradient descent at iteration  263\n",
      "Learned weights =  [-7.42286926e-02  2.22143148e+02  4.51216075e+01]\n",
      "Done with gradient descent at iteration  263\n",
      "Learned weights =  [-7.42286926e-02  2.22268740e+02  4.51216075e+01]\n",
      "Done with gradient descent at iteration  263\n",
      "Learned weights =  [-7.42286926e-02  2.22268740e+02  4.49852297e+01]\n",
      "Done with gradient descent at iteration  264\n",
      "Learned weights =  [-7.46563139e-02  2.22268740e+02  4.49852297e+01]\n",
      "Done with gradient descent at iteration  264\n",
      "Learned weights =  [-7.46563139e-02  2.22393587e+02  4.49852297e+01]\n",
      "Done with gradient descent at iteration  264\n",
      "Learned weights =  [-7.46563139e-02  2.22393587e+02  4.48496608e+01]\n",
      "Done with gradient descent at iteration  265\n",
      "Learned weights =  [-7.50836030e-02  2.22393587e+02  4.48496608e+01]\n",
      "Done with gradient descent at iteration  265\n",
      "Learned weights =  [-7.50836030e-02  2.22517694e+02  4.48496608e+01]\n",
      "Done with gradient descent at iteration  265\n",
      "Learned weights =  [-7.50836030e-02  2.22517694e+02  4.47148960e+01]\n",
      "Done with gradient descent at iteration  266\n",
      "Learned weights =  [-7.55105618e-02  2.22517694e+02  4.47148960e+01]\n",
      "Done with gradient descent at iteration  266\n",
      "Learned weights =  [-7.55105618e-02  2.22641064e+02  4.47148960e+01]\n",
      "Done with gradient descent at iteration  266\n",
      "Learned weights =  [-7.55105618e-02  2.22641064e+02  4.45809306e+01]\n",
      "Done with gradient descent at iteration  267\n",
      "Learned weights =  [-7.59371925e-02  2.22641064e+02  4.45809306e+01]\n",
      "Done with gradient descent at iteration  267\n",
      "Learned weights =  [-7.59371925e-02  2.22763702e+02  4.45809306e+01]\n",
      "Done with gradient descent at iteration  267\n",
      "Learned weights =  [-7.59371925e-02  2.22763702e+02  4.44477598e+01]\n",
      "Done with gradient descent at iteration  268\n",
      "Learned weights =  [-7.63634968e-02  2.22763702e+02  4.44477598e+01]\n",
      "Done with gradient descent at iteration  268\n",
      "Learned weights =  [-7.63634968e-02  2.22885613e+02  4.44477598e+01]\n",
      "Done with gradient descent at iteration  268\n",
      "Learned weights =  [-7.63634968e-02  2.22885613e+02  4.43153790e+01]\n",
      "Done with gradient descent at iteration  269\n",
      "Learned weights =  [-7.67894767e-02  2.22885613e+02  4.43153790e+01]\n",
      "Done with gradient descent at iteration  269\n",
      "Learned weights =  [-7.67894767e-02  2.23006801e+02  4.43153790e+01]\n",
      "Done with gradient descent at iteration  269\n",
      "Learned weights =  [-7.67894767e-02  2.23006801e+02  4.41837834e+01]\n",
      "Done with gradient descent at iteration  270\n",
      "Learned weights =  [-7.72151342e-02  2.23006801e+02  4.41837834e+01]\n",
      "Done with gradient descent at iteration  270\n",
      "Learned weights =  [-7.72151342e-02  2.23127270e+02  4.41837834e+01]\n",
      "Done with gradient descent at iteration  270\n",
      "Learned weights =  [-7.72151342e-02  2.23127270e+02  4.40529683e+01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  271\n",
      "Learned weights =  [-7.76404711e-02  2.23127270e+02  4.40529683e+01]\n",
      "Done with gradient descent at iteration  271\n",
      "Learned weights =  [-7.76404711e-02  2.23247024e+02  4.40529683e+01]\n",
      "Done with gradient descent at iteration  271\n",
      "Learned weights =  [-7.76404711e-02  2.23247024e+02  4.39229292e+01]\n",
      "Done with gradient descent at iteration  272\n",
      "Learned weights =  [-7.80654894e-02  2.23247024e+02  4.39229292e+01]\n",
      "Done with gradient descent at iteration  272\n",
      "Learned weights =  [-7.80654894e-02  2.23366069e+02  4.39229292e+01]\n",
      "Done with gradient descent at iteration  272\n",
      "Learned weights =  [-7.80654894e-02  2.23366069e+02  4.37936614e+01]\n",
      "Done with gradient descent at iteration  273\n",
      "Learned weights =  [-7.84901910e-02  2.23366069e+02  4.37936614e+01]\n",
      "Done with gradient descent at iteration  273\n",
      "Learned weights =  [-7.84901910e-02  2.23484407e+02  4.37936614e+01]\n",
      "Done with gradient descent at iteration  273\n",
      "Learned weights =  [-7.84901910e-02  2.23484407e+02  4.36651604e+01]\n",
      "Done with gradient descent at iteration  274\n",
      "Learned weights =  [-7.89145777e-02  2.23484407e+02  4.36651604e+01]\n",
      "Done with gradient descent at iteration  274\n",
      "Learned weights =  [-7.89145777e-02  2.23602043e+02  4.36651604e+01]\n",
      "Done with gradient descent at iteration  274\n",
      "Learned weights =  [-7.89145777e-02  2.23602043e+02  4.35374216e+01]\n",
      "Done with gradient descent at iteration  275\n",
      "Learned weights =  [-7.93386514e-02  2.23602043e+02  4.35374216e+01]\n",
      "Done with gradient descent at iteration  275\n",
      "Learned weights =  [-7.93386514e-02  2.23718981e+02  4.35374216e+01]\n",
      "Done with gradient descent at iteration  275\n",
      "Learned weights =  [-7.93386514e-02  2.23718981e+02  4.34104405e+01]\n",
      "Done with gradient descent at iteration  276\n",
      "Learned weights =  [-7.97624139e-02  2.23718981e+02  4.34104405e+01]\n",
      "Done with gradient descent at iteration  276\n",
      "Learned weights =  [-7.97624139e-02  2.23835226e+02  4.34104405e+01]\n",
      "Done with gradient descent at iteration  276\n",
      "Learned weights =  [-7.97624139e-02  2.23835226e+02  4.32842126e+01]\n",
      "Done with gradient descent at iteration  277\n",
      "Learned weights =  [-8.01858672e-02  2.23835226e+02  4.32842126e+01]\n",
      "Done with gradient descent at iteration  277\n",
      "Learned weights =  [-8.01858672e-02  2.23950781e+02  4.32842126e+01]\n",
      "Done with gradient descent at iteration  277\n",
      "Learned weights =  [-8.01858672e-02  2.23950781e+02  4.31587334e+01]\n",
      "Done with gradient descent at iteration  278\n",
      "Learned weights =  [-8.06090130e-02  2.23950781e+02  4.31587334e+01]\n",
      "Done with gradient descent at iteration  278\n",
      "Learned weights =  [-8.06090130e-02  2.24065651e+02  4.31587334e+01]\n",
      "Done with gradient descent at iteration  278\n",
      "Learned weights =  [-8.06090130e-02  2.24065651e+02  4.30339985e+01]\n",
      "Done with gradient descent at iteration  279\n",
      "Learned weights =  [-8.10318531e-02  2.24065651e+02  4.30339985e+01]\n",
      "Done with gradient descent at iteration  279\n",
      "Learned weights =  [-8.10318531e-02  2.24179839e+02  4.30339985e+01]\n",
      "Done with gradient descent at iteration  279\n",
      "Learned weights =  [-8.10318531e-02  2.24179839e+02  4.29100035e+01]\n",
      "Done with gradient descent at iteration  280\n",
      "Learned weights =  [-8.14543894e-02  2.24179839e+02  4.29100035e+01]\n",
      "Done with gradient descent at iteration  280\n",
      "Learned weights =  [-8.14543894e-02  2.24293350e+02  4.29100035e+01]\n",
      "Done with gradient descent at iteration  280\n",
      "Learned weights =  [-8.14543894e-02  2.24293350e+02  4.27867440e+01]\n",
      "Done with gradient descent at iteration  281\n",
      "Learned weights =  [-8.18766237e-02  2.24293350e+02  4.27867440e+01]\n",
      "Done with gradient descent at iteration  281\n",
      "Learned weights =  [-8.18766237e-02  2.24406188e+02  4.27867440e+01]\n",
      "Done with gradient descent at iteration  281\n",
      "Learned weights =  [-8.18766237e-02  2.24406188e+02  4.26642156e+01]\n",
      "Done with gradient descent at iteration  282\n",
      "Learned weights =  [-8.22985578e-02  2.24406188e+02  4.26642156e+01]\n",
      "Done with gradient descent at iteration  282\n",
      "Learned weights =  [-8.22985578e-02  2.24518356e+02  4.26642156e+01]\n",
      "Done with gradient descent at iteration  282\n",
      "Learned weights =  [-8.22985578e-02  2.24518356e+02  4.25424139e+01]\n",
      "Done with gradient descent at iteration  283\n",
      "Learned weights =  [-8.27201934e-02  2.24518356e+02  4.25424139e+01]\n",
      "Done with gradient descent at iteration  283\n",
      "Learned weights =  [-8.27201934e-02  2.24629859e+02  4.25424139e+01]\n",
      "Done with gradient descent at iteration  283\n",
      "Learned weights =  [-8.27201934e-02  2.24629859e+02  4.24213348e+01]\n",
      "Done with gradient descent at iteration  284\n",
      "Learned weights =  [-8.31415323e-02  2.24629859e+02  4.24213348e+01]\n",
      "Done with gradient descent at iteration  284\n",
      "Learned weights =  [-8.31415323e-02  2.24740701e+02  4.24213348e+01]\n",
      "Done with gradient descent at iteration  284\n",
      "Learned weights =  [-8.31415323e-02  2.24740701e+02  4.23009738e+01]\n",
      "Done with gradient descent at iteration  285\n",
      "Learned weights =  [-8.35625763e-02  2.24740701e+02  4.23009738e+01]\n",
      "Done with gradient descent at iteration  285\n",
      "Learned weights =  [-8.35625763e-02  2.24850885e+02  4.23009738e+01]\n",
      "Done with gradient descent at iteration  285\n",
      "Learned weights =  [-8.35625763e-02  2.24850885e+02  4.21813268e+01]\n",
      "Done with gradient descent at iteration  286\n",
      "Learned weights =  [-8.39833271e-02  2.24850885e+02  4.21813268e+01]\n",
      "Done with gradient descent at iteration  286\n",
      "Learned weights =  [-8.39833271e-02  2.24960416e+02  4.21813268e+01]\n",
      "Done with gradient descent at iteration  286\n",
      "Learned weights =  [-8.39833271e-02  2.24960416e+02  4.20623895e+01]\n",
      "Done with gradient descent at iteration  287\n",
      "Learned weights =  [-8.44037865e-02  2.24960416e+02  4.20623895e+01]\n",
      "Done with gradient descent at iteration  287\n",
      "Learned weights =  [-8.44037865e-02  2.25069297e+02  4.20623895e+01]\n",
      "Done with gradient descent at iteration  287\n",
      "Learned weights =  [-8.44037865e-02  2.25069297e+02  4.19441577e+01]\n",
      "Done with gradient descent at iteration  288\n",
      "Learned weights =  [-8.48239562e-02  2.25069297e+02  4.19441577e+01]\n",
      "Done with gradient descent at iteration  288\n",
      "Learned weights =  [-8.48239562e-02  2.25177532e+02  4.19441577e+01]\n",
      "Done with gradient descent at iteration  288\n",
      "Learned weights =  [-8.48239562e-02  2.25177532e+02  4.18266271e+01]\n",
      "Done with gradient descent at iteration  289\n",
      "Learned weights =  [-8.52438379e-02  2.25177532e+02  4.18266271e+01]\n",
      "Done with gradient descent at iteration  289\n",
      "Learned weights =  [-8.52438379e-02  2.25285125e+02  4.18266271e+01]\n",
      "Done with gradient descent at iteration  289\n",
      "Learned weights =  [-8.52438379e-02  2.25285125e+02  4.17097937e+01]\n",
      "Done with gradient descent at iteration  290\n",
      "Learned weights =  [-8.56634333e-02  2.25285125e+02  4.17097937e+01]\n",
      "Done with gradient descent at iteration  290\n",
      "Learned weights =  [-8.56634333e-02  2.25392080e+02  4.17097937e+01]\n",
      "Done with gradient descent at iteration  290\n",
      "Learned weights =  [-8.56634333e-02  2.25392080e+02  4.15936534e+01]\n",
      "Done with gradient descent at iteration  291\n",
      "Learned weights =  [-8.60827441e-02  2.25392080e+02  4.15936534e+01]\n",
      "Done with gradient descent at iteration  291\n",
      "Learned weights =  [-8.60827441e-02  2.25498401e+02  4.15936534e+01]\n",
      "Done with gradient descent at iteration  291\n",
      "Learned weights =  [-8.60827441e-02  2.25498401e+02  4.14782019e+01]\n",
      "Done with gradient descent at iteration  292\n",
      "Learned weights =  [-8.65017721e-02  2.25498401e+02  4.14782019e+01]\n",
      "Done with gradient descent at iteration  292\n",
      "Learned weights =  [-8.65017721e-02  2.25604091e+02  4.14782019e+01]\n",
      "Done with gradient descent at iteration  292\n",
      "Learned weights =  [-8.65017721e-02  2.25604091e+02  4.13634352e+01]\n",
      "Done with gradient descent at iteration  293\n",
      "Learned weights =  [-8.69205188e-02  2.25604091e+02  4.13634352e+01]\n",
      "Done with gradient descent at iteration  293\n",
      "Learned weights =  [-8.69205188e-02  2.25709154e+02  4.13634352e+01]\n",
      "Done with gradient descent at iteration  293\n",
      "Learned weights =  [-8.69205188e-02  2.25709154e+02  4.12493493e+01]\n",
      "Done with gradient descent at iteration  294\n",
      "Learned weights =  [-8.73389860e-02  2.25709154e+02  4.12493493e+01]\n",
      "Done with gradient descent at iteration  294\n",
      "Learned weights =  [-8.73389860e-02  2.25813594e+02  4.12493493e+01]\n",
      "Done with gradient descent at iteration  294\n",
      "Learned weights =  [-8.73389860e-02  2.25813594e+02  4.11359400e+01]\n",
      "Done with gradient descent at iteration  295\n",
      "Learned weights =  [-8.77571753e-02  2.25813594e+02  4.11359400e+01]\n",
      "Done with gradient descent at iteration  295\n",
      "Learned weights =  [-8.77571753e-02  2.25917414e+02  4.11359400e+01]\n",
      "Done with gradient descent at iteration  295\n",
      "Learned weights =  [-8.77571753e-02  2.25917414e+02  4.10232035e+01]\n",
      "Done with gradient descent at iteration  296\n",
      "Learned weights =  [-8.81750883e-02  2.25917414e+02  4.10232035e+01]\n",
      "Done with gradient descent at iteration  296\n",
      "Learned weights =  [-8.81750883e-02  2.26020618e+02  4.10232035e+01]\n",
      "Done with gradient descent at iteration  296\n",
      "Learned weights =  [-8.81750883e-02  2.26020618e+02  4.09111357e+01]\n",
      "Done with gradient descent at iteration  297\n",
      "Learned weights =  [-8.85927268e-02  2.26020618e+02  4.09111357e+01]\n",
      "Done with gradient descent at iteration  297\n",
      "Learned weights =  [-8.85927268e-02  2.26123211e+02  4.09111357e+01]\n",
      "Done with gradient descent at iteration  297\n",
      "Learned weights =  [-8.85927268e-02  2.26123211e+02  4.07997326e+01]\n",
      "Done with gradient descent at iteration  298\n",
      "Learned weights =  [-8.90100922e-02  2.26123211e+02  4.07997326e+01]\n",
      "Done with gradient descent at iteration  298\n",
      "Learned weights =  [-8.90100922e-02  2.26225195e+02  4.07997326e+01]\n",
      "Done with gradient descent at iteration  298\n",
      "Learned weights =  [-8.90100922e-02  2.26225195e+02  4.06889903e+01]\n",
      "Done with gradient descent at iteration  299\n",
      "Learned weights =  [-8.94271864e-02  2.26225195e+02  4.06889903e+01]\n",
      "Done with gradient descent at iteration  299\n",
      "Learned weights =  [-8.94271864e-02  2.26326573e+02  4.06889903e+01]\n",
      "Done with gradient descent at iteration  299\n",
      "Learned weights =  [-8.94271864e-02  2.26326573e+02  4.05789049e+01]\n",
      "Iteration = 300\n",
      "Cost function =  1207473080962631.5\n",
      "Done with gradient descent at iteration  300\n",
      "Learned weights =  [-8.98440107e-02  2.26326573e+02  4.05789049e+01]\n",
      "Done with gradient descent at iteration  300\n",
      "Learned weights =  [-8.98440107e-02  2.26427351e+02  4.05789049e+01]\n",
      "Done with gradient descent at iteration  300\n",
      "Learned weights =  [-8.98440107e-02  2.26427351e+02  4.04694725e+01]\n",
      "Done with gradient descent at iteration  301\n",
      "Learned weights =  [-9.02605670e-02  2.26427351e+02  4.04694725e+01]\n",
      "Done with gradient descent at iteration  301\n",
      "Learned weights =  [-9.02605670e-02  2.26527531e+02  4.04694725e+01]\n",
      "Done with gradient descent at iteration  301\n",
      "Learned weights =  [-9.02605670e-02  2.26527531e+02  4.03606892e+01]\n",
      "Done with gradient descent at iteration  302\n",
      "Learned weights =  [-9.06768566e-02  2.26527531e+02  4.03606892e+01]\n",
      "Done with gradient descent at iteration  302\n",
      "Learned weights =  [-9.06768566e-02  2.26627116e+02  4.03606892e+01]\n",
      "Done with gradient descent at iteration  302\n",
      "Learned weights =  [-9.06768566e-02  2.26627116e+02  4.02525511e+01]\n",
      "Done with gradient descent at iteration  303\n",
      "Learned weights =  [-9.10928814e-02  2.26627116e+02  4.02525511e+01]\n",
      "Done with gradient descent at iteration  303\n",
      "Learned weights =  [-9.10928814e-02  2.26726111e+02  4.02525511e+01]\n",
      "Done with gradient descent at iteration  303\n",
      "Learned weights =  [-9.10928814e-02  2.26726111e+02  4.01450545e+01]\n",
      "Done with gradient descent at iteration  304\n",
      "Learned weights =  [-9.15086427e-02  2.26726111e+02  4.01450545e+01]\n",
      "Done with gradient descent at iteration  304\n",
      "Learned weights =  [-9.15086427e-02  2.26824519e+02  4.01450545e+01]\n",
      "Done with gradient descent at iteration  304\n",
      "Learned weights =  [-9.15086427e-02  2.26824519e+02  4.00381955e+01]\n",
      "Done with gradient descent at iteration  305\n",
      "Learned weights =  [-9.19241421e-02  2.26824519e+02  4.00381955e+01]\n",
      "Done with gradient descent at iteration  305\n",
      "Learned weights =  [-9.19241421e-02  2.26922343e+02  4.00381955e+01]\n",
      "Done with gradient descent at iteration  305\n",
      "Learned weights =  [-9.19241421e-02  2.26922343e+02  3.99319703e+01]\n",
      "Done with gradient descent at iteration  306\n",
      "Learned weights =  [-9.23393813e-02  2.26922343e+02  3.99319703e+01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  306\n",
      "Learned weights =  [-9.23393813e-02  2.27019586e+02  3.99319703e+01]\n",
      "Done with gradient descent at iteration  306\n",
      "Learned weights =  [-9.23393813e-02  2.27019586e+02  3.98263752e+01]\n",
      "Done with gradient descent at iteration  307\n",
      "Learned weights =  [-9.27543617e-02  2.27019586e+02  3.98263752e+01]\n",
      "Done with gradient descent at iteration  307\n",
      "Learned weights =  [-9.27543617e-02  2.27116253e+02  3.98263752e+01]\n",
      "Done with gradient descent at iteration  307\n",
      "Learned weights =  [-9.27543617e-02  2.27116253e+02  3.97214065e+01]\n",
      "Done with gradient descent at iteration  308\n",
      "Learned weights =  [-9.31690850e-02  2.27116253e+02  3.97214065e+01]\n",
      "Done with gradient descent at iteration  308\n",
      "Learned weights =  [-9.31690850e-02  2.27212347e+02  3.97214065e+01]\n",
      "Done with gradient descent at iteration  308\n",
      "Learned weights =  [-9.31690850e-02  2.27212347e+02  3.96170604e+01]\n",
      "Done with gradient descent at iteration  309\n",
      "Learned weights =  [-9.35835525e-02  2.27212347e+02  3.96170604e+01]\n",
      "Done with gradient descent at iteration  309\n",
      "Learned weights =  [-9.35835525e-02  2.27307870e+02  3.96170604e+01]\n",
      "Done with gradient descent at iteration  309\n",
      "Learned weights =  [-9.35835525e-02  2.27307870e+02  3.95133333e+01]\n",
      "Done with gradient descent at iteration  310\n",
      "Learned weights =  [-9.39977659e-02  2.27307870e+02  3.95133333e+01]\n",
      "Done with gradient descent at iteration  310\n",
      "Learned weights =  [-9.39977659e-02  2.27402827e+02  3.95133333e+01]\n",
      "Done with gradient descent at iteration  310\n",
      "Learned weights =  [-9.39977659e-02  2.27402827e+02  3.94102214e+01]\n",
      "Done with gradient descent at iteration  311\n",
      "Learned weights =  [-9.44117266e-02  2.27402827e+02  3.94102214e+01]\n",
      "Done with gradient descent at iteration  311\n",
      "Learned weights =  [-9.44117266e-02  2.27497221e+02  3.94102214e+01]\n",
      "Done with gradient descent at iteration  311\n",
      "Learned weights =  [-9.44117266e-02  2.27497221e+02  3.93077211e+01]\n",
      "Done with gradient descent at iteration  312\n",
      "Learned weights =  [-9.48254362e-02  2.27497221e+02  3.93077211e+01]\n",
      "Done with gradient descent at iteration  312\n",
      "Learned weights =  [-9.48254362e-02  2.27591055e+02  3.93077211e+01]\n",
      "Done with gradient descent at iteration  312\n",
      "Learned weights =  [-9.48254362e-02  2.27591055e+02  3.92058288e+01]\n",
      "Done with gradient descent at iteration  313\n",
      "Learned weights =  [-9.52388961e-02  2.27591055e+02  3.92058288e+01]\n",
      "Done with gradient descent at iteration  313\n",
      "Learned weights =  [-9.52388961e-02  2.27684332e+02  3.92058288e+01]\n",
      "Done with gradient descent at iteration  313\n",
      "Learned weights =  [-9.52388961e-02  2.27684332e+02  3.91045409e+01]\n",
      "Done with gradient descent at iteration  314\n",
      "Learned weights =  [-9.56521078e-02  2.27684332e+02  3.91045409e+01]\n",
      "Done with gradient descent at iteration  314\n",
      "Learned weights =  [-9.56521078e-02  2.27777056e+02  3.91045409e+01]\n",
      "Done with gradient descent at iteration  314\n",
      "Learned weights =  [-9.56521078e-02  2.27777056e+02  3.90038538e+01]\n",
      "Done with gradient descent at iteration  315\n",
      "Learned weights =  [-9.60650728e-02  2.27777056e+02  3.90038538e+01]\n",
      "Done with gradient descent at iteration  315\n",
      "Learned weights =  [-9.60650728e-02  2.27869230e+02  3.90038538e+01]\n",
      "Done with gradient descent at iteration  315\n",
      "Learned weights =  [-9.60650728e-02  2.27869230e+02  3.89037639e+01]\n",
      "Done with gradient descent at iteration  316\n",
      "Learned weights =  [-9.64777925e-02  2.27869230e+02  3.89037639e+01]\n",
      "Done with gradient descent at iteration  316\n",
      "Learned weights =  [-9.64777925e-02  2.27960857e+02  3.89037639e+01]\n",
      "Done with gradient descent at iteration  316\n",
      "Learned weights =  [-9.64777925e-02  2.27960857e+02  3.88042677e+01]\n",
      "Done with gradient descent at iteration  317\n",
      "Learned weights =  [-9.68902685e-02  2.27960857e+02  3.88042677e+01]\n",
      "Done with gradient descent at iteration  317\n",
      "Learned weights =  [-9.68902685e-02  2.28051941e+02  3.88042677e+01]\n",
      "Done with gradient descent at iteration  317\n",
      "Learned weights =  [-9.68902685e-02  2.28051941e+02  3.87053617e+01]\n",
      "Done with gradient descent at iteration  318\n",
      "Learned weights =  [-9.73025021e-02  2.28051941e+02  3.87053617e+01]\n",
      "Done with gradient descent at iteration  318\n",
      "Learned weights =  [-9.73025021e-02  2.28142484e+02  3.87053617e+01]\n",
      "Done with gradient descent at iteration  318\n",
      "Learned weights =  [-9.73025021e-02  2.28142484e+02  3.86070423e+01]\n",
      "Done with gradient descent at iteration  319\n",
      "Learned weights =  [-9.77144947e-02  2.28142484e+02  3.86070423e+01]\n",
      "Done with gradient descent at iteration  319\n",
      "Learned weights =  [-9.77144947e-02  2.28232490e+02  3.86070423e+01]\n",
      "Done with gradient descent at iteration  319\n",
      "Learned weights =  [-9.77144947e-02  2.28232490e+02  3.85093062e+01]\n",
      "Done with gradient descent at iteration  320\n",
      "Learned weights =  [-9.81262479e-02  2.28232490e+02  3.85093062e+01]\n",
      "Done with gradient descent at iteration  320\n",
      "Learned weights =  [-9.81262479e-02  2.28321963e+02  3.85093062e+01]\n",
      "Done with gradient descent at iteration  320\n",
      "Learned weights =  [-9.81262479e-02  2.28321963e+02  3.84121497e+01]\n",
      "Done with gradient descent at iteration  321\n",
      "Learned weights =  [-9.85377631e-02  2.28321963e+02  3.84121497e+01]\n",
      "Done with gradient descent at iteration  321\n",
      "Learned weights =  [-9.85377631e-02  2.28410904e+02  3.84121497e+01]\n",
      "Done with gradient descent at iteration  321\n",
      "Learned weights =  [-9.85377631e-02  2.28410904e+02  3.83155696e+01]\n",
      "Done with gradient descent at iteration  322\n",
      "Learned weights =  [-9.89490415e-02  2.28410904e+02  3.83155696e+01]\n",
      "Done with gradient descent at iteration  322\n",
      "Learned weights =  [-9.89490415e-02  2.28499319e+02  3.83155696e+01]\n",
      "Done with gradient descent at iteration  322\n",
      "Learned weights =  [-9.89490415e-02  2.28499319e+02  3.82195623e+01]\n",
      "Done with gradient descent at iteration  323\n",
      "Learned weights =  [-9.93600848e-02  2.28499319e+02  3.82195623e+01]\n",
      "Done with gradient descent at iteration  323\n",
      "Learned weights =  [-9.93600848e-02  2.28587208e+02  3.82195623e+01]\n",
      "Done with gradient descent at iteration  323\n",
      "Learned weights =  [-9.93600848e-02  2.28587208e+02  3.81241246e+01]\n",
      "Done with gradient descent at iteration  324\n",
      "Learned weights =  [-9.97708941e-02  2.28587208e+02  3.81241246e+01]\n",
      "Done with gradient descent at iteration  324\n",
      "Learned weights =  [-9.97708941e-02  2.28674577e+02  3.81241246e+01]\n",
      "Done with gradient descent at iteration  324\n",
      "Learned weights =  [-9.97708941e-02  2.28674577e+02  3.80292529e+01]\n",
      "Done with gradient descent at iteration  325\n",
      "Learned weights =  [-1.00181471e-01  2.28674577e+02  3.80292529e+01]\n",
      "Done with gradient descent at iteration  325\n",
      "Learned weights =  [-1.00181471e-01  2.28761427e+02  3.80292529e+01]\n",
      "Done with gradient descent at iteration  325\n",
      "Learned weights =  [-1.00181471e-01  2.28761427e+02  3.79349439e+01]\n",
      "Done with gradient descent at iteration  326\n",
      "Learned weights =  [-1.00591817e-01  2.28761427e+02  3.79349439e+01]\n",
      "Done with gradient descent at iteration  326\n",
      "Learned weights =  [-1.00591817e-01  2.28847762e+02  3.79349439e+01]\n",
      "Done with gradient descent at iteration  326\n",
      "Learned weights =  [-1.00591817e-01  2.28847762e+02  3.78411944e+01]\n",
      "Done with gradient descent at iteration  327\n",
      "Learned weights =  [-1.01001933e-01  2.28847762e+02  3.78411944e+01]\n",
      "Done with gradient descent at iteration  327\n",
      "Learned weights =  [-1.01001933e-01  2.28933585e+02  3.78411944e+01]\n",
      "Done with gradient descent at iteration  327\n",
      "Learned weights =  [-1.01001933e-01  2.28933585e+02  3.77480009e+01]\n",
      "Done with gradient descent at iteration  328\n",
      "Learned weights =  [-1.01411821e-01  2.28933585e+02  3.77480009e+01]\n",
      "Done with gradient descent at iteration  328\n",
      "Learned weights =  [-1.01411821e-01  2.29018899e+02  3.77480009e+01]\n",
      "Done with gradient descent at iteration  328\n",
      "Learned weights =  [-1.01411821e-01  2.29018899e+02  3.76553602e+01]\n",
      "Done with gradient descent at iteration  329\n",
      "Learned weights =  [-1.01821481e-01  2.29018899e+02  3.76553602e+01]\n",
      "Done with gradient descent at iteration  329\n",
      "Learned weights =  [-1.01821481e-01  2.29103707e+02  3.76553602e+01]\n",
      "Done with gradient descent at iteration  329\n",
      "Learned weights =  [-1.01821481e-01  2.29103707e+02  3.75632690e+01]\n",
      "Done with gradient descent at iteration  330\n",
      "Learned weights =  [-1.02230917e-01  2.29103707e+02  3.75632690e+01]\n",
      "Done with gradient descent at iteration  330\n",
      "Learned weights =  [-1.02230917e-01  2.29188011e+02  3.75632690e+01]\n",
      "Done with gradient descent at iteration  330\n",
      "Learned weights =  [-1.02230917e-01  2.29188011e+02  3.74717241e+01]\n",
      "Done with gradient descent at iteration  331\n",
      "Learned weights =  [-1.02640127e-01  2.29188011e+02  3.74717241e+01]\n",
      "Done with gradient descent at iteration  331\n",
      "Learned weights =  [-1.02640127e-01  2.29271816e+02  3.74717241e+01]\n",
      "Done with gradient descent at iteration  331\n",
      "Learned weights =  [-1.02640127e-01  2.29271816e+02  3.73807222e+01]\n",
      "Done with gradient descent at iteration  332\n",
      "Learned weights =  [-1.03049115e-01  2.29271816e+02  3.73807222e+01]\n",
      "Done with gradient descent at iteration  332\n",
      "Learned weights =  [-1.03049115e-01  2.29355124e+02  3.73807222e+01]\n",
      "Done with gradient descent at iteration  332\n",
      "Learned weights =  [-1.03049115e-01  2.29355124e+02  3.72902601e+01]\n",
      "Done with gradient descent at iteration  333\n",
      "Learned weights =  [-1.03457881e-01  2.29355124e+02  3.72902601e+01]\n",
      "Done with gradient descent at iteration  333\n",
      "Learned weights =  [-1.03457881e-01  2.29437937e+02  3.72902601e+01]\n",
      "Done with gradient descent at iteration  333\n",
      "Learned weights =  [-1.03457881e-01  2.29437937e+02  3.72003345e+01]\n",
      "Done with gradient descent at iteration  334\n",
      "Learned weights =  [-1.03866427e-01  2.29437937e+02  3.72003345e+01]\n",
      "Done with gradient descent at iteration  334\n",
      "Learned weights =  [-1.03866427e-01  2.29520260e+02  3.72003345e+01]\n",
      "Done with gradient descent at iteration  334\n",
      "Learned weights =  [-1.03866427e-01  2.29520260e+02  3.71109423e+01]\n",
      "Done with gradient descent at iteration  335\n",
      "Learned weights =  [-1.04274754e-01  2.29520260e+02  3.71109423e+01]\n",
      "Done with gradient descent at iteration  335\n",
      "Learned weights =  [-1.04274754e-01  2.29602093e+02  3.71109423e+01]\n",
      "Done with gradient descent at iteration  335\n",
      "Learned weights =  [-1.04274754e-01  2.29602093e+02  3.70220804e+01]\n",
      "Done with gradient descent at iteration  336\n",
      "Learned weights =  [-1.04682863e-01  2.29602093e+02  3.70220804e+01]\n",
      "Done with gradient descent at iteration  336\n",
      "Learned weights =  [-1.04682863e-01  2.29683442e+02  3.70220804e+01]\n",
      "Done with gradient descent at iteration  336\n",
      "Learned weights =  [-1.04682863e-01  2.29683442e+02  3.69337456e+01]\n",
      "Done with gradient descent at iteration  337\n",
      "Learned weights =  [-1.05090755e-01  2.29683442e+02  3.69337456e+01]\n",
      "Done with gradient descent at iteration  337\n",
      "Learned weights =  [-1.05090755e-01  2.29764308e+02  3.69337456e+01]\n",
      "Done with gradient descent at iteration  337\n",
      "Learned weights =  [-1.05090755e-01  2.29764308e+02  3.68459348e+01]\n",
      "Done with gradient descent at iteration  338\n",
      "Learned weights =  [-1.05498433e-01  2.29764308e+02  3.68459348e+01]\n",
      "Done with gradient descent at iteration  338\n",
      "Learned weights =  [-1.05498433e-01  2.29844694e+02  3.68459348e+01]\n",
      "Done with gradient descent at iteration  338\n",
      "Learned weights =  [-1.05498433e-01  2.29844694e+02  3.67586448e+01]\n",
      "Done with gradient descent at iteration  339\n",
      "Learned weights =  [-1.05905896e-01  2.29844694e+02  3.67586448e+01]\n",
      "Done with gradient descent at iteration  339\n",
      "Learned weights =  [-1.05905896e-01  2.29924604e+02  3.67586448e+01]\n",
      "Done with gradient descent at iteration  339\n",
      "Learned weights =  [-1.05905896e-01  2.29924604e+02  3.66718725e+01]\n",
      "Done with gradient descent at iteration  340\n",
      "Learned weights =  [-1.06313147e-01  2.29924604e+02  3.66718725e+01]\n",
      "Done with gradient descent at iteration  340\n",
      "Learned weights =  [-1.06313147e-01  2.30004040e+02  3.66718725e+01]\n",
      "Done with gradient descent at iteration  340\n",
      "Learned weights =  [-1.06313147e-01  2.30004040e+02  3.65856150e+01]\n",
      "Done with gradient descent at iteration  341\n",
      "Learned weights =  [-1.06720187e-01  2.30004040e+02  3.65856150e+01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  341\n",
      "Learned weights =  [-1.06720187e-01  2.30083004e+02  3.65856150e+01]\n",
      "Done with gradient descent at iteration  341\n",
      "Learned weights =  [-1.06720187e-01  2.30083004e+02  3.64998691e+01]\n",
      "Done with gradient descent at iteration  342\n",
      "Learned weights =  [-1.07127016e-01  2.30083004e+02  3.64998691e+01]\n",
      "Done with gradient descent at iteration  342\n",
      "Learned weights =  [-1.07127016e-01  2.30161500e+02  3.64998691e+01]\n",
      "Done with gradient descent at iteration  342\n",
      "Learned weights =  [-1.07127016e-01  2.30161500e+02  3.64146318e+01]\n",
      "Done with gradient descent at iteration  343\n",
      "Learned weights =  [-1.07533637e-01  2.30161500e+02  3.64146318e+01]\n",
      "Done with gradient descent at iteration  343\n",
      "Learned weights =  [-1.07533637e-01  2.30239530e+02  3.64146318e+01]\n",
      "Done with gradient descent at iteration  343\n",
      "Learned weights =  [-1.07533637e-01  2.30239530e+02  3.63299001e+01]\n",
      "Done with gradient descent at iteration  344\n",
      "Learned weights =  [-1.07940050e-01  2.30239530e+02  3.63299001e+01]\n",
      "Done with gradient descent at iteration  344\n",
      "Learned weights =  [-1.07940050e-01  2.30317098e+02  3.63299001e+01]\n",
      "Done with gradient descent at iteration  344\n",
      "Learned weights =  [-1.07940050e-01  2.30317098e+02  3.62456710e+01]\n",
      "Done with gradient descent at iteration  345\n",
      "Learned weights =  [-1.08346256e-01  2.30317098e+02  3.62456710e+01]\n",
      "Done with gradient descent at iteration  345\n",
      "Learned weights =  [-1.08346256e-01  2.30394205e+02  3.62456710e+01]\n",
      "Done with gradient descent at iteration  345\n",
      "Learned weights =  [-1.08346256e-01  2.30394205e+02  3.61619415e+01]\n",
      "Done with gradient descent at iteration  346\n",
      "Learned weights =  [-1.08752258e-01  2.30394205e+02  3.61619415e+01]\n",
      "Done with gradient descent at iteration  346\n",
      "Learned weights =  [-1.08752258e-01  2.30470855e+02  3.61619415e+01]\n",
      "Done with gradient descent at iteration  346\n",
      "Learned weights =  [-1.08752258e-01  2.30470855e+02  3.60787087e+01]\n",
      "Done with gradient descent at iteration  347\n",
      "Learned weights =  [-1.09158055e-01  2.30470855e+02  3.60787087e+01]\n",
      "Done with gradient descent at iteration  347\n",
      "Learned weights =  [-1.09158055e-01  2.30547051e+02  3.60787087e+01]\n",
      "Done with gradient descent at iteration  347\n",
      "Learned weights =  [-1.09158055e-01  2.30547051e+02  3.59959696e+01]\n",
      "Done with gradient descent at iteration  348\n",
      "Learned weights =  [-1.09563650e-01  2.30547051e+02  3.59959696e+01]\n",
      "Done with gradient descent at iteration  348\n",
      "Learned weights =  [-1.09563650e-01  2.30622794e+02  3.59959696e+01]\n",
      "Done with gradient descent at iteration  348\n",
      "Learned weights =  [-1.09563650e-01  2.30622794e+02  3.59137212e+01]\n",
      "Done with gradient descent at iteration  349\n",
      "Learned weights =  [-1.09969043e-01  2.30622794e+02  3.59137212e+01]\n",
      "Done with gradient descent at iteration  349\n",
      "Learned weights =  [-1.09969043e-01  2.30698089e+02  3.59137212e+01]\n",
      "Done with gradient descent at iteration  349\n",
      "Learned weights =  [-1.09969043e-01  2.30698089e+02  3.58319607e+01]\n",
      "Done with gradient descent at iteration  350\n",
      "Learned weights =  [-1.10374236e-01  2.30698089e+02  3.58319607e+01]\n",
      "Done with gradient descent at iteration  350\n",
      "Learned weights =  [-1.10374236e-01  2.30772936e+02  3.58319607e+01]\n",
      "Done with gradient descent at iteration  350\n",
      "Learned weights =  [-1.10374236e-01  2.30772936e+02  3.57506852e+01]\n",
      "Done with gradient descent at iteration  351\n",
      "Learned weights =  [-1.10779229e-01  2.30772936e+02  3.57506852e+01]\n",
      "Done with gradient descent at iteration  351\n",
      "Learned weights =  [-1.10779229e-01  2.30847340e+02  3.57506852e+01]\n",
      "Done with gradient descent at iteration  351\n",
      "Learned weights =  [-1.10779229e-01  2.30847340e+02  3.56698918e+01]\n",
      "Done with gradient descent at iteration  352\n",
      "Learned weights =  [-1.11184025e-01  2.30847340e+02  3.56698918e+01]\n",
      "Done with gradient descent at iteration  352\n",
      "Learned weights =  [-1.11184025e-01  2.30921302e+02  3.56698918e+01]\n",
      "Done with gradient descent at iteration  352\n",
      "Learned weights =  [-1.11184025e-01  2.30921302e+02  3.55895776e+01]\n",
      "Done with gradient descent at iteration  353\n",
      "Learned weights =  [-1.11588624e-01  2.30921302e+02  3.55895776e+01]\n",
      "Done with gradient descent at iteration  353\n",
      "Learned weights =  [-1.11588624e-01  2.30994826e+02  3.55895776e+01]\n",
      "Done with gradient descent at iteration  353\n",
      "Learned weights =  [-1.11588624e-01  2.30994826e+02  3.55097398e+01]\n",
      "Done with gradient descent at iteration  354\n",
      "Learned weights =  [-1.11993027e-01  2.30994826e+02  3.55097398e+01]\n",
      "Done with gradient descent at iteration  354\n",
      "Learned weights =  [-1.11993027e-01  2.31067913e+02  3.55097398e+01]\n",
      "Done with gradient descent at iteration  354\n",
      "Learned weights =  [-1.11993027e-01  2.31067913e+02  3.54303755e+01]\n",
      "Done with gradient descent at iteration  355\n",
      "Learned weights =  [-1.12397236e-01  2.31067913e+02  3.54303755e+01]\n",
      "Done with gradient descent at iteration  355\n",
      "Learned weights =  [-1.12397236e-01  2.31140567e+02  3.54303755e+01]\n",
      "Done with gradient descent at iteration  355\n",
      "Learned weights =  [-1.12397236e-01  2.31140567e+02  3.53514820e+01]\n",
      "Done with gradient descent at iteration  356\n",
      "Learned weights =  [-1.12801252e-01  2.31140567e+02  3.53514820e+01]\n",
      "Done with gradient descent at iteration  356\n",
      "Learned weights =  [-1.12801252e-01  2.31212790e+02  3.53514820e+01]\n",
      "Done with gradient descent at iteration  356\n",
      "Learned weights =  [-1.12801252e-01  2.31212790e+02  3.52730565e+01]\n",
      "Done with gradient descent at iteration  357\n",
      "Learned weights =  [-1.13205075e-01  2.31212790e+02  3.52730565e+01]\n",
      "Done with gradient descent at iteration  357\n",
      "Learned weights =  [-1.13205075e-01  2.31284585e+02  3.52730565e+01]\n",
      "Done with gradient descent at iteration  357\n",
      "Learned weights =  [-1.13205075e-01  2.31284585e+02  3.51950962e+01]\n",
      "Done with gradient descent at iteration  358\n",
      "Learned weights =  [-1.13608707e-01  2.31284585e+02  3.51950962e+01]\n",
      "Done with gradient descent at iteration  358\n",
      "Learned weights =  [-1.13608707e-01  2.31355953e+02  3.51950962e+01]\n",
      "Done with gradient descent at iteration  358\n",
      "Learned weights =  [-1.13608707e-01  2.31355953e+02  3.51175982e+01]\n",
      "Done with gradient descent at iteration  359\n",
      "Learned weights =  [-1.14012150e-01  2.31355953e+02  3.51175982e+01]\n",
      "Done with gradient descent at iteration  359\n",
      "Learned weights =  [-1.14012150e-01  2.31426899e+02  3.51175982e+01]\n",
      "Done with gradient descent at iteration  359\n",
      "Learned weights =  [-1.14012150e-01  2.31426899e+02  3.50405600e+01]\n",
      "Done with gradient descent at iteration  360\n",
      "Learned weights =  [-1.14415403e-01  2.31426899e+02  3.50405600e+01]\n",
      "Done with gradient descent at iteration  360\n",
      "Learned weights =  [-1.14415403e-01  2.31497423e+02  3.50405600e+01]\n",
      "Done with gradient descent at iteration  360\n",
      "Learned weights =  [-1.14415403e-01  2.31497423e+02  3.49639788e+01]\n",
      "Done with gradient descent at iteration  361\n",
      "Learned weights =  [-1.14818469e-01  2.31497423e+02  3.49639788e+01]\n",
      "Done with gradient descent at iteration  361\n",
      "Learned weights =  [-1.14818469e-01  2.31567530e+02  3.49639788e+01]\n",
      "Done with gradient descent at iteration  361\n",
      "Learned weights =  [-1.14818469e-01  2.31567530e+02  3.48878517e+01]\n",
      "Done with gradient descent at iteration  362\n",
      "Learned weights =  [-1.15221349e-01  2.31567530e+02  3.48878517e+01]\n",
      "Done with gradient descent at iteration  362\n",
      "Learned weights =  [-1.15221349e-01  2.31637220e+02  3.48878517e+01]\n",
      "Done with gradient descent at iteration  362\n",
      "Learned weights =  [-1.15221349e-01  2.31637220e+02  3.48121763e+01]\n",
      "Done with gradient descent at iteration  363\n",
      "Learned weights =  [-1.15624043e-01  2.31637220e+02  3.48121763e+01]\n",
      "Done with gradient descent at iteration  363\n",
      "Learned weights =  [-1.15624043e-01  2.31706497e+02  3.48121763e+01]\n",
      "Done with gradient descent at iteration  363\n",
      "Learned weights =  [-1.15624043e-01  2.31706497e+02  3.47369497e+01]\n",
      "Done with gradient descent at iteration  364\n",
      "Learned weights =  [-1.16026552e-01  2.31706497e+02  3.47369497e+01]\n",
      "Done with gradient descent at iteration  364\n",
      "Learned weights =  [-1.16026552e-01  2.31775363e+02  3.47369497e+01]\n",
      "Done with gradient descent at iteration  364\n",
      "Learned weights =  [-1.16026552e-01  2.31775363e+02  3.46621693e+01]\n",
      "Done with gradient descent at iteration  365\n",
      "Learned weights =  [-1.16428879e-01  2.31775363e+02  3.46621693e+01]\n",
      "Done with gradient descent at iteration  365\n",
      "Learned weights =  [-1.16428879e-01  2.31843821e+02  3.46621693e+01]\n",
      "Done with gradient descent at iteration  365\n",
      "Learned weights =  [-1.16428879e-01  2.31843821e+02  3.45878325e+01]\n",
      "Done with gradient descent at iteration  366\n",
      "Learned weights =  [-1.16831023e-01  2.31843821e+02  3.45878325e+01]\n",
      "Done with gradient descent at iteration  366\n",
      "Learned weights =  [-1.16831023e-01  2.31911872e+02  3.45878325e+01]\n",
      "Done with gradient descent at iteration  366\n",
      "Learned weights =  [-1.16831023e-01  2.31911872e+02  3.45139366e+01]\n",
      "Done with gradient descent at iteration  367\n",
      "Learned weights =  [-1.17232986e-01  2.31911872e+02  3.45139366e+01]\n",
      "Done with gradient descent at iteration  367\n",
      "Learned weights =  [-1.17232986e-01  2.31979520e+02  3.45139366e+01]\n",
      "Done with gradient descent at iteration  367\n",
      "Learned weights =  [-1.17232986e-01  2.31979520e+02  3.44404791e+01]\n",
      "Done with gradient descent at iteration  368\n",
      "Learned weights =  [-1.17634770e-01  2.31979520e+02  3.44404791e+01]\n",
      "Done with gradient descent at iteration  368\n",
      "Learned weights =  [-1.17634770e-01  2.32046767e+02  3.44404791e+01]\n",
      "Done with gradient descent at iteration  368\n",
      "Learned weights =  [-1.17634770e-01  2.32046767e+02  3.43674572e+01]\n",
      "Done with gradient descent at iteration  369\n",
      "Learned weights =  [-1.18036374e-01  2.32046767e+02  3.43674572e+01]\n",
      "Done with gradient descent at iteration  369\n",
      "Learned weights =  [-1.18036374e-01  2.32113615e+02  3.43674572e+01]\n",
      "Done with gradient descent at iteration  369\n",
      "Learned weights =  [-1.18036374e-01  2.32113615e+02  3.42948686e+01]\n",
      "Done with gradient descent at iteration  370\n",
      "Learned weights =  [-1.18437800e-01  2.32113615e+02  3.42948686e+01]\n",
      "Done with gradient descent at iteration  370\n",
      "Learned weights =  [-1.18437800e-01  2.32180066e+02  3.42948686e+01]\n",
      "Done with gradient descent at iteration  370\n",
      "Learned weights =  [-1.18437800e-01  2.32180066e+02  3.42227104e+01]\n",
      "Done with gradient descent at iteration  371\n",
      "Learned weights =  [-1.18839050e-01  2.32180066e+02  3.42227104e+01]\n",
      "Done with gradient descent at iteration  371\n",
      "Learned weights =  [-1.18839050e-01  2.32246123e+02  3.42227104e+01]\n",
      "Done with gradient descent at iteration  371\n",
      "Learned weights =  [-1.18839050e-01  2.32246123e+02  3.41509803e+01]\n",
      "Done with gradient descent at iteration  372\n",
      "Learned weights =  [-1.19240124e-01  2.32246123e+02  3.41509803e+01]\n",
      "Done with gradient descent at iteration  372\n",
      "Learned weights =  [-1.19240124e-01  2.32311789e+02  3.41509803e+01]\n",
      "Done with gradient descent at iteration  372\n",
      "Learned weights =  [-1.19240124e-01  2.32311789e+02  3.40796757e+01]\n",
      "Done with gradient descent at iteration  373\n",
      "Learned weights =  [-1.19641023e-01  2.32311789e+02  3.40796757e+01]\n",
      "Done with gradient descent at iteration  373\n",
      "Learned weights =  [-1.19641023e-01  2.32377064e+02  3.40796757e+01]\n",
      "Done with gradient descent at iteration  373\n",
      "Learned weights =  [-1.19641023e-01  2.32377064e+02  3.40087940e+01]\n",
      "Done with gradient descent at iteration  374\n",
      "Learned weights =  [-1.20041749e-01  2.32377064e+02  3.40087940e+01]\n",
      "Done with gradient descent at iteration  374\n",
      "Learned weights =  [-1.20041749e-01  2.32441953e+02  3.40087940e+01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  374\n",
      "Learned weights =  [-1.20041749e-01  2.32441953e+02  3.39383327e+01]\n",
      "Done with gradient descent at iteration  375\n",
      "Learned weights =  [-1.20442301e-01  2.32441953e+02  3.39383327e+01]\n",
      "Done with gradient descent at iteration  375\n",
      "Learned weights =  [-1.20442301e-01  2.32506457e+02  3.39383327e+01]\n",
      "Done with gradient descent at iteration  375\n",
      "Learned weights =  [-1.20442301e-01  2.32506457e+02  3.38682894e+01]\n",
      "Done with gradient descent at iteration  376\n",
      "Learned weights =  [-1.20842683e-01  2.32506457e+02  3.38682894e+01]\n",
      "Done with gradient descent at iteration  376\n",
      "Learned weights =  [-1.20842683e-01  2.32570578e+02  3.38682894e+01]\n",
      "Done with gradient descent at iteration  376\n",
      "Learned weights =  [-1.20842683e-01  2.32570578e+02  3.37986616e+01]\n",
      "Done with gradient descent at iteration  377\n",
      "Learned weights =  [-1.21242893e-01  2.32570578e+02  3.37986616e+01]\n",
      "Done with gradient descent at iteration  377\n",
      "Learned weights =  [-1.21242893e-01  2.32634319e+02  3.37986616e+01]\n",
      "Done with gradient descent at iteration  377\n",
      "Learned weights =  [-1.21242893e-01  2.32634319e+02  3.37294467e+01]\n",
      "Done with gradient descent at iteration  378\n",
      "Learned weights =  [-1.21642934e-01  2.32634319e+02  3.37294467e+01]\n",
      "Done with gradient descent at iteration  378\n",
      "Learned weights =  [-1.21642934e-01  2.32697681e+02  3.37294467e+01]\n",
      "Done with gradient descent at iteration  378\n",
      "Learned weights =  [-1.21642934e-01  2.32697681e+02  3.36606424e+01]\n",
      "Done with gradient descent at iteration  379\n",
      "Learned weights =  [-1.22042806e-01  2.32697681e+02  3.36606424e+01]\n",
      "Done with gradient descent at iteration  379\n",
      "Learned weights =  [-1.22042806e-01  2.32760668e+02  3.36606424e+01]\n",
      "Done with gradient descent at iteration  379\n",
      "Learned weights =  [-1.22042806e-01  2.32760668e+02  3.35922463e+01]\n",
      "Done with gradient descent at iteration  380\n",
      "Learned weights =  [-1.22442511e-01  2.32760668e+02  3.35922463e+01]\n",
      "Done with gradient descent at iteration  380\n",
      "Learned weights =  [-1.22442511e-01  2.32823281e+02  3.35922463e+01]\n",
      "Done with gradient descent at iteration  380\n",
      "Learned weights =  [-1.22442511e-01  2.32823281e+02  3.35242558e+01]\n",
      "Done with gradient descent at iteration  381\n",
      "Learned weights =  [-1.22842049e-01  2.32823281e+02  3.35242558e+01]\n",
      "Done with gradient descent at iteration  381\n",
      "Learned weights =  [-1.22842049e-01  2.32885523e+02  3.35242558e+01]\n",
      "Done with gradient descent at iteration  381\n",
      "Learned weights =  [-1.22842049e-01  2.32885523e+02  3.34566686e+01]\n",
      "Done with gradient descent at iteration  382\n",
      "Learned weights =  [-1.23241422e-01  2.32885523e+02  3.34566686e+01]\n",
      "Done with gradient descent at iteration  382\n",
      "Learned weights =  [-1.23241422e-01  2.32947396e+02  3.34566686e+01]\n",
      "Done with gradient descent at iteration  382\n",
      "Learned weights =  [-1.23241422e-01  2.32947396e+02  3.33894824e+01]\n",
      "Done with gradient descent at iteration  383\n",
      "Learned weights =  [-1.23640630e-01  2.32947396e+02  3.33894824e+01]\n",
      "Done with gradient descent at iteration  383\n",
      "Learned weights =  [-1.23640630e-01  2.33008902e+02  3.33894824e+01]\n",
      "Done with gradient descent at iteration  383\n",
      "Learned weights =  [-1.23640630e-01  2.33008902e+02  3.33226946e+01]\n",
      "Done with gradient descent at iteration  384\n",
      "Learned weights =  [-1.24039674e-01  2.33008902e+02  3.33226946e+01]\n",
      "Done with gradient descent at iteration  384\n",
      "Learned weights =  [-1.24039674e-01  2.33070042e+02  3.33226946e+01]\n",
      "Done with gradient descent at iteration  384\n",
      "Learned weights =  [-1.24039674e-01  2.33070042e+02  3.32563030e+01]\n",
      "Done with gradient descent at iteration  385\n",
      "Learned weights =  [-1.24438556e-01  2.33070042e+02  3.32563030e+01]\n",
      "Done with gradient descent at iteration  385\n",
      "Learned weights =  [-1.24438556e-01  2.33130821e+02  3.32563030e+01]\n",
      "Done with gradient descent at iteration  385\n",
      "Learned weights =  [-1.24438556e-01  2.33130821e+02  3.31903052e+01]\n",
      "Done with gradient descent at iteration  386\n",
      "Learned weights =  [-1.24837276e-01  2.33130821e+02  3.31903052e+01]\n",
      "Done with gradient descent at iteration  386\n",
      "Learned weights =  [-1.24837276e-01  2.33191238e+02  3.31903052e+01]\n",
      "Done with gradient descent at iteration  386\n",
      "Learned weights =  [-1.24837276e-01  2.33191238e+02  3.31246989e+01]\n",
      "Done with gradient descent at iteration  387\n",
      "Learned weights =  [-1.25235835e-01  2.33191238e+02  3.31246989e+01]\n",
      "Done with gradient descent at iteration  387\n",
      "Learned weights =  [-1.25235835e-01  2.33251297e+02  3.31246989e+01]\n",
      "Done with gradient descent at iteration  387\n",
      "Learned weights =  [-1.25235835e-01  2.33251297e+02  3.30594817e+01]\n",
      "Done with gradient descent at iteration  388\n",
      "Learned weights =  [-1.25634234e-01  2.33251297e+02  3.30594817e+01]\n",
      "Done with gradient descent at iteration  388\n",
      "Learned weights =  [-1.25634234e-01  2.33311000e+02  3.30594817e+01]\n",
      "Done with gradient descent at iteration  388\n",
      "Learned weights =  [-1.25634234e-01  2.33311000e+02  3.29946514e+01]\n",
      "Done with gradient descent at iteration  389\n",
      "Learned weights =  [-1.26032475e-01  2.33311000e+02  3.29946514e+01]\n",
      "Done with gradient descent at iteration  389\n",
      "Learned weights =  [-1.26032475e-01  2.33370349e+02  3.29946514e+01]\n",
      "Done with gradient descent at iteration  389\n",
      "Learned weights =  [-1.26032475e-01  2.33370349e+02  3.29302056e+01]\n",
      "Done with gradient descent at iteration  390\n",
      "Learned weights =  [-1.26430557e-01  2.33370349e+02  3.29302056e+01]\n",
      "Done with gradient descent at iteration  390\n",
      "Learned weights =  [-1.26430557e-01  2.33429346e+02  3.29302056e+01]\n",
      "Done with gradient descent at iteration  390\n",
      "Learned weights =  [-1.26430557e-01  2.33429346e+02  3.28661421e+01]\n",
      "Done with gradient descent at iteration  391\n",
      "Learned weights =  [-1.26828483e-01  2.33429346e+02  3.28661421e+01]\n",
      "Done with gradient descent at iteration  391\n",
      "Learned weights =  [-1.26828483e-01  2.33487993e+02  3.28661421e+01]\n",
      "Done with gradient descent at iteration  391\n",
      "Learned weights =  [-1.26828483e-01  2.33487993e+02  3.28024586e+01]\n",
      "Done with gradient descent at iteration  392\n",
      "Learned weights =  [-1.27226252e-01  2.33487993e+02  3.28024586e+01]\n",
      "Done with gradient descent at iteration  392\n",
      "Learned weights =  [-1.27226252e-01  2.33546292e+02  3.28024586e+01]\n",
      "Done with gradient descent at iteration  392\n",
      "Learned weights =  [-1.27226252e-01  2.33546292e+02  3.27391528e+01]\n",
      "Done with gradient descent at iteration  393\n",
      "Learned weights =  [-1.27623867e-01  2.33546292e+02  3.27391528e+01]\n",
      "Done with gradient descent at iteration  393\n",
      "Learned weights =  [-1.27623867e-01  2.33604245e+02  3.27391528e+01]\n",
      "Done with gradient descent at iteration  393\n",
      "Learned weights =  [-1.27623867e-01  2.33604245e+02  3.26762226e+01]\n",
      "Done with gradient descent at iteration  394\n",
      "Learned weights =  [-1.28021327e-01  2.33604245e+02  3.26762226e+01]\n",
      "Done with gradient descent at iteration  394\n",
      "Learned weights =  [-1.28021327e-01  2.33661855e+02  3.26762226e+01]\n",
      "Done with gradient descent at iteration  394\n",
      "Learned weights =  [-1.28021327e-01  2.33661855e+02  3.26136656e+01]\n",
      "Done with gradient descent at iteration  395\n",
      "Learned weights =  [-1.28418634e-01  2.33661855e+02  3.26136656e+01]\n",
      "Done with gradient descent at iteration  395\n",
      "Learned weights =  [-1.28418634e-01  2.33719123e+02  3.26136656e+01]\n",
      "Done with gradient descent at iteration  395\n",
      "Learned weights =  [-1.28418634e-01  2.33719123e+02  3.25514797e+01]\n",
      "Done with gradient descent at iteration  396\n",
      "Learned weights =  [-1.28815789e-01  2.33719123e+02  3.25514797e+01]\n",
      "Done with gradient descent at iteration  396\n",
      "Learned weights =  [-1.28815789e-01  2.33776051e+02  3.25514797e+01]\n",
      "Done with gradient descent at iteration  396\n",
      "Learned weights =  [-1.28815789e-01  2.33776051e+02  3.24896626e+01]\n",
      "Done with gradient descent at iteration  397\n",
      "Learned weights =  [-1.29212792e-01  2.33776051e+02  3.24896626e+01]\n",
      "Done with gradient descent at iteration  397\n",
      "Learned weights =  [-1.29212792e-01  2.33832641e+02  3.24896626e+01]\n",
      "Done with gradient descent at iteration  397\n",
      "Learned weights =  [-1.29212792e-01  2.33832641e+02  3.24282122e+01]\n",
      "Done with gradient descent at iteration  398\n",
      "Learned weights =  [-1.29609644e-01  2.33832641e+02  3.24282122e+01]\n",
      "Done with gradient descent at iteration  398\n",
      "Learned weights =  [-1.29609644e-01  2.33888896e+02  3.24282122e+01]\n",
      "Done with gradient descent at iteration  398\n",
      "Learned weights =  [-1.29609644e-01  2.33888896e+02  3.23671263e+01]\n",
      "Done with gradient descent at iteration  399\n",
      "Learned weights =  [-1.30006347e-01  2.33888896e+02  3.23671263e+01]\n",
      "Done with gradient descent at iteration  399\n",
      "Learned weights =  [-1.30006347e-01  2.33944817e+02  3.23671263e+01]\n",
      "Done with gradient descent at iteration  399\n",
      "Learned weights =  [-1.30006347e-01  2.33944817e+02  3.23064027e+01]\n",
      "Iteration = 400\n",
      "Cost function =  1206175125770960.0\n",
      "Done with gradient descent at iteration  400\n",
      "Learned weights =  [-1.30402901e-01  2.33944817e+02  3.23064027e+01]\n",
      "Done with gradient descent at iteration  400\n",
      "Learned weights =  [-1.30402901e-01  2.34000406e+02  3.23064027e+01]\n",
      "Done with gradient descent at iteration  400\n",
      "Learned weights =  [-1.30402901e-01  2.34000406e+02  3.22460394e+01]\n",
      "Done with gradient descent at iteration  401\n",
      "Learned weights =  [-1.30799307e-01  2.34000406e+02  3.22460394e+01]\n",
      "Done with gradient descent at iteration  401\n",
      "Learned weights =  [-1.30799307e-01  2.34055666e+02  3.22460394e+01]\n",
      "Done with gradient descent at iteration  401\n",
      "Learned weights =  [-1.30799307e-01  2.34055666e+02  3.21860341e+01]\n",
      "Done with gradient descent at iteration  402\n",
      "Learned weights =  [-1.31195566e-01  2.34055666e+02  3.21860341e+01]\n",
      "Done with gradient descent at iteration  402\n",
      "Learned weights =  [-1.31195566e-01  2.34110598e+02  3.21860341e+01]\n",
      "Done with gradient descent at iteration  402\n",
      "Learned weights =  [-1.31195566e-01  2.34110598e+02  3.21263847e+01]\n",
      "Done with gradient descent at iteration  403\n",
      "Learned weights =  [-1.31591679e-01  2.34110598e+02  3.21263847e+01]\n",
      "Done with gradient descent at iteration  403\n",
      "Learned weights =  [-1.31591679e-01  2.34165204e+02  3.21263847e+01]\n",
      "Done with gradient descent at iteration  403\n",
      "Learned weights =  [-1.31591679e-01  2.34165204e+02  3.20670891e+01]\n",
      "Done with gradient descent at iteration  404\n",
      "Learned weights =  [-1.31987647e-01  2.34165204e+02  3.20670891e+01]\n",
      "Done with gradient descent at iteration  404\n",
      "Learned weights =  [-1.31987647e-01  2.34219486e+02  3.20670891e+01]\n",
      "Done with gradient descent at iteration  404\n",
      "Learned weights =  [-1.31987647e-01  2.34219486e+02  3.20081452e+01]\n",
      "Done with gradient descent at iteration  405\n",
      "Learned weights =  [-1.32383470e-01  2.34219486e+02  3.20081452e+01]\n",
      "Done with gradient descent at iteration  405\n",
      "Learned weights =  [-1.32383470e-01  2.34273446e+02  3.20081452e+01]\n",
      "Done with gradient descent at iteration  405\n",
      "Learned weights =  [-1.32383470e-01  2.34273446e+02  3.19495510e+01]\n",
      "Done with gradient descent at iteration  406\n",
      "Learned weights =  [-1.32779150e-01  2.34273446e+02  3.19495510e+01]\n",
      "Done with gradient descent at iteration  406\n",
      "Learned weights =  [-1.32779150e-01  2.34327086e+02  3.19495510e+01]\n",
      "Done with gradient descent at iteration  406\n",
      "Learned weights =  [-1.32779150e-01  2.34327086e+02  3.18913043e+01]\n",
      "Done with gradient descent at iteration  407\n",
      "Learned weights =  [-1.33174687e-01  2.34327086e+02  3.18913043e+01]\n",
      "Done with gradient descent at iteration  407\n",
      "Learned weights =  [-1.33174687e-01  2.34380408e+02  3.18913043e+01]\n",
      "Done with gradient descent at iteration  407\n",
      "Learned weights =  [-1.33174687e-01  2.34380408e+02  3.18334031e+01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  408\n",
      "Learned weights =  [-1.33570082e-01  2.34380408e+02  3.18334031e+01]\n",
      "Done with gradient descent at iteration  408\n",
      "Learned weights =  [-1.33570082e-01  2.34433414e+02  3.18334031e+01]\n",
      "Done with gradient descent at iteration  408\n",
      "Learned weights =  [-1.33570082e-01  2.34433414e+02  3.17758454e+01]\n",
      "Done with gradient descent at iteration  409\n",
      "Learned weights =  [-1.33965336e-01  2.34433414e+02  3.17758454e+01]\n",
      "Done with gradient descent at iteration  409\n",
      "Learned weights =  [-1.33965336e-01  2.34486105e+02  3.17758454e+01]\n",
      "Done with gradient descent at iteration  409\n",
      "Learned weights =  [-1.33965336e-01  2.34486105e+02  3.17186291e+01]\n",
      "Done with gradient descent at iteration  410\n",
      "Learned weights =  [-1.34360450e-01  2.34486105e+02  3.17186291e+01]\n",
      "Done with gradient descent at iteration  410\n",
      "Learned weights =  [-1.34360450e-01  2.34538484e+02  3.17186291e+01]\n",
      "Done with gradient descent at iteration  410\n",
      "Learned weights =  [-1.34360450e-01  2.34538484e+02  3.16617522e+01]\n",
      "Done with gradient descent at iteration  411\n",
      "Learned weights =  [-1.34755424e-01  2.34538484e+02  3.16617522e+01]\n",
      "Done with gradient descent at iteration  411\n",
      "Learned weights =  [-1.34755424e-01  2.34590552e+02  3.16617522e+01]\n",
      "Done with gradient descent at iteration  411\n",
      "Learned weights =  [-1.34755424e-01  2.34590552e+02  3.16052126e+01]\n",
      "Done with gradient descent at iteration  412\n",
      "Learned weights =  [-1.35150260e-01  2.34590552e+02  3.16052126e+01]\n",
      "Done with gradient descent at iteration  412\n",
      "Learned weights =  [-1.35150260e-01  2.34642311e+02  3.16052126e+01]\n",
      "Done with gradient descent at iteration  412\n",
      "Learned weights =  [-1.35150260e-01  2.34642311e+02  3.15490084e+01]\n",
      "Done with gradient descent at iteration  413\n",
      "Learned weights =  [-1.35544958e-01  2.34642311e+02  3.15490084e+01]\n",
      "Done with gradient descent at iteration  413\n",
      "Learned weights =  [-1.35544958e-01  2.34693763e+02  3.15490084e+01]\n",
      "Done with gradient descent at iteration  413\n",
      "Learned weights =  [-1.35544958e-01  2.34693763e+02  3.14931376e+01]\n",
      "Done with gradient descent at iteration  414\n",
      "Learned weights =  [-1.35939519e-01  2.34693763e+02  3.14931376e+01]\n",
      "Done with gradient descent at iteration  414\n",
      "Learned weights =  [-1.35939519e-01  2.34744910e+02  3.14931376e+01]\n",
      "Done with gradient descent at iteration  414\n",
      "Learned weights =  [-1.35939519e-01  2.34744910e+02  3.14375982e+01]\n",
      "Done with gradient descent at iteration  415\n",
      "Learned weights =  [-1.36333945e-01  2.34744910e+02  3.14375982e+01]\n",
      "Done with gradient descent at iteration  415\n",
      "Learned weights =  [-1.36333945e-01  2.34795754e+02  3.14375982e+01]\n",
      "Done with gradient descent at iteration  415\n",
      "Learned weights =  [-1.36333945e-01  2.34795754e+02  3.13823882e+01]\n",
      "Done with gradient descent at iteration  416\n",
      "Learned weights =  [-1.36728234e-01  2.34795754e+02  3.13823882e+01]\n",
      "Done with gradient descent at iteration  416\n",
      "Learned weights =  [-1.36728234e-01  2.34846296e+02  3.13823882e+01]\n",
      "Done with gradient descent at iteration  416\n",
      "Learned weights =  [-1.36728234e-01  2.34846296e+02  3.13275057e+01]\n",
      "Done with gradient descent at iteration  417\n",
      "Learned weights =  [-1.37122390e-01  2.34846296e+02  3.13275057e+01]\n",
      "Done with gradient descent at iteration  417\n",
      "Learned weights =  [-1.37122390e-01  2.34896538e+02  3.13275057e+01]\n",
      "Done with gradient descent at iteration  417\n",
      "Learned weights =  [-1.37122390e-01  2.34896538e+02  3.12729487e+01]\n",
      "Done with gradient descent at iteration  418\n",
      "Learned weights =  [-1.37516412e-01  2.34896538e+02  3.12729487e+01]\n",
      "Done with gradient descent at iteration  418\n",
      "Learned weights =  [-1.37516412e-01  2.34946482e+02  3.12729487e+01]\n",
      "Done with gradient descent at iteration  418\n",
      "Learned weights =  [-1.37516412e-01  2.34946482e+02  3.12187154e+01]\n",
      "Done with gradient descent at iteration  419\n",
      "Learned weights =  [-1.37910300e-01  2.34946482e+02  3.12187154e+01]\n",
      "Done with gradient descent at iteration  419\n",
      "Learned weights =  [-1.37910300e-01  2.34996130e+02  3.12187154e+01]\n",
      "Done with gradient descent at iteration  419\n",
      "Learned weights =  [-1.37910300e-01  2.34996130e+02  3.11648037e+01]\n",
      "Done with gradient descent at iteration  420\n",
      "Learned weights =  [-1.38304057e-01  2.34996130e+02  3.11648037e+01]\n",
      "Done with gradient descent at iteration  420\n",
      "Learned weights =  [-1.38304057e-01  2.35045483e+02  3.11648037e+01]\n",
      "Done with gradient descent at iteration  420\n",
      "Learned weights =  [-1.38304057e-01  2.35045483e+02  3.11112119e+01]\n",
      "Done with gradient descent at iteration  421\n",
      "Learned weights =  [-1.38697683e-01  2.35045483e+02  3.11112119e+01]\n",
      "Done with gradient descent at iteration  421\n",
      "Learned weights =  [-1.38697683e-01  2.35094544e+02  3.11112119e+01]\n",
      "Done with gradient descent at iteration  421\n",
      "Learned weights =  [-1.38697683e-01  2.35094544e+02  3.10579379e+01]\n",
      "Done with gradient descent at iteration  422\n",
      "Learned weights =  [-1.39091177e-01  2.35094544e+02  3.10579379e+01]\n",
      "Done with gradient descent at iteration  422\n",
      "Learned weights =  [-1.39091177e-01  2.35143314e+02  3.10579379e+01]\n",
      "Done with gradient descent at iteration  422\n",
      "Learned weights =  [-1.39091177e-01  2.35143314e+02  3.10049799e+01]\n",
      "Done with gradient descent at iteration  423\n",
      "Learned weights =  [-1.39484543e-01  2.35143314e+02  3.10049799e+01]\n",
      "Done with gradient descent at iteration  423\n",
      "Learned weights =  [-1.39484543e-01  2.35191794e+02  3.10049799e+01]\n",
      "Done with gradient descent at iteration  423\n",
      "Learned weights =  [-1.39484543e-01  2.35191794e+02  3.09523360e+01]\n",
      "Done with gradient descent at iteration  424\n",
      "Learned weights =  [-1.39877779e-01  2.35191794e+02  3.09523360e+01]\n",
      "Done with gradient descent at iteration  424\n",
      "Learned weights =  [-1.39877779e-01  2.35239987e+02  3.09523360e+01]\n",
      "Done with gradient descent at iteration  424\n",
      "Learned weights =  [-1.39877779e-01  2.35239987e+02  3.09000044e+01]\n",
      "Done with gradient descent at iteration  425\n",
      "Learned weights =  [-1.40270887e-01  2.35239987e+02  3.09000044e+01]\n",
      "Done with gradient descent at iteration  425\n",
      "Learned weights =  [-1.40270887e-01  2.35287894e+02  3.09000044e+01]\n",
      "Done with gradient descent at iteration  425\n",
      "Learned weights =  [-1.40270887e-01  2.35287894e+02  3.08479832e+01]\n",
      "Done with gradient descent at iteration  426\n",
      "Learned weights =  [-1.40663867e-01  2.35287894e+02  3.08479832e+01]\n",
      "Done with gradient descent at iteration  426\n",
      "Learned weights =  [-1.40663867e-01  2.35335517e+02  3.08479832e+01]\n",
      "Done with gradient descent at iteration  426\n",
      "Learned weights =  [-1.40663867e-01  2.35335517e+02  3.07962706e+01]\n",
      "Done with gradient descent at iteration  427\n",
      "Learned weights =  [-1.41056721e-01  2.35335517e+02  3.07962706e+01]\n",
      "Done with gradient descent at iteration  427\n",
      "Learned weights =  [-1.41056721e-01  2.35382857e+02  3.07962706e+01]\n",
      "Done with gradient descent at iteration  427\n",
      "Learned weights =  [-1.41056721e-01  2.35382857e+02  3.07448647e+01]\n",
      "Done with gradient descent at iteration  428\n",
      "Learned weights =  [-1.41449449e-01  2.35382857e+02  3.07448647e+01]\n",
      "Done with gradient descent at iteration  428\n",
      "Learned weights =  [-1.41449449e-01  2.35429917e+02  3.07448647e+01]\n",
      "Done with gradient descent at iteration  428\n",
      "Learned weights =  [-1.41449449e-01  2.35429917e+02  3.06937638e+01]\n",
      "Done with gradient descent at iteration  429\n",
      "Learned weights =  [-1.41842051e-01  2.35429917e+02  3.06937638e+01]\n",
      "Done with gradient descent at iteration  429\n",
      "Learned weights =  [-1.41842051e-01  2.35476697e+02  3.06937638e+01]\n",
      "Done with gradient descent at iteration  429\n",
      "Learned weights =  [-1.41842051e-01  2.35476697e+02  3.06429659e+01]\n",
      "Done with gradient descent at iteration  430\n",
      "Learned weights =  [-1.42234529e-01  2.35476697e+02  3.06429659e+01]\n",
      "Done with gradient descent at iteration  430\n",
      "Learned weights =  [-1.42234529e-01  2.35523200e+02  3.06429659e+01]\n",
      "Done with gradient descent at iteration  430\n",
      "Learned weights =  [-1.42234529e-01  2.35523200e+02  3.05924694e+01]\n",
      "Done with gradient descent at iteration  431\n",
      "Learned weights =  [-1.42626883e-01  2.35523200e+02  3.05924694e+01]\n",
      "Done with gradient descent at iteration  431\n",
      "Learned weights =  [-1.42626883e-01  2.35569427e+02  3.05924694e+01]\n",
      "Done with gradient descent at iteration  431\n",
      "Learned weights =  [-1.42626883e-01  2.35569427e+02  3.05422723e+01]\n",
      "Done with gradient descent at iteration  432\n",
      "Learned weights =  [-1.43019115e-01  2.35569427e+02  3.05422723e+01]\n",
      "Done with gradient descent at iteration  432\n",
      "Learned weights =  [-1.43019115e-01  2.35615380e+02  3.05422723e+01]\n",
      "Done with gradient descent at iteration  432\n",
      "Learned weights =  [-1.43019115e-01  2.35615380e+02  3.04923731e+01]\n",
      "Done with gradient descent at iteration  433\n",
      "Learned weights =  [-1.43411224e-01  2.35615380e+02  3.04923731e+01]\n",
      "Done with gradient descent at iteration  433\n",
      "Learned weights =  [-1.43411224e-01  2.35661060e+02  3.04923731e+01]\n",
      "Done with gradient descent at iteration  433\n",
      "Learned weights =  [-1.43411224e-01  2.35661060e+02  3.04427698e+01]\n",
      "Done with gradient descent at iteration  434\n",
      "Learned weights =  [-1.43803211e-01  2.35661060e+02  3.04427698e+01]\n",
      "Done with gradient descent at iteration  434\n",
      "Learned weights =  [-1.43803211e-01  2.35706470e+02  3.04427698e+01]\n",
      "Done with gradient descent at iteration  434\n",
      "Learned weights =  [-1.43803211e-01  2.35706470e+02  3.03934607e+01]\n",
      "Done with gradient descent at iteration  435\n",
      "Learned weights =  [-1.44195078e-01  2.35706470e+02  3.03934607e+01]\n",
      "Done with gradient descent at iteration  435\n",
      "Learned weights =  [-1.44195078e-01  2.35751610e+02  3.03934607e+01]\n",
      "Done with gradient descent at iteration  435\n",
      "Learned weights =  [-1.44195078e-01  2.35751610e+02  3.03444441e+01]\n",
      "Done with gradient descent at iteration  436\n",
      "Learned weights =  [-1.44586825e-01  2.35751610e+02  3.03444441e+01]\n",
      "Done with gradient descent at iteration  436\n",
      "Learned weights =  [-1.44586825e-01  2.35796482e+02  3.03444441e+01]\n",
      "Done with gradient descent at iteration  436\n",
      "Learned weights =  [-1.44586825e-01  2.35796482e+02  3.02957183e+01]\n",
      "Done with gradient descent at iteration  437\n",
      "Learned weights =  [-1.44978452e-01  2.35796482e+02  3.02957183e+01]\n",
      "Done with gradient descent at iteration  437\n",
      "Learned weights =  [-1.44978452e-01  2.35841088e+02  3.02957183e+01]\n",
      "Done with gradient descent at iteration  437\n",
      "Learned weights =  [-1.44978452e-01  2.35841088e+02  3.02472815e+01]\n",
      "Done with gradient descent at iteration  438\n",
      "Learned weights =  [-1.45369960e-01  2.35841088e+02  3.02472815e+01]\n",
      "Done with gradient descent at iteration  438\n",
      "Learned weights =  [-1.45369960e-01  2.35885429e+02  3.02472815e+01]\n",
      "Done with gradient descent at iteration  438\n",
      "Learned weights =  [-1.45369960e-01  2.35885429e+02  3.01991320e+01]\n",
      "Done with gradient descent at iteration  439\n",
      "Learned weights =  [-1.45761351e-01  2.35885429e+02  3.01991320e+01]\n",
      "Done with gradient descent at iteration  439\n",
      "Learned weights =  [-1.45761351e-01  2.35929508e+02  3.01991320e+01]\n",
      "Done with gradient descent at iteration  439\n",
      "Learned weights =  [-1.45761351e-01  2.35929508e+02  3.01512681e+01]\n",
      "Done with gradient descent at iteration  440\n",
      "Learned weights =  [-1.46152624e-01  2.35929508e+02  3.01512681e+01]\n",
      "Done with gradient descent at iteration  440\n",
      "Learned weights =  [-1.46152624e-01  2.35973325e+02  3.01512681e+01]\n",
      "Done with gradient descent at iteration  440\n",
      "Learned weights =  [-1.46152624e-01  2.35973325e+02  3.01036881e+01]\n",
      "Done with gradient descent at iteration  441\n",
      "Learned weights =  [-1.46543781e-01  2.35973325e+02  3.01036881e+01]\n",
      "Done with gradient descent at iteration  441\n",
      "Learned weights =  [-1.46543781e-01  2.36016882e+02  3.01036881e+01]\n",
      "Done with gradient descent at iteration  441\n",
      "Learned weights =  [-1.46543781e-01  2.36016882e+02  3.00563903e+01]\n",
      "Done with gradient descent at iteration  442\n",
      "Learned weights =  [-1.46934821e-01  2.36016882e+02  3.00563903e+01]\n",
      "Done with gradient descent at iteration  442\n",
      "Learned weights =  [-1.46934821e-01  2.36060181e+02  3.00563903e+01]\n",
      "Done with gradient descent at iteration  442\n",
      "Learned weights =  [-1.46934821e-01  2.36060181e+02  3.00093731e+01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  443\n",
      "Learned weights =  [-1.47325747e-01  2.36060181e+02  3.00093731e+01]\n",
      "Done with gradient descent at iteration  443\n",
      "Learned weights =  [-1.47325747e-01  2.36103223e+02  3.00093731e+01]\n",
      "Done with gradient descent at iteration  443\n",
      "Learned weights =  [-1.47325747e-01  2.36103223e+02  2.99626348e+01]\n",
      "Done with gradient descent at iteration  444\n",
      "Learned weights =  [-1.47716558e-01  2.36103223e+02  2.99626348e+01]\n",
      "Done with gradient descent at iteration  444\n",
      "Learned weights =  [-1.47716558e-01  2.36146009e+02  2.99626348e+01]\n",
      "Done with gradient descent at iteration  444\n",
      "Learned weights =  [-1.47716558e-01  2.36146009e+02  2.99161737e+01]\n",
      "Done with gradient descent at iteration  445\n",
      "Learned weights =  [-1.48107255e-01  2.36146009e+02  2.99161737e+01]\n",
      "Done with gradient descent at iteration  445\n",
      "Learned weights =  [-1.48107255e-01  2.36188542e+02  2.99161737e+01]\n",
      "Done with gradient descent at iteration  445\n",
      "Learned weights =  [-1.48107255e-01  2.36188542e+02  2.98699882e+01]\n",
      "Done with gradient descent at iteration  446\n",
      "Learned weights =  [-1.48497839e-01  2.36188542e+02  2.98699882e+01]\n",
      "Done with gradient descent at iteration  446\n",
      "Learned weights =  [-1.48497839e-01  2.36230823e+02  2.98699882e+01]\n",
      "Done with gradient descent at iteration  446\n",
      "Learned weights =  [-1.48497839e-01  2.36230823e+02  2.98240766e+01]\n",
      "Done with gradient descent at iteration  447\n",
      "Learned weights =  [-1.48888311e-01  2.36230823e+02  2.98240766e+01]\n",
      "Done with gradient descent at iteration  447\n",
      "Learned weights =  [-1.48888311e-01  2.36272853e+02  2.98240766e+01]\n",
      "Done with gradient descent at iteration  447\n",
      "Learned weights =  [-1.48888311e-01  2.36272853e+02  2.97784374e+01]\n",
      "Done with gradient descent at iteration  448\n",
      "Learned weights =  [-1.49278670e-01  2.36272853e+02  2.97784374e+01]\n",
      "Done with gradient descent at iteration  448\n",
      "Learned weights =  [-1.49278670e-01  2.36314633e+02  2.97784374e+01]\n",
      "Done with gradient descent at iteration  448\n",
      "Learned weights =  [-1.49278670e-01  2.36314633e+02  2.97330689e+01]\n",
      "Done with gradient descent at iteration  449\n",
      "Learned weights =  [-1.49668919e-01  2.36314633e+02  2.97330689e+01]\n",
      "Done with gradient descent at iteration  449\n",
      "Learned weights =  [-1.49668919e-01  2.36356166e+02  2.97330689e+01]\n",
      "Done with gradient descent at iteration  449\n",
      "Learned weights =  [-1.49668919e-01  2.36356166e+02  2.96879695e+01]\n",
      "Done with gradient descent at iteration  450\n",
      "Learned weights =  [-1.50059057e-01  2.36356166e+02  2.96879695e+01]\n",
      "Done with gradient descent at iteration  450\n",
      "Learned weights =  [-1.50059057e-01  2.36397452e+02  2.96879695e+01]\n",
      "Done with gradient descent at iteration  450\n",
      "Learned weights =  [-1.50059057e-01  2.36397452e+02  2.96431376e+01]\n",
      "Done with gradient descent at iteration  451\n",
      "Learned weights =  [-1.50449085e-01  2.36397452e+02  2.96431376e+01]\n",
      "Done with gradient descent at iteration  451\n",
      "Learned weights =  [-1.50449085e-01  2.36438493e+02  2.96431376e+01]\n",
      "Done with gradient descent at iteration  451\n",
      "Learned weights =  [-1.50449085e-01  2.36438493e+02  2.95985716e+01]\n",
      "Done with gradient descent at iteration  452\n",
      "Learned weights =  [-1.50839004e-01  2.36438493e+02  2.95985716e+01]\n",
      "Done with gradient descent at iteration  452\n",
      "Learned weights =  [-1.50839004e-01  2.36479291e+02  2.95985716e+01]\n",
      "Done with gradient descent at iteration  452\n",
      "Learned weights =  [-1.50839004e-01  2.36479291e+02  2.95542700e+01]\n",
      "Done with gradient descent at iteration  453\n",
      "Learned weights =  [-1.51228814e-01  2.36479291e+02  2.95542700e+01]\n",
      "Done with gradient descent at iteration  453\n",
      "Learned weights =  [-1.51228814e-01  2.36519847e+02  2.95542700e+01]\n",
      "Done with gradient descent at iteration  453\n",
      "Learned weights =  [-1.51228814e-01  2.36519847e+02  2.95102312e+01]\n",
      "Done with gradient descent at iteration  454\n",
      "Learned weights =  [-1.51618517e-01  2.36519847e+02  2.95102312e+01]\n",
      "Done with gradient descent at iteration  454\n",
      "Learned weights =  [-1.51618517e-01  2.36560163e+02  2.95102312e+01]\n",
      "Done with gradient descent at iteration  454\n",
      "Learned weights =  [-1.51618517e-01  2.36560163e+02  2.94664536e+01]\n",
      "Done with gradient descent at iteration  455\n",
      "Learned weights =  [-1.52008112e-01  2.36560163e+02  2.94664536e+01]\n",
      "Done with gradient descent at iteration  455\n",
      "Learned weights =  [-1.52008112e-01  2.36600239e+02  2.94664536e+01]\n",
      "Done with gradient descent at iteration  455\n",
      "Learned weights =  [-1.52008112e-01  2.36600239e+02  2.94229356e+01]\n",
      "Done with gradient descent at iteration  456\n",
      "Learned weights =  [-1.52397601e-01  2.36600239e+02  2.94229356e+01]\n",
      "Done with gradient descent at iteration  456\n",
      "Learned weights =  [-1.52397601e-01  2.36640077e+02  2.94229356e+01]\n",
      "Done with gradient descent at iteration  456\n",
      "Learned weights =  [-1.52397601e-01  2.36640077e+02  2.93796758e+01]\n",
      "Done with gradient descent at iteration  457\n",
      "Learned weights =  [-1.52786983e-01  2.36640077e+02  2.93796758e+01]\n",
      "Done with gradient descent at iteration  457\n",
      "Learned weights =  [-1.52786983e-01  2.36679680e+02  2.93796758e+01]\n",
      "Done with gradient descent at iteration  457\n",
      "Learned weights =  [-1.52786983e-01  2.36679680e+02  2.93366726e+01]\n",
      "Done with gradient descent at iteration  458\n",
      "Learned weights =  [-1.53176260e-01  2.36679680e+02  2.93366726e+01]\n",
      "Done with gradient descent at iteration  458\n",
      "Learned weights =  [-1.53176260e-01  2.36719047e+02  2.93366726e+01]\n",
      "Done with gradient descent at iteration  458\n",
      "Learned weights =  [-1.53176260e-01  2.36719047e+02  2.92939244e+01]\n",
      "Done with gradient descent at iteration  459\n",
      "Learned weights =  [-1.53565433e-01  2.36719047e+02  2.92939244e+01]\n",
      "Done with gradient descent at iteration  459\n",
      "Learned weights =  [-1.53565433e-01  2.36758181e+02  2.92939244e+01]\n",
      "Done with gradient descent at iteration  459\n",
      "Learned weights =  [-1.53565433e-01  2.36758181e+02  2.92514298e+01]\n",
      "Done with gradient descent at iteration  460\n",
      "Learned weights =  [-1.53954501e-01  2.36758181e+02  2.92514298e+01]\n",
      "Done with gradient descent at iteration  460\n",
      "Learned weights =  [-1.53954501e-01  2.36797083e+02  2.92514298e+01]\n",
      "Done with gradient descent at iteration  460\n",
      "Learned weights =  [-1.53954501e-01  2.36797083e+02  2.92091873e+01]\n",
      "Done with gradient descent at iteration  461\n",
      "Learned weights =  [-1.54343466e-01  2.36797083e+02  2.92091873e+01]\n",
      "Done with gradient descent at iteration  461\n",
      "Learned weights =  [-1.54343466e-01  2.36835753e+02  2.92091873e+01]\n",
      "Done with gradient descent at iteration  461\n",
      "Learned weights =  [-1.54343466e-01  2.36835753e+02  2.91671954e+01]\n",
      "Done with gradient descent at iteration  462\n",
      "Learned weights =  [-1.54732328e-01  2.36835753e+02  2.91671954e+01]\n",
      "Done with gradient descent at iteration  462\n",
      "Learned weights =  [-1.54732328e-01  2.36874195e+02  2.91671954e+01]\n",
      "Done with gradient descent at iteration  462\n",
      "Learned weights =  [-1.54732328e-01  2.36874195e+02  2.91254525e+01]\n",
      "Done with gradient descent at iteration  463\n",
      "Learned weights =  [-1.55121088e-01  2.36874195e+02  2.91254525e+01]\n",
      "Done with gradient descent at iteration  463\n",
      "Learned weights =  [-1.55121088e-01  2.36912409e+02  2.91254525e+01]\n",
      "Done with gradient descent at iteration  463\n",
      "Learned weights =  [-1.55121088e-01  2.36912409e+02  2.90839572e+01]\n",
      "Done with gradient descent at iteration  464\n",
      "Learned weights =  [-1.55509746e-01  2.36912409e+02  2.90839572e+01]\n",
      "Done with gradient descent at iteration  464\n",
      "Learned weights =  [-1.55509746e-01  2.36950395e+02  2.90839572e+01]\n",
      "Done with gradient descent at iteration  464\n",
      "Learned weights =  [-1.55509746e-01  2.36950395e+02  2.90427081e+01]\n",
      "Done with gradient descent at iteration  465\n",
      "Learned weights =  [-1.55898302e-01  2.36950395e+02  2.90427081e+01]\n",
      "Done with gradient descent at iteration  465\n",
      "Learned weights =  [-1.55898302e-01  2.36988157e+02  2.90427081e+01]\n",
      "Done with gradient descent at iteration  465\n",
      "Learned weights =  [-1.55898302e-01  2.36988157e+02  2.90017036e+01]\n",
      "Done with gradient descent at iteration  466\n",
      "Learned weights =  [-1.56286759e-01  2.36988157e+02  2.90017036e+01]\n",
      "Done with gradient descent at iteration  466\n",
      "Learned weights =  [-1.56286759e-01  2.37025695e+02  2.90017036e+01]\n",
      "Done with gradient descent at iteration  466\n",
      "Learned weights =  [-1.56286759e-01  2.37025695e+02  2.89609424e+01]\n",
      "Done with gradient descent at iteration  467\n",
      "Learned weights =  [-1.56675115e-01  2.37025695e+02  2.89609424e+01]\n",
      "Done with gradient descent at iteration  467\n",
      "Learned weights =  [-1.56675115e-01  2.37063010e+02  2.89609424e+01]\n",
      "Done with gradient descent at iteration  467\n",
      "Learned weights =  [-1.56675115e-01  2.37063010e+02  2.89204229e+01]\n",
      "Done with gradient descent at iteration  468\n",
      "Learned weights =  [-1.57063373e-01  2.37063010e+02  2.89204229e+01]\n",
      "Done with gradient descent at iteration  468\n",
      "Learned weights =  [-1.57063373e-01  2.37100103e+02  2.89204229e+01]\n",
      "Done with gradient descent at iteration  468\n",
      "Learned weights =  [-1.57063373e-01  2.37100103e+02  2.88801438e+01]\n",
      "Done with gradient descent at iteration  469\n",
      "Learned weights =  [-1.57451531e-01  2.37100103e+02  2.88801438e+01]\n",
      "Done with gradient descent at iteration  469\n",
      "Learned weights =  [-1.57451531e-01  2.37136977e+02  2.88801438e+01]\n",
      "Done with gradient descent at iteration  469\n",
      "Learned weights =  [-1.57451531e-01  2.37136977e+02  2.88401036e+01]\n",
      "Done with gradient descent at iteration  470\n",
      "Learned weights =  [-1.57839592e-01  2.37136977e+02  2.88401036e+01]\n",
      "Done with gradient descent at iteration  470\n",
      "Learned weights =  [-1.57839592e-01  2.37173632e+02  2.88401036e+01]\n",
      "Done with gradient descent at iteration  470\n",
      "Learned weights =  [-1.57839592e-01  2.37173632e+02  2.88003009e+01]\n",
      "Done with gradient descent at iteration  471\n",
      "Learned weights =  [-1.58227554e-01  2.37173632e+02  2.88003009e+01]\n",
      "Done with gradient descent at iteration  471\n",
      "Learned weights =  [-1.58227554e-01  2.37210069e+02  2.88003009e+01]\n",
      "Done with gradient descent at iteration  471\n",
      "Learned weights =  [-1.58227554e-01  2.37210069e+02  2.87607343e+01]\n",
      "Done with gradient descent at iteration  472\n",
      "Learned weights =  [-1.58615420e-01  2.37210069e+02  2.87607343e+01]\n",
      "Done with gradient descent at iteration  472\n",
      "Learned weights =  [-1.58615420e-01  2.37246290e+02  2.87607343e+01]\n",
      "Done with gradient descent at iteration  472\n",
      "Learned weights =  [-1.58615420e-01  2.37246290e+02  2.87214024e+01]\n",
      "Done with gradient descent at iteration  473\n",
      "Learned weights =  [-1.59003190e-01  2.37246290e+02  2.87214024e+01]\n",
      "Done with gradient descent at iteration  473\n",
      "Learned weights =  [-1.59003190e-01  2.37282297e+02  2.87214024e+01]\n",
      "Done with gradient descent at iteration  473\n",
      "Learned weights =  [-1.59003190e-01  2.37282297e+02  2.86823038e+01]\n",
      "Done with gradient descent at iteration  474\n",
      "Learned weights =  [-1.59390864e-01  2.37282297e+02  2.86823038e+01]\n",
      "Done with gradient descent at iteration  474\n",
      "Learned weights =  [-1.59390864e-01  2.37318090e+02  2.86823038e+01]\n",
      "Done with gradient descent at iteration  474\n",
      "Learned weights =  [-1.59390864e-01  2.37318090e+02  2.86434371e+01]\n",
      "Done with gradient descent at iteration  475\n",
      "Learned weights =  [-1.59778442e-01  2.37318090e+02  2.86434371e+01]\n",
      "Done with gradient descent at iteration  475\n",
      "Learned weights =  [-1.59778442e-01  2.37353670e+02  2.86434371e+01]\n",
      "Done with gradient descent at iteration  475\n",
      "Learned weights =  [-1.59778442e-01  2.37353670e+02  2.86048010e+01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  476\n",
      "Learned weights =  [-1.60165926e-01  2.37353670e+02  2.86048010e+01]\n",
      "Done with gradient descent at iteration  476\n",
      "Learned weights =  [-1.60165926e-01  2.37389040e+02  2.86048010e+01]\n",
      "Done with gradient descent at iteration  476\n",
      "Learned weights =  [-1.60165926e-01  2.37389040e+02  2.85663940e+01]\n",
      "Done with gradient descent at iteration  477\n",
      "Learned weights =  [-1.60553316e-01  2.37389040e+02  2.85663940e+01]\n",
      "Done with gradient descent at iteration  477\n",
      "Learned weights =  [-1.60553316e-01  2.37424199e+02  2.85663940e+01]\n",
      "Done with gradient descent at iteration  477\n",
      "Learned weights =  [-1.60553316e-01  2.37424199e+02  2.85282149e+01]\n",
      "Done with gradient descent at iteration  478\n",
      "Learned weights =  [-1.60940612e-01  2.37424199e+02  2.85282149e+01]\n",
      "Done with gradient descent at iteration  478\n",
      "Learned weights =  [-1.60940612e-01  2.37459151e+02  2.85282149e+01]\n",
      "Done with gradient descent at iteration  478\n",
      "Learned weights =  [-1.60940612e-01  2.37459151e+02  2.84902622e+01]\n",
      "Done with gradient descent at iteration  479\n",
      "Learned weights =  [-1.61327815e-01  2.37459151e+02  2.84902622e+01]\n",
      "Done with gradient descent at iteration  479\n",
      "Learned weights =  [-1.61327815e-01  2.37493894e+02  2.84902622e+01]\n",
      "Done with gradient descent at iteration  479\n",
      "Learned weights =  [-1.61327815e-01  2.37493894e+02  2.84525346e+01]\n",
      "Done with gradient descent at iteration  480\n",
      "Learned weights =  [-1.61714926e-01  2.37493894e+02  2.84525346e+01]\n",
      "Done with gradient descent at iteration  480\n",
      "Learned weights =  [-1.61714926e-01  2.37528432e+02  2.84525346e+01]\n",
      "Done with gradient descent at iteration  480\n",
      "Learned weights =  [-1.61714926e-01  2.37528432e+02  2.84150308e+01]\n",
      "Done with gradient descent at iteration  481\n",
      "Learned weights =  [-1.62101945e-01  2.37528432e+02  2.84150308e+01]\n",
      "Done with gradient descent at iteration  481\n",
      "Learned weights =  [-1.62101945e-01  2.37562765e+02  2.84150308e+01]\n",
      "Done with gradient descent at iteration  481\n",
      "Learned weights =  [-1.62101945e-01  2.37562765e+02  2.83777495e+01]\n",
      "Done with gradient descent at iteration  482\n",
      "Learned weights =  [-1.62488872e-01  2.37562765e+02  2.83777495e+01]\n",
      "Done with gradient descent at iteration  482\n",
      "Learned weights =  [-1.62488872e-01  2.37596894e+02  2.83777495e+01]\n",
      "Done with gradient descent at iteration  482\n",
      "Learned weights =  [-1.62488872e-01  2.37596894e+02  2.83406893e+01]\n",
      "Done with gradient descent at iteration  483\n",
      "Learned weights =  [-1.62875709e-01  2.37596894e+02  2.83406893e+01]\n",
      "Done with gradient descent at iteration  483\n",
      "Learned weights =  [-1.62875709e-01  2.37630821e+02  2.83406893e+01]\n",
      "Done with gradient descent at iteration  483\n",
      "Learned weights =  [-1.62875709e-01  2.37630821e+02  2.83038489e+01]\n",
      "Done with gradient descent at iteration  484\n",
      "Learned weights =  [-1.63262455e-01  2.37630821e+02  2.83038489e+01]\n",
      "Done with gradient descent at iteration  484\n",
      "Learned weights =  [-1.63262455e-01  2.37664547e+02  2.83038489e+01]\n",
      "Done with gradient descent at iteration  484\n",
      "Learned weights =  [-1.63262455e-01  2.37664547e+02  2.82672271e+01]\n",
      "Done with gradient descent at iteration  485\n",
      "Learned weights =  [-1.63649112e-01  2.37664547e+02  2.82672271e+01]\n",
      "Done with gradient descent at iteration  485\n",
      "Learned weights =  [-1.63649112e-01  2.37698072e+02  2.82672271e+01]\n",
      "Done with gradient descent at iteration  485\n",
      "Learned weights =  [-1.63649112e-01  2.37698072e+02  2.82308224e+01]\n",
      "Done with gradient descent at iteration  486\n",
      "Learned weights =  [-1.64035680e-01  2.37698072e+02  2.82308224e+01]\n",
      "Done with gradient descent at iteration  486\n",
      "Learned weights =  [-1.64035680e-01  2.37731399e+02  2.82308224e+01]\n",
      "Done with gradient descent at iteration  486\n",
      "Learned weights =  [-1.64035680e-01  2.37731399e+02  2.81946337e+01]\n",
      "Done with gradient descent at iteration  487\n",
      "Learned weights =  [-1.64422158e-01  2.37731399e+02  2.81946337e+01]\n",
      "Done with gradient descent at iteration  487\n",
      "Learned weights =  [-1.64422158e-01  2.37764528e+02  2.81946337e+01]\n",
      "Done with gradient descent at iteration  487\n",
      "Learned weights =  [-1.64422158e-01  2.37764528e+02  2.81586597e+01]\n",
      "Done with gradient descent at iteration  488\n",
      "Learned weights =  [-1.64808549e-01  2.37764528e+02  2.81586597e+01]\n",
      "Done with gradient descent at iteration  488\n",
      "Learned weights =  [-1.64808549e-01  2.37797460e+02  2.81586597e+01]\n",
      "Done with gradient descent at iteration  488\n",
      "Learned weights =  [-1.64808549e-01  2.37797460e+02  2.81228991e+01]\n",
      "Done with gradient descent at iteration  489\n",
      "Learned weights =  [-1.65194852e-01  2.37797460e+02  2.81228991e+01]\n",
      "Done with gradient descent at iteration  489\n",
      "Learned weights =  [-1.65194852e-01  2.37830197e+02  2.81228991e+01]\n",
      "Done with gradient descent at iteration  489\n",
      "Learned weights =  [-1.65194852e-01  2.37830197e+02  2.80873506e+01]\n",
      "Done with gradient descent at iteration  490\n",
      "Learned weights =  [-1.65581068e-01  2.37830197e+02  2.80873506e+01]\n",
      "Done with gradient descent at iteration  490\n",
      "Learned weights =  [-1.65581068e-01  2.37862740e+02  2.80873506e+01]\n",
      "Done with gradient descent at iteration  490\n",
      "Learned weights =  [-1.65581068e-01  2.37862740e+02  2.80520129e+01]\n",
      "Done with gradient descent at iteration  491\n",
      "Learned weights =  [-1.65967197e-01  2.37862740e+02  2.80520129e+01]\n",
      "Done with gradient descent at iteration  491\n",
      "Learned weights =  [-1.65967197e-01  2.37895090e+02  2.80520129e+01]\n",
      "Done with gradient descent at iteration  491\n",
      "Learned weights =  [-1.65967197e-01  2.37895090e+02  2.80168848e+01]\n",
      "Done with gradient descent at iteration  492\n",
      "Learned weights =  [-1.66353241e-01  2.37895090e+02  2.80168848e+01]\n",
      "Done with gradient descent at iteration  492\n",
      "Learned weights =  [-1.66353241e-01  2.37927248e+02  2.80168848e+01]\n",
      "Done with gradient descent at iteration  492\n",
      "Learned weights =  [-1.66353241e-01  2.37927248e+02  2.79819651e+01]\n",
      "Done with gradient descent at iteration  493\n",
      "Learned weights =  [-1.66739198e-01  2.37927248e+02  2.79819651e+01]\n",
      "Done with gradient descent at iteration  493\n",
      "Learned weights =  [-1.66739198e-01  2.37959216e+02  2.79819651e+01]\n",
      "Done with gradient descent at iteration  493\n",
      "Learned weights =  [-1.66739198e-01  2.37959216e+02  2.79472526e+01]\n",
      "Done with gradient descent at iteration  494\n",
      "Learned weights =  [-1.67125071e-01  2.37959216e+02  2.79472526e+01]\n",
      "Done with gradient descent at iteration  494\n",
      "Learned weights =  [-1.67125071e-01  2.37990993e+02  2.79472526e+01]\n",
      "Done with gradient descent at iteration  494\n",
      "Learned weights =  [-1.67125071e-01  2.37990993e+02  2.79127459e+01]\n",
      "Done with gradient descent at iteration  495\n",
      "Learned weights =  [-1.67510859e-01  2.37990993e+02  2.79127459e+01]\n",
      "Done with gradient descent at iteration  495\n",
      "Learned weights =  [-1.67510859e-01  2.38022583e+02  2.79127459e+01]\n",
      "Done with gradient descent at iteration  495\n",
      "Learned weights =  [-1.67510859e-01  2.38022583e+02  2.78784439e+01]\n",
      "Done with gradient descent at iteration  496\n",
      "Learned weights =  [-1.67896563e-01  2.38022583e+02  2.78784439e+01]\n",
      "Done with gradient descent at iteration  496\n",
      "Learned weights =  [-1.67896563e-01  2.38053984e+02  2.78784439e+01]\n",
      "Done with gradient descent at iteration  496\n",
      "Learned weights =  [-1.67896563e-01  2.38053984e+02  2.78443454e+01]\n",
      "Done with gradient descent at iteration  497\n",
      "Learned weights =  [-1.68282184e-01  2.38053984e+02  2.78443454e+01]\n",
      "Done with gradient descent at iteration  497\n",
      "Learned weights =  [-1.68282184e-01  2.38085200e+02  2.78443454e+01]\n",
      "Done with gradient descent at iteration  497\n",
      "Learned weights =  [-1.68282184e-01  2.38085200e+02  2.78104492e+01]\n",
      "Done with gradient descent at iteration  498\n",
      "Learned weights =  [-1.68667721e-01  2.38085200e+02  2.78104492e+01]\n",
      "Done with gradient descent at iteration  498\n",
      "Learned weights =  [-1.68667721e-01  2.38116230e+02  2.78104492e+01]\n",
      "Done with gradient descent at iteration  498\n",
      "Learned weights =  [-1.68667721e-01  2.38116230e+02  2.77767540e+01]\n",
      "Done with gradient descent at iteration  499\n",
      "Learned weights =  [-1.69053176e-01  2.38116230e+02  2.77767540e+01]\n",
      "Done with gradient descent at iteration  499\n",
      "Learned weights =  [-1.69053176e-01  2.38147077e+02  2.77767540e+01]\n",
      "Done with gradient descent at iteration  499\n",
      "Learned weights =  [-1.69053176e-01  2.38147077e+02  2.77432587e+01]\n",
      "Iteration = 500\n",
      "Cost function =  1205780190233996.0\n",
      "Done with gradient descent at iteration  500\n",
      "Learned weights =  [-1.69438549e-01  2.38147077e+02  2.77432587e+01]\n",
      "Done with gradient descent at iteration  500\n",
      "Learned weights =  [-1.69438549e-01  2.38177740e+02  2.77432587e+01]\n",
      "Done with gradient descent at iteration  500\n",
      "Learned weights =  [-1.69438549e-01  2.38177740e+02  2.77099620e+01]\n",
      "Done with gradient descent at iteration  501\n",
      "Learned weights =  [-1.6982384e-01  2.3817774e+02  2.7709962e+01]\n",
      "Done with gradient descent at iteration  501\n",
      "Learned weights =  [-1.69823840e-01  2.38208221e+02  2.77099620e+01]\n",
      "Done with gradient descent at iteration  501\n",
      "Learned weights =  [-1.69823840e-01  2.38208221e+02  2.76768629e+01]\n",
      "Done with gradient descent at iteration  502\n",
      "Learned weights =  [-1.70209050e-01  2.38208221e+02  2.76768629e+01]\n",
      "Done with gradient descent at iteration  502\n",
      "Learned weights =  [-1.70209050e-01  2.38238522e+02  2.76768629e+01]\n",
      "Done with gradient descent at iteration  502\n",
      "Learned weights =  [-1.70209050e-01  2.38238522e+02  2.76439600e+01]\n",
      "Done with gradient descent at iteration  503\n",
      "Learned weights =  [-1.70594179e-01  2.38238522e+02  2.76439600e+01]\n",
      "Done with gradient descent at iteration  503\n",
      "Learned weights =  [-1.70594179e-01  2.38268643e+02  2.76439600e+01]\n",
      "Done with gradient descent at iteration  503\n",
      "Learned weights =  [-1.70594179e-01  2.38268643e+02  2.76112524e+01]\n",
      "Done with gradient descent at iteration  504\n",
      "Learned weights =  [-1.70979229e-01  2.38268643e+02  2.76112524e+01]\n",
      "Done with gradient descent at iteration  504\n",
      "Learned weights =  [-1.70979229e-01  2.38298585e+02  2.76112524e+01]\n",
      "Done with gradient descent at iteration  504\n",
      "Learned weights =  [-1.70979229e-01  2.38298585e+02  2.75787388e+01]\n",
      "Done with gradient descent at iteration  505\n",
      "Learned weights =  [-1.71364198e-01  2.38298585e+02  2.75787388e+01]\n",
      "Done with gradient descent at iteration  505\n",
      "Learned weights =  [-1.71364198e-01  2.38328350e+02  2.75787388e+01]\n",
      "Done with gradient descent at iteration  505\n",
      "Learned weights =  [-1.71364198e-01  2.38328350e+02  2.75464180e+01]\n",
      "Done with gradient descent at iteration  506\n",
      "Learned weights =  [-1.71749089e-01  2.38328350e+02  2.75464180e+01]\n",
      "Done with gradient descent at iteration  506\n",
      "Learned weights =  [-1.71749089e-01  2.38357938e+02  2.75464180e+01]\n",
      "Done with gradient descent at iteration  506\n",
      "Learned weights =  [-1.71749089e-01  2.38357938e+02  2.75142889e+01]\n",
      "Done with gradient descent at iteration  507\n",
      "Learned weights =  [-1.72133900e-01  2.38357938e+02  2.75142889e+01]\n",
      "Done with gradient descent at iteration  507\n",
      "Learned weights =  [-1.72133900e-01  2.38387351e+02  2.75142889e+01]\n",
      "Done with gradient descent at iteration  507\n",
      "Learned weights =  [-1.72133900e-01  2.38387351e+02  2.74823504e+01]\n",
      "Done with gradient descent at iteration  508\n",
      "Learned weights =  [-1.72518634e-01  2.38387351e+02  2.74823504e+01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  508\n",
      "Learned weights =  [-1.72518634e-01  2.38416589e+02  2.74823504e+01]\n",
      "Done with gradient descent at iteration  508\n",
      "Learned weights =  [-1.72518634e-01  2.38416589e+02  2.74506014e+01]\n",
      "Done with gradient descent at iteration  509\n",
      "Learned weights =  [-1.72903290e-01  2.38416589e+02  2.74506014e+01]\n",
      "Done with gradient descent at iteration  509\n",
      "Learned weights =  [-1.72903290e-01  2.38445654e+02  2.74506014e+01]\n",
      "Done with gradient descent at iteration  509\n",
      "Learned weights =  [-1.72903290e-01  2.38445654e+02  2.74190407e+01]\n",
      "Done with gradient descent at iteration  510\n",
      "Learned weights =  [-1.73287868e-01  2.38445654e+02  2.74190407e+01]\n",
      "Done with gradient descent at iteration  510\n",
      "Learned weights =  [-1.73287868e-01  2.38474546e+02  2.74190407e+01]\n",
      "Done with gradient descent at iteration  510\n",
      "Learned weights =  [-1.73287868e-01  2.38474546e+02  2.73876671e+01]\n",
      "Done with gradient descent at iteration  511\n",
      "Learned weights =  [-1.73672369e-01  2.38474546e+02  2.73876671e+01]\n",
      "Done with gradient descent at iteration  511\n",
      "Learned weights =  [-1.73672369e-01  2.38503267e+02  2.73876671e+01]\n",
      "Done with gradient descent at iteration  511\n",
      "Learned weights =  [-1.73672369e-01  2.38503267e+02  2.73564797e+01]\n",
      "Done with gradient descent at iteration  512\n",
      "Learned weights =  [-1.74056794e-01  2.38503267e+02  2.73564797e+01]\n",
      "Done with gradient descent at iteration  512\n",
      "Learned weights =  [-1.74056794e-01  2.38531818e+02  2.73564797e+01]\n",
      "Done with gradient descent at iteration  512\n",
      "Learned weights =  [-1.74056794e-01  2.38531818e+02  2.73254773e+01]\n",
      "Done with gradient descent at iteration  513\n",
      "Learned weights =  [-1.74441143e-01  2.38531818e+02  2.73254773e+01]\n",
      "Done with gradient descent at iteration  513\n",
      "Learned weights =  [-1.74441143e-01  2.38560199e+02  2.73254773e+01]\n",
      "Done with gradient descent at iteration  513\n",
      "Learned weights =  [-1.74441143e-01  2.38560199e+02  2.72946587e+01]\n",
      "Done with gradient descent at iteration  514\n",
      "Learned weights =  [-1.74825417e-01  2.38560199e+02  2.72946587e+01]\n",
      "Done with gradient descent at iteration  514\n",
      "Learned weights =  [-1.74825417e-01  2.38588412e+02  2.72946587e+01]\n",
      "Done with gradient descent at iteration  514\n",
      "Learned weights =  [-1.74825417e-01  2.38588412e+02  2.72640230e+01]\n",
      "Done with gradient descent at iteration  515\n",
      "Learned weights =  [-1.75209615e-01  2.38588412e+02  2.72640230e+01]\n",
      "Done with gradient descent at iteration  515\n",
      "Learned weights =  [-1.75209615e-01  2.38616457e+02  2.72640230e+01]\n",
      "Done with gradient descent at iteration  515\n",
      "Learned weights =  [-1.75209615e-01  2.38616457e+02  2.72335690e+01]\n",
      "Done with gradient descent at iteration  516\n",
      "Learned weights =  [-1.75593739e-01  2.38616457e+02  2.72335690e+01]\n",
      "Done with gradient descent at iteration  516\n",
      "Learned weights =  [-1.75593739e-01  2.38644336e+02  2.72335690e+01]\n",
      "Done with gradient descent at iteration  516\n",
      "Learned weights =  [-1.75593739e-01  2.38644336e+02  2.72032956e+01]\n",
      "Done with gradient descent at iteration  517\n",
      "Learned weights =  [-1.75977789e-01  2.38644336e+02  2.72032956e+01]\n",
      "Done with gradient descent at iteration  517\n",
      "Learned weights =  [-1.75977789e-01  2.38672050e+02  2.72032956e+01]\n",
      "Done with gradient descent at iteration  517\n",
      "Learned weights =  [-1.75977789e-01  2.38672050e+02  2.71732018e+01]\n",
      "Done with gradient descent at iteration  518\n",
      "Learned weights =  [-1.76361765e-01  2.38672050e+02  2.71732018e+01]\n",
      "Done with gradient descent at iteration  518\n",
      "Learned weights =  [-1.76361765e-01  2.38699600e+02  2.71732018e+01]\n",
      "Done with gradient descent at iteration  518\n",
      "Learned weights =  [-1.76361765e-01  2.38699600e+02  2.71432865e+01]\n",
      "Done with gradient descent at iteration  519\n",
      "Learned weights =  [-1.76745667e-01  2.38699600e+02  2.71432865e+01]\n",
      "Done with gradient descent at iteration  519\n",
      "Learned weights =  [-1.76745667e-01  2.38726986e+02  2.71432865e+01]\n",
      "Done with gradient descent at iteration  519\n",
      "Learned weights =  [-1.76745667e-01  2.38726986e+02  2.71135487e+01]\n",
      "Done with gradient descent at iteration  520\n",
      "Learned weights =  [-1.77129497e-01  2.38726986e+02  2.71135487e+01]\n",
      "Done with gradient descent at iteration  520\n",
      "Learned weights =  [-1.77129497e-01  2.38754209e+02  2.71135487e+01]\n",
      "Done with gradient descent at iteration  520\n",
      "Learned weights =  [-1.77129497e-01  2.38754209e+02  2.70839872e+01]\n",
      "Done with gradient descent at iteration  521\n",
      "Learned weights =  [-1.77513255e-01  2.38754209e+02  2.70839872e+01]\n",
      "Done with gradient descent at iteration  521\n",
      "Learned weights =  [-1.77513255e-01  2.38781271e+02  2.70839872e+01]\n",
      "Done with gradient descent at iteration  521\n",
      "Learned weights =  [-1.77513255e-01  2.38781271e+02  2.70546011e+01]\n",
      "Done with gradient descent at iteration  522\n",
      "Learned weights =  [-1.77896940e-01  2.38781271e+02  2.70546011e+01]\n",
      "Done with gradient descent at iteration  522\n",
      "Learned weights =  [-1.77896940e-01  2.38808173e+02  2.70546011e+01]\n",
      "Done with gradient descent at iteration  522\n",
      "Learned weights =  [-1.77896940e-01  2.38808173e+02  2.70253893e+01]\n",
      "Done with gradient descent at iteration  523\n",
      "Learned weights =  [-1.78280554e-01  2.38808173e+02  2.70253893e+01]\n",
      "Done with gradient descent at iteration  523\n",
      "Learned weights =  [-1.78280554e-01  2.38834915e+02  2.70253893e+01]\n",
      "Done with gradient descent at iteration  523\n",
      "Learned weights =  [-1.78280554e-01  2.38834915e+02  2.69963508e+01]\n",
      "Done with gradient descent at iteration  524\n",
      "Learned weights =  [-1.78664096e-01  2.38834915e+02  2.69963508e+01]\n",
      "Done with gradient descent at iteration  524\n",
      "Learned weights =  [-1.78664096e-01  2.38861498e+02  2.69963508e+01]\n",
      "Done with gradient descent at iteration  524\n",
      "Learned weights =  [-1.78664096e-01  2.38861498e+02  2.69674845e+01]\n",
      "Done with gradient descent at iteration  525\n",
      "Learned weights =  [-1.79047568e-01  2.38861498e+02  2.69674845e+01]\n",
      "Done with gradient descent at iteration  525\n",
      "Learned weights =  [-1.79047568e-01  2.38887924e+02  2.69674845e+01]\n",
      "Done with gradient descent at iteration  525\n",
      "Learned weights =  [-1.79047568e-01  2.38887924e+02  2.69387894e+01]\n",
      "Done with gradient descent at iteration  526\n",
      "Learned weights =  [-1.79430970e-01  2.38887924e+02  2.69387894e+01]\n",
      "Done with gradient descent at iteration  526\n",
      "Learned weights =  [-1.79430970e-01  2.38914193e+02  2.69387894e+01]\n",
      "Done with gradient descent at iteration  526\n",
      "Learned weights =  [-1.79430970e-01  2.38914193e+02  2.69102646e+01]\n",
      "Done with gradient descent at iteration  527\n",
      "Learned weights =  [-1.79814301e-01  2.38914193e+02  2.69102646e+01]\n",
      "Done with gradient descent at iteration  527\n",
      "Learned weights =  [-1.79814301e-01  2.38940306e+02  2.69102646e+01]\n",
      "Done with gradient descent at iteration  527\n",
      "Learned weights =  [-1.79814301e-01  2.38940306e+02  2.68819089e+01]\n",
      "Done with gradient descent at iteration  528\n",
      "Learned weights =  [-1.80197563e-01  2.38940306e+02  2.68819089e+01]\n",
      "Done with gradient descent at iteration  528\n",
      "Learned weights =  [-1.80197563e-01  2.38966265e+02  2.68819089e+01]\n",
      "Done with gradient descent at iteration  528\n",
      "Learned weights =  [-1.80197563e-01  2.38966265e+02  2.68537215e+01]\n",
      "Done with gradient descent at iteration  529\n",
      "Learned weights =  [-1.80580756e-01  2.38966265e+02  2.68537215e+01]\n",
      "Done with gradient descent at iteration  529\n",
      "Learned weights =  [-1.80580756e-01  2.38992069e+02  2.68537215e+01]\n",
      "Done with gradient descent at iteration  529\n",
      "Learned weights =  [-1.80580756e-01  2.38992069e+02  2.68257012e+01]\n",
      "Done with gradient descent at iteration  530\n",
      "Learned weights =  [-1.80963881e-01  2.38992069e+02  2.68257012e+01]\n",
      "Done with gradient descent at iteration  530\n",
      "Learned weights =  [-1.80963881e-01  2.39017720e+02  2.68257012e+01]\n",
      "Done with gradient descent at iteration  530\n",
      "Learned weights =  [-1.80963881e-01  2.39017720e+02  2.67978471e+01]\n",
      "Done with gradient descent at iteration  531\n",
      "Learned weights =  [-1.81346937e-01  2.39017720e+02  2.67978471e+01]\n",
      "Done with gradient descent at iteration  531\n",
      "Learned weights =  [-1.81346937e-01  2.39043219e+02  2.67978471e+01]\n",
      "Done with gradient descent at iteration  531\n",
      "Learned weights =  [-1.81346937e-01  2.39043219e+02  2.67701583e+01]\n",
      "Done with gradient descent at iteration  532\n",
      "Learned weights =  [-1.81729925e-01  2.39043219e+02  2.67701583e+01]\n",
      "Done with gradient descent at iteration  532\n",
      "Learned weights =  [-1.81729925e-01  2.39068567e+02  2.67701583e+01]\n",
      "Done with gradient descent at iteration  532\n",
      "Learned weights =  [-1.81729925e-01  2.39068567e+02  2.67426337e+01]\n",
      "Done with gradient descent at iteration  533\n",
      "Learned weights =  [-1.82112846e-01  2.39068567e+02  2.67426337e+01]\n",
      "Done with gradient descent at iteration  533\n",
      "Learned weights =  [-1.82112846e-01  2.39093764e+02  2.67426337e+01]\n",
      "Done with gradient descent at iteration  533\n",
      "Learned weights =  [-1.82112846e-01  2.39093764e+02  2.67152724e+01]\n",
      "Done with gradient descent at iteration  534\n",
      "Learned weights =  [-1.82495700e-01  2.39093764e+02  2.67152724e+01]\n",
      "Done with gradient descent at iteration  534\n",
      "Learned weights =  [-1.82495700e-01  2.39118812e+02  2.67152724e+01]\n",
      "Done with gradient descent at iteration  534\n",
      "Learned weights =  [-1.82495700e-01  2.39118812e+02  2.66880733e+01]\n",
      "Done with gradient descent at iteration  535\n",
      "Learned weights =  [-1.82878487e-01  2.39118812e+02  2.66880733e+01]\n",
      "Done with gradient descent at iteration  535\n",
      "Learned weights =  [-1.82878487e-01  2.39143712e+02  2.66880733e+01]\n",
      "Done with gradient descent at iteration  535\n",
      "Learned weights =  [-1.82878487e-01  2.39143712e+02  2.66610356e+01]\n",
      "Done with gradient descent at iteration  536\n",
      "Learned weights =  [-1.83261208e-01  2.39143712e+02  2.66610356e+01]\n",
      "Done with gradient descent at iteration  536\n",
      "Learned weights =  [-1.83261208e-01  2.39168464e+02  2.66610356e+01]\n",
      "Done with gradient descent at iteration  536\n",
      "Learned weights =  [-1.83261208e-01  2.39168464e+02  2.66341583e+01]\n",
      "Done with gradient descent at iteration  537\n",
      "Learned weights =  [-1.83643863e-01  2.39168464e+02  2.66341583e+01]\n",
      "Done with gradient descent at iteration  537\n",
      "Learned weights =  [-1.83643863e-01  2.39193069e+02  2.66341583e+01]\n",
      "Done with gradient descent at iteration  537\n",
      "Learned weights =  [-1.83643863e-01  2.39193069e+02  2.66074404e+01]\n",
      "Done with gradient descent at iteration  538\n",
      "Learned weights =  [-1.84026453e-01  2.39193069e+02  2.66074404e+01]\n",
      "Done with gradient descent at iteration  538\n",
      "Learned weights =  [-1.84026453e-01  2.39217528e+02  2.66074404e+01]\n",
      "Done with gradient descent at iteration  538\n",
      "Learned weights =  [-1.84026453e-01  2.39217528e+02  2.65808810e+01]\n",
      "Done with gradient descent at iteration  539\n",
      "Learned weights =  [-1.84408977e-01  2.39217528e+02  2.65808810e+01]\n",
      "Done with gradient descent at iteration  539\n",
      "Learned weights =  [-1.84408977e-01  2.39241842e+02  2.65808810e+01]\n",
      "Done with gradient descent at iteration  539\n",
      "Learned weights =  [-1.84408977e-01  2.39241842e+02  2.65544791e+01]\n",
      "Done with gradient descent at iteration  540\n",
      "Learned weights =  [-1.84791437e-01  2.39241842e+02  2.65544791e+01]\n",
      "Done with gradient descent at iteration  540\n",
      "Learned weights =  [-1.84791437e-01  2.39266011e+02  2.65544791e+01]\n",
      "Done with gradient descent at iteration  540\n",
      "Learned weights =  [-1.84791437e-01  2.39266011e+02  2.65282338e+01]\n",
      "Done with gradient descent at iteration  541\n",
      "Learned weights =  [-1.85173833e-01  2.39266011e+02  2.65282338e+01]\n",
      "Done with gradient descent at iteration  541\n",
      "Learned weights =  [-1.85173833e-01  2.39290038e+02  2.65282338e+01]\n",
      "Done with gradient descent at iteration  541\n",
      "Learned weights =  [-1.85173833e-01  2.39290038e+02  2.65021442e+01]\n",
      "Done with gradient descent at iteration  542\n",
      "Learned weights =  [-1.85556164e-01  2.39290038e+02  2.65021442e+01]\n",
      "Done with gradient descent at iteration  542\n",
      "Learned weights =  [-1.85556164e-01  2.39313921e+02  2.65021442e+01]\n",
      "Done with gradient descent at iteration  542\n",
      "Learned weights =  [-1.85556164e-01  2.39313921e+02  2.64762094e+01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  543\n",
      "Learned weights =  [-1.85938432e-01  2.39313921e+02  2.64762094e+01]\n",
      "Done with gradient descent at iteration  543\n",
      "Learned weights =  [-1.85938432e-01  2.39337663e+02  2.64762094e+01]\n",
      "Done with gradient descent at iteration  543\n",
      "Learned weights =  [-1.85938432e-01  2.39337663e+02  2.64504283e+01]\n",
      "Done with gradient descent at iteration  544\n",
      "Learned weights =  [-1.86320637e-01  2.39337663e+02  2.64504283e+01]\n",
      "Done with gradient descent at iteration  544\n",
      "Learned weights =  [-1.86320637e-01  2.39361265e+02  2.64504283e+01]\n",
      "Done with gradient descent at iteration  544\n",
      "Learned weights =  [-1.86320637e-01  2.39361265e+02  2.64248003e+01]\n",
      "Done with gradient descent at iteration  545\n",
      "Learned weights =  [-1.86702779e-01  2.39361265e+02  2.64248003e+01]\n",
      "Done with gradient descent at iteration  545\n",
      "Learned weights =  [-1.86702779e-01  2.39384726e+02  2.64248003e+01]\n",
      "Done with gradient descent at iteration  545\n",
      "Learned weights =  [-1.86702779e-01  2.39384726e+02  2.63993242e+01]\n",
      "Done with gradient descent at iteration  546\n",
      "Learned weights =  [-1.87084859e-01  2.39384726e+02  2.63993242e+01]\n",
      "Done with gradient descent at iteration  546\n",
      "Learned weights =  [-1.87084859e-01  2.39408048e+02  2.63993242e+01]\n",
      "Done with gradient descent at iteration  546\n",
      "Learned weights =  [-1.87084859e-01  2.39408048e+02  2.63739992e+01]\n",
      "Done with gradient descent at iteration  547\n",
      "Learned weights =  [-1.87466877e-01  2.39408048e+02  2.63739992e+01]\n",
      "Done with gradient descent at iteration  547\n",
      "Learned weights =  [-1.87466877e-01  2.39431232e+02  2.63739992e+01]\n",
      "Done with gradient descent at iteration  547\n",
      "Learned weights =  [-1.87466877e-01  2.39431232e+02  2.63488245e+01]\n",
      "Done with gradient descent at iteration  548\n",
      "Learned weights =  [-1.87848832e-01  2.39431232e+02  2.63488245e+01]\n",
      "Done with gradient descent at iteration  548\n",
      "Learned weights =  [-1.87848832e-01  2.39454278e+02  2.63488245e+01]\n",
      "Done with gradient descent at iteration  548\n",
      "Learned weights =  [-1.87848832e-01  2.39454278e+02  2.63237991e+01]\n",
      "Done with gradient descent at iteration  549\n",
      "Learned weights =  [-1.88230727e-01  2.39454278e+02  2.63237991e+01]\n",
      "Done with gradient descent at iteration  549\n",
      "Learned weights =  [-1.88230727e-01  2.39477188e+02  2.63237991e+01]\n",
      "Done with gradient descent at iteration  549\n",
      "Learned weights =  [-1.88230727e-01  2.39477188e+02  2.62989221e+01]\n",
      "Done with gradient descent at iteration  550\n",
      "Learned weights =  [-1.88612561e-01  2.39477188e+02  2.62989221e+01]\n",
      "Done with gradient descent at iteration  550\n",
      "Learned weights =  [-1.88612561e-01  2.39499962e+02  2.62989221e+01]\n",
      "Done with gradient descent at iteration  550\n",
      "Learned weights =  [-1.88612561e-01  2.39499962e+02  2.62741927e+01]\n",
      "Done with gradient descent at iteration  551\n",
      "Learned weights =  [-1.88994334e-01  2.39499962e+02  2.62741927e+01]\n",
      "Done with gradient descent at iteration  551\n",
      "Learned weights =  [-1.88994334e-01  2.39522600e+02  2.62741927e+01]\n",
      "Done with gradient descent at iteration  551\n",
      "Learned weights =  [-1.88994334e-01  2.39522600e+02  2.62496100e+01]\n",
      "Done with gradient descent at iteration  552\n",
      "Learned weights =  [-1.89376046e-01  2.39522600e+02  2.62496100e+01]\n",
      "Done with gradient descent at iteration  552\n",
      "Learned weights =  [-1.89376046e-01  2.39545105e+02  2.62496100e+01]\n",
      "Done with gradient descent at iteration  552\n",
      "Learned weights =  [-1.89376046e-01  2.39545105e+02  2.62251730e+01]\n",
      "Done with gradient descent at iteration  553\n",
      "Learned weights =  [-1.89757699e-01  2.39545105e+02  2.62251730e+01]\n",
      "Done with gradient descent at iteration  553\n",
      "Learned weights =  [-1.89757699e-01  2.39567476e+02  2.62251730e+01]\n",
      "Done with gradient descent at iteration  553\n",
      "Learned weights =  [-1.89757699e-01  2.39567476e+02  2.62008811e+01]\n",
      "Done with gradient descent at iteration  554\n",
      "Learned weights =  [-1.90139293e-01  2.39567476e+02  2.62008811e+01]\n",
      "Done with gradient descent at iteration  554\n",
      "Learned weights =  [-1.90139293e-01  2.39589714e+02  2.62008811e+01]\n",
      "Done with gradient descent at iteration  554\n",
      "Learned weights =  [-1.90139293e-01  2.39589714e+02  2.61767332e+01]\n",
      "Done with gradient descent at iteration  555\n",
      "Learned weights =  [-1.90520827e-01  2.39589714e+02  2.61767332e+01]\n",
      "Done with gradient descent at iteration  555\n",
      "Learned weights =  [-1.90520827e-01  2.39611820e+02  2.61767332e+01]\n",
      "Done with gradient descent at iteration  555\n",
      "Learned weights =  [-1.90520827e-01  2.39611820e+02  2.61527286e+01]\n",
      "Done with gradient descent at iteration  556\n",
      "Learned weights =  [-1.90902302e-01  2.39611820e+02  2.61527286e+01]\n",
      "Done with gradient descent at iteration  556\n",
      "Learned weights =  [-1.90902302e-01  2.39633795e+02  2.61527286e+01]\n",
      "Done with gradient descent at iteration  556\n",
      "Learned weights =  [-1.90902302e-01  2.39633795e+02  2.61288663e+01]\n",
      "Done with gradient descent at iteration  557\n",
      "Learned weights =  [-1.91283719e-01  2.39633795e+02  2.61288663e+01]\n",
      "Done with gradient descent at iteration  557\n",
      "Learned weights =  [-1.91283719e-01  2.39655640e+02  2.61288663e+01]\n",
      "Done with gradient descent at iteration  557\n",
      "Learned weights =  [-1.91283719e-01  2.39655640e+02  2.61051456e+01]\n",
      "Done with gradient descent at iteration  558\n",
      "Learned weights =  [-1.91665078e-01  2.39655640e+02  2.61051456e+01]\n",
      "Done with gradient descent at iteration  558\n",
      "Learned weights =  [-1.91665078e-01  2.39677355e+02  2.61051456e+01]\n",
      "Done with gradient descent at iteration  558\n",
      "Learned weights =  [-1.91665078e-01  2.39677355e+02  2.60815656e+01]\n",
      "Done with gradient descent at iteration  559\n",
      "Learned weights =  [-1.92046379e-01  2.39677355e+02  2.60815656e+01]\n",
      "Done with gradient descent at iteration  559\n",
      "Learned weights =  [-1.92046379e-01  2.39698941e+02  2.60815656e+01]\n",
      "Done with gradient descent at iteration  559\n",
      "Learned weights =  [-1.92046379e-01  2.39698941e+02  2.60581255e+01]\n",
      "Done with gradient descent at iteration  560\n",
      "Learned weights =  [-1.92427623e-01  2.39698941e+02  2.60581255e+01]\n",
      "Done with gradient descent at iteration  560\n",
      "Learned weights =  [-1.92427623e-01  2.39720400e+02  2.60581255e+01]\n",
      "Done with gradient descent at iteration  560\n",
      "Learned weights =  [-1.92427623e-01  2.39720400e+02  2.60348244e+01]\n",
      "Done with gradient descent at iteration  561\n",
      "Learned weights =  [-1.92808809e-01  2.39720400e+02  2.60348244e+01]\n",
      "Done with gradient descent at iteration  561\n",
      "Learned weights =  [-1.92808809e-01  2.39741731e+02  2.60348244e+01]\n",
      "Done with gradient descent at iteration  561\n",
      "Learned weights =  [-1.92808809e-01  2.39741731e+02  2.60116615e+01]\n",
      "Done with gradient descent at iteration  562\n",
      "Learned weights =  [-1.93189939e-01  2.39741731e+02  2.60116615e+01]\n",
      "Done with gradient descent at iteration  562\n",
      "Learned weights =  [-1.93189939e-01  2.39762935e+02  2.60116615e+01]\n",
      "Done with gradient descent at iteration  562\n",
      "Learned weights =  [-1.93189939e-01  2.39762935e+02  2.59886360e+01]\n",
      "Done with gradient descent at iteration  563\n",
      "Learned weights =  [-1.93571012e-01  2.39762935e+02  2.59886360e+01]\n",
      "Done with gradient descent at iteration  563\n",
      "Learned weights =  [-1.93571012e-01  2.39784014e+02  2.59886360e+01]\n",
      "Done with gradient descent at iteration  563\n",
      "Learned weights =  [-1.93571012e-01  2.39784014e+02  2.59657471e+01]\n",
      "Done with gradient descent at iteration  564\n",
      "Learned weights =  [-1.93952029e-01  2.39784014e+02  2.59657471e+01]\n",
      "Done with gradient descent at iteration  564\n",
      "Learned weights =  [-1.93952029e-01  2.39804968e+02  2.59657471e+01]\n",
      "Done with gradient descent at iteration  564\n",
      "Learned weights =  [-1.93952029e-01  2.39804968e+02  2.59429939e+01]\n",
      "Done with gradient descent at iteration  565\n",
      "Learned weights =  [-1.94332991e-01  2.39804968e+02  2.59429939e+01]\n",
      "Done with gradient descent at iteration  565\n",
      "Learned weights =  [-1.94332991e-01  2.39825797e+02  2.59429939e+01]\n",
      "Done with gradient descent at iteration  565\n",
      "Learned weights =  [-1.94332991e-01  2.39825797e+02  2.59203758e+01]\n",
      "Done with gradient descent at iteration  566\n",
      "Learned weights =  [-1.94713897e-01  2.39825797e+02  2.59203758e+01]\n",
      "Done with gradient descent at iteration  566\n",
      "Learned weights =  [-1.94713897e-01  2.39846503e+02  2.59203758e+01]\n",
      "Done with gradient descent at iteration  566\n",
      "Learned weights =  [-1.94713897e-01  2.39846503e+02  2.58978917e+01]\n",
      "Done with gradient descent at iteration  567\n",
      "Learned weights =  [-1.95094748e-01  2.39846503e+02  2.58978917e+01]\n",
      "Done with gradient descent at iteration  567\n",
      "Learned weights =  [-1.95094748e-01  2.39867086e+02  2.58978917e+01]\n",
      "Done with gradient descent at iteration  567\n",
      "Learned weights =  [-1.95094748e-01  2.39867086e+02  2.58755411e+01]\n",
      "Done with gradient descent at iteration  568\n",
      "Learned weights =  [-1.95475544e-01  2.39867086e+02  2.58755411e+01]\n",
      "Done with gradient descent at iteration  568\n",
      "Learned weights =  [-1.95475544e-01  2.39887547e+02  2.58755411e+01]\n",
      "Done with gradient descent at iteration  568\n",
      "Learned weights =  [-1.95475544e-01  2.39887547e+02  2.58533230e+01]\n",
      "Done with gradient descent at iteration  569\n",
      "Learned weights =  [-1.95856286e-01  2.39887547e+02  2.58533230e+01]\n",
      "Done with gradient descent at iteration  569\n",
      "Learned weights =  [-1.95856286e-01  2.39907887e+02  2.58533230e+01]\n",
      "Done with gradient descent at iteration  569\n",
      "Learned weights =  [-1.95856286e-01  2.39907887e+02  2.58312367e+01]\n",
      "Done with gradient descent at iteration  570\n",
      "Learned weights =  [-1.96236973e-01  2.39907887e+02  2.58312367e+01]\n",
      "Done with gradient descent at iteration  570\n",
      "Learned weights =  [-1.96236973e-01  2.39928106e+02  2.58312367e+01]\n",
      "Done with gradient descent at iteration  570\n",
      "Learned weights =  [-1.96236973e-01  2.39928106e+02  2.58092814e+01]\n",
      "Done with gradient descent at iteration  571\n",
      "Learned weights =  [-1.96617607e-01  2.39928106e+02  2.58092814e+01]\n",
      "Done with gradient descent at iteration  571\n",
      "Learned weights =  [-1.96617607e-01  2.39948205e+02  2.58092814e+01]\n",
      "Done with gradient descent at iteration  571\n",
      "Learned weights =  [-1.96617607e-01  2.39948205e+02  2.57874564e+01]\n",
      "Done with gradient descent at iteration  572\n",
      "Learned weights =  [-1.96998187e-01  2.39948205e+02  2.57874564e+01]\n",
      "Done with gradient descent at iteration  572\n",
      "Learned weights =  [-1.96998187e-01  2.39968185e+02  2.57874564e+01]\n",
      "Done with gradient descent at iteration  572\n",
      "Learned weights =  [-1.96998187e-01  2.39968185e+02  2.57657608e+01]\n",
      "Done with gradient descent at iteration  573\n",
      "Learned weights =  [-1.97378715e-01  2.39968185e+02  2.57657608e+01]\n",
      "Done with gradient descent at iteration  573\n",
      "Learned weights =  [-1.97378715e-01  2.39988046e+02  2.57657608e+01]\n",
      "Done with gradient descent at iteration  573\n",
      "Learned weights =  [-1.97378715e-01  2.39988046e+02  2.57441939e+01]\n",
      "Done with gradient descent at iteration  574\n",
      "Learned weights =  [-1.97759189e-01  2.39988046e+02  2.57441939e+01]\n",
      "Done with gradient descent at iteration  574\n",
      "Learned weights =  [-1.97759189e-01  2.40007790e+02  2.57441939e+01]\n",
      "Done with gradient descent at iteration  574\n",
      "Learned weights =  [-1.97759189e-01  2.40007790e+02  2.57227549e+01]\n",
      "Done with gradient descent at iteration  575\n",
      "Learned weights =  [-1.98139611e-01  2.40007790e+02  2.57227549e+01]\n",
      "Done with gradient descent at iteration  575\n",
      "Learned weights =  [-1.98139611e-01  2.40027416e+02  2.57227549e+01]\n",
      "Done with gradient descent at iteration  575\n",
      "Learned weights =  [-1.98139611e-01  2.40027416e+02  2.57014431e+01]\n",
      "Done with gradient descent at iteration  576\n",
      "Learned weights =  [-1.98519980e-01  2.40027416e+02  2.57014431e+01]\n",
      "Done with gradient descent at iteration  576\n",
      "Learned weights =  [-1.98519980e-01  2.40046926e+02  2.57014431e+01]\n",
      "Done with gradient descent at iteration  576\n",
      "Learned weights =  [-1.98519980e-01  2.40046926e+02  2.56802577e+01]\n",
      "Done with gradient descent at iteration  577\n",
      "Learned weights =  [-1.98900298e-01  2.40046926e+02  2.56802577e+01]\n",
      "Done with gradient descent at iteration  577\n",
      "Learned weights =  [-1.98900298e-01  2.40066321e+02  2.56802577e+01]\n",
      "Done with gradient descent at iteration  577\n",
      "Learned weights =  [-1.98900298e-01  2.40066321e+02  2.56591980e+01]\n",
      "Done with gradient descent at iteration  578\n",
      "Learned weights =  [-1.99280564e-01  2.40066321e+02  2.56591980e+01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  578\n",
      "Learned weights =  [-1.99280564e-01  2.40085600e+02  2.56591980e+01]\n",
      "Done with gradient descent at iteration  578\n",
      "Learned weights =  [-1.99280564e-01  2.40085600e+02  2.56382632e+01]\n",
      "Done with gradient descent at iteration  579\n",
      "Learned weights =  [-1.99660779e-01  2.40085600e+02  2.56382632e+01]\n",
      "Done with gradient descent at iteration  579\n",
      "Learned weights =  [-1.99660779e-01  2.40104765e+02  2.56382632e+01]\n",
      "Done with gradient descent at iteration  579\n",
      "Learned weights =  [-1.99660779e-01  2.40104765e+02  2.56174525e+01]\n",
      "Done with gradient descent at iteration  580\n",
      "Learned weights =  [-2.00040943e-01  2.40104765e+02  2.56174525e+01]\n",
      "Done with gradient descent at iteration  580\n",
      "Learned weights =  [-2.00040943e-01  2.40123816e+02  2.56174525e+01]\n",
      "Done with gradient descent at iteration  580\n",
      "Learned weights =  [-2.00040943e-01  2.40123816e+02  2.55967653e+01]\n",
      "Done with gradient descent at iteration  581\n",
      "Learned weights =  [-2.00421056e-01  2.40123816e+02  2.55967653e+01]\n",
      "Done with gradient descent at iteration  581\n",
      "Learned weights =  [-2.00421056e-01  2.40142754e+02  2.55967653e+01]\n",
      "Done with gradient descent at iteration  581\n",
      "Learned weights =  [-2.00421056e-01  2.40142754e+02  2.55762008e+01]\n",
      "Done with gradient descent at iteration  582\n",
      "Learned weights =  [-2.00801118e-01  2.40142754e+02  2.55762008e+01]\n",
      "Done with gradient descent at iteration  582\n",
      "Learned weights =  [-2.00801118e-01  2.40161580e+02  2.55762008e+01]\n",
      "Done with gradient descent at iteration  582\n",
      "Learned weights =  [-2.00801118e-01  2.40161580e+02  2.55557583e+01]\n",
      "Done with gradient descent at iteration  583\n",
      "Learned weights =  [-2.01181131e-01  2.40161580e+02  2.55557583e+01]\n",
      "Done with gradient descent at iteration  583\n",
      "Learned weights =  [-2.01181131e-01  2.40180294e+02  2.55557583e+01]\n",
      "Done with gradient descent at iteration  583\n",
      "Learned weights =  [-2.01181131e-01  2.40180294e+02  2.55354371e+01]\n",
      "Done with gradient descent at iteration  584\n",
      "Learned weights =  [-2.01561094e-01  2.40180294e+02  2.55354371e+01]\n",
      "Done with gradient descent at iteration  584\n",
      "Learned weights =  [-2.01561094e-01  2.40198897e+02  2.55354371e+01]\n",
      "Done with gradient descent at iteration  584\n",
      "Learned weights =  [-2.01561094e-01  2.40198897e+02  2.55152364e+01]\n",
      "Done with gradient descent at iteration  585\n",
      "Learned weights =  [-2.01941007e-01  2.40198897e+02  2.55152364e+01]\n",
      "Done with gradient descent at iteration  585\n",
      "Learned weights =  [-2.01941007e-01  2.40217390e+02  2.55152364e+01]\n",
      "Done with gradient descent at iteration  585\n",
      "Learned weights =  [-2.01941007e-01  2.40217390e+02  2.54951555e+01]\n",
      "Done with gradient descent at iteration  586\n",
      "Learned weights =  [-2.02320871e-01  2.40217390e+02  2.54951555e+01]\n",
      "Done with gradient descent at iteration  586\n",
      "Learned weights =  [-2.02320871e-01  2.40235773e+02  2.54951555e+01]\n",
      "Done with gradient descent at iteration  586\n",
      "Learned weights =  [-2.02320871e-01  2.40235773e+02  2.54751937e+01]\n",
      "Done with gradient descent at iteration  587\n",
      "Learned weights =  [-2.02700686e-01  2.40235773e+02  2.54751937e+01]\n",
      "Done with gradient descent at iteration  587\n",
      "Learned weights =  [-2.02700686e-01  2.40254047e+02  2.54751937e+01]\n",
      "Done with gradient descent at iteration  587\n",
      "Learned weights =  [-2.02700686e-01  2.40254047e+02  2.54553503e+01]\n",
      "Done with gradient descent at iteration  588\n",
      "Learned weights =  [-2.03080453e-01  2.40254047e+02  2.54553503e+01]\n",
      "Done with gradient descent at iteration  588\n",
      "Learned weights =  [-2.03080453e-01  2.40272213e+02  2.54553503e+01]\n",
      "Done with gradient descent at iteration  588\n",
      "Learned weights =  [-2.03080453e-01  2.40272213e+02  2.54356247e+01]\n",
      "Done with gradient descent at iteration  589\n",
      "Learned weights =  [-2.03460171e-01  2.40272213e+02  2.54356247e+01]\n",
      "Done with gradient descent at iteration  589\n",
      "Learned weights =  [-2.03460171e-01  2.40290271e+02  2.54356247e+01]\n",
      "Done with gradient descent at iteration  589\n",
      "Learned weights =  [-2.03460171e-01  2.40290271e+02  2.54160160e+01]\n",
      "Done with gradient descent at iteration  590\n",
      "Learned weights =  [-2.03839841e-01  2.40290271e+02  2.54160160e+01]\n",
      "Done with gradient descent at iteration  590\n",
      "Learned weights =  [-2.03839841e-01  2.40308222e+02  2.54160160e+01]\n",
      "Done with gradient descent at iteration  590\n",
      "Learned weights =  [-2.03839841e-01  2.40308222e+02  2.53965237e+01]\n",
      "Done with gradient descent at iteration  591\n",
      "Learned weights =  [-2.04219463e-01  2.40308222e+02  2.53965237e+01]\n",
      "Done with gradient descent at iteration  591\n",
      "Learned weights =  [-2.04219463e-01  2.40326066e+02  2.53965237e+01]\n",
      "Done with gradient descent at iteration  591\n",
      "Learned weights =  [-2.04219463e-01  2.40326066e+02  2.53771469e+01]\n",
      "Done with gradient descent at iteration  592\n",
      "Learned weights =  [-2.04599038e-01  2.40326066e+02  2.53771469e+01]\n",
      "Done with gradient descent at iteration  592\n",
      "Learned weights =  [-2.04599038e-01  2.40343805e+02  2.53771469e+01]\n",
      "Done with gradient descent at iteration  592\n",
      "Learned weights =  [-2.04599038e-01  2.40343805e+02  2.53578851e+01]\n",
      "Done with gradient descent at iteration  593\n",
      "Learned weights =  [-2.04978566e-01  2.40343805e+02  2.53578851e+01]\n",
      "Done with gradient descent at iteration  593\n",
      "Learned weights =  [-2.04978566e-01  2.40361438e+02  2.53578851e+01]\n",
      "Done with gradient descent at iteration  593\n",
      "Learned weights =  [-2.04978566e-01  2.40361438e+02  2.53387376e+01]\n",
      "Done with gradient descent at iteration  594\n",
      "Learned weights =  [-2.05358047e-01  2.40361438e+02  2.53387376e+01]\n",
      "Done with gradient descent at iteration  594\n",
      "Learned weights =  [-2.05358047e-01  2.40378967e+02  2.53387376e+01]\n",
      "Done with gradient descent at iteration  594\n",
      "Learned weights =  [-2.05358047e-01  2.40378967e+02  2.53197036e+01]\n",
      "Done with gradient descent at iteration  595\n",
      "Learned weights =  [-2.05737481e-01  2.40378967e+02  2.53197036e+01]\n",
      "Done with gradient descent at iteration  595\n",
      "Learned weights =  [-2.05737481e-01  2.40396392e+02  2.53197036e+01]\n",
      "Done with gradient descent at iteration  595\n",
      "Learned weights =  [-2.05737481e-01  2.40396392e+02  2.53007826e+01]\n",
      "Done with gradient descent at iteration  596\n",
      "Learned weights =  [-2.06116869e-01  2.40396392e+02  2.53007826e+01]\n",
      "Done with gradient descent at iteration  596\n",
      "Learned weights =  [-2.06116869e-01  2.40413713e+02  2.53007826e+01]\n",
      "Done with gradient descent at iteration  596\n",
      "Learned weights =  [-2.06116869e-01  2.40413713e+02  2.52819737e+01]\n",
      "Done with gradient descent at iteration  597\n",
      "Learned weights =  [-2.06496211e-01  2.40413713e+02  2.52819737e+01]\n",
      "Done with gradient descent at iteration  597\n",
      "Learned weights =  [-2.06496211e-01  2.40430932e+02  2.52819737e+01]\n",
      "Done with gradient descent at iteration  597\n",
      "Learned weights =  [-2.06496211e-01  2.40430932e+02  2.52632765e+01]\n",
      "Done with gradient descent at iteration  598\n",
      "Learned weights =  [-2.06875506e-01  2.40430932e+02  2.52632765e+01]\n",
      "Done with gradient descent at iteration  598\n",
      "Learned weights =  [-2.06875506e-01  2.40448048e+02  2.52632765e+01]\n",
      "Done with gradient descent at iteration  598\n",
      "Learned weights =  [-2.06875506e-01  2.40448048e+02  2.52446901e+01]\n",
      "Done with gradient descent at iteration  599\n",
      "Learned weights =  [-2.07254757e-01  2.40448048e+02  2.52446901e+01]\n",
      "Done with gradient descent at iteration  599\n",
      "Learned weights =  [-2.07254757e-01  2.40465063e+02  2.52446901e+01]\n",
      "Done with gradient descent at iteration  599\n",
      "Learned weights =  [-2.07254757e-01  2.40465063e+02  2.52262140e+01]\n",
      "Iteration = 600\n",
      "Cost function =  1205660014471676.0\n",
      "Done with gradient descent at iteration  600\n",
      "Learned weights =  [-2.07633962e-01  2.40465063e+02  2.52262140e+01]\n",
      "Done with gradient descent at iteration  600\n",
      "Learned weights =  [-2.07633962e-01  2.40481977e+02  2.52262140e+01]\n",
      "Done with gradient descent at iteration  600\n",
      "Learned weights =  [-2.07633962e-01  2.40481977e+02  2.52078475e+01]\n",
      "Done with gradient descent at iteration  601\n",
      "Learned weights =  [-2.08013122e-01  2.40481977e+02  2.52078475e+01]\n",
      "Done with gradient descent at iteration  601\n",
      "Learned weights =  [-2.08013122e-01  2.40498791e+02  2.52078475e+01]\n",
      "Done with gradient descent at iteration  601\n",
      "Learned weights =  [-2.08013122e-01  2.40498791e+02  2.51895899e+01]\n",
      "Done with gradient descent at iteration  602\n",
      "Learned weights =  [-2.08392237e-01  2.40498791e+02  2.51895899e+01]\n",
      "Done with gradient descent at iteration  602\n",
      "Learned weights =  [-2.08392237e-01  2.40515505e+02  2.51895899e+01]\n",
      "Done with gradient descent at iteration  602\n",
      "Learned weights =  [-2.08392237e-01  2.40515505e+02  2.51714406e+01]\n",
      "Done with gradient descent at iteration  603\n",
      "Learned weights =  [-2.08771308e-01  2.40515505e+02  2.51714406e+01]\n",
      "Done with gradient descent at iteration  603\n",
      "Learned weights =  [-2.08771308e-01  2.40532120e+02  2.51714406e+01]\n",
      "Done with gradient descent at iteration  603\n",
      "Learned weights =  [-2.08771308e-01  2.40532120e+02  2.51533990e+01]\n",
      "Done with gradient descent at iteration  604\n",
      "Learned weights =  [-2.09150335e-01  2.40532120e+02  2.51533990e+01]\n",
      "Done with gradient descent at iteration  604\n",
      "Learned weights =  [-2.09150335e-01  2.40548636e+02  2.51533990e+01]\n",
      "Done with gradient descent at iteration  604\n",
      "Learned weights =  [-2.09150335e-01  2.40548636e+02  2.51354644e+01]\n",
      "Done with gradient descent at iteration  605\n",
      "Learned weights =  [-2.09529317e-01  2.40548636e+02  2.51354644e+01]\n",
      "Done with gradient descent at iteration  605\n",
      "Learned weights =  [-2.09529317e-01  2.40565054e+02  2.51354644e+01]\n",
      "Done with gradient descent at iteration  605\n",
      "Learned weights =  [-2.09529317e-01  2.40565054e+02  2.51176362e+01]\n",
      "Done with gradient descent at iteration  606\n",
      "Learned weights =  [-2.09908256e-01  2.40565054e+02  2.51176362e+01]\n",
      "Done with gradient descent at iteration  606\n",
      "Learned weights =  [-2.09908256e-01  2.40581375e+02  2.51176362e+01]\n",
      "Done with gradient descent at iteration  606\n",
      "Learned weights =  [-2.09908256e-01  2.40581375e+02  2.50999137e+01]\n",
      "Done with gradient descent at iteration  607\n",
      "Learned weights =  [-2.10287152e-01  2.40581375e+02  2.50999137e+01]\n",
      "Done with gradient descent at iteration  607\n",
      "Learned weights =  [-2.10287152e-01  2.40597599e+02  2.50999137e+01]\n",
      "Done with gradient descent at iteration  607\n",
      "Learned weights =  [-2.10287152e-01  2.40597599e+02  2.50822964e+01]\n",
      "Done with gradient descent at iteration  608\n",
      "Learned weights =  [-2.10666004e-01  2.40597599e+02  2.50822964e+01]\n",
      "Done with gradient descent at iteration  608\n",
      "Learned weights =  [-2.10666004e-01  2.40613727e+02  2.50822964e+01]\n",
      "Done with gradient descent at iteration  608\n",
      "Learned weights =  [-2.10666004e-01  2.40613727e+02  2.50647835e+01]\n",
      "Done with gradient descent at iteration  609\n",
      "Learned weights =  [-2.11044814e-01  2.40613727e+02  2.50647835e+01]\n",
      "Done with gradient descent at iteration  609\n",
      "Learned weights =  [-2.11044814e-01  2.40629760e+02  2.50647835e+01]\n",
      "Done with gradient descent at iteration  609\n",
      "Learned weights =  [-2.11044814e-01  2.40629760e+02  2.50473745e+01]\n",
      "Done with gradient descent at iteration  610\n",
      "Learned weights =  [-2.11423581e-01  2.40629760e+02  2.50473745e+01]\n",
      "Done with gradient descent at iteration  610\n",
      "Learned weights =  [-2.11423581e-01  2.40645697e+02  2.50473745e+01]\n",
      "Done with gradient descent at iteration  610\n",
      "Learned weights =  [-2.11423581e-01  2.40645697e+02  2.50300688e+01]\n",
      "Done with gradient descent at iteration  611\n",
      "Learned weights =  [-2.11802305e-01  2.40645697e+02  2.50300688e+01]\n",
      "Done with gradient descent at iteration  611\n",
      "Learned weights =  [-2.11802305e-01  2.40661539e+02  2.50300688e+01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  611\n",
      "Learned weights =  [-2.11802305e-01  2.40661539e+02  2.50128658e+01]\n",
      "Done with gradient descent at iteration  612\n",
      "Learned weights =  [-2.12180988e-01  2.40661539e+02  2.50128658e+01]\n",
      "Done with gradient descent at iteration  612\n",
      "Learned weights =  [-2.12180988e-01  2.40677288e+02  2.50128658e+01]\n",
      "Done with gradient descent at iteration  612\n",
      "Learned weights =  [-2.12180988e-01  2.40677288e+02  2.49957648e+01]\n",
      "Done with gradient descent at iteration  613\n",
      "Learned weights =  [-2.12559628e-01  2.40677288e+02  2.49957648e+01]\n",
      "Done with gradient descent at iteration  613\n",
      "Learned weights =  [-2.12559628e-01  2.40692943e+02  2.49957648e+01]\n",
      "Done with gradient descent at iteration  613\n",
      "Learned weights =  [-2.12559628e-01  2.40692943e+02  2.49787652e+01]\n",
      "Done with gradient descent at iteration  614\n",
      "Learned weights =  [-2.12938227e-01  2.40692943e+02  2.49787652e+01]\n",
      "Done with gradient descent at iteration  614\n",
      "Learned weights =  [-2.12938227e-01  2.40708506e+02  2.49787652e+01]\n",
      "Done with gradient descent at iteration  614\n",
      "Learned weights =  [-2.12938227e-01  2.40708506e+02  2.49618664e+01]\n",
      "Done with gradient descent at iteration  615\n",
      "Learned weights =  [-2.13316784e-01  2.40708506e+02  2.49618664e+01]\n",
      "Done with gradient descent at iteration  615\n",
      "Learned weights =  [-2.13316784e-01  2.40723976e+02  2.49618664e+01]\n",
      "Done with gradient descent at iteration  615\n",
      "Learned weights =  [-2.13316784e-01  2.40723976e+02  2.49450679e+01]\n",
      "Done with gradient descent at iteration  616\n",
      "Learned weights =  [-2.13695300e-01  2.40723976e+02  2.49450679e+01]\n",
      "Done with gradient descent at iteration  616\n",
      "Learned weights =  [-2.13695300e-01  2.40739354e+02  2.49450679e+01]\n",
      "Done with gradient descent at iteration  616\n",
      "Learned weights =  [-2.13695300e-01  2.40739354e+02  2.49283691e+01]\n",
      "Done with gradient descent at iteration  617\n",
      "Learned weights =  [-2.14073775e-01  2.40739354e+02  2.49283691e+01]\n",
      "Done with gradient descent at iteration  617\n",
      "Learned weights =  [-2.14073775e-01  2.40754641e+02  2.49283691e+01]\n",
      "Done with gradient descent at iteration  617\n",
      "Learned weights =  [-2.14073775e-01  2.40754641e+02  2.49117693e+01]\n",
      "Done with gradient descent at iteration  618\n",
      "Learned weights =  [-2.14452210e-01  2.40754641e+02  2.49117693e+01]\n",
      "Done with gradient descent at iteration  618\n",
      "Learned weights =  [-2.14452210e-01  2.40769837e+02  2.49117693e+01]\n",
      "Done with gradient descent at iteration  618\n",
      "Learned weights =  [-2.14452210e-01  2.40769837e+02  2.48952679e+01]\n",
      "Done with gradient descent at iteration  619\n",
      "Learned weights =  [-2.14830604e-01  2.40769837e+02  2.48952679e+01]\n",
      "Done with gradient descent at iteration  619\n",
      "Learned weights =  [-2.14830604e-01  2.40784944e+02  2.48952679e+01]\n",
      "Done with gradient descent at iteration  619\n",
      "Learned weights =  [-2.14830604e-01  2.40784944e+02  2.48788644e+01]\n",
      "Done with gradient descent at iteration  620\n",
      "Learned weights =  [-2.15208958e-01  2.40784944e+02  2.48788644e+01]\n",
      "Done with gradient descent at iteration  620\n",
      "Learned weights =  [-2.15208958e-01  2.40799960e+02  2.48788644e+01]\n",
      "Done with gradient descent at iteration  620\n",
      "Learned weights =  [-2.15208958e-01  2.40799960e+02  2.48625583e+01]\n",
      "Done with gradient descent at iteration  621\n",
      "Learned weights =  [-2.15587272e-01  2.40799960e+02  2.48625583e+01]\n",
      "Done with gradient descent at iteration  621\n",
      "Learned weights =  [-2.15587272e-01  2.40814888e+02  2.48625583e+01]\n",
      "Done with gradient descent at iteration  621\n",
      "Learned weights =  [-2.15587272e-01  2.40814888e+02  2.48463488e+01]\n",
      "Done with gradient descent at iteration  622\n",
      "Learned weights =  [-2.15965546e-01  2.40814888e+02  2.48463488e+01]\n",
      "Done with gradient descent at iteration  622\n",
      "Learned weights =  [-2.15965546e-01  2.40829727e+02  2.48463488e+01]\n",
      "Done with gradient descent at iteration  622\n",
      "Learned weights =  [-2.15965546e-01  2.40829727e+02  2.48302355e+01]\n",
      "Done with gradient descent at iteration  623\n",
      "Learned weights =  [-2.16343781e-01  2.40829727e+02  2.48302355e+01]\n",
      "Done with gradient descent at iteration  623\n",
      "Learned weights =  [-2.16343781e-01  2.40844478e+02  2.48302355e+01]\n",
      "Done with gradient descent at iteration  623\n",
      "Learned weights =  [-2.16343781e-01  2.40844478e+02  2.48142178e+01]\n",
      "Done with gradient descent at iteration  624\n",
      "Learned weights =  [-2.16721976e-01  2.40844478e+02  2.48142178e+01]\n",
      "Done with gradient descent at iteration  624\n",
      "Learned weights =  [-2.16721976e-01  2.40859142e+02  2.48142178e+01]\n",
      "Done with gradient descent at iteration  624\n",
      "Learned weights =  [-2.16721976e-01  2.40859142e+02  2.47982951e+01]\n",
      "Done with gradient descent at iteration  625\n",
      "Learned weights =  [-2.17100133e-01  2.40859142e+02  2.47982951e+01]\n",
      "Done with gradient descent at iteration  625\n",
      "Learned weights =  [-2.17100133e-01  2.40873718e+02  2.47982951e+01]\n",
      "Done with gradient descent at iteration  625\n",
      "Learned weights =  [-2.17100133e-01  2.40873718e+02  2.47824668e+01]\n",
      "Done with gradient descent at iteration  626\n",
      "Learned weights =  [-2.17478251e-01  2.40873718e+02  2.47824668e+01]\n",
      "Done with gradient descent at iteration  626\n",
      "Learned weights =  [-2.17478251e-01  2.40888208e+02  2.47824668e+01]\n",
      "Done with gradient descent at iteration  626\n",
      "Learned weights =  [-2.17478251e-01  2.40888208e+02  2.47667325e+01]\n",
      "Done with gradient descent at iteration  627\n",
      "Learned weights =  [-2.17856330e-01  2.40888208e+02  2.47667325e+01]\n",
      "Done with gradient descent at iteration  627\n",
      "Learned weights =  [-2.17856330e-01  2.40902612e+02  2.47667325e+01]\n",
      "Done with gradient descent at iteration  627\n",
      "Learned weights =  [-2.17856330e-01  2.40902612e+02  2.47510914e+01]\n",
      "Done with gradient descent at iteration  628\n",
      "Learned weights =  [-2.18234371e-01  2.40902612e+02  2.47510914e+01]\n",
      "Done with gradient descent at iteration  628\n",
      "Learned weights =  [-2.18234371e-01  2.40916931e+02  2.47510914e+01]\n",
      "Done with gradient descent at iteration  628\n",
      "Learned weights =  [-2.18234371e-01  2.40916931e+02  2.47355432e+01]\n",
      "Done with gradient descent at iteration  629\n",
      "Learned weights =  [-2.18612373e-01  2.40916931e+02  2.47355432e+01]\n",
      "Done with gradient descent at iteration  629\n",
      "Learned weights =  [-2.18612373e-01  2.40931165e+02  2.47355432e+01]\n",
      "Done with gradient descent at iteration  629\n",
      "Learned weights =  [-2.18612373e-01  2.40931165e+02  2.47200871e+01]\n",
      "Done with gradient descent at iteration  630\n",
      "Learned weights =  [-2.18990338e-01  2.40931165e+02  2.47200871e+01]\n",
      "Done with gradient descent at iteration  630\n",
      "Learned weights =  [-2.18990338e-01  2.40945314e+02  2.47200871e+01]\n",
      "Done with gradient descent at iteration  630\n",
      "Learned weights =  [-2.18990338e-01  2.40945314e+02  2.47047228e+01]\n",
      "Done with gradient descent at iteration  631\n",
      "Learned weights =  [-2.19368265e-01  2.40945314e+02  2.47047228e+01]\n",
      "Done with gradient descent at iteration  631\n",
      "Learned weights =  [-2.19368265e-01  2.40959380e+02  2.47047228e+01]\n",
      "Done with gradient descent at iteration  631\n",
      "Learned weights =  [-2.19368265e-01  2.40959380e+02  2.46894495e+01]\n",
      "Done with gradient descent at iteration  632\n",
      "Learned weights =  [-2.19746155e-01  2.40959380e+02  2.46894495e+01]\n",
      "Done with gradient descent at iteration  632\n",
      "Learned weights =  [-2.19746155e-01  2.40973362e+02  2.46894495e+01]\n",
      "Done with gradient descent at iteration  632\n",
      "Learned weights =  [-2.19746155e-01  2.40973362e+02  2.46742669e+01]\n",
      "Done with gradient descent at iteration  633\n",
      "Learned weights =  [-2.20124008e-01  2.40973362e+02  2.46742669e+01]\n",
      "Done with gradient descent at iteration  633\n",
      "Learned weights =  [-2.20124008e-01  2.40987261e+02  2.46742669e+01]\n",
      "Done with gradient descent at iteration  633\n",
      "Learned weights =  [-2.20124008e-01  2.40987261e+02  2.46591743e+01]\n",
      "Done with gradient descent at iteration  634\n",
      "Learned weights =  [-2.20501823e-01  2.40987261e+02  2.46591743e+01]\n",
      "Done with gradient descent at iteration  634\n",
      "Learned weights =  [-2.20501823e-01  2.41001077e+02  2.46591743e+01]\n",
      "Done with gradient descent at iteration  634\n",
      "Learned weights =  [-2.20501823e-01  2.41001077e+02  2.46441713e+01]\n",
      "Done with gradient descent at iteration  635\n",
      "Learned weights =  [-2.20879602e-01  2.41001077e+02  2.46441713e+01]\n",
      "Done with gradient descent at iteration  635\n",
      "Learned weights =  [-2.20879602e-01  2.41014812e+02  2.46441713e+01]\n",
      "Done with gradient descent at iteration  635\n",
      "Learned weights =  [-2.20879602e-01  2.41014812e+02  2.46292572e+01]\n",
      "Done with gradient descent at iteration  636\n",
      "Learned weights =  [-2.21257344e-01  2.41014812e+02  2.46292572e+01]\n",
      "Done with gradient descent at iteration  636\n",
      "Learned weights =  [-2.21257344e-01  2.41028465e+02  2.46292572e+01]\n",
      "Done with gradient descent at iteration  636\n",
      "Learned weights =  [-2.21257344e-01  2.41028465e+02  2.46144317e+01]\n",
      "Done with gradient descent at iteration  637\n",
      "Learned weights =  [-2.21635050e-01  2.41028465e+02  2.46144317e+01]\n",
      "Done with gradient descent at iteration  637\n",
      "Learned weights =  [-2.21635050e-01  2.41042037e+02  2.46144317e+01]\n",
      "Done with gradient descent at iteration  637\n",
      "Learned weights =  [-2.21635050e-01  2.41042037e+02  2.45996940e+01]\n",
      "Done with gradient descent at iteration  638\n",
      "Learned weights =  [-2.22012720e-01  2.41042037e+02  2.45996940e+01]\n",
      "Done with gradient descent at iteration  638\n",
      "Learned weights =  [-2.22012720e-01  2.41055529e+02  2.45996940e+01]\n",
      "Done with gradient descent at iteration  638\n",
      "Learned weights =  [-2.22012720e-01  2.41055529e+02  2.45850438e+01]\n",
      "Done with gradient descent at iteration  639\n",
      "Learned weights =  [-2.22390354e-01  2.41055529e+02  2.45850438e+01]\n",
      "Done with gradient descent at iteration  639\n",
      "Learned weights =  [-2.22390354e-01  2.41068941e+02  2.45850438e+01]\n",
      "Done with gradient descent at iteration  639\n",
      "Learned weights =  [-2.22390354e-01  2.41068941e+02  2.45704804e+01]\n",
      "Done with gradient descent at iteration  640\n",
      "Learned weights =  [-2.22767952e-01  2.41068941e+02  2.45704804e+01]\n",
      "Done with gradient descent at iteration  640\n",
      "Learned weights =  [-2.22767952e-01  2.41082273e+02  2.45704804e+01]\n",
      "Done with gradient descent at iteration  640\n",
      "Learned weights =  [-2.22767952e-01  2.41082273e+02  2.45560035e+01]\n",
      "Done with gradient descent at iteration  641\n",
      "Learned weights =  [-2.23145515e-01  2.41082273e+02  2.45560035e+01]\n",
      "Done with gradient descent at iteration  641\n",
      "Learned weights =  [-2.23145515e-01  2.41095526e+02  2.45560035e+01]\n",
      "Done with gradient descent at iteration  641\n",
      "Learned weights =  [-2.23145515e-01  2.41095526e+02  2.45416124e+01]\n",
      "Done with gradient descent at iteration  642\n",
      "Learned weights =  [-2.23523043e-01  2.41095526e+02  2.45416124e+01]\n",
      "Done with gradient descent at iteration  642\n",
      "Learned weights =  [-2.23523043e-01  2.41108700e+02  2.45416124e+01]\n",
      "Done with gradient descent at iteration  642\n",
      "Learned weights =  [-2.23523043e-01  2.41108700e+02  2.45273067e+01]\n",
      "Done with gradient descent at iteration  643\n",
      "Learned weights =  [-2.23900535e-01  2.41108700e+02  2.45273067e+01]\n",
      "Done with gradient descent at iteration  643\n",
      "Learned weights =  [-2.23900535e-01  2.41121797e+02  2.45273067e+01]\n",
      "Done with gradient descent at iteration  643\n",
      "Learned weights =  [-2.23900535e-01  2.41121797e+02  2.45130859e+01]\n",
      "Done with gradient descent at iteration  644\n",
      "Learned weights =  [-2.24277993e-01  2.41121797e+02  2.45130859e+01]\n",
      "Done with gradient descent at iteration  644\n",
      "Learned weights =  [-2.24277993e-01  2.41134815e+02  2.45130859e+01]\n",
      "Done with gradient descent at iteration  644\n",
      "Learned weights =  [-2.24277993e-01  2.41134815e+02  2.44989494e+01]\n",
      "Done with gradient descent at iteration  645\n",
      "Learned weights =  [-2.24655416e-01  2.41134815e+02  2.44989494e+01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  645\n",
      "Learned weights =  [-2.24655416e-01  2.41147757e+02  2.44989494e+01]\n",
      "Done with gradient descent at iteration  645\n",
      "Learned weights =  [-2.24655416e-01  2.41147757e+02  2.44848967e+01]\n",
      "Done with gradient descent at iteration  646\n",
      "Learned weights =  [-2.25032804e-01  2.41147757e+02  2.44848967e+01]\n",
      "Done with gradient descent at iteration  646\n",
      "Learned weights =  [-2.25032804e-01  2.41160621e+02  2.44848967e+01]\n",
      "Done with gradient descent at iteration  646\n",
      "Learned weights =  [-2.25032804e-01  2.41160621e+02  2.44709274e+01]\n",
      "Done with gradient descent at iteration  647\n",
      "Learned weights =  [-2.25410159e-01  2.41160621e+02  2.44709274e+01]\n",
      "Done with gradient descent at iteration  647\n",
      "Learned weights =  [-2.25410159e-01  2.41173409e+02  2.44709274e+01]\n",
      "Done with gradient descent at iteration  647\n",
      "Learned weights =  [-2.25410159e-01  2.41173409e+02  2.44570410e+01]\n",
      "Done with gradient descent at iteration  648\n",
      "Learned weights =  [-2.25787479e-01  2.41173409e+02  2.44570410e+01]\n",
      "Done with gradient descent at iteration  648\n",
      "Learned weights =  [-2.25787479e-01  2.41186122e+02  2.44570410e+01]\n",
      "Done with gradient descent at iteration  648\n",
      "Learned weights =  [-2.25787479e-01  2.41186122e+02  2.44432369e+01]\n",
      "Done with gradient descent at iteration  649\n",
      "Learned weights =  [-2.26164765e-01  2.41186122e+02  2.44432369e+01]\n",
      "Done with gradient descent at iteration  649\n",
      "Learned weights =  [-2.26164765e-01  2.41198759e+02  2.44432369e+01]\n",
      "Done with gradient descent at iteration  649\n",
      "Learned weights =  [-2.26164765e-01  2.41198759e+02  2.44295147e+01]\n",
      "Done with gradient descent at iteration  650\n",
      "Learned weights =  [-2.26542018e-01  2.41198759e+02  2.44295147e+01]\n",
      "Done with gradient descent at iteration  650\n",
      "Learned weights =  [-2.26542018e-01  2.41211321e+02  2.44295147e+01]\n",
      "Done with gradient descent at iteration  650\n",
      "Learned weights =  [-2.26542018e-01  2.41211321e+02  2.44158739e+01]\n",
      "Done with gradient descent at iteration  651\n",
      "Learned weights =  [-2.26919237e-01  2.41211321e+02  2.44158739e+01]\n",
      "Done with gradient descent at iteration  651\n",
      "Learned weights =  [-2.26919237e-01  2.41223809e+02  2.44158739e+01]\n",
      "Done with gradient descent at iteration  651\n",
      "Learned weights =  [-2.26919237e-01  2.41223809e+02  2.44023141e+01]\n",
      "Done with gradient descent at iteration  652\n",
      "Learned weights =  [-2.27296424e-01  2.41223809e+02  2.44023141e+01]\n",
      "Done with gradient descent at iteration  652\n",
      "Learned weights =  [-2.27296424e-01  2.41236222e+02  2.44023141e+01]\n",
      "Done with gradient descent at iteration  652\n",
      "Learned weights =  [-2.27296424e-01  2.41236222e+02  2.43888346e+01]\n",
      "Done with gradient descent at iteration  653\n",
      "Learned weights =  [-2.27673577e-01  2.41236222e+02  2.43888346e+01]\n",
      "Done with gradient descent at iteration  653\n",
      "Learned weights =  [-2.27673577e-01  2.41248562e+02  2.43888346e+01]\n",
      "Done with gradient descent at iteration  653\n",
      "Learned weights =  [-2.27673577e-01  2.41248562e+02  2.43754351e+01]\n",
      "Done with gradient descent at iteration  654\n",
      "Learned weights =  [-2.28050697e-01  2.41248562e+02  2.43754351e+01]\n",
      "Done with gradient descent at iteration  654\n",
      "Learned weights =  [-2.28050697e-01  2.41260829e+02  2.43754351e+01]\n",
      "Done with gradient descent at iteration  654\n",
      "Learned weights =  [-2.28050697e-01  2.41260829e+02  2.43621151e+01]\n",
      "Done with gradient descent at iteration  655\n",
      "Learned weights =  [-2.28427785e-01  2.41260829e+02  2.43621151e+01]\n",
      "Done with gradient descent at iteration  655\n",
      "Learned weights =  [-2.28427785e-01  2.41273023e+02  2.43621151e+01]\n",
      "Done with gradient descent at iteration  655\n",
      "Learned weights =  [-2.28427785e-01  2.41273023e+02  2.43488741e+01]\n",
      "Done with gradient descent at iteration  656\n",
      "Learned weights =  [-2.28804840e-01  2.41273023e+02  2.43488741e+01]\n",
      "Done with gradient descent at iteration  656\n",
      "Learned weights =  [-2.28804840e-01  2.41285144e+02  2.43488741e+01]\n",
      "Done with gradient descent at iteration  656\n",
      "Learned weights =  [-2.28804840e-01  2.41285144e+02  2.43357117e+01]\n",
      "Done with gradient descent at iteration  657\n",
      "Learned weights =  [-2.29181863e-01  2.41285144e+02  2.43357117e+01]\n",
      "Done with gradient descent at iteration  657\n",
      "Learned weights =  [-2.29181863e-01  2.41297194e+02  2.43357117e+01]\n",
      "Done with gradient descent at iteration  657\n",
      "Learned weights =  [-2.29181863e-01  2.41297194e+02  2.43226273e+01]\n",
      "Done with gradient descent at iteration  658\n",
      "Learned weights =  [-2.29558854e-01  2.41297194e+02  2.43226273e+01]\n",
      "Done with gradient descent at iteration  658\n",
      "Learned weights =  [-2.29558854e-01  2.41309172e+02  2.43226273e+01]\n",
      "Done with gradient descent at iteration  658\n",
      "Learned weights =  [-2.29558854e-01  2.41309172e+02  2.43096205e+01]\n",
      "Done with gradient descent at iteration  659\n",
      "Learned weights =  [-2.29935813e-01  2.41309172e+02  2.43096205e+01]\n",
      "Done with gradient descent at iteration  659\n",
      "Learned weights =  [-2.29935813e-01  2.41321079e+02  2.43096205e+01]\n",
      "Done with gradient descent at iteration  659\n",
      "Learned weights =  [-2.29935813e-01  2.41321079e+02  2.42966909e+01]\n",
      "Done with gradient descent at iteration  660\n",
      "Learned weights =  [-2.30312740e-01  2.41321079e+02  2.42966909e+01]\n",
      "Done with gradient descent at iteration  660\n",
      "Learned weights =  [-2.30312740e-01  2.41332916e+02  2.42966909e+01]\n",
      "Done with gradient descent at iteration  660\n",
      "Learned weights =  [-2.30312740e-01  2.41332916e+02  2.42838380e+01]\n",
      "Done with gradient descent at iteration  661\n",
      "Learned weights =  [-2.30689636e-01  2.41332916e+02  2.42838380e+01]\n",
      "Done with gradient descent at iteration  661\n",
      "Learned weights =  [-2.30689636e-01  2.41344682e+02  2.42838380e+01]\n",
      "Done with gradient descent at iteration  661\n",
      "Learned weights =  [-2.30689636e-01  2.41344682e+02  2.42710613e+01]\n",
      "Done with gradient descent at iteration  662\n",
      "Learned weights =  [-2.31066501e-01  2.41344682e+02  2.42710613e+01]\n",
      "Done with gradient descent at iteration  662\n",
      "Learned weights =  [-2.31066501e-01  2.41356379e+02  2.42710613e+01]\n",
      "Done with gradient descent at iteration  662\n",
      "Learned weights =  [-2.31066501e-01  2.41356379e+02  2.42583604e+01]\n",
      "Done with gradient descent at iteration  663\n",
      "Learned weights =  [-2.31443334e-01  2.41356379e+02  2.42583604e+01]\n",
      "Done with gradient descent at iteration  663\n",
      "Learned weights =  [-2.31443334e-01  2.41368006e+02  2.42583604e+01]\n",
      "Done with gradient descent at iteration  663\n",
      "Learned weights =  [-2.31443334e-01  2.41368006e+02  2.42457348e+01]\n",
      "Done with gradient descent at iteration  664\n",
      "Learned weights =  [-2.31820137e-01  2.41368006e+02  2.42457348e+01]\n",
      "Done with gradient descent at iteration  664\n",
      "Learned weights =  [-2.31820137e-01  2.41379564e+02  2.42457348e+01]\n",
      "Done with gradient descent at iteration  664\n",
      "Learned weights =  [-2.31820137e-01  2.41379564e+02  2.42331842e+01]\n",
      "Done with gradient descent at iteration  665\n",
      "Learned weights =  [-2.32196908e-01  2.41379564e+02  2.42331842e+01]\n",
      "Done with gradient descent at iteration  665\n",
      "Learned weights =  [-2.32196908e-01  2.41391054e+02  2.42331842e+01]\n",
      "Done with gradient descent at iteration  665\n",
      "Learned weights =  [-2.32196908e-01  2.41391054e+02  2.42207079e+01]\n",
      "Done with gradient descent at iteration  666\n",
      "Learned weights =  [-2.32573649e-01  2.41391054e+02  2.42207079e+01]\n",
      "Done with gradient descent at iteration  666\n",
      "Learned weights =  [-2.32573649e-01  2.41402475e+02  2.42207079e+01]\n",
      "Done with gradient descent at iteration  666\n",
      "Learned weights =  [-2.32573649e-01  2.41402475e+02  2.42083057e+01]\n",
      "Done with gradient descent at iteration  667\n",
      "Learned weights =  [-2.32950360e-01  2.41402475e+02  2.42083057e+01]\n",
      "Done with gradient descent at iteration  667\n",
      "Learned weights =  [-2.32950360e-01  2.41413829e+02  2.42083057e+01]\n",
      "Done with gradient descent at iteration  667\n",
      "Learned weights =  [-2.32950360e-01  2.41413829e+02  2.41959771e+01]\n",
      "Done with gradient descent at iteration  668\n",
      "Learned weights =  [-2.33327041e-01  2.41413829e+02  2.41959771e+01]\n",
      "Done with gradient descent at iteration  668\n",
      "Learned weights =  [-2.33327041e-01  2.41425115e+02  2.41959771e+01]\n",
      "Done with gradient descent at iteration  668\n",
      "Learned weights =  [-2.33327041e-01  2.41425115e+02  2.41837215e+01]\n",
      "Done with gradient descent at iteration  669\n",
      "Learned weights =  [-2.33703691e-01  2.41425115e+02  2.41837215e+01]\n",
      "Done with gradient descent at iteration  669\n",
      "Learned weights =  [-2.33703691e-01  2.41436335e+02  2.41837215e+01]\n",
      "Done with gradient descent at iteration  669\n",
      "Learned weights =  [-2.33703691e-01  2.41436335e+02  2.41715387e+01]\n",
      "Done with gradient descent at iteration  670\n",
      "Learned weights =  [-2.34080312e-01  2.41436335e+02  2.41715387e+01]\n",
      "Done with gradient descent at iteration  670\n",
      "Learned weights =  [-2.34080312e-01  2.41447488e+02  2.41715387e+01]\n",
      "Done with gradient descent at iteration  670\n",
      "Learned weights =  [-2.34080312e-01  2.41447488e+02  2.41594281e+01]\n",
      "Done with gradient descent at iteration  671\n",
      "Learned weights =  [-2.34456903e-01  2.41447488e+02  2.41594281e+01]\n",
      "Done with gradient descent at iteration  671\n",
      "Learned weights =  [-2.34456903e-01  2.41458574e+02  2.41594281e+01]\n",
      "Done with gradient descent at iteration  671\n",
      "Learned weights =  [-2.34456903e-01  2.41458574e+02  2.41473894e+01]\n",
      "Done with gradient descent at iteration  672\n",
      "Learned weights =  [-2.34833464e-01  2.41458574e+02  2.41473894e+01]\n",
      "Done with gradient descent at iteration  672\n",
      "Learned weights =  [-2.34833464e-01  2.41469595e+02  2.41473894e+01]\n",
      "Done with gradient descent at iteration  672\n",
      "Learned weights =  [-2.34833464e-01  2.41469595e+02  2.41354221e+01]\n",
      "Done with gradient descent at iteration  673\n",
      "Learned weights =  [-2.35209997e-01  2.41469595e+02  2.41354221e+01]\n",
      "Done with gradient descent at iteration  673\n",
      "Learned weights =  [-2.35209997e-01  2.41480551e+02  2.41354221e+01]\n",
      "Done with gradient descent at iteration  673\n",
      "Learned weights =  [-2.35209997e-01  2.41480551e+02  2.41235257e+01]\n",
      "Done with gradient descent at iteration  674\n",
      "Learned weights =  [-2.35586500e-01  2.41480551e+02  2.41235257e+01]\n",
      "Done with gradient descent at iteration  674\n",
      "Learned weights =  [-2.35586500e-01  2.41491442e+02  2.41235257e+01]\n",
      "Done with gradient descent at iteration  674\n",
      "Learned weights =  [-2.35586500e-01  2.41491442e+02  2.41117000e+01]\n",
      "Done with gradient descent at iteration  675\n",
      "Learned weights =  [-2.35962974e-01  2.41491442e+02  2.41117000e+01]\n",
      "Done with gradient descent at iteration  675\n",
      "Learned weights =  [-2.35962974e-01  2.41502268e+02  2.41117000e+01]\n",
      "Done with gradient descent at iteration  675\n",
      "Learned weights =  [-2.35962974e-01  2.41502268e+02  2.40999444e+01]\n",
      "Done with gradient descent at iteration  676\n",
      "Learned weights =  [-2.36339419e-01  2.41502268e+02  2.40999444e+01]\n",
      "Done with gradient descent at iteration  676\n",
      "Learned weights =  [-2.36339419e-01  2.41513029e+02  2.40999444e+01]\n",
      "Done with gradient descent at iteration  676\n",
      "Learned weights =  [-2.36339419e-01  2.41513029e+02  2.40882585e+01]\n",
      "Done with gradient descent at iteration  677\n",
      "Learned weights =  [-2.36715835e-01  2.41513029e+02  2.40882585e+01]\n",
      "Done with gradient descent at iteration  677\n",
      "Learned weights =  [-2.36715835e-01  2.41523727e+02  2.40882585e+01]\n",
      "Done with gradient descent at iteration  677\n",
      "Learned weights =  [-2.36715835e-01  2.41523727e+02  2.40766419e+01]\n",
      "Done with gradient descent at iteration  678\n",
      "Learned weights =  [-2.37092224e-01  2.41523727e+02  2.40766419e+01]\n",
      "Done with gradient descent at iteration  678\n",
      "Learned weights =  [-2.37092224e-01  2.41534362e+02  2.40766419e+01]\n",
      "Done with gradient descent at iteration  678\n",
      "Learned weights =  [-2.37092224e-01  2.41534362e+02  2.40650942e+01]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  679\n",
      "Learned weights =  [-2.37468583e-01  2.41534362e+02  2.40650942e+01]\n",
      "Done with gradient descent at iteration  679\n",
      "Learned weights =  [-2.37468583e-01  2.41544933e+02  2.40650942e+01]\n",
      "Done with gradient descent at iteration  679\n",
      "Learned weights =  [-2.37468583e-01  2.41544933e+02  2.40536150e+01]\n",
      "Done with gradient descent at iteration  680\n",
      "Learned weights =  [-2.37844915e-01  2.41544933e+02  2.40536150e+01]\n",
      "Done with gradient descent at iteration  680\n",
      "Learned weights =  [-2.37844915e-01  2.41555442e+02  2.40536150e+01]\n",
      "Done with gradient descent at iteration  680\n",
      "Learned weights =  [-2.37844915e-01  2.41555442e+02  2.40422040e+01]\n",
      "Done with gradient descent at iteration  681\n",
      "Learned weights =  [-2.38221219e-01  2.41555442e+02  2.40422040e+01]\n",
      "Done with gradient descent at iteration  681\n",
      "Learned weights =  [-2.38221219e-01  2.41565888e+02  2.40422040e+01]\n",
      "Done with gradient descent at iteration  681\n",
      "Learned weights =  [-2.38221219e-01  2.41565888e+02  2.40308606e+01]\n",
      "Done with gradient descent at iteration  682\n",
      "Learned weights =  [-2.38597495e-01  2.41565888e+02  2.40308606e+01]\n",
      "Done with gradient descent at iteration  682\n",
      "Learned weights =  [-2.38597495e-01  2.41576273e+02  2.40308606e+01]\n",
      "Done with gradient descent at iteration  682\n",
      "Learned weights =  [-2.38597495e-01  2.41576273e+02  2.40195845e+01]\n",
      "Done with gradient descent at iteration  683\n",
      "Learned weights =  [-2.38973743e-01  2.41576273e+02  2.40195845e+01]\n",
      "Done with gradient descent at iteration  683\n",
      "Learned weights =  [-2.38973743e-01  2.41586596e+02  2.40195845e+01]\n",
      "Done with gradient descent at iteration  683\n",
      "Learned weights =  [-2.38973743e-01  2.41586596e+02  2.40083752e+01]\n",
      "Done with gradient descent at iteration  684\n",
      "Learned weights =  [-2.39349964e-01  2.41586596e+02  2.40083752e+01]\n",
      "Done with gradient descent at iteration  684\n",
      "Learned weights =  [-2.39349964e-01  2.41596857e+02  2.40083752e+01]\n",
      "Done with gradient descent at iteration  684\n",
      "Learned weights =  [-2.39349964e-01  2.41596857e+02  2.39972325e+01]\n",
      "Done with gradient descent at iteration  685\n",
      "Learned weights =  [-2.39726157e-01  2.41596857e+02  2.39972325e+01]\n",
      "Done with gradient descent at iteration  685\n",
      "Learned weights =  [-2.39726157e-01  2.41607058e+02  2.39972325e+01]\n",
      "Done with gradient descent at iteration  685\n",
      "Learned weights =  [-2.39726157e-01  2.41607058e+02  2.39861558e+01]\n",
      "Done with gradient descent at iteration  686\n",
      "Learned weights =  [-2.40102324e-01  2.41607058e+02  2.39861558e+01]\n",
      "Done with gradient descent at iteration  686\n",
      "Learned weights =  [-2.40102324e-01  2.41617198e+02  2.39861558e+01]\n",
      "Done with gradient descent at iteration  686\n",
      "Learned weights =  [-2.40102324e-01  2.41617198e+02  2.39751449e+01]\n",
      "Done with gradient descent at iteration  687\n",
      "Learned weights =  [-2.40478463e-01  2.41617198e+02  2.39751449e+01]\n",
      "Done with gradient descent at iteration  687\n",
      "Learned weights =  [-2.40478463e-01  2.41627278e+02  2.39751449e+01]\n",
      "Done with gradient descent at iteration  687\n",
      "Learned weights =  [-2.40478463e-01  2.41627278e+02  2.39641993e+01]\n",
      "Done with gradient descent at iteration  688\n",
      "Learned weights =  [-2.40854576e-01  2.41627278e+02  2.39641993e+01]\n",
      "Done with gradient descent at iteration  688\n",
      "Learned weights =  [-2.40854576e-01  2.41637299e+02  2.39641993e+01]\n",
      "Done with gradient descent at iteration  688\n",
      "Learned weights =  [-2.40854576e-01  2.41637299e+02  2.39533186e+01]\n",
      "Done with gradient descent at iteration  689\n",
      "Learned weights =  [-2.41230662e-01  2.41637299e+02  2.39533186e+01]\n",
      "Done with gradient descent at iteration  689\n",
      "Learned weights =  [-2.41230662e-01  2.41647260e+02  2.39533186e+01]\n",
      "Done with gradient descent at iteration  689\n",
      "Learned weights =  [-2.41230662e-01  2.41647260e+02  2.39425024e+01]\n",
      "Done with gradient descent at iteration  690\n",
      "Learned weights =  [-2.41606721e-01  2.41647260e+02  2.39425024e+01]\n",
      "Done with gradient descent at iteration  690\n",
      "Learned weights =  [-2.41606721e-01  2.41657161e+02  2.39425024e+01]\n",
      "Done with gradient descent at iteration  690\n",
      "Learned weights =  [-2.41606721e-01  2.41657161e+02  2.39317504e+01]\n",
      "Done with gradient descent at iteration  691\n",
      "Learned weights =  [ -0.24198275 241.65716128  23.93175042]\n",
      "Done with gradient descent at iteration  691\n",
      "Learned weights =  [ -0.24198275 241.66700433  23.93175042]\n",
      "Done with gradient descent at iteration  691\n",
      "Learned weights =  [ -0.24198275 241.66700433  23.9210622 ]\n",
      "Done with gradient descent at iteration  692\n",
      "Learned weights =  [ -0.24235876 241.66700433  23.9210622 ]\n",
      "Done with gradient descent at iteration  692\n",
      "Learned weights =  [ -0.24235876 241.67678899  23.9210622 ]\n",
      "Done with gradient descent at iteration  692\n",
      "Learned weights =  [ -0.24235876 241.67678899  23.91043738]\n",
      "Done with gradient descent at iteration  693\n",
      "Learned weights =  [ -0.24273474 241.67678899  23.91043738]\n",
      "Done with gradient descent at iteration  693\n",
      "Learned weights =  [ -0.24273474 241.68651561  23.91043738]\n",
      "Done with gradient descent at iteration  693\n",
      "Learned weights =  [ -0.24273474 241.68651561  23.89987557]\n",
      "Done with gradient descent at iteration  694\n",
      "Learned weights =  [ -0.2431107  241.68651561  23.89987557]\n",
      "Done with gradient descent at iteration  694\n",
      "Learned weights =  [ -0.2431107  241.69618453  23.89987557]\n",
      "Done with gradient descent at iteration  694\n",
      "Learned weights =  [ -0.2431107  241.69618453  23.88937642]\n",
      "Done with gradient descent at iteration  695\n",
      "Learned weights =  [ -0.24348663 241.69618453  23.88937642]\n",
      "Done with gradient descent at iteration  695\n",
      "Learned weights =  [ -0.24348663 241.70579611  23.88937642]\n",
      "Done with gradient descent at iteration  695\n",
      "Learned weights =  [ -0.24348663 241.70579611  23.87893955]\n",
      "Done with gradient descent at iteration  696\n",
      "Learned weights =  [ -0.24386253 241.70579611  23.87893955]\n",
      "Done with gradient descent at iteration  696\n",
      "Learned weights =  [ -0.24386253 241.71535067  23.87893955]\n",
      "Done with gradient descent at iteration  696\n",
      "Learned weights =  [ -0.24386253 241.71535067  23.86856458]\n",
      "Done with gradient descent at iteration  697\n",
      "Learned weights =  [ -0.24423841 241.71535067  23.86856458]\n",
      "Done with gradient descent at iteration  697\n",
      "Learned weights =  [ -0.24423841 241.72484856  23.86856458]\n",
      "Done with gradient descent at iteration  697\n",
      "Learned weights =  [ -0.24423841 241.72484856  23.85825115]\n",
      "Done with gradient descent at iteration  698\n",
      "Learned weights =  [ -0.24461426 241.72484856  23.85825115]\n",
      "Done with gradient descent at iteration  698\n",
      "Learned weights =  [ -0.24461426 241.73429011  23.85825115]\n",
      "Done with gradient descent at iteration  698\n",
      "Learned weights =  [ -0.24461426 241.73429011  23.8479989 ]\n",
      "Done with gradient descent at iteration  699\n",
      "Learned weights =  [ -0.24499009 241.73429011  23.8479989 ]\n",
      "Done with gradient descent at iteration  699\n",
      "Learned weights =  [ -0.24499009 241.74367566  23.8479989 ]\n",
      "Done with gradient descent at iteration  699\n",
      "Learned weights =  [ -0.24499009 241.74367566  23.83780746]\n",
      "Iteration = 700\n",
      "Cost function =  1205623439252682.0\n",
      "Done with gradient descent at iteration  700\n",
      "Learned weights =  [ -0.24536589 241.74367566  23.83780746]\n",
      "Done with gradient descent at iteration  700\n",
      "Learned weights =  [ -0.24536589 241.75300554  23.83780746]\n",
      "Done with gradient descent at iteration  700\n",
      "Learned weights =  [ -0.24536589 241.75300554  23.82767648]\n",
      "Done with gradient descent at iteration  701\n",
      "Learned weights =  [ -0.24574167 241.75300554  23.82767648]\n",
      "Done with gradient descent at iteration  701\n",
      "Learned weights =  [ -0.24574167 241.76228008  23.82767648]\n",
      "Done with gradient descent at iteration  701\n",
      "Learned weights =  [ -0.24574167 241.76228008  23.81760559]\n",
      "Done with gradient descent at iteration  702\n",
      "Learned weights =  [ -0.24611742 241.76228008  23.81760559]\n",
      "Done with gradient descent at iteration  702\n",
      "Learned weights =  [ -0.24611742 241.7714996   23.81760559]\n",
      "Done with gradient descent at iteration  702\n",
      "Learned weights =  [ -0.24611742 241.7714996   23.80759443]\n",
      "Done with gradient descent at iteration  703\n",
      "Learned weights =  [ -0.24649315 241.7714996   23.80759443]\n",
      "Done with gradient descent at iteration  703\n",
      "Learned weights =  [ -0.24649315 241.78066444  23.80759443]\n",
      "Done with gradient descent at iteration  703\n",
      "Learned weights =  [ -0.24649315 241.78066444  23.79764266]\n",
      "Done with gradient descent at iteration  704\n",
      "Learned weights =  [ -0.24686886 241.78066444  23.79764266]\n",
      "Done with gradient descent at iteration  704\n",
      "Learned weights =  [ -0.24686886 241.78977492  23.79764266]\n",
      "Done with gradient descent at iteration  704\n",
      "Learned weights =  [ -0.24686886 241.78977492  23.78774992]\n",
      "Done with gradient descent at iteration  705\n",
      "Learned weights =  [ -0.24724454 241.78977492  23.78774992]\n",
      "Done with gradient descent at iteration  705\n",
      "Learned weights =  [ -0.24724454 241.79883136  23.78774992]\n",
      "Done with gradient descent at iteration  705\n",
      "Learned weights =  [ -0.24724454 241.79883136  23.77791586]\n",
      "Done with gradient descent at iteration  706\n",
      "Learned weights =  [ -0.24762019 241.79883136  23.77791586]\n",
      "Done with gradient descent at iteration  706\n",
      "Learned weights =  [ -0.24762019 241.80783407  23.77791586]\n",
      "Done with gradient descent at iteration  706\n",
      "Learned weights =  [ -0.24762019 241.80783407  23.76814013]\n",
      "Done with gradient descent at iteration  707\n",
      "Learned weights =  [ -0.24799583 241.80783407  23.76814013]\n",
      "Done with gradient descent at iteration  707\n",
      "Learned weights =  [ -0.24799583 241.81678339  23.76814013]\n",
      "Done with gradient descent at iteration  707\n",
      "Learned weights =  [ -0.24799583 241.81678339  23.75842239]\n",
      "Done with gradient descent at iteration  708\n",
      "Learned weights =  [ -0.24837143 241.81678339  23.75842239]\n",
      "Done with gradient descent at iteration  708\n",
      "Learned weights =  [ -0.24837143 241.82567963  23.75842239]\n",
      "Done with gradient descent at iteration  708\n",
      "Learned weights =  [ -0.24837143 241.82567963  23.74876229]\n",
      "Done with gradient descent at iteration  709\n",
      "Learned weights =  [ -0.24874702 241.82567963  23.74876229]\n",
      "Done with gradient descent at iteration  709\n",
      "Learned weights =  [ -0.24874702 241.83452309  23.74876229]\n",
      "Done with gradient descent at iteration  709\n",
      "Learned weights =  [ -0.24874702 241.83452309  23.73915949]\n",
      "Done with gradient descent at iteration  710\n",
      "Learned weights =  [ -0.24912258 241.83452309  23.73915949]\n",
      "Done with gradient descent at iteration  710\n",
      "Learned weights =  [ -0.24912258 241.8433141   23.73915949]\n",
      "Done with gradient descent at iteration  710\n",
      "Learned weights =  [ -0.24912258 241.8433141   23.72961365]\n",
      "Done with gradient descent at iteration  711\n",
      "Learned weights =  [ -0.24949812 241.8433141   23.72961365]\n",
      "Done with gradient descent at iteration  711\n",
      "Learned weights =  [ -0.24949812 241.85205297  23.72961365]\n",
      "Done with gradient descent at iteration  711\n",
      "Learned weights =  [ -0.24949812 241.85205297  23.72012443]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  712\n",
      "Learned weights =  [ -0.24987363 241.85205297  23.72012443]\n",
      "Done with gradient descent at iteration  712\n",
      "Learned weights =  [ -0.24987363 241.86074     23.72012443]\n",
      "Done with gradient descent at iteration  712\n",
      "Learned weights =  [ -0.24987363 241.86074     23.7106915 ]\n",
      "Done with gradient descent at iteration  713\n",
      "Learned weights =  [ -0.25024912 241.86074     23.7106915 ]\n",
      "Done with gradient descent at iteration  713\n",
      "Learned weights =  [ -0.25024912 241.8693755   23.7106915 ]\n",
      "Done with gradient descent at iteration  713\n",
      "Learned weights =  [ -0.25024912 241.8693755   23.70131453]\n",
      "Done with gradient descent at iteration  714\n",
      "Learned weights =  [ -0.25062459 241.8693755   23.70131453]\n",
      "Done with gradient descent at iteration  714\n",
      "Learned weights =  [ -0.25062459 241.87795978  23.70131453]\n",
      "Done with gradient descent at iteration  714\n",
      "Learned weights =  [ -0.25062459 241.87795978  23.69199317]\n",
      "Done with gradient descent at iteration  715\n",
      "Learned weights =  [ -0.25100004 241.87795978  23.69199317]\n",
      "Done with gradient descent at iteration  715\n",
      "Learned weights =  [ -0.25100004 241.88649314  23.69199317]\n",
      "Done with gradient descent at iteration  715\n",
      "Learned weights =  [ -0.25100004 241.88649314  23.68272711]\n",
      "Done with gradient descent at iteration  716\n",
      "Learned weights =  [ -0.25137546 241.88649314  23.68272711]\n",
      "Done with gradient descent at iteration  716\n",
      "Learned weights =  [ -0.25137546 241.89497589  23.68272711]\n",
      "Done with gradient descent at iteration  716\n",
      "Learned weights =  [ -0.25137546 241.89497589  23.67351601]\n",
      "Done with gradient descent at iteration  717\n",
      "Learned weights =  [ -0.25175086 241.89497589  23.67351601]\n",
      "Done with gradient descent at iteration  717\n",
      "Learned weights =  [ -0.25175086 241.90340832  23.67351601]\n",
      "Done with gradient descent at iteration  717\n",
      "Learned weights =  [ -0.25175086 241.90340832  23.66435955]\n",
      "Done with gradient descent at iteration  718\n",
      "Learned weights =  [ -0.25212624 241.90340832  23.66435955]\n",
      "Done with gradient descent at iteration  718\n",
      "Learned weights =  [ -0.25212624 241.91179073  23.66435955]\n",
      "Done with gradient descent at iteration  718\n",
      "Learned weights =  [ -0.25212624 241.91179073  23.6552574 ]\n",
      "Done with gradient descent at iteration  719\n",
      "Learned weights =  [ -0.25250159 241.91179073  23.6552574 ]\n",
      "Done with gradient descent at iteration  719\n",
      "Learned weights =  [ -0.25250159 241.92012343  23.6552574 ]\n",
      "Done with gradient descent at iteration  719\n",
      "Learned weights =  [ -0.25250159 241.92012343  23.64620924]\n",
      "Done with gradient descent at iteration  720\n",
      "Learned weights =  [ -0.25287693 241.92012343  23.64620924]\n",
      "Done with gradient descent at iteration  720\n",
      "Learned weights =  [ -0.25287693 241.92840669  23.64620924]\n",
      "Done with gradient descent at iteration  720\n",
      "Learned weights =  [ -0.25287693 241.92840669  23.63721475]\n",
      "Done with gradient descent at iteration  721\n",
      "Learned weights =  [ -0.25325224 241.92840669  23.63721475]\n",
      "Done with gradient descent at iteration  721\n",
      "Learned weights =  [ -0.25325224 241.93664082  23.63721475]\n",
      "Done with gradient descent at iteration  721\n",
      "Learned weights =  [ -0.25325224 241.93664082  23.62827362]\n",
      "Done with gradient descent at iteration  722\n",
      "Learned weights =  [ -0.25362753 241.93664082  23.62827362]\n",
      "Done with gradient descent at iteration  722\n",
      "Learned weights =  [ -0.25362753 241.94482612  23.62827362]\n",
      "Done with gradient descent at iteration  722\n",
      "Learned weights =  [ -0.25362753 241.94482612  23.61938552]\n",
      "Done with gradient descent at iteration  723\n",
      "Learned weights =  [ -0.25400279 241.94482612  23.61938552]\n",
      "Done with gradient descent at iteration  723\n",
      "Learned weights =  [ -0.25400279 241.95296286  23.61938552]\n",
      "Done with gradient descent at iteration  723\n",
      "Learned weights =  [ -0.25400279 241.95296286  23.61055014]\n",
      "Done with gradient descent at iteration  724\n",
      "Learned weights =  [ -0.25437804 241.95296286  23.61055014]\n",
      "Done with gradient descent at iteration  724\n",
      "Learned weights =  [ -0.25437804 241.96105133  23.61055014]\n",
      "Done with gradient descent at iteration  724\n",
      "Learned weights =  [ -0.25437804 241.96105133  23.60176717]\n",
      "Done with gradient descent at iteration  725\n",
      "Learned weights =  [ -0.25475327 241.96105133  23.60176717]\n",
      "Done with gradient descent at iteration  725\n",
      "Learned weights =  [ -0.25475327 241.96909183  23.60176717]\n",
      "Done with gradient descent at iteration  725\n",
      "Learned weights =  [ -0.25475327 241.96909183  23.5930363 ]\n",
      "Done with gradient descent at iteration  726\n",
      "Learned weights =  [ -0.25512847 241.96909183  23.5930363 ]\n",
      "Done with gradient descent at iteration  726\n",
      "Learned weights =  [ -0.25512847 241.97708464  23.5930363 ]\n",
      "Done with gradient descent at iteration  726\n",
      "Learned weights =  [ -0.25512847 241.97708464  23.58435722]\n",
      "Done with gradient descent at iteration  727\n",
      "Learned weights =  [ -0.25550365 241.97708464  23.58435722]\n",
      "Done with gradient descent at iteration  727\n",
      "Learned weights =  [ -0.25550365 241.98503003  23.58435722]\n",
      "Done with gradient descent at iteration  727\n",
      "Learned weights =  [ -0.25550365 241.98503003  23.57572962]\n",
      "Done with gradient descent at iteration  728\n",
      "Learned weights =  [ -0.25587881 241.98503003  23.57572962]\n",
      "Done with gradient descent at iteration  728\n",
      "Learned weights =  [ -0.25587881 241.9929283   23.57572962]\n",
      "Done with gradient descent at iteration  728\n",
      "Learned weights =  [ -0.25587881 241.9929283   23.56715319]\n",
      "Done with gradient descent at iteration  729\n",
      "Learned weights =  [ -0.25625395 241.9929283   23.56715319]\n",
      "Done with gradient descent at iteration  729\n",
      "Learned weights =  [ -0.25625395 242.00077972  23.56715319]\n",
      "Done with gradient descent at iteration  729\n",
      "Learned weights =  [ -0.25625395 242.00077972  23.55862764]\n",
      "Done with gradient descent at iteration  730\n",
      "Learned weights =  [ -0.25662907 242.00077972  23.55862764]\n",
      "Done with gradient descent at iteration  730\n",
      "Learned weights =  [ -0.25662907 242.00858457  23.55862764]\n",
      "Done with gradient descent at iteration  730\n",
      "Learned weights =  [ -0.25662907 242.00858457  23.55015266]\n",
      "Done with gradient descent at iteration  731\n",
      "Learned weights =  [ -0.25700417 242.00858457  23.55015266]\n",
      "Done with gradient descent at iteration  731\n",
      "Learned weights =  [ -0.25700417 242.01634312  23.55015266]\n",
      "Done with gradient descent at iteration  731\n",
      "Learned weights =  [ -0.25700417 242.01634312  23.54172795]\n",
      "Done with gradient descent at iteration  732\n",
      "Learned weights =  [ -0.25737924 242.01634312  23.54172795]\n",
      "Done with gradient descent at iteration  732\n",
      "Learned weights =  [ -0.25737924 242.02405565  23.54172795]\n",
      "Done with gradient descent at iteration  732\n",
      "Learned weights =  [ -0.25737924 242.02405565  23.53335321]\n",
      "Done with gradient descent at iteration  733\n",
      "Learned weights =  [ -0.2577543  242.02405565  23.53335321]\n",
      "Done with gradient descent at iteration  733\n",
      "Learned weights =  [ -0.2577543  242.03172244  23.53335321]\n",
      "Done with gradient descent at iteration  733\n",
      "Learned weights =  [ -0.2577543  242.03172244  23.52502815]\n",
      "Done with gradient descent at iteration  734\n",
      "Learned weights =  [ -0.25812934 242.03172244  23.52502815]\n",
      "Done with gradient descent at iteration  734\n",
      "Learned weights =  [ -0.25812934 242.03934375  23.52502815]\n",
      "Done with gradient descent at iteration  734\n",
      "Learned weights =  [ -0.25812934 242.03934375  23.51675247]\n",
      "Done with gradient descent at iteration  735\n",
      "Learned weights =  [ -0.25850435 242.03934375  23.51675247]\n",
      "Done with gradient descent at iteration  735\n",
      "Learned weights =  [ -0.25850435 242.04691985  23.51675247]\n",
      "Done with gradient descent at iteration  735\n",
      "Learned weights =  [ -0.25850435 242.04691985  23.50852588]\n",
      "Done with gradient descent at iteration  736\n",
      "Learned weights =  [ -0.25887935 242.04691985  23.50852588]\n",
      "Done with gradient descent at iteration  736\n",
      "Learned weights =  [ -0.25887935 242.05445101  23.50852588]\n",
      "Done with gradient descent at iteration  736\n",
      "Learned weights =  [ -0.25887935 242.05445101  23.50034808]\n",
      "Done with gradient descent at iteration  737\n",
      "Learned weights =  [ -0.25925433 242.05445101  23.50034808]\n",
      "Done with gradient descent at iteration  737\n",
      "Learned weights =  [ -0.25925433 242.06193751  23.50034808]\n",
      "Done with gradient descent at iteration  737\n",
      "Learned weights =  [ -0.25925433 242.06193751  23.4922188 ]\n",
      "Done with gradient descent at iteration  738\n",
      "Learned weights =  [ -0.25962928 242.06193751  23.4922188 ]\n",
      "Done with gradient descent at iteration  738\n",
      "Learned weights =  [ -0.25962928 242.06937959  23.4922188 ]\n",
      "Done with gradient descent at iteration  738\n",
      "Learned weights =  [ -0.25962928 242.06937959  23.48413773]\n",
      "Done with gradient descent at iteration  739\n",
      "Learned weights =  [ -0.26000422 242.06937959  23.48413773]\n",
      "Done with gradient descent at iteration  739\n",
      "Learned weights =  [ -0.26000422 242.07677753  23.48413773]\n",
      "Done with gradient descent at iteration  739\n",
      "Learned weights =  [ -0.26000422 242.07677753  23.4761046 ]\n",
      "Done with gradient descent at iteration  740\n",
      "Learned weights =  [ -0.26037913 242.07677753  23.4761046 ]\n",
      "Done with gradient descent at iteration  740\n",
      "Learned weights =  [ -0.26037913 242.0841316   23.4761046 ]\n",
      "Done with gradient descent at iteration  740\n",
      "Learned weights =  [ -0.26037913 242.0841316   23.46811912]\n",
      "Done with gradient descent at iteration  741\n",
      "Learned weights =  [ -0.26075403 242.0841316   23.46811912]\n",
      "Done with gradient descent at iteration  741\n",
      "Learned weights =  [ -0.26075403 242.09144204  23.46811912]\n",
      "Done with gradient descent at iteration  741\n",
      "Learned weights =  [ -0.26075403 242.09144204  23.46018101]\n",
      "Done with gradient descent at iteration  742\n",
      "Learned weights =  [ -0.26112891 242.09144204  23.46018101]\n",
      "Done with gradient descent at iteration  742\n",
      "Learned weights =  [ -0.26112891 242.09870911  23.46018101]\n",
      "Done with gradient descent at iteration  742\n",
      "Learned weights =  [ -0.26112891 242.09870911  23.45228998]\n",
      "Done with gradient descent at iteration  743\n",
      "Learned weights =  [ -0.26150377 242.09870911  23.45228998]\n",
      "Done with gradient descent at iteration  743\n",
      "Learned weights =  [ -0.26150377 242.10593309  23.45228998]\n",
      "Done with gradient descent at iteration  743\n",
      "Learned weights =  [ -0.26150377 242.10593309  23.44444576]\n",
      "Done with gradient descent at iteration  744\n",
      "Learned weights =  [ -0.26187861 242.10593309  23.44444576]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  744\n",
      "Learned weights =  [ -0.26187861 242.11311421  23.44444576]\n",
      "Done with gradient descent at iteration  744\n",
      "Learned weights =  [ -0.26187861 242.11311421  23.43664807]\n",
      "Done with gradient descent at iteration  745\n",
      "Learned weights =  [ -0.26225343 242.11311421  23.43664807]\n",
      "Done with gradient descent at iteration  745\n",
      "Learned weights =  [ -0.26225343 242.12025274  23.43664807]\n",
      "Done with gradient descent at iteration  745\n",
      "Learned weights =  [ -0.26225343 242.12025274  23.42889664]\n",
      "Done with gradient descent at iteration  746\n",
      "Learned weights =  [ -0.26262823 242.12025274  23.42889664]\n",
      "Done with gradient descent at iteration  746\n",
      "Learned weights =  [ -0.26262823 242.12734892  23.42889664]\n",
      "Done with gradient descent at iteration  746\n",
      "Learned weights =  [ -0.26262823 242.12734892  23.42119118]\n",
      "Done with gradient descent at iteration  747\n",
      "Learned weights =  [ -0.26300301 242.12734892  23.42119118]\n",
      "Done with gradient descent at iteration  747\n",
      "Learned weights =  [ -0.26300301 242.13440302  23.42119118]\n",
      "Done with gradient descent at iteration  747\n",
      "Learned weights =  [ -0.26300301 242.13440302  23.41353143]\n",
      "Done with gradient descent at iteration  748\n",
      "Learned weights =  [ -0.26337777 242.13440302  23.41353143]\n",
      "Done with gradient descent at iteration  748\n",
      "Learned weights =  [ -0.26337777 242.14141527  23.41353143]\n",
      "Done with gradient descent at iteration  748\n",
      "Learned weights =  [ -0.26337777 242.14141527  23.40591712]\n",
      "Done with gradient descent at iteration  749\n",
      "Learned weights =  [ -0.26375252 242.14141527  23.40591712]\n",
      "Done with gradient descent at iteration  749\n",
      "Learned weights =  [ -0.26375252 242.14838593  23.40591712]\n",
      "Done with gradient descent at iteration  749\n",
      "Learned weights =  [ -0.26375252 242.14838593  23.39834797]\n",
      "Done with gradient descent at iteration  750\n",
      "Learned weights =  [ -0.26412724 242.14838593  23.39834797]\n",
      "Done with gradient descent at iteration  750\n",
      "Learned weights =  [ -0.26412724 242.15531524  23.39834797]\n",
      "Done with gradient descent at iteration  750\n",
      "Learned weights =  [ -0.26412724 242.15531524  23.39082371]\n",
      "Done with gradient descent at iteration  751\n",
      "Learned weights =  [ -0.26450195 242.15531524  23.39082371]\n",
      "Done with gradient descent at iteration  751\n",
      "Learned weights =  [ -0.26450195 242.16220345  23.39082371]\n",
      "Done with gradient descent at iteration  751\n",
      "Learned weights =  [ -0.26450195 242.16220345  23.38334409]\n",
      "Done with gradient descent at iteration  752\n",
      "Learned weights =  [ -0.26487664 242.16220345  23.38334409]\n",
      "Done with gradient descent at iteration  752\n",
      "Learned weights =  [ -0.26487664 242.1690508   23.38334409]\n",
      "Done with gradient descent at iteration  752\n",
      "Learned weights =  [ -0.26487664 242.1690508   23.37590884]\n",
      "Done with gradient descent at iteration  753\n",
      "Learned weights =  [ -0.26525131 242.1690508   23.37590884]\n",
      "Done with gradient descent at iteration  753\n",
      "Learned weights =  [ -0.26525131 242.17585753  23.37590884]\n",
      "Done with gradient descent at iteration  753\n",
      "Learned weights =  [ -0.26525131 242.17585753  23.36851769]\n",
      "Done with gradient descent at iteration  754\n",
      "Learned weights =  [ -0.26562596 242.17585753  23.36851769]\n",
      "Done with gradient descent at iteration  754\n",
      "Learned weights =  [ -0.26562596 242.1826239   23.36851769]\n",
      "Done with gradient descent at iteration  754\n",
      "Learned weights =  [ -0.26562596 242.1826239   23.36117038]\n",
      "Done with gradient descent at iteration  755\n",
      "Learned weights =  [ -0.2660006  242.1826239   23.36117038]\n",
      "Done with gradient descent at iteration  755\n",
      "Learned weights =  [ -0.2660006  242.18935012  23.36117038]\n",
      "Done with gradient descent at iteration  755\n",
      "Learned weights =  [ -0.2660006  242.18935012  23.35386666]\n",
      "Done with gradient descent at iteration  756\n",
      "Learned weights =  [ -0.26637521 242.18935012  23.35386666]\n",
      "Done with gradient descent at iteration  756\n",
      "Learned weights =  [ -0.26637521 242.19603645  23.35386666]\n",
      "Done with gradient descent at iteration  756\n",
      "Learned weights =  [ -0.26637521 242.19603645  23.34660626]\n",
      "Done with gradient descent at iteration  757\n",
      "Learned weights =  [ -0.26674981 242.19603645  23.34660626]\n",
      "Done with gradient descent at iteration  757\n",
      "Learned weights =  [ -0.26674981 242.20268312  23.34660626]\n",
      "Done with gradient descent at iteration  757\n",
      "Learned weights =  [ -0.26674981 242.20268312  23.33938892]\n",
      "Done with gradient descent at iteration  758\n",
      "Learned weights =  [ -0.2671244  242.20268312  23.33938892]\n",
      "Done with gradient descent at iteration  758\n",
      "Learned weights =  [ -0.2671244  242.20929036  23.33938892]\n",
      "Done with gradient descent at iteration  758\n",
      "Learned weights =  [ -0.2671244  242.20929036  23.3322144 ]\n",
      "Done with gradient descent at iteration  759\n",
      "Learned weights =  [ -0.26749896 242.20929036  23.3322144 ]\n",
      "Done with gradient descent at iteration  759\n",
      "Learned weights =  [ -0.26749896 242.21585842  23.3322144 ]\n",
      "Done with gradient descent at iteration  759\n",
      "Learned weights =  [ -0.26749896 242.21585842  23.32508243]\n",
      "Done with gradient descent at iteration  760\n",
      "Learned weights =  [ -0.26787351 242.21585842  23.32508243]\n",
      "Done with gradient descent at iteration  760\n",
      "Learned weights =  [ -0.26787351 242.22238751  23.32508243]\n",
      "Done with gradient descent at iteration  760\n",
      "Learned weights =  [ -0.26787351 242.22238751  23.31799277]\n",
      "Done with gradient descent at iteration  761\n",
      "Learned weights =  [ -0.26824803 242.22238751  23.31799277]\n",
      "Done with gradient descent at iteration  761\n",
      "Learned weights =  [ -0.26824803 242.22887788  23.31799277]\n",
      "Done with gradient descent at iteration  761\n",
      "Learned weights =  [ -0.26824803 242.22887788  23.31094516]\n",
      "Done with gradient descent at iteration  762\n",
      "Learned weights =  [ -0.26862255 242.22887788  23.31094516]\n",
      "Done with gradient descent at iteration  762\n",
      "Learned weights =  [ -0.26862255 242.23532974  23.31094516]\n",
      "Done with gradient descent at iteration  762\n",
      "Learned weights =  [ -0.26862255 242.23532974  23.30393936]\n",
      "Done with gradient descent at iteration  763\n",
      "Learned weights =  [ -0.26899704 242.23532974  23.30393936]\n",
      "Done with gradient descent at iteration  763\n",
      "Learned weights =  [ -0.26899704 242.24174334  23.30393936]\n",
      "Done with gradient descent at iteration  763\n",
      "Learned weights =  [ -0.26899704 242.24174334  23.29697511]\n",
      "Done with gradient descent at iteration  764\n",
      "Learned weights =  [ -0.26937152 242.24174334  23.29697511]\n",
      "Done with gradient descent at iteration  764\n",
      "Learned weights =  [ -0.26937152 242.2481189   23.29697511]\n",
      "Done with gradient descent at iteration  764\n",
      "Learned weights =  [ -0.26937152 242.2481189   23.29005217]\n",
      "Done with gradient descent at iteration  765\n",
      "Learned weights =  [ -0.26974598 242.2481189   23.29005217]\n",
      "Done with gradient descent at iteration  765\n",
      "Learned weights =  [ -0.26974598 242.25445664  23.29005217]\n",
      "Done with gradient descent at iteration  765\n",
      "Learned weights =  [ -0.26974598 242.25445664  23.2831703 ]\n",
      "Done with gradient descent at iteration  766\n",
      "Learned weights =  [ -0.27012042 242.25445664  23.2831703 ]\n",
      "Done with gradient descent at iteration  766\n",
      "Learned weights =  [ -0.27012042 242.26075678  23.2831703 ]\n",
      "Done with gradient descent at iteration  766\n",
      "Learned weights =  [ -0.27012042 242.26075678  23.27632925]\n",
      "Done with gradient descent at iteration  767\n",
      "Learned weights =  [ -0.27049485 242.26075678  23.27632925]\n",
      "Done with gradient descent at iteration  767\n",
      "Learned weights =  [ -0.27049485 242.26701956  23.27632925]\n",
      "Done with gradient descent at iteration  767\n",
      "Learned weights =  [ -0.27049485 242.26701956  23.26952877]\n",
      "Done with gradient descent at iteration  768\n",
      "Learned weights =  [ -0.27086926 242.26701956  23.26952877]\n",
      "Done with gradient descent at iteration  768\n",
      "Learned weights =  [ -0.27086926 242.27324518  23.26952877]\n",
      "Done with gradient descent at iteration  768\n",
      "Learned weights =  [ -0.27086926 242.27324518  23.26276864]\n",
      "Done with gradient descent at iteration  769\n",
      "Learned weights =  [ -0.27124365 242.27324518  23.26276864]\n",
      "Done with gradient descent at iteration  769\n",
      "Learned weights =  [ -0.27124365 242.27943388  23.26276864]\n",
      "Done with gradient descent at iteration  769\n",
      "Learned weights =  [ -0.27124365 242.27943388  23.2560486 ]\n",
      "Done with gradient descent at iteration  770\n",
      "Learned weights =  [ -0.27161803 242.27943388  23.2560486 ]\n",
      "Done with gradient descent at iteration  770\n",
      "Learned weights =  [ -0.27161803 242.28558588  23.2560486 ]\n",
      "Done with gradient descent at iteration  770\n",
      "Learned weights =  [ -0.27161803 242.28558588  23.24936843]\n",
      "Done with gradient descent at iteration  771\n",
      "Learned weights =  [ -0.27199239 242.28558588  23.24936843]\n",
      "Done with gradient descent at iteration  771\n",
      "Learned weights =  [ -0.27199239 242.29170138  23.24936843]\n",
      "Done with gradient descent at iteration  771\n",
      "Learned weights =  [ -0.27199239 242.29170138  23.24272788]\n",
      "Done with gradient descent at iteration  772\n",
      "Learned weights =  [ -0.27236674 242.29170138  23.24272788]\n",
      "Done with gradient descent at iteration  772\n",
      "Learned weights =  [ -0.27236674 242.2977806   23.24272788]\n",
      "Done with gradient descent at iteration  772\n",
      "Learned weights =  [ -0.27236674 242.2977806   23.23612672]\n",
      "Done with gradient descent at iteration  773\n",
      "Learned weights =  [ -0.27274107 242.2977806   23.23612672]\n",
      "Done with gradient descent at iteration  773\n",
      "Learned weights =  [ -0.27274107 242.30382377  23.23612672]\n",
      "Done with gradient descent at iteration  773\n",
      "Learned weights =  [ -0.27274107 242.30382377  23.22956472]\n",
      "Done with gradient descent at iteration  774\n",
      "Learned weights =  [ -0.27311538 242.30382377  23.22956472]\n",
      "Done with gradient descent at iteration  774\n",
      "Learned weights =  [ -0.27311538 242.30983109  23.22956472]\n",
      "Done with gradient descent at iteration  774\n",
      "Learned weights =  [ -0.27311538 242.30983109  23.22304164]\n",
      "Done with gradient descent at iteration  775\n",
      "Learned weights =  [ -0.27348967 242.30983109  23.22304164]\n",
      "Done with gradient descent at iteration  775\n",
      "Learned weights =  [ -0.27348967 242.31580278  23.22304164]\n",
      "Done with gradient descent at iteration  775\n",
      "Learned weights =  [ -0.27348967 242.31580278  23.21655726]\n",
      "Done with gradient descent at iteration  776\n",
      "Learned weights =  [ -0.27386395 242.31580278  23.21655726]\n",
      "Done with gradient descent at iteration  776\n",
      "Learned weights =  [ -0.27386395 242.32173904  23.21655726]\n",
      "Done with gradient descent at iteration  776\n",
      "Learned weights =  [ -0.27386395 242.32173904  23.21011134]\n",
      "Done with gradient descent at iteration  777\n",
      "Learned weights =  [ -0.27423822 242.32173904  23.21011134]\n",
      "Done with gradient descent at iteration  777\n",
      "Learned weights =  [ -0.27423822 242.3276401   23.21011134]\n",
      "Done with gradient descent at iteration  777\n",
      "Learned weights =  [ -0.27423822 242.3276401   23.20370365]\n",
      "Done with gradient descent at iteration  778\n",
      "Learned weights =  [ -0.27461247 242.3276401   23.20370365]\n",
      "Done with gradient descent at iteration  778\n",
      "Learned weights =  [ -0.27461247 242.33350615  23.20370365]\n",
      "Done with gradient descent at iteration  778\n",
      "Learned weights =  [ -0.27461247 242.33350615  23.19733397]\n",
      "Done with gradient descent at iteration  779\n",
      "Learned weights =  [ -0.2749867  242.33350615  23.19733397]\n",
      "Done with gradient descent at iteration  779\n",
      "Learned weights =  [ -0.2749867  242.33933741  23.19733397]\n",
      "Done with gradient descent at iteration  779\n",
      "Learned weights =  [ -0.2749867  242.33933741  23.19100208]\n",
      "Done with gradient descent at iteration  780\n",
      "Learned weights =  [ -0.27536092 242.33933741  23.19100208]\n",
      "Done with gradient descent at iteration  780\n",
      "Learned weights =  [ -0.27536092 242.34513408  23.19100208]\n",
      "Done with gradient descent at iteration  780\n",
      "Learned weights =  [ -0.27536092 242.34513408  23.18470774]\n",
      "Done with gradient descent at iteration  781\n",
      "Learned weights =  [ -0.27573512 242.34513408  23.18470774]\n",
      "Done with gradient descent at iteration  781\n",
      "Learned weights =  [ -0.27573512 242.35089637  23.18470774]\n",
      "Done with gradient descent at iteration  781\n",
      "Learned weights =  [ -0.27573512 242.35089637  23.17845074]\n",
      "Done with gradient descent at iteration  782\n",
      "Learned weights =  [ -0.27610931 242.35089637  23.17845074]\n",
      "Done with gradient descent at iteration  782\n",
      "Learned weights =  [ -0.27610931 242.35662447  23.17845074]\n",
      "Done with gradient descent at iteration  782\n",
      "Learned weights =  [ -0.27610931 242.35662447  23.17223086]\n",
      "Done with gradient descent at iteration  783\n",
      "Learned weights =  [ -0.27648348 242.35662447  23.17223086]\n",
      "Done with gradient descent at iteration  783\n",
      "Learned weights =  [ -0.27648348 242.3623186   23.17223086]\n",
      "Done with gradient descent at iteration  783\n",
      "Learned weights =  [ -0.27648348 242.3623186   23.16604787]\n",
      "Done with gradient descent at iteration  784\n",
      "Learned weights =  [ -0.27685764 242.3623186   23.16604787]\n",
      "Done with gradient descent at iteration  784\n",
      "Learned weights =  [ -0.27685764 242.36797896  23.16604787]\n",
      "Done with gradient descent at iteration  784\n",
      "Learned weights =  [ -0.27685764 242.36797896  23.15990155]\n",
      "Done with gradient descent at iteration  785\n",
      "Learned weights =  [ -0.27723178 242.36797896  23.15990155]\n",
      "Done with gradient descent at iteration  785\n",
      "Learned weights =  [ -0.27723178 242.37360574  23.15990155]\n",
      "Done with gradient descent at iteration  785\n",
      "Learned weights =  [ -0.27723178 242.37360574  23.1537917 ]\n",
      "Done with gradient descent at iteration  786\n",
      "Learned weights =  [ -0.27760591 242.37360574  23.1537917 ]\n",
      "Done with gradient descent at iteration  786\n",
      "Learned weights =  [ -0.27760591 242.37919914  23.1537917 ]\n",
      "Done with gradient descent at iteration  786\n",
      "Learned weights =  [ -0.27760591 242.37919914  23.14771808]\n",
      "Done with gradient descent at iteration  787\n",
      "Learned weights =  [ -0.27798002 242.37919914  23.14771808]\n",
      "Done with gradient descent at iteration  787\n",
      "Learned weights =  [ -0.27798002 242.38475937  23.14771808]\n",
      "Done with gradient descent at iteration  787\n",
      "Learned weights =  [ -0.27798002 242.38475937  23.1416805 ]\n",
      "Done with gradient descent at iteration  788\n",
      "Learned weights =  [ -0.27835411 242.38475937  23.1416805 ]\n",
      "Done with gradient descent at iteration  788\n",
      "Learned weights =  [ -0.27835411 242.39028662  23.1416805 ]\n",
      "Done with gradient descent at iteration  788\n",
      "Learned weights =  [ -0.27835411 242.39028662  23.13567872]\n",
      "Done with gradient descent at iteration  789\n",
      "Learned weights =  [ -0.2787282  242.39028662  23.13567872]\n",
      "Done with gradient descent at iteration  789\n",
      "Learned weights =  [ -0.2787282  242.39578108  23.13567872]\n",
      "Done with gradient descent at iteration  789\n",
      "Learned weights =  [ -0.2787282  242.39578108  23.12971255]\n",
      "Done with gradient descent at iteration  790\n",
      "Learned weights =  [ -0.27910226 242.39578108  23.12971255]\n",
      "Done with gradient descent at iteration  790\n",
      "Learned weights =  [ -0.27910226 242.40124295  23.12971255]\n",
      "Done with gradient descent at iteration  790\n",
      "Learned weights =  [ -0.27910226 242.40124295  23.12378177]\n",
      "Done with gradient descent at iteration  791\n",
      "Learned weights =  [ -0.27947632 242.40124295  23.12378177]\n",
      "Done with gradient descent at iteration  791\n",
      "Learned weights =  [ -0.27947632 242.40667242  23.12378177]\n",
      "Done with gradient descent at iteration  791\n",
      "Learned weights =  [ -0.27947632 242.40667242  23.11788616]\n",
      "Done with gradient descent at iteration  792\n",
      "Learned weights =  [ -0.27985036 242.40667242  23.11788616]\n",
      "Done with gradient descent at iteration  792\n",
      "Learned weights =  [ -0.27985036 242.41206969  23.11788616]\n",
      "Done with gradient descent at iteration  792\n",
      "Learned weights =  [ -0.27985036 242.41206969  23.11202553]\n",
      "Done with gradient descent at iteration  793\n",
      "Learned weights =  [ -0.28022438 242.41206969  23.11202553]\n",
      "Done with gradient descent at iteration  793\n",
      "Learned weights =  [ -0.28022438 242.41743494  23.11202553]\n",
      "Done with gradient descent at iteration  793\n",
      "Learned weights =  [ -0.28022438 242.41743494  23.10619966]\n",
      "Done with gradient descent at iteration  794\n",
      "Learned weights =  [ -0.28059839 242.41743494  23.10619966]\n",
      "Done with gradient descent at iteration  794\n",
      "Learned weights =  [ -0.28059839 242.42276837  23.10619966]\n",
      "Done with gradient descent at iteration  794\n",
      "Learned weights =  [ -0.28059839 242.42276837  23.10040835]\n",
      "Done with gradient descent at iteration  795\n",
      "Learned weights =  [ -0.28097239 242.42276837  23.10040835]\n",
      "Done with gradient descent at iteration  795\n",
      "Learned weights =  [ -0.28097239 242.42807016  23.10040835]\n",
      "Done with gradient descent at iteration  795\n",
      "Learned weights =  [ -0.28097239 242.42807016  23.0946514 ]\n",
      "Done with gradient descent at iteration  796\n",
      "Learned weights =  [ -0.28134637 242.42807016  23.0946514 ]\n",
      "Done with gradient descent at iteration  796\n",
      "Learned weights =  [ -0.28134637 242.43334051  23.0946514 ]\n",
      "Done with gradient descent at iteration  796\n",
      "Learned weights =  [ -0.28134637 242.43334051  23.08892859]\n",
      "Done with gradient descent at iteration  797\n",
      "Learned weights =  [ -0.28172034 242.43334051  23.08892859]\n",
      "Done with gradient descent at iteration  797\n",
      "Learned weights =  [ -0.28172034 242.43857959  23.08892859]\n",
      "Done with gradient descent at iteration  797\n",
      "Learned weights =  [ -0.28172034 242.43857959  23.08323972]\n",
      "Done with gradient descent at iteration  798\n",
      "Learned weights =  [ -0.28209429 242.43857959  23.08323972]\n",
      "Done with gradient descent at iteration  798\n",
      "Learned weights =  [ -0.28209429 242.4437876   23.08323972]\n",
      "Done with gradient descent at iteration  798\n",
      "Learned weights =  [ -0.28209429 242.4437876   23.0775846 ]\n",
      "Done with gradient descent at iteration  799\n",
      "Learned weights =  [ -0.28246823 242.4437876   23.0775846 ]\n",
      "Done with gradient descent at iteration  799\n",
      "Learned weights =  [ -0.28246823 242.44896472  23.0775846 ]\n",
      "Done with gradient descent at iteration  799\n",
      "Learned weights =  [ -0.28246823 242.44896472  23.07196303]\n",
      "Iteration = 800\n",
      "Cost function =  1205612300984401.0\n",
      "Done with gradient descent at iteration  800\n",
      "Learned weights =  [ -0.28284216 242.44896472  23.07196303]\n",
      "Done with gradient descent at iteration  800\n",
      "Learned weights =  [ -0.28284216 242.45411113  23.07196303]\n",
      "Done with gradient descent at iteration  800\n",
      "Learned weights =  [ -0.28284216 242.45411113  23.0663748 ]\n",
      "Done with gradient descent at iteration  801\n",
      "Learned weights =  [ -0.28321607 242.45411113  23.0663748 ]\n",
      "Done with gradient descent at iteration  801\n",
      "Learned weights =  [ -0.28321607 242.45922701  23.0663748 ]\n",
      "Done with gradient descent at iteration  801\n",
      "Learned weights =  [ -0.28321607 242.45922701  23.06081972]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  802\n",
      "Learned weights =  [ -0.28358997 242.45922701  23.06081972]\n",
      "Done with gradient descent at iteration  802\n",
      "Learned weights =  [ -0.28358997 242.46431254  23.06081972]\n",
      "Done with gradient descent at iteration  802\n",
      "Learned weights =  [ -0.28358997 242.46431254  23.05529759]\n",
      "Done with gradient descent at iteration  803\n",
      "Learned weights =  [ -0.28396385 242.46431254  23.05529759]\n",
      "Done with gradient descent at iteration  803\n",
      "Learned weights =  [ -0.28396385 242.46936791  23.05529759]\n",
      "Done with gradient descent at iteration  803\n",
      "Learned weights =  [ -0.28396385 242.46936791  23.04980822]\n",
      "Done with gradient descent at iteration  804\n",
      "Learned weights =  [ -0.28433772 242.46936791  23.04980822]\n",
      "Done with gradient descent at iteration  804\n",
      "Learned weights =  [ -0.28433772 242.4743933   23.04980822]\n",
      "Done with gradient descent at iteration  804\n",
      "Learned weights =  [ -0.28433772 242.4743933   23.04435141]\n",
      "Done with gradient descent at iteration  805\n",
      "Learned weights =  [ -0.28471158 242.4743933   23.04435141]\n",
      "Done with gradient descent at iteration  805\n",
      "Learned weights =  [ -0.28471158 242.47938888  23.04435141]\n",
      "Done with gradient descent at iteration  805\n",
      "Learned weights =  [ -0.28471158 242.47938888  23.03892697]\n",
      "Done with gradient descent at iteration  806\n",
      "Learned weights =  [ -0.28508543 242.47938888  23.03892697]\n",
      "Done with gradient descent at iteration  806\n",
      "Learned weights =  [ -0.28508543 242.48435482  23.03892697]\n",
      "Done with gradient descent at iteration  806\n",
      "Learned weights =  [ -0.28508543 242.48435482  23.0335347 ]\n",
      "Done with gradient descent at iteration  807\n",
      "Learned weights =  [ -0.28545926 242.48435482  23.0335347 ]\n",
      "Done with gradient descent at iteration  807\n",
      "Learned weights =  [ -0.28545926 242.48929131  23.0335347 ]\n",
      "Done with gradient descent at iteration  807\n",
      "Learned weights =  [ -0.28545926 242.48929131  23.02817442]\n",
      "Done with gradient descent at iteration  808\n",
      "Learned weights =  [ -0.28583308 242.48929131  23.02817442]\n",
      "Done with gradient descent at iteration  808\n",
      "Learned weights =  [ -0.28583308 242.49419852  23.02817442]\n",
      "Done with gradient descent at iteration  808\n",
      "Learned weights =  [ -0.28583308 242.49419852  23.02284594]\n",
      "Done with gradient descent at iteration  809\n",
      "Learned weights =  [ -0.28620688 242.49419852  23.02284594]\n",
      "Done with gradient descent at iteration  809\n",
      "Learned weights =  [ -0.28620688 242.49907662  23.02284594]\n",
      "Done with gradient descent at iteration  809\n",
      "Learned weights =  [ -0.28620688 242.49907662  23.01754906]\n",
      "Done with gradient descent at iteration  810\n",
      "Learned weights =  [ -0.28658068 242.49907662  23.01754906]\n",
      "Done with gradient descent at iteration  810\n",
      "Learned weights =  [ -0.28658068 242.50392578  23.01754906]\n",
      "Done with gradient descent at iteration  810\n",
      "Learned weights =  [ -0.28658068 242.50392578  23.0122836 ]\n",
      "Done with gradient descent at iteration  811\n",
      "Learned weights =  [ -0.28695446 242.50392578  23.0122836 ]\n",
      "Done with gradient descent at iteration  811\n",
      "Learned weights =  [ -0.28695446 242.50874619  23.0122836 ]\n",
      "Done with gradient descent at iteration  811\n",
      "Learned weights =  [ -0.28695446 242.50874619  23.00704938]\n",
      "Done with gradient descent at iteration  812\n",
      "Learned weights =  [ -0.28732822 242.50874619  23.00704938]\n",
      "Done with gradient descent at iteration  812\n",
      "Learned weights =  [ -0.28732822 242.513538    23.00704938]\n",
      "Done with gradient descent at iteration  812\n",
      "Learned weights =  [ -0.28732822 242.513538    23.0018462 ]\n",
      "Done with gradient descent at iteration  813\n",
      "Learned weights =  [ -0.28770198 242.513538    23.0018462 ]\n",
      "Done with gradient descent at iteration  813\n",
      "Learned weights =  [ -0.28770198 242.51830138  23.0018462 ]\n",
      "Done with gradient descent at iteration  813\n",
      "Learned weights =  [ -0.28770198 242.51830138  22.99667389]\n",
      "Done with gradient descent at iteration  814\n",
      "Learned weights =  [ -0.28807572 242.51830138  22.99667389]\n",
      "Done with gradient descent at iteration  814\n",
      "Learned weights =  [ -0.28807572 242.52303652  22.99667389]\n",
      "Done with gradient descent at iteration  814\n",
      "Learned weights =  [ -0.28807572 242.52303652  22.99153226]\n",
      "Done with gradient descent at iteration  815\n",
      "Learned weights =  [ -0.28844945 242.52303652  22.99153226]\n",
      "Done with gradient descent at iteration  815\n",
      "Learned weights =  [ -0.28844945 242.52774356  22.99153226]\n",
      "Done with gradient descent at iteration  815\n",
      "Learned weights =  [ -0.28844945 242.52774356  22.98642112]\n",
      "Done with gradient descent at iteration  816\n",
      "Learned weights =  [ -0.28882317 242.52774356  22.98642112]\n",
      "Done with gradient descent at iteration  816\n",
      "Learned weights =  [ -0.28882317 242.53242269  22.98642112]\n",
      "Done with gradient descent at iteration  816\n",
      "Learned weights =  [ -0.28882317 242.53242269  22.98134031]\n",
      "Done with gradient descent at iteration  817\n",
      "Learned weights =  [ -0.28919687 242.53242269  22.98134031]\n",
      "Done with gradient descent at iteration  817\n",
      "Learned weights =  [ -0.28919687 242.53707406  22.98134031]\n",
      "Done with gradient descent at iteration  817\n",
      "Learned weights =  [ -0.28919687 242.53707406  22.97628963]\n",
      "Done with gradient descent at iteration  818\n",
      "Learned weights =  [ -0.28957056 242.53707406  22.97628963]\n",
      "Done with gradient descent at iteration  818\n",
      "Learned weights =  [ -0.28957056 242.54169784  22.97628963]\n",
      "Done with gradient descent at iteration  818\n",
      "Learned weights =  [ -0.28957056 242.54169784  22.97126892]\n",
      "Done with gradient descent at iteration  819\n",
      "Learned weights =  [ -0.28994424 242.54169784  22.97126892]\n",
      "Done with gradient descent at iteration  819\n",
      "Learned weights =  [ -0.28994424 242.5462942   22.97126892]\n",
      "Done with gradient descent at iteration  819\n",
      "Learned weights =  [ -0.28994424 242.5462942   22.96627798]\n",
      "Done with gradient descent at iteration  820\n",
      "Learned weights =  [ -0.29031791 242.5462942   22.96627798]\n",
      "Done with gradient descent at iteration  820\n",
      "Learned weights =  [ -0.29031791 242.55086329  22.96627798]\n",
      "Done with gradient descent at iteration  820\n",
      "Learned weights =  [ -0.29031791 242.55086329  22.96131665]\n",
      "Done with gradient descent at iteration  821\n",
      "Learned weights =  [ -0.29069156 242.55086329  22.96131665]\n",
      "Done with gradient descent at iteration  821\n",
      "Learned weights =  [ -0.29069156 242.55540528  22.96131665]\n",
      "Done with gradient descent at iteration  821\n",
      "Learned weights =  [ -0.29069156 242.55540528  22.95638475]\n",
      "Done with gradient descent at iteration  822\n",
      "Learned weights =  [ -0.29106521 242.55540528  22.95638475]\n",
      "Done with gradient descent at iteration  822\n",
      "Learned weights =  [ -0.29106521 242.55992033  22.95638475]\n",
      "Done with gradient descent at iteration  822\n",
      "Learned weights =  [ -0.29106521 242.55992033  22.9514821 ]\n",
      "Done with gradient descent at iteration  823\n",
      "Learned weights =  [ -0.29143884 242.55992033  22.9514821 ]\n",
      "Done with gradient descent at iteration  823\n",
      "Learned weights =  [ -0.29143884 242.5644086   22.9514821 ]\n",
      "Done with gradient descent at iteration  823\n",
      "Learned weights =  [ -0.29143884 242.5644086   22.94660854]\n",
      "Done with gradient descent at iteration  824\n",
      "Learned weights =  [ -0.29181246 242.5644086   22.94660854]\n",
      "Done with gradient descent at iteration  824\n",
      "Learned weights =  [ -0.29181246 242.56887024  22.94660854]\n",
      "Done with gradient descent at iteration  824\n",
      "Learned weights =  [ -0.29181246 242.56887024  22.94176388]\n",
      "Done with gradient descent at iteration  825\n",
      "Learned weights =  [ -0.29218606 242.56887024  22.94176388]\n",
      "Done with gradient descent at iteration  825\n",
      "Learned weights =  [ -0.29218606 242.57330542  22.94176388]\n",
      "Done with gradient descent at iteration  825\n",
      "Learned weights =  [ -0.29218606 242.57330542  22.93694797]\n",
      "Done with gradient descent at iteration  826\n",
      "Learned weights =  [ -0.29255966 242.57330542  22.93694797]\n",
      "Done with gradient descent at iteration  826\n",
      "Learned weights =  [ -0.29255966 242.57771429  22.93694797]\n",
      "Done with gradient descent at iteration  826\n",
      "Learned weights =  [ -0.29255966 242.57771429  22.93216062]\n",
      "Done with gradient descent at iteration  827\n",
      "Learned weights =  [ -0.29293324 242.57771429  22.93216062]\n",
      "Done with gradient descent at iteration  827\n",
      "Learned weights =  [ -0.29293324 242.58209702  22.93216062]\n",
      "Done with gradient descent at iteration  827\n",
      "Learned weights =  [ -0.29293324 242.58209702  22.92740166]\n",
      "Done with gradient descent at iteration  828\n",
      "Learned weights =  [ -0.29330681 242.58209702  22.92740166]\n",
      "Done with gradient descent at iteration  828\n",
      "Learned weights =  [ -0.29330681 242.58645374  22.92740166]\n",
      "Done with gradient descent at iteration  828\n",
      "Learned weights =  [ -0.29330681 242.58645374  22.92267094]\n",
      "Done with gradient descent at iteration  829\n",
      "Learned weights =  [ -0.29368037 242.58645374  22.92267094]\n",
      "Done with gradient descent at iteration  829\n",
      "Learned weights =  [ -0.29368037 242.59078462  22.92267094]\n",
      "Done with gradient descent at iteration  829\n",
      "Learned weights =  [ -0.29368037 242.59078462  22.91796827]\n",
      "Done with gradient descent at iteration  830\n",
      "Learned weights =  [ -0.29405392 242.59078462  22.91796827]\n",
      "Done with gradient descent at iteration  830\n",
      "Learned weights =  [ -0.29405392 242.59508982  22.91796827]\n",
      "Done with gradient descent at iteration  830\n",
      "Learned weights =  [ -0.29405392 242.59508982  22.91329351]\n",
      "Done with gradient descent at iteration  831\n",
      "Learned weights =  [ -0.29442746 242.59508982  22.91329351]\n",
      "Done with gradient descent at iteration  831\n",
      "Learned weights =  [ -0.29442746 242.59936947  22.91329351]\n",
      "Done with gradient descent at iteration  831\n",
      "Learned weights =  [ -0.29442746 242.59936947  22.90864647]\n",
      "Done with gradient descent at iteration  832\n",
      "Learned weights =  [ -0.29480099 242.59936947  22.90864647]\n",
      "Done with gradient descent at iteration  832\n",
      "Learned weights =  [ -0.29480099 242.60362375  22.90864647]\n",
      "Done with gradient descent at iteration  832\n",
      "Learned weights =  [ -0.29480099 242.60362375  22.90402699]\n",
      "Done with gradient descent at iteration  833\n",
      "Learned weights =  [ -0.2951745  242.60362375  22.90402699]\n",
      "Done with gradient descent at iteration  833\n",
      "Learned weights =  [ -0.2951745  242.60785278  22.90402699]\n",
      "Done with gradient descent at iteration  833\n",
      "Learned weights =  [ -0.2951745  242.60785278  22.89943492]\n",
      "Done with gradient descent at iteration  834\n",
      "Learned weights =  [ -0.295548   242.60785278  22.89943492]\n",
      "Done with gradient descent at iteration  834\n",
      "Learned weights =  [ -0.295548   242.61205674  22.89943492]\n",
      "Done with gradient descent at iteration  834\n",
      "Learned weights =  [ -0.295548   242.61205674  22.89487009]\n",
      "Done with gradient descent at iteration  835\n",
      "Learned weights =  [ -0.2959215  242.61205674  22.89487009]\n",
      "Done with gradient descent at iteration  835\n",
      "Learned weights =  [ -0.2959215  242.61623575  22.89487009]\n",
      "Done with gradient descent at iteration  835\n",
      "Learned weights =  [ -0.2959215  242.61623575  22.89033234]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  836\n",
      "Learned weights =  [ -0.29629498 242.61623575  22.89033234]\n",
      "Done with gradient descent at iteration  836\n",
      "Learned weights =  [ -0.29629498 242.62038998  22.89033234]\n",
      "Done with gradient descent at iteration  836\n",
      "Learned weights =  [ -0.29629498 242.62038998  22.8858215 ]\n",
      "Done with gradient descent at iteration  837\n",
      "Learned weights =  [ -0.29666845 242.62038998  22.8858215 ]\n",
      "Done with gradient descent at iteration  837\n",
      "Learned weights =  [ -0.29666845 242.62451957  22.8858215 ]\n",
      "Done with gradient descent at iteration  837\n",
      "Learned weights =  [ -0.29666845 242.62451957  22.88133742]\n",
      "Done with gradient descent at iteration  838\n",
      "Learned weights =  [ -0.29704191 242.62451957  22.88133742]\n",
      "Done with gradient descent at iteration  838\n",
      "Learned weights =  [ -0.29704191 242.62862466  22.88133742]\n",
      "Done with gradient descent at iteration  838\n",
      "Learned weights =  [ -0.29704191 242.62862466  22.87687994]\n",
      "Done with gradient descent at iteration  839\n",
      "Learned weights =  [ -0.29741535 242.62862466  22.87687994]\n",
      "Done with gradient descent at iteration  839\n",
      "Learned weights =  [ -0.29741535 242.63270541  22.87687994]\n",
      "Done with gradient descent at iteration  839\n",
      "Learned weights =  [ -0.29741535 242.63270541  22.87244889]\n",
      "Done with gradient descent at iteration  840\n",
      "Learned weights =  [ -0.29778879 242.63270541  22.87244889]\n",
      "Done with gradient descent at iteration  840\n",
      "Learned weights =  [ -0.29778879 242.63676194  22.87244889]\n",
      "Done with gradient descent at iteration  840\n",
      "Learned weights =  [ -0.29778879 242.63676194  22.86804414]\n",
      "Done with gradient descent at iteration  841\n",
      "Learned weights =  [ -0.29816222 242.63676194  22.86804414]\n",
      "Done with gradient descent at iteration  841\n",
      "Learned weights =  [ -0.29816222 242.64079442  22.86804414]\n",
      "Done with gradient descent at iteration  841\n",
      "Learned weights =  [ -0.29816222 242.64079442  22.86366551]\n",
      "Done with gradient descent at iteration  842\n",
      "Learned weights =  [ -0.29853563 242.64079442  22.86366551]\n",
      "Done with gradient descent at iteration  842\n",
      "Learned weights =  [ -0.29853563 242.64480298  22.86366551]\n",
      "Done with gradient descent at iteration  842\n",
      "Learned weights =  [ -0.29853563 242.64480298  22.85931285]\n",
      "Done with gradient descent at iteration  843\n",
      "Learned weights =  [ -0.29890904 242.64480298  22.85931285]\n",
      "Done with gradient descent at iteration  843\n",
      "Learned weights =  [ -0.29890904 242.64878776  22.85931285]\n",
      "Done with gradient descent at iteration  843\n",
      "Learned weights =  [ -0.29890904 242.64878776  22.85498602]\n",
      "Done with gradient descent at iteration  844\n",
      "Learned weights =  [ -0.29928243 242.64878776  22.85498602]\n",
      "Done with gradient descent at iteration  844\n",
      "Learned weights =  [ -0.29928243 242.6527489   22.85498602]\n",
      "Done with gradient descent at iteration  844\n",
      "Learned weights =  [ -0.29928243 242.6527489   22.85068485]\n",
      "Done with gradient descent at iteration  845\n",
      "Learned weights =  [ -0.29965582 242.6527489   22.85068485]\n",
      "Done with gradient descent at iteration  845\n",
      "Learned weights =  [ -0.29965582 242.65668655  22.85068485]\n",
      "Done with gradient descent at iteration  845\n",
      "Learned weights =  [ -0.29965582 242.65668655  22.84640919]\n",
      "Done with gradient descent at iteration  846\n",
      "Learned weights =  [ -0.30002919 242.65668655  22.84640919]\n",
      "Done with gradient descent at iteration  846\n",
      "Learned weights =  [ -0.30002919 242.66060084  22.84640919]\n",
      "Done with gradient descent at iteration  846\n",
      "Learned weights =  [ -0.30002919 242.66060084  22.84215889]\n",
      "Done with gradient descent at iteration  847\n",
      "Learned weights =  [ -0.30040255 242.66060084  22.84215889]\n",
      "Done with gradient descent at iteration  847\n",
      "Learned weights =  [ -0.30040255 242.66449192  22.84215889]\n",
      "Done with gradient descent at iteration  847\n",
      "Learned weights =  [ -0.30040255 242.66449192  22.83793381]\n",
      "Done with gradient descent at iteration  848\n",
      "Learned weights =  [ -0.30077591 242.66449192  22.83793381]\n",
      "Done with gradient descent at iteration  848\n",
      "Learned weights =  [ -0.30077591 242.66835991  22.83793381]\n",
      "Done with gradient descent at iteration  848\n",
      "Learned weights =  [ -0.30077591 242.66835991  22.83373379]\n",
      "Done with gradient descent at iteration  849\n",
      "Learned weights =  [ -0.30114925 242.66835991  22.83373379]\n",
      "Done with gradient descent at iteration  849\n",
      "Learned weights =  [ -0.30114925 242.67220496  22.83373379]\n",
      "Done with gradient descent at iteration  849\n",
      "Learned weights =  [ -0.30114925 242.67220496  22.82955868]\n",
      "Done with gradient descent at iteration  850\n",
      "Learned weights =  [ -0.30152258 242.67220496  22.82955868]\n",
      "Done with gradient descent at iteration  850\n",
      "Learned weights =  [ -0.30152258 242.67602721  22.82955868]\n",
      "Done with gradient descent at iteration  850\n",
      "Learned weights =  [ -0.30152258 242.67602721  22.82540834]\n",
      "Done with gradient descent at iteration  851\n",
      "Learned weights =  [ -0.3018959  242.67602721  22.82540834]\n",
      "Done with gradient descent at iteration  851\n",
      "Learned weights =  [ -0.3018959  242.67982678  22.82540834]\n",
      "Done with gradient descent at iteration  851\n",
      "Learned weights =  [ -0.3018959  242.67982678  22.82128262]\n",
      "Done with gradient descent at iteration  852\n",
      "Learned weights =  [ -0.30226921 242.67982678  22.82128262]\n",
      "Done with gradient descent at iteration  852\n",
      "Learned weights =  [ -0.30226921 242.68360381  22.82128262]\n",
      "Done with gradient descent at iteration  852\n",
      "Learned weights =  [ -0.30226921 242.68360381  22.81718137]\n",
      "Done with gradient descent at iteration  853\n",
      "Learned weights =  [ -0.30264252 242.68360381  22.81718137]\n",
      "Done with gradient descent at iteration  853\n",
      "Learned weights =  [ -0.30264252 242.68735845  22.81718137]\n",
      "Done with gradient descent at iteration  853\n",
      "Learned weights =  [ -0.30264252 242.68735845  22.81310445]\n",
      "Done with gradient descent at iteration  854\n",
      "Learned weights =  [ -0.30301581 242.68735845  22.81310445]\n",
      "Done with gradient descent at iteration  854\n",
      "Learned weights =  [ -0.30301581 242.69109081  22.81310445]\n",
      "Done with gradient descent at iteration  854\n",
      "Learned weights =  [ -0.30301581 242.69109081  22.80905171]\n",
      "Done with gradient descent at iteration  855\n",
      "Learned weights =  [ -0.30338909 242.69109081  22.80905171]\n",
      "Done with gradient descent at iteration  855\n",
      "Learned weights =  [ -0.30338909 242.69480103  22.80905171]\n",
      "Done with gradient descent at iteration  855\n",
      "Learned weights =  [ -0.30338909 242.69480103  22.80502301]\n",
      "Done with gradient descent at iteration  856\n",
      "Learned weights =  [ -0.30376236 242.69480103  22.80502301]\n",
      "Done with gradient descent at iteration  856\n",
      "Learned weights =  [ -0.30376236 242.69848924  22.80502301]\n",
      "Done with gradient descent at iteration  856\n",
      "Learned weights =  [ -0.30376236 242.69848924  22.80101821]\n",
      "Done with gradient descent at iteration  857\n",
      "Learned weights =  [ -0.30413562 242.69848924  22.80101821]\n",
      "Done with gradient descent at iteration  857\n",
      "Learned weights =  [ -0.30413562 242.70215558  22.80101821]\n",
      "Done with gradient descent at iteration  857\n",
      "Learned weights =  [ -0.30413562 242.70215558  22.79703717]\n",
      "Done with gradient descent at iteration  858\n",
      "Learned weights =  [ -0.30450888 242.70215558  22.79703717]\n",
      "Done with gradient descent at iteration  858\n",
      "Learned weights =  [ -0.30450888 242.70580017  22.79703717]\n",
      "Done with gradient descent at iteration  858\n",
      "Learned weights =  [ -0.30450888 242.70580017  22.79307974]\n",
      "Done with gradient descent at iteration  859\n",
      "Learned weights =  [ -0.30488212 242.70580017  22.79307974]\n",
      "Done with gradient descent at iteration  859\n",
      "Learned weights =  [ -0.30488212 242.70942314  22.79307974]\n",
      "Done with gradient descent at iteration  859\n",
      "Learned weights =  [ -0.30488212 242.70942314  22.78914578]\n",
      "Done with gradient descent at iteration  860\n",
      "Learned weights =  [ -0.30525535 242.70942314  22.78914578]\n",
      "Done with gradient descent at iteration  860\n",
      "Learned weights =  [ -0.30525535 242.71302462  22.78914578]\n",
      "Done with gradient descent at iteration  860\n",
      "Learned weights =  [ -0.30525535 242.71302462  22.78523517]\n",
      "Done with gradient descent at iteration  861\n",
      "Learned weights =  [ -0.30562857 242.71302462  22.78523517]\n",
      "Done with gradient descent at iteration  861\n",
      "Learned weights =  [ -0.30562857 242.71660474  22.78523517]\n",
      "Done with gradient descent at iteration  861\n",
      "Learned weights =  [ -0.30562857 242.71660474  22.78134774]\n",
      "Done with gradient descent at iteration  862\n",
      "Learned weights =  [ -0.30600179 242.71660474  22.78134774]\n",
      "Done with gradient descent at iteration  862\n",
      "Learned weights =  [ -0.30600179 242.72016362  22.78134774]\n",
      "Done with gradient descent at iteration  862\n",
      "Learned weights =  [ -0.30600179 242.72016362  22.77748338]\n",
      "Done with gradient descent at iteration  863\n",
      "Learned weights =  [ -0.30637499 242.72016362  22.77748338]\n",
      "Done with gradient descent at iteration  863\n",
      "Learned weights =  [ -0.30637499 242.7237014   22.77748338]\n",
      "Done with gradient descent at iteration  863\n",
      "Learned weights =  [ -0.30637499 242.7237014   22.77364194]\n",
      "Done with gradient descent at iteration  864\n",
      "Learned weights =  [ -0.30674819 242.7237014   22.77364194]\n",
      "Done with gradient descent at iteration  864\n",
      "Learned weights =  [ -0.30674819 242.72721819  22.77364194]\n",
      "Done with gradient descent at iteration  864\n",
      "Learned weights =  [ -0.30674819 242.72721819  22.76982329]\n",
      "Done with gradient descent at iteration  865\n",
      "Learned weights =  [ -0.30712137 242.72721819  22.76982329]\n",
      "Done with gradient descent at iteration  865\n",
      "Learned weights =  [ -0.30712137 242.73071412  22.76982329]\n",
      "Done with gradient descent at iteration  865\n",
      "Learned weights =  [ -0.30712137 242.73071412  22.76602728]\n",
      "Done with gradient descent at iteration  866\n",
      "Learned weights =  [ -0.30749455 242.73071412  22.76602728]\n",
      "Done with gradient descent at iteration  866\n",
      "Learned weights =  [ -0.30749455 242.73418931  22.76602728]\n",
      "Done with gradient descent at iteration  866\n",
      "Learned weights =  [ -0.30749455 242.73418931  22.7622538 ]\n",
      "Done with gradient descent at iteration  867\n",
      "Learned weights =  [ -0.30786772 242.73418931  22.7622538 ]\n",
      "Done with gradient descent at iteration  867\n",
      "Learned weights =  [ -0.30786772 242.73764389  22.7622538 ]\n",
      "Done with gradient descent at iteration  867\n",
      "Learned weights =  [ -0.30786772 242.73764389  22.7585027 ]\n",
      "Done with gradient descent at iteration  868\n",
      "Learned weights =  [ -0.30824088 242.73764389  22.7585027 ]\n",
      "Done with gradient descent at iteration  868\n",
      "Learned weights =  [ -0.30824088 242.74107798  22.7585027 ]\n",
      "Done with gradient descent at iteration  868\n",
      "Learned weights =  [ -0.30824088 242.74107798  22.75477385]\n",
      "Done with gradient descent at iteration  869\n",
      "Learned weights =  [ -0.30861402 242.74107798  22.75477385]\n",
      "Done with gradient descent at iteration  869\n",
      "Learned weights =  [ -0.30861402 242.7444917   22.75477385]\n",
      "Done with gradient descent at iteration  869\n",
      "Learned weights =  [ -0.30861402 242.7444917   22.75106712]\n",
      "Done with gradient descent at iteration  870\n",
      "Learned weights =  [ -0.30898716 242.7444917   22.75106712]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  870\n",
      "Learned weights =  [ -0.30898716 242.74788517  22.75106712]\n",
      "Done with gradient descent at iteration  870\n",
      "Learned weights =  [ -0.30898716 242.74788517  22.74738237]\n",
      "Done with gradient descent at iteration  871\n",
      "Learned weights =  [ -0.30936029 242.74788517  22.74738237]\n",
      "Done with gradient descent at iteration  871\n",
      "Learned weights =  [ -0.30936029 242.75125851  22.74738237]\n",
      "Done with gradient descent at iteration  871\n",
      "Learned weights =  [ -0.30936029 242.75125851  22.74371949]\n",
      "Done with gradient descent at iteration  872\n",
      "Learned weights =  [ -0.30973342 242.75125851  22.74371949]\n",
      "Done with gradient descent at iteration  872\n",
      "Learned weights =  [ -0.30973342 242.75461184  22.74371949]\n",
      "Done with gradient descent at iteration  872\n",
      "Learned weights =  [ -0.30973342 242.75461184  22.74007833]\n",
      "Done with gradient descent at iteration  873\n",
      "Learned weights =  [ -0.31010653 242.75461184  22.74007833]\n",
      "Done with gradient descent at iteration  873\n",
      "Learned weights =  [ -0.31010653 242.75794529  22.74007833]\n",
      "Done with gradient descent at iteration  873\n",
      "Learned weights =  [ -0.31010653 242.75794529  22.73645877]\n",
      "Done with gradient descent at iteration  874\n",
      "Learned weights =  [ -0.31047963 242.75794529  22.73645877]\n",
      "Done with gradient descent at iteration  874\n",
      "Learned weights =  [ -0.31047963 242.76125896  22.73645877]\n",
      "Done with gradient descent at iteration  874\n",
      "Learned weights =  [ -0.31047963 242.76125896  22.73286068]\n",
      "Done with gradient descent at iteration  875\n",
      "Learned weights =  [ -0.31085273 242.76125896  22.73286068]\n",
      "Done with gradient descent at iteration  875\n",
      "Learned weights =  [ -0.31085273 242.76455297  22.73286068]\n",
      "Done with gradient descent at iteration  875\n",
      "Learned weights =  [ -0.31085273 242.76455297  22.72928393]\n",
      "Done with gradient descent at iteration  876\n",
      "Learned weights =  [ -0.31122581 242.76455297  22.72928393]\n",
      "Done with gradient descent at iteration  876\n",
      "Learned weights =  [ -0.31122581 242.76782745  22.72928393]\n",
      "Done with gradient descent at iteration  876\n",
      "Learned weights =  [ -0.31122581 242.76782745  22.7257284 ]\n",
      "Done with gradient descent at iteration  877\n",
      "Learned weights =  [ -0.31159889 242.76782745  22.7257284 ]\n",
      "Done with gradient descent at iteration  877\n",
      "Learned weights =  [ -0.31159889 242.7710825   22.7257284 ]\n",
      "Done with gradient descent at iteration  877\n",
      "Learned weights =  [ -0.31159889 242.7710825   22.72219397]\n",
      "Done with gradient descent at iteration  878\n",
      "Learned weights =  [ -0.31197196 242.7710825   22.72219397]\n",
      "Done with gradient descent at iteration  878\n",
      "Learned weights =  [ -0.31197196 242.77431825  22.72219397]\n",
      "Done with gradient descent at iteration  878\n",
      "Learned weights =  [ -0.31197196 242.77431825  22.71868049]\n",
      "Done with gradient descent at iteration  879\n",
      "Learned weights =  [ -0.31234502 242.77431825  22.71868049]\n",
      "Done with gradient descent at iteration  879\n",
      "Learned weights =  [ -0.31234502 242.7775348   22.71868049]\n",
      "Done with gradient descent at iteration  879\n",
      "Learned weights =  [ -0.31234502 242.7775348   22.71518786]\n",
      "Done with gradient descent at iteration  880\n",
      "Learned weights =  [ -0.31271807 242.7775348   22.71518786]\n",
      "Done with gradient descent at iteration  880\n",
      "Learned weights =  [ -0.31271807 242.78073227  22.71518786]\n",
      "Done with gradient descent at iteration  880\n",
      "Learned weights =  [ -0.31271807 242.78073227  22.71171595]\n",
      "Done with gradient descent at iteration  881\n",
      "Learned weights =  [ -0.31309112 242.78073227  22.71171595]\n",
      "Done with gradient descent at iteration  881\n",
      "Learned weights =  [ -0.31309112 242.78391078  22.71171595]\n",
      "Done with gradient descent at iteration  881\n",
      "Learned weights =  [ -0.31309112 242.78391078  22.70826463]\n",
      "Done with gradient descent at iteration  882\n",
      "Learned weights =  [ -0.31346415 242.78391078  22.70826463]\n",
      "Done with gradient descent at iteration  882\n",
      "Learned weights =  [ -0.31346415 242.78707043  22.70826463]\n",
      "Done with gradient descent at iteration  882\n",
      "Learned weights =  [ -0.31346415 242.78707043  22.70483378]\n",
      "Done with gradient descent at iteration  883\n",
      "Learned weights =  [ -0.31383718 242.78707043  22.70483378]\n",
      "Done with gradient descent at iteration  883\n",
      "Learned weights =  [ -0.31383718 242.79021135  22.70483378]\n",
      "Done with gradient descent at iteration  883\n",
      "Learned weights =  [ -0.31383718 242.79021135  22.70142329]\n",
      "Done with gradient descent at iteration  884\n",
      "Learned weights =  [ -0.3142102  242.79021135  22.70142329]\n",
      "Done with gradient descent at iteration  884\n",
      "Learned weights =  [ -0.3142102  242.79333363  22.70142329]\n",
      "Done with gradient descent at iteration  884\n",
      "Learned weights =  [ -0.3142102  242.79333363  22.69803302]\n",
      "Done with gradient descent at iteration  885\n",
      "Learned weights =  [ -0.31458321 242.79333363  22.69803302]\n",
      "Done with gradient descent at iteration  885\n",
      "Learned weights =  [ -0.31458321 242.79643739  22.69803302]\n",
      "Done with gradient descent at iteration  885\n",
      "Learned weights =  [ -0.31458321 242.79643739  22.69466287]\n",
      "Done with gradient descent at iteration  886\n",
      "Learned weights =  [ -0.31495621 242.79643739  22.69466287]\n",
      "Done with gradient descent at iteration  886\n",
      "Learned weights =  [ -0.31495621 242.79952274  22.69466287]\n",
      "Done with gradient descent at iteration  886\n",
      "Learned weights =  [ -0.31495621 242.79952274  22.69131271]\n",
      "Done with gradient descent at iteration  887\n",
      "Learned weights =  [ -0.3153292  242.79952274  22.69131271]\n",
      "Done with gradient descent at iteration  887\n",
      "Learned weights =  [ -0.3153292  242.80258979  22.69131271]\n",
      "Done with gradient descent at iteration  887\n",
      "Learned weights =  [ -0.3153292  242.80258979  22.68798242]\n",
      "Done with gradient descent at iteration  888\n",
      "Learned weights =  [ -0.31570219 242.80258979  22.68798242]\n",
      "Done with gradient descent at iteration  888\n",
      "Learned weights =  [ -0.31570219 242.80563865  22.68798242]\n",
      "Done with gradient descent at iteration  888\n",
      "Learned weights =  [ -0.31570219 242.80563865  22.68467188]\n",
      "Done with gradient descent at iteration  889\n",
      "Learned weights =  [ -0.31607516 242.80563865  22.68467188]\n",
      "Done with gradient descent at iteration  889\n",
      "Learned weights =  [ -0.31607516 242.80866942  22.68467188]\n",
      "Done with gradient descent at iteration  889\n",
      "Learned weights =  [ -0.31607516 242.80866942  22.68138099]\n",
      "Done with gradient descent at iteration  890\n",
      "Learned weights =  [ -0.31644813 242.80866942  22.68138099]\n",
      "Done with gradient descent at iteration  890\n",
      "Learned weights =  [ -0.31644813 242.81168222  22.68138099]\n",
      "Done with gradient descent at iteration  890\n",
      "Learned weights =  [ -0.31644813 242.81168222  22.67810961]\n",
      "Done with gradient descent at iteration  891\n",
      "Learned weights =  [ -0.31682109 242.81168222  22.67810961]\n",
      "Done with gradient descent at iteration  891\n",
      "Learned weights =  [ -0.31682109 242.81467714  22.67810961]\n",
      "Done with gradient descent at iteration  891\n",
      "Learned weights =  [ -0.31682109 242.81467714  22.67485764]\n",
      "Done with gradient descent at iteration  892\n",
      "Learned weights =  [ -0.31719405 242.81467714  22.67485764]\n",
      "Done with gradient descent at iteration  892\n",
      "Learned weights =  [ -0.31719405 242.81765431  22.67485764]\n",
      "Done with gradient descent at iteration  892\n",
      "Learned weights =  [ -0.31719405 242.81765431  22.67162496]\n",
      "Done with gradient descent at iteration  893\n",
      "Learned weights =  [ -0.31756699 242.81765431  22.67162496]\n",
      "Done with gradient descent at iteration  893\n",
      "Learned weights =  [ -0.31756699 242.82061381  22.67162496]\n",
      "Done with gradient descent at iteration  893\n",
      "Learned weights =  [ -0.31756699 242.82061381  22.66841146]\n",
      "Done with gradient descent at iteration  894\n",
      "Learned weights =  [ -0.31793993 242.82061381  22.66841146]\n",
      "Done with gradient descent at iteration  894\n",
      "Learned weights =  [ -0.31793993 242.82355576  22.66841146]\n",
      "Done with gradient descent at iteration  894\n",
      "Learned weights =  [ -0.31793993 242.82355576  22.66521701]\n",
      "Done with gradient descent at iteration  895\n",
      "Learned weights =  [ -0.31831286 242.82355576  22.66521701]\n",
      "Done with gradient descent at iteration  895\n",
      "Learned weights =  [ -0.31831286 242.82648025  22.66521701]\n",
      "Done with gradient descent at iteration  895\n",
      "Learned weights =  [ -0.31831286 242.82648025  22.66204152]\n",
      "Done with gradient descent at iteration  896\n",
      "Learned weights =  [ -0.31868578 242.82648025  22.66204152]\n",
      "Done with gradient descent at iteration  896\n",
      "Learned weights =  [ -0.31868578 242.8293874   22.66204152]\n",
      "Done with gradient descent at iteration  896\n",
      "Learned weights =  [ -0.31868578 242.8293874   22.65888486]\n",
      "Done with gradient descent at iteration  897\n",
      "Learned weights =  [ -0.31905869 242.8293874   22.65888486]\n",
      "Done with gradient descent at iteration  897\n",
      "Learned weights =  [ -0.31905869 242.83227731  22.65888486]\n",
      "Done with gradient descent at iteration  897\n",
      "Learned weights =  [ -0.31905869 242.83227731  22.65574693]\n",
      "Done with gradient descent at iteration  898\n",
      "Learned weights =  [ -0.3194316  242.83227731  22.65574693]\n",
      "Done with gradient descent at iteration  898\n",
      "Learned weights =  [ -0.3194316  242.83515008  22.65574693]\n",
      "Done with gradient descent at iteration  898\n",
      "Learned weights =  [ -0.3194316  242.83515008  22.65262761]\n",
      "Done with gradient descent at iteration  899\n",
      "Learned weights =  [ -0.3198045  242.83515008  22.65262761]\n",
      "Done with gradient descent at iteration  899\n",
      "Learned weights =  [ -0.3198045  242.8380058   22.65262761]\n",
      "Done with gradient descent at iteration  899\n",
      "Learned weights =  [ -0.3198045 242.8380058  22.6495268]\n",
      "Iteration = 900\n",
      "Cost function =  1205608902360341.2\n",
      "Done with gradient descent at iteration  900\n",
      "Learned weights =  [ -0.32017739 242.8380058   22.6495268 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  900\n",
      "Learned weights =  [ -0.32017739 242.84084459  22.6495268 ]\n",
      "Done with gradient descent at iteration  900\n",
      "Learned weights =  [ -0.32017739 242.84084459  22.64644438]\n",
      "Done with gradient descent at iteration  901\n",
      "Learned weights =  [ -0.32055027 242.84084459  22.64644438]\n",
      "Done with gradient descent at iteration  901\n",
      "Learned weights =  [ -0.32055027 242.84366653  22.64644438]\n",
      "Done with gradient descent at iteration  901\n",
      "Learned weights =  [ -0.32055027 242.84366653  22.64338024]\n",
      "Done with gradient descent at iteration  902\n",
      "Learned weights =  [ -0.32092315 242.84366653  22.64338024]\n",
      "Done with gradient descent at iteration  902\n",
      "Learned weights =  [ -0.32092315 242.84647174  22.64338024]\n",
      "Done with gradient descent at iteration  902\n",
      "Learned weights =  [ -0.32092315 242.84647174  22.64033428]\n",
      "Done with gradient descent at iteration  903\n",
      "Learned weights =  [ -0.32129602 242.84647174  22.64033428]\n",
      "Done with gradient descent at iteration  903\n",
      "Learned weights =  [ -0.32129602 242.84926031  22.64033428]\n",
      "Done with gradient descent at iteration  903\n",
      "Learned weights =  [ -0.32129602 242.84926031  22.63730639]\n",
      "Done with gradient descent at iteration  904\n",
      "Learned weights =  [ -0.32166888 242.84926031  22.63730639]\n",
      "Done with gradient descent at iteration  904\n",
      "Learned weights =  [ -0.32166888 242.85203234  22.63730639]\n",
      "Done with gradient descent at iteration  904\n",
      "Learned weights =  [ -0.32166888 242.85203234  22.63429646]\n",
      "Done with gradient descent at iteration  905\n",
      "Learned weights =  [ -0.32204173 242.85203234  22.63429646]\n",
      "Done with gradient descent at iteration  905\n",
      "Learned weights =  [ -0.32204173 242.85478793  22.63429646]\n",
      "Done with gradient descent at iteration  905\n",
      "Learned weights =  [ -0.32204173 242.85478793  22.63130438]\n",
      "Done with gradient descent at iteration  906\n",
      "Learned weights =  [ -0.32241458 242.85478793  22.63130438]\n",
      "Done with gradient descent at iteration  906\n",
      "Learned weights =  [ -0.32241458 242.85752717  22.63130438]\n",
      "Done with gradient descent at iteration  906\n",
      "Learned weights =  [ -0.32241458 242.85752717  22.62833005]\n",
      "Done with gradient descent at iteration  907\n",
      "Learned weights =  [ -0.32278741 242.85752717  22.62833005]\n",
      "Done with gradient descent at iteration  907\n",
      "Learned weights =  [ -0.32278741 242.86025016  22.62833005]\n",
      "Done with gradient descent at iteration  907\n",
      "Learned weights =  [ -0.32278741 242.86025016  22.62537337]\n",
      "Done with gradient descent at iteration  908\n",
      "Learned weights =  [ -0.32316025 242.86025016  22.62537337]\n",
      "Done with gradient descent at iteration  908\n",
      "Learned weights =  [ -0.32316025 242.86295701  22.62537337]\n",
      "Done with gradient descent at iteration  908\n",
      "Learned weights =  [ -0.32316025 242.86295701  22.62243422]\n",
      "Done with gradient descent at iteration  909\n",
      "Learned weights =  [ -0.32353307 242.86295701  22.62243422]\n",
      "Done with gradient descent at iteration  909\n",
      "Learned weights =  [ -0.32353307 242.86564779  22.62243422]\n",
      "Done with gradient descent at iteration  909\n",
      "Learned weights =  [ -0.32353307 242.86564779  22.61951251]\n",
      "Done with gradient descent at iteration  910\n",
      "Learned weights =  [ -0.32390589 242.86564779  22.61951251]\n",
      "Done with gradient descent at iteration  910\n",
      "Learned weights =  [ -0.32390589 242.86832262  22.61951251]\n",
      "Done with gradient descent at iteration  910\n",
      "Learned weights =  [ -0.32390589 242.86832262  22.61660813]\n",
      "Done with gradient descent at iteration  911\n",
      "Learned weights =  [ -0.3242787  242.86832262  22.61660813]\n",
      "Done with gradient descent at iteration  911\n",
      "Learned weights =  [ -0.3242787  242.87098158  22.61660813]\n",
      "Done with gradient descent at iteration  911\n",
      "Learned weights =  [ -0.3242787  242.87098158  22.61372098]\n",
      "Done with gradient descent at iteration  912\n",
      "Learned weights =  [ -0.3246515  242.87098158  22.61372098]\n",
      "Done with gradient descent at iteration  912\n",
      "Learned weights =  [ -0.3246515  242.87362477  22.61372098]\n",
      "Done with gradient descent at iteration  912\n",
      "Learned weights =  [ -0.3246515  242.87362477  22.61085095]\n",
      "Done with gradient descent at iteration  913\n",
      "Learned weights =  [ -0.3250243  242.87362477  22.61085095]\n",
      "Done with gradient descent at iteration  913\n",
      "Learned weights =  [ -0.3250243  242.87625228  22.61085095]\n",
      "Done with gradient descent at iteration  913\n",
      "Learned weights =  [ -0.3250243  242.87625228  22.60799795]\n",
      "Done with gradient descent at iteration  914\n",
      "Learned weights =  [ -0.32539709 242.87625228  22.60799795]\n",
      "Done with gradient descent at iteration  914\n",
      "Learned weights =  [ -0.32539709 242.8788642   22.60799795]\n",
      "Done with gradient descent at iteration  914\n",
      "Learned weights =  [ -0.32539709 242.8788642   22.60516188]\n",
      "Done with gradient descent at iteration  915\n",
      "Learned weights =  [ -0.32576987 242.8788642   22.60516188]\n",
      "Done with gradient descent at iteration  915\n",
      "Learned weights =  [ -0.32576987 242.88146063  22.60516188]\n",
      "Done with gradient descent at iteration  915\n",
      "Learned weights =  [ -0.32576987 242.88146063  22.60234262]\n",
      "Done with gradient descent at iteration  916\n",
      "Learned weights =  [ -0.32614264 242.88146063  22.60234262]\n",
      "Done with gradient descent at iteration  916\n",
      "Learned weights =  [ -0.32614264 242.88404166  22.60234262]\n",
      "Done with gradient descent at iteration  916\n",
      "Learned weights =  [ -0.32614264 242.88404166  22.5995401 ]\n",
      "Done with gradient descent at iteration  917\n",
      "Learned weights =  [ -0.32651541 242.88404166  22.5995401 ]\n",
      "Done with gradient descent at iteration  917\n",
      "Learned weights =  [ -0.32651541 242.88660739  22.5995401 ]\n",
      "Done with gradient descent at iteration  917\n",
      "Learned weights =  [ -0.32651541 242.88660739  22.59675419]\n",
      "Done with gradient descent at iteration  918\n",
      "Learned weights =  [ -0.32688817 242.88660739  22.59675419]\n",
      "Done with gradient descent at iteration  918\n",
      "Learned weights =  [ -0.32688817 242.88915789  22.59675419]\n",
      "Done with gradient descent at iteration  918\n",
      "Learned weights =  [ -0.32688817 242.88915789  22.59398481]\n",
      "Done with gradient descent at iteration  919\n",
      "Learned weights =  [ -0.32726093 242.88915789  22.59398481]\n",
      "Done with gradient descent at iteration  919\n",
      "Learned weights =  [ -0.32726093 242.89169326  22.59398481]\n",
      "Done with gradient descent at iteration  919\n",
      "Learned weights =  [ -0.32726093 242.89169326  22.59123186]\n",
      "Done with gradient descent at iteration  920\n",
      "Learned weights =  [ -0.32763368 242.89169326  22.59123186]\n",
      "Done with gradient descent at iteration  920\n",
      "Learned weights =  [ -0.32763368 242.8942136   22.59123186]\n",
      "Done with gradient descent at iteration  920\n",
      "Learned weights =  [ -0.32763368 242.8942136   22.58849524]\n",
      "Done with gradient descent at iteration  921\n",
      "Learned weights =  [ -0.32800642 242.8942136   22.58849524]\n",
      "Done with gradient descent at iteration  921\n",
      "Learned weights =  [ -0.32800642 242.89671899  22.58849524]\n",
      "Done with gradient descent at iteration  921\n",
      "Learned weights =  [ -0.32800642 242.89671899  22.58577485]\n",
      "Done with gradient descent at iteration  922\n",
      "Learned weights =  [ -0.32837915 242.89671899  22.58577485]\n",
      "Done with gradient descent at iteration  922\n",
      "Learned weights =  [ -0.32837915 242.89920951  22.58577485]\n",
      "Done with gradient descent at iteration  922\n",
      "Learned weights =  [ -0.32837915 242.89920951  22.5830706 ]\n",
      "Done with gradient descent at iteration  923\n",
      "Learned weights =  [ -0.32875188 242.89920951  22.5830706 ]\n",
      "Done with gradient descent at iteration  923\n",
      "Learned weights =  [ -0.32875188 242.90168526  22.5830706 ]\n",
      "Done with gradient descent at iteration  923\n",
      "Learned weights =  [ -0.32875188 242.90168526  22.58038239]\n",
      "Done with gradient descent at iteration  924\n",
      "Learned weights =  [ -0.3291246  242.90168526  22.58038239]\n",
      "Done with gradient descent at iteration  924\n",
      "Learned weights =  [ -0.3291246  242.90414633  22.58038239]\n",
      "Done with gradient descent at iteration  924\n",
      "Learned weights =  [ -0.3291246  242.90414633  22.57771013]\n",
      "Done with gradient descent at iteration  925\n",
      "Learned weights =  [ -0.32949732 242.90414633  22.57771013]\n",
      "Done with gradient descent at iteration  925\n",
      "Learned weights =  [ -0.32949732 242.9065928   22.57771013]\n",
      "Done with gradient descent at iteration  925\n",
      "Learned weights =  [ -0.32949732 242.9065928   22.57505371]\n",
      "Done with gradient descent at iteration  926\n",
      "Learned weights =  [ -0.32987003 242.9065928   22.57505371]\n",
      "Done with gradient descent at iteration  926\n",
      "Learned weights =  [ -0.32987003 242.90902476  22.57505371]\n",
      "Done with gradient descent at iteration  926\n",
      "Learned weights =  [ -0.32987003 242.90902476  22.57241306]\n",
      "Done with gradient descent at iteration  927\n",
      "Learned weights =  [ -0.33024273 242.90902476  22.57241306]\n",
      "Done with gradient descent at iteration  927\n",
      "Learned weights =  [ -0.33024273 242.91144229  22.57241306]\n",
      "Done with gradient descent at iteration  927\n",
      "Learned weights =  [ -0.33024273 242.91144229  22.56978807]\n",
      "Done with gradient descent at iteration  928\n",
      "Learned weights =  [ -0.33061542 242.91144229  22.56978807]\n",
      "Done with gradient descent at iteration  928\n",
      "Learned weights =  [ -0.33061542 242.91384548  22.56978807]\n",
      "Done with gradient descent at iteration  928\n",
      "Learned weights =  [ -0.33061542 242.91384548  22.56717865]\n",
      "Done with gradient descent at iteration  929\n",
      "Learned weights =  [ -0.33098811 242.91384548  22.56717865]\n",
      "Done with gradient descent at iteration  929\n",
      "Learned weights =  [ -0.33098811 242.91623442  22.56717865]\n",
      "Done with gradient descent at iteration  929\n",
      "Learned weights =  [ -0.33098811 242.91623442  22.56458471]\n",
      "Done with gradient descent at iteration  930\n",
      "Learned weights =  [ -0.33136079 242.91623442  22.56458471]\n",
      "Done with gradient descent at iteration  930\n",
      "Learned weights =  [ -0.33136079 242.91860919  22.56458471]\n",
      "Done with gradient descent at iteration  930\n",
      "Learned weights =  [ -0.33136079 242.91860919  22.56200616]\n",
      "Done with gradient descent at iteration  931\n",
      "Learned weights =  [ -0.33173347 242.91860919  22.56200616]\n",
      "Done with gradient descent at iteration  931\n",
      "Learned weights =  [ -0.33173347 242.92096987  22.56200616]\n",
      "Done with gradient descent at iteration  931\n",
      "Learned weights =  [ -0.33173347 242.92096987  22.5594429 ]\n",
      "Done with gradient descent at iteration  932\n",
      "Learned weights =  [ -0.33210614 242.92096987  22.5594429 ]\n",
      "Done with gradient descent at iteration  932\n",
      "Learned weights =  [ -0.33210614 242.92331655  22.5594429 ]\n",
      "Done with gradient descent at iteration  932\n",
      "Learned weights =  [ -0.33210614 242.92331655  22.55689485]\n",
      "Done with gradient descent at iteration  933\n",
      "Learned weights =  [ -0.3324788  242.92331655  22.55689485]\n",
      "Done with gradient descent at iteration  933\n",
      "Learned weights =  [ -0.3324788  242.92564931  22.55689485]\n",
      "Done with gradient descent at iteration  933\n",
      "Learned weights =  [ -0.3324788  242.92564931  22.55436192]\n",
      "Done with gradient descent at iteration  934\n",
      "Learned weights =  [ -0.33285146 242.92564931  22.55436192]\n",
      "Done with gradient descent at iteration  934\n",
      "Learned weights =  [ -0.33285146 242.92796824  22.55436192]\n",
      "Done with gradient descent at iteration  934\n",
      "Learned weights =  [ -0.33285146 242.92796824  22.551844  ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  935\n",
      "Learned weights =  [ -0.33322411 242.92796824  22.551844  ]\n",
      "Done with gradient descent at iteration  935\n",
      "Learned weights =  [ -0.33322411 242.93027341  22.551844  ]\n",
      "Done with gradient descent at iteration  935\n",
      "Learned weights =  [ -0.33322411 242.93027341  22.54934103]\n",
      "Done with gradient descent at iteration  936\n",
      "Learned weights =  [ -0.33359676 242.93027341  22.54934103]\n",
      "Done with gradient descent at iteration  936\n",
      "Learned weights =  [ -0.33359676 242.9325649   22.54934103]\n",
      "Done with gradient descent at iteration  936\n",
      "Learned weights =  [ -0.33359676 242.9325649   22.5468529 ]\n",
      "Done with gradient descent at iteration  937\n",
      "Learned weights =  [ -0.3339694 242.9325649  22.5468529]\n",
      "Done with gradient descent at iteration  937\n",
      "Learned weights =  [ -0.3339694 242.9348428  22.5468529]\n",
      "Done with gradient descent at iteration  937\n",
      "Learned weights =  [ -0.3339694  242.9348428   22.54437953]\n",
      "Done with gradient descent at iteration  938\n",
      "Learned weights =  [ -0.33434203 242.9348428   22.54437953]\n",
      "Done with gradient descent at iteration  938\n",
      "Learned weights =  [ -0.33434203 242.9371072   22.54437953]\n",
      "Done with gradient descent at iteration  938\n",
      "Learned weights =  [ -0.33434203 242.9371072   22.54192084]\n",
      "Done with gradient descent at iteration  939\n",
      "Learned weights =  [ -0.33471466 242.9371072   22.54192084]\n",
      "Done with gradient descent at iteration  939\n",
      "Learned weights =  [ -0.33471466 242.93935816  22.54192084]\n",
      "Done with gradient descent at iteration  939\n",
      "Learned weights =  [ -0.33471466 242.93935816  22.53947673]\n",
      "Done with gradient descent at iteration  940\n",
      "Learned weights =  [ -0.33508728 242.93935816  22.53947673]\n",
      "Done with gradient descent at iteration  940\n",
      "Learned weights =  [ -0.33508728 242.94159576  22.53947673]\n",
      "Done with gradient descent at iteration  940\n",
      "Learned weights =  [ -0.33508728 242.94159576  22.53704711]\n",
      "Done with gradient descent at iteration  941\n",
      "Learned weights =  [ -0.33545989 242.94159576  22.53704711]\n",
      "Done with gradient descent at iteration  941\n",
      "Learned weights =  [ -0.33545989 242.9438201   22.53704711]\n",
      "Done with gradient descent at iteration  941\n",
      "Learned weights =  [ -0.33545989 242.9438201   22.53463191]\n",
      "Done with gradient descent at iteration  942\n",
      "Learned weights =  [ -0.3358325  242.9438201   22.53463191]\n",
      "Done with gradient descent at iteration  942\n",
      "Learned weights =  [ -0.3358325  242.94603124  22.53463191]\n",
      "Done with gradient descent at iteration  942\n",
      "Learned weights =  [ -0.3358325  242.94603124  22.53223104]\n",
      "Done with gradient descent at iteration  943\n",
      "Learned weights =  [ -0.33620511 242.94603124  22.53223104]\n",
      "Done with gradient descent at iteration  943\n",
      "Learned weights =  [ -0.33620511 242.94822927  22.53223104]\n",
      "Done with gradient descent at iteration  943\n",
      "Learned weights =  [ -0.33620511 242.94822927  22.52984441]\n",
      "Done with gradient descent at iteration  944\n",
      "Learned weights =  [ -0.3365777  242.94822927  22.52984441]\n",
      "Done with gradient descent at iteration  944\n",
      "Learned weights =  [ -0.3365777  242.95041426  22.52984441]\n",
      "Done with gradient descent at iteration  944\n",
      "Learned weights =  [ -0.3365777  242.95041426  22.52747193]\n",
      "Done with gradient descent at iteration  945\n",
      "Learned weights =  [ -0.3369503  242.95041426  22.52747193]\n",
      "Done with gradient descent at iteration  945\n",
      "Learned weights =  [ -0.3369503  242.95258629  22.52747193]\n",
      "Done with gradient descent at iteration  945\n",
      "Learned weights =  [ -0.3369503  242.95258629  22.52511353]\n",
      "Done with gradient descent at iteration  946\n",
      "Learned weights =  [ -0.33732288 242.95258629  22.52511353]\n",
      "Done with gradient descent at iteration  946\n",
      "Learned weights =  [ -0.33732288 242.95474543  22.52511353]\n",
      "Done with gradient descent at iteration  946\n",
      "Learned weights =  [ -0.33732288 242.95474543  22.52276912]\n",
      "Done with gradient descent at iteration  947\n",
      "Learned weights =  [ -0.33769546 242.95474543  22.52276912]\n",
      "Done with gradient descent at iteration  947\n",
      "Learned weights =  [ -0.33769546 242.95689177  22.52276912]\n",
      "Done with gradient descent at iteration  947\n",
      "Learned weights =  [ -0.33769546 242.95689177  22.52043862]\n",
      "Done with gradient descent at iteration  948\n",
      "Learned weights =  [ -0.33806804 242.95689177  22.52043862]\n",
      "Done with gradient descent at iteration  948\n",
      "Learned weights =  [ -0.33806804 242.95902538  22.52043862]\n",
      "Done with gradient descent at iteration  948\n",
      "Learned weights =  [ -0.33806804 242.95902538  22.51812194]\n",
      "Done with gradient descent at iteration  949\n",
      "Learned weights =  [ -0.33844061 242.95902538  22.51812194]\n",
      "Done with gradient descent at iteration  949\n",
      "Learned weights =  [ -0.33844061 242.96114633  22.51812194]\n",
      "Done with gradient descent at iteration  949\n",
      "Learned weights =  [ -0.33844061 242.96114633  22.515819  ]\n",
      "Done with gradient descent at iteration  950\n",
      "Learned weights =  [ -0.33881317 242.96114633  22.515819  ]\n",
      "Done with gradient descent at iteration  950\n",
      "Learned weights =  [ -0.33881317 242.9632547   22.515819  ]\n",
      "Done with gradient descent at iteration  950\n",
      "Learned weights =  [ -0.33881317 242.9632547   22.51352972]\n",
      "Done with gradient descent at iteration  951\n",
      "Learned weights =  [ -0.33918573 242.9632547   22.51352972]\n",
      "Done with gradient descent at iteration  951\n",
      "Learned weights =  [ -0.33918573 242.96535057  22.51352972]\n",
      "Done with gradient descent at iteration  951\n",
      "Learned weights =  [ -0.33918573 242.96535057  22.51125403]\n",
      "Done with gradient descent at iteration  952\n",
      "Learned weights =  [ -0.33955828 242.96535057  22.51125403]\n",
      "Done with gradient descent at iteration  952\n",
      "Learned weights =  [ -0.33955828 242.967434    22.51125403]\n",
      "Done with gradient descent at iteration  952\n",
      "Learned weights =  [ -0.33955828 242.967434    22.50899183]\n",
      "Done with gradient descent at iteration  953\n",
      "Learned weights =  [ -0.33993083 242.967434    22.50899183]\n",
      "Done with gradient descent at iteration  953\n",
      "Learned weights =  [ -0.33993083 242.96950507  22.50899183]\n",
      "Done with gradient descent at iteration  953\n",
      "Learned weights =  [ -0.33993083 242.96950507  22.50674305]\n",
      "Done with gradient descent at iteration  954\n",
      "Learned weights =  [ -0.34030337 242.96950507  22.50674305]\n",
      "Done with gradient descent at iteration  954\n",
      "Learned weights =  [ -0.34030337 242.97156386  22.50674305]\n",
      "Done with gradient descent at iteration  954\n",
      "Learned weights =  [ -0.34030337 242.97156386  22.50450762]\n",
      "Done with gradient descent at iteration  955\n",
      "Learned weights =  [ -0.3406759  242.97156386  22.50450762]\n",
      "Done with gradient descent at iteration  955\n",
      "Learned weights =  [ -0.3406759  242.97361044  22.50450762]\n",
      "Done with gradient descent at iteration  955\n",
      "Learned weights =  [ -0.3406759  242.97361044  22.50228544]\n",
      "Done with gradient descent at iteration  956\n",
      "Learned weights =  [ -0.34104843 242.97361044  22.50228544]\n",
      "Done with gradient descent at iteration  956\n",
      "Learned weights =  [ -0.34104843 242.97564488  22.50228544]\n",
      "Done with gradient descent at iteration  956\n",
      "Learned weights =  [ -0.34104843 242.97564488  22.50007644]\n",
      "Done with gradient descent at iteration  957\n",
      "Learned weights =  [ -0.34142096 242.97564488  22.50007644]\n",
      "Done with gradient descent at iteration  957\n",
      "Learned weights =  [ -0.34142096 242.97766725  22.50007644]\n",
      "Done with gradient descent at iteration  957\n",
      "Learned weights =  [ -0.34142096 242.97766725  22.49788055]\n",
      "Done with gradient descent at iteration  958\n",
      "Learned weights =  [ -0.34179348 242.97766725  22.49788055]\n",
      "Done with gradient descent at iteration  958\n",
      "Learned weights =  [ -0.34179348 242.97967763  22.49788055]\n",
      "Done with gradient descent at iteration  958\n",
      "Learned weights =  [ -0.34179348 242.97967763  22.49569769]\n",
      "Done with gradient descent at iteration  959\n",
      "Learned weights =  [ -0.34216599 242.97967763  22.49569769]\n",
      "Done with gradient descent at iteration  959\n",
      "Learned weights =  [ -0.34216599 242.98167608  22.49569769]\n",
      "Done with gradient descent at iteration  959\n",
      "Learned weights =  [ -0.34216599 242.98167608  22.49352777]\n",
      "Done with gradient descent at iteration  960\n",
      "Learned weights =  [ -0.3425385  242.98167608  22.49352777]\n",
      "Done with gradient descent at iteration  960\n",
      "Learned weights =  [ -0.3425385  242.98366268  22.49352777]\n",
      "Done with gradient descent at iteration  960\n",
      "Learned weights =  [ -0.3425385  242.98366268  22.49137073]\n",
      "Done with gradient descent at iteration  961\n",
      "Learned weights =  [ -0.342911   242.98366268  22.49137073]\n",
      "Done with gradient descent at iteration  961\n",
      "Learned weights =  [ -0.342911   242.98563749  22.49137073]\n",
      "Done with gradient descent at iteration  961\n",
      "Learned weights =  [ -0.342911   242.98563749  22.48922648]\n",
      "Done with gradient descent at iteration  962\n",
      "Learned weights =  [ -0.3432835  242.98563749  22.48922648]\n",
      "Done with gradient descent at iteration  962\n",
      "Learned weights =  [ -0.3432835  242.98760059  22.48922648]\n",
      "Done with gradient descent at iteration  962\n",
      "Learned weights =  [ -0.3432835  242.98760059  22.48709495]\n",
      "Done with gradient descent at iteration  963\n",
      "Learned weights =  [ -0.34365599 242.98760059  22.48709495]\n",
      "Done with gradient descent at iteration  963\n",
      "Learned weights =  [ -0.34365599 242.98955205  22.48709495]\n",
      "Done with gradient descent at iteration  963\n",
      "Learned weights =  [ -0.34365599 242.98955205  22.48497606]\n",
      "Done with gradient descent at iteration  964\n",
      "Learned weights =  [ -0.34402848 242.98955205  22.48497606]\n",
      "Done with gradient descent at iteration  964\n",
      "Learned weights =  [ -0.34402848 242.99149193  22.48497606]\n",
      "Done with gradient descent at iteration  964\n",
      "Learned weights =  [ -0.34402848 242.99149193  22.48286975]\n",
      "Done with gradient descent at iteration  965\n",
      "Learned weights =  [ -0.34440096 242.99149193  22.48286975]\n",
      "Done with gradient descent at iteration  965\n",
      "Learned weights =  [ -0.34440096 242.9934203   22.48286975]\n",
      "Done with gradient descent at iteration  965\n",
      "Learned weights =  [ -0.34440096 242.9934203   22.48077593]\n",
      "Done with gradient descent at iteration  966\n",
      "Learned weights =  [ -0.34477344 242.9934203   22.48077593]\n",
      "Done with gradient descent at iteration  966\n",
      "Learned weights =  [ -0.34477344 242.99533724  22.48077593]\n",
      "Done with gradient descent at iteration  966\n",
      "Learned weights =  [ -0.34477344 242.99533724  22.47869453]\n",
      "Done with gradient descent at iteration  967\n",
      "Learned weights =  [ -0.34514591 242.99533724  22.47869453]\n",
      "Done with gradient descent at iteration  967\n",
      "Learned weights =  [ -0.34514591 242.99724281  22.47869453]\n",
      "Done with gradient descent at iteration  967\n",
      "Learned weights =  [ -0.34514591 242.99724281  22.47662547]\n",
      "Done with gradient descent at iteration  968\n",
      "Learned weights =  [ -0.34551838 242.99724281  22.47662547]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  968\n",
      "Learned weights =  [ -0.34551838 242.99913707  22.47662547]\n",
      "Done with gradient descent at iteration  968\n",
      "Learned weights =  [ -0.34551838 242.99913707  22.47456869]\n",
      "Done with gradient descent at iteration  969\n",
      "Learned weights =  [ -0.34589084 242.99913707  22.47456869]\n",
      "Done with gradient descent at iteration  969\n",
      "Learned weights =  [ -0.34589084 243.0010201   22.47456869]\n",
      "Done with gradient descent at iteration  969\n",
      "Learned weights =  [ -0.34589084 243.0010201   22.47252412]\n",
      "Done with gradient descent at iteration  970\n",
      "Learned weights =  [ -0.3462633  243.0010201   22.47252412]\n",
      "Done with gradient descent at iteration  970\n",
      "Learned weights =  [ -0.3462633  243.00289195  22.47252412]\n",
      "Done with gradient descent at iteration  970\n",
      "Learned weights =  [ -0.3462633  243.00289195  22.47049166]\n",
      "Done with gradient descent at iteration  971\n",
      "Learned weights =  [ -0.34663575 243.00289195  22.47049166]\n",
      "Done with gradient descent at iteration  971\n",
      "Learned weights =  [ -0.34663575 243.00475271  22.47049166]\n",
      "Done with gradient descent at iteration  971\n",
      "Learned weights =  [ -0.34663575 243.00475271  22.46847127]\n",
      "Done with gradient descent at iteration  972\n",
      "Learned weights =  [ -0.3470082  243.00475271  22.46847127]\n",
      "Done with gradient descent at iteration  972\n",
      "Learned weights =  [ -0.3470082  243.00660243  22.46847127]\n",
      "Done with gradient descent at iteration  972\n",
      "Learned weights =  [ -0.3470082  243.00660243  22.46646286]\n",
      "Done with gradient descent at iteration  973\n",
      "Learned weights =  [ -0.34738064 243.00660243  22.46646286]\n",
      "Done with gradient descent at iteration  973\n",
      "Learned weights =  [ -0.34738064 243.00844117  22.46646286]\n",
      "Done with gradient descent at iteration  973\n",
      "Learned weights =  [ -0.34738064 243.00844117  22.46446637]\n",
      "Done with gradient descent at iteration  974\n",
      "Learned weights =  [ -0.34775308 243.00844117  22.46446637]\n",
      "Done with gradient descent at iteration  974\n",
      "Learned weights =  [ -0.34775308 243.01026901  22.46446637]\n",
      "Done with gradient descent at iteration  974\n",
      "Learned weights =  [ -0.34775308 243.01026901  22.46248172]\n",
      "Done with gradient descent at iteration  975\n",
      "Learned weights =  [ -0.34812551 243.01026901  22.46248172]\n",
      "Done with gradient descent at iteration  975\n",
      "Learned weights =  [ -0.34812551 243.01208601  22.46248172]\n",
      "Done with gradient descent at iteration  975\n",
      "Learned weights =  [ -0.34812551 243.01208601  22.46050884]\n",
      "Done with gradient descent at iteration  976\n",
      "Learned weights =  [ -0.34849794 243.01208601  22.46050884]\n",
      "Done with gradient descent at iteration  976\n",
      "Learned weights =  [ -0.34849794 243.01389223  22.46050884]\n",
      "Done with gradient descent at iteration  976\n",
      "Learned weights =  [ -0.34849794 243.01389223  22.45854766]\n",
      "Done with gradient descent at iteration  977\n",
      "Learned weights =  [ -0.34887036 243.01389223  22.45854766]\n",
      "Done with gradient descent at iteration  977\n",
      "Learned weights =  [ -0.34887036 243.01568773  22.45854766]\n",
      "Done with gradient descent at iteration  977\n",
      "Learned weights =  [ -0.34887036 243.01568773  22.45659812]\n",
      "Done with gradient descent at iteration  978\n",
      "Learned weights =  [ -0.34924278 243.01568773  22.45659812]\n",
      "Done with gradient descent at iteration  978\n",
      "Learned weights =  [ -0.34924278 243.01747259  22.45659812]\n",
      "Done with gradient descent at iteration  978\n",
      "Learned weights =  [ -0.34924278 243.01747259  22.45466015]\n",
      "Done with gradient descent at iteration  979\n",
      "Learned weights =  [ -0.34961519 243.01747259  22.45466015]\n",
      "Done with gradient descent at iteration  979\n",
      "Learned weights =  [ -0.34961519 243.01924686  22.45466015]\n",
      "Done with gradient descent at iteration  979\n",
      "Learned weights =  [ -0.34961519 243.01924686  22.45273367]\n",
      "Done with gradient descent at iteration  980\n",
      "Learned weights =  [ -0.3499876  243.01924686  22.45273367]\n",
      "Done with gradient descent at iteration  980\n",
      "Learned weights =  [ -0.3499876  243.0210106   22.45273367]\n",
      "Done with gradient descent at iteration  980\n",
      "Learned weights =  [ -0.3499876  243.0210106   22.45081861]\n",
      "Done with gradient descent at iteration  981\n",
      "Learned weights =  [ -0.35036    243.0210106   22.45081861]\n",
      "Done with gradient descent at iteration  981\n",
      "Learned weights =  [ -0.35036    243.02276388  22.45081861]\n",
      "Done with gradient descent at iteration  981\n",
      "Learned weights =  [ -0.35036    243.02276388  22.44891492]\n",
      "Done with gradient descent at iteration  982\n",
      "Learned weights =  [ -0.3507324  243.02276388  22.44891492]\n",
      "Done with gradient descent at iteration  982\n",
      "Learned weights =  [ -0.3507324  243.02450677  22.44891492]\n",
      "Done with gradient descent at iteration  982\n",
      "Learned weights =  [ -0.3507324  243.02450677  22.44702252]\n",
      "Done with gradient descent at iteration  983\n",
      "Learned weights =  [ -0.3511048  243.02450677  22.44702252]\n",
      "Done with gradient descent at iteration  983\n",
      "Learned weights =  [ -0.3511048  243.02623931  22.44702252]\n",
      "Done with gradient descent at iteration  983\n",
      "Learned weights =  [ -0.3511048  243.02623931  22.44514135]\n",
      "Done with gradient descent at iteration  984\n",
      "Learned weights =  [ -0.35147719 243.02623931  22.44514135]\n",
      "Done with gradient descent at iteration  984\n",
      "Learned weights =  [ -0.35147719 243.02796158  22.44514135]\n",
      "Done with gradient descent at iteration  984\n",
      "Learned weights =  [ -0.35147719 243.02796158  22.44327133]\n",
      "Done with gradient descent at iteration  985\n",
      "Learned weights =  [ -0.35184957 243.02796158  22.44327133]\n",
      "Done with gradient descent at iteration  985\n",
      "Learned weights =  [ -0.35184957 243.02967363  22.44327133]\n",
      "Done with gradient descent at iteration  985\n",
      "Learned weights =  [ -0.35184957 243.02967363  22.44141241]\n",
      "Done with gradient descent at iteration  986\n",
      "Learned weights =  [ -0.35222195 243.02967363  22.44141241]\n",
      "Done with gradient descent at iteration  986\n",
      "Learned weights =  [ -0.35222195 243.03137553  22.44141241]\n",
      "Done with gradient descent at iteration  986\n",
      "Learned weights =  [ -0.35222195 243.03137553  22.43956452]\n",
      "Done with gradient descent at iteration  987\n",
      "Learned weights =  [ -0.35259433 243.03137553  22.43956452]\n",
      "Done with gradient descent at iteration  987\n",
      "Learned weights =  [ -0.35259433 243.03306733  22.43956452]\n",
      "Done with gradient descent at iteration  987\n",
      "Learned weights =  [ -0.35259433 243.03306733  22.43772759]\n",
      "Done with gradient descent at iteration  988\n",
      "Learned weights =  [ -0.3529667  243.03306733  22.43772759]\n",
      "Done with gradient descent at iteration  988\n",
      "Learned weights =  [ -0.3529667  243.0347491   22.43772759]\n",
      "Done with gradient descent at iteration  988\n",
      "Learned weights =  [ -0.3529667  243.0347491   22.43590155]\n",
      "Done with gradient descent at iteration  989\n",
      "Learned weights =  [ -0.35333907 243.0347491   22.43590155]\n",
      "Done with gradient descent at iteration  989\n",
      "Learned weights =  [ -0.35333907 243.03642089  22.43590155]\n",
      "Done with gradient descent at iteration  989\n",
      "Learned weights =  [ -0.35333907 243.03642089  22.43408635]\n",
      "Done with gradient descent at iteration  990\n",
      "Learned weights =  [ -0.35371143 243.03642089  22.43408635]\n",
      "Done with gradient descent at iteration  990\n",
      "Learned weights =  [ -0.35371143 243.03808277  22.43408635]\n",
      "Done with gradient descent at iteration  990\n",
      "Learned weights =  [ -0.35371143 243.03808277  22.43228192]\n",
      "Done with gradient descent at iteration  991\n",
      "Learned weights =  [ -0.35408379 243.03808277  22.43228192]\n",
      "Done with gradient descent at iteration  991\n",
      "Learned weights =  [ -0.35408379 243.03973479  22.43228192]\n",
      "Done with gradient descent at iteration  991\n",
      "Learned weights =  [ -0.35408379 243.03973479  22.43048819]\n",
      "Done with gradient descent at iteration  992\n",
      "Learned weights =  [ -0.35445614 243.03973479  22.43048819]\n",
      "Done with gradient descent at iteration  992\n",
      "Learned weights =  [ -0.35445614 243.04137701  22.43048819]\n",
      "Done with gradient descent at iteration  992\n",
      "Learned weights =  [ -0.35445614 243.04137701  22.4287051 ]\n",
      "Done with gradient descent at iteration  993\n",
      "Learned weights =  [ -0.35482849 243.04137701  22.4287051 ]\n",
      "Done with gradient descent at iteration  993\n",
      "Learned weights =  [ -0.35482849 243.04300949  22.4287051 ]\n",
      "Done with gradient descent at iteration  993\n",
      "Learned weights =  [ -0.35482849 243.04300949  22.42693258]\n",
      "Done with gradient descent at iteration  994\n",
      "Learned weights =  [ -0.35520084 243.04300949  22.42693258]\n",
      "Done with gradient descent at iteration  994\n",
      "Learned weights =  [ -0.35520084 243.04463228  22.42693258]\n",
      "Done with gradient descent at iteration  994\n",
      "Learned weights =  [ -0.35520084 243.04463228  22.42517058]\n",
      "Done with gradient descent at iteration  995\n",
      "Learned weights =  [ -0.35557318 243.04463228  22.42517058]\n",
      "Done with gradient descent at iteration  995\n",
      "Learned weights =  [ -0.35557318 243.04624545  22.42517058]\n",
      "Done with gradient descent at iteration  995\n",
      "Learned weights =  [ -0.35557318 243.04624545  22.42341904]\n",
      "Done with gradient descent at iteration  996\n",
      "Learned weights =  [ -0.35594551 243.04624545  22.42341904]\n",
      "Done with gradient descent at iteration  996\n",
      "Learned weights =  [ -0.35594551 243.04784905  22.42341904]\n",
      "Done with gradient descent at iteration  996\n",
      "Learned weights =  [ -0.35594551 243.04784905  22.42167788]\n",
      "Done with gradient descent at iteration  997\n",
      "Learned weights =  [ -0.35631785 243.04784905  22.42167788]\n",
      "Done with gradient descent at iteration  997\n",
      "Learned weights =  [ -0.35631785 243.04944314  22.42167788]\n",
      "Done with gradient descent at iteration  997\n",
      "Learned weights =  [ -0.35631785 243.04944314  22.41994706]\n",
      "Done with gradient descent at iteration  998\n",
      "Learned weights =  [ -0.35669018 243.04944314  22.41994706]\n",
      "Done with gradient descent at iteration  998\n",
      "Learned weights =  [ -0.35669018 243.05102777  22.41994706]\n",
      "Done with gradient descent at iteration  998\n",
      "Learned weights =  [ -0.35669018 243.05102777  22.4182265 ]\n",
      "Done with gradient descent at iteration  999\n",
      "Learned weights =  [ -0.3570625  243.05102777  22.4182265 ]\n",
      "Done with gradient descent at iteration  999\n",
      "Learned weights =  [ -0.3570625  243.05260301  22.4182265 ]\n",
      "Done with gradient descent at iteration  999\n",
      "Learned weights =  [ -0.3570625  243.05260301  22.41651615]\n",
      "Iteration = 1000\n",
      "Cost function =  1205607858660559.5\n",
      "Done with gradient descent at iteration  1000\n",
      "Learned weights =  [ -0.35743482 243.05260301  22.41651615]\n",
      "Done with gradient descent at iteration  1000\n",
      "Learned weights =  [ -0.35743482 243.0541689   22.41651615]\n",
      "Done with gradient descent at iteration  1000\n",
      "Learned weights =  [ -0.35743482 243.0541689   22.41481594]\n"
     ]
    }
   ],
   "source": [
    "multiple_weights_0_penalty = ridge_regression_gradient_descent(feature_matrix, output, initial_weights, step_size, 0, max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's consider high regularization.  Set the `l2_penalty` to `1e11` and run your ridge regression algorithm to learn the weights of your model.  Call your weights:\n",
    "\n",
    "`multiple_weights_high_penalty`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gradient descent with l2_penalty = 100000000000.0\n",
      "Iteration = 1\n",
      "Cost function =  7433051851026171.0\n",
      "Done with gradient descent at iteration  1\n",
      "Learned weights =  [0.0187527 0.        0.       ]\n",
      "Done with gradient descent at iteration  1\n",
      "Learned weights =  [1.87526989e-02 4.73325137e+01 0.00000000e+00]\n",
      "Done with gradient descent at iteration  1\n",
      "Learned weights =  [1.87526989e-02 4.73325137e+01 4.23911280e+01]\n",
      "Iteration = 2\n",
      "Cost function =  4460489790285892.0\n",
      "Done with gradient descent at iteration  2\n",
      "Learned weights =  [3.11553143e-02 4.73325137e+01 4.23911280e+01]\n",
      "Done with gradient descent at iteration  2\n",
      "Learned weights =  [3.11553143e-02 6.98872133e+01 4.23911280e+01]\n",
      "Done with gradient descent at iteration  2\n",
      "Learned weights =  [3.11553143e-02 6.98872133e+01 6.22108016e+01]\n",
      "Iteration = 3\n",
      "Cost function =  3796674468844605.5\n",
      "Done with gradient descent at iteration  3\n",
      "Learned weights =  [4.05582778e-02 6.98872133e+01 6.22108016e+01]\n",
      "Done with gradient descent at iteration  3\n",
      "Learned weights =  [4.05582778e-02 8.06958471e+01 6.22108016e+01]\n",
      "Done with gradient descent at iteration  3\n",
      "Learned weights =  [4.05582778e-02 8.06958471e+01 7.14092854e+01]\n",
      "Iteration = 4\n",
      "Cost function =  3648319530437361.0\n",
      "Done with gradient descent at iteration  4\n",
      "Learned weights =  [4.85444304e-02 8.06958471e+01 7.14092854e+01]\n",
      "Done with gradient descent at iteration  4\n",
      "Learned weights =  [4.85444304e-02 8.59235400e+01 7.14092854e+01]\n",
      "Done with gradient descent at iteration  4\n",
      "Learned weights =  [4.85444304e-02 8.59235400e+01 7.56237653e+01]\n",
      "Iteration = 5\n",
      "Cost function =  3615091103216103.0\n",
      "Done with gradient descent at iteration  5\n",
      "Learned weights =  [5.58615181e-02 8.59235400e+01 7.56237653e+01]\n",
      "Done with gradient descent at iteration  5\n",
      "Learned weights =  [5.58615181e-02 8.84895333e+01 7.56237653e+01]\n",
      "Done with gradient descent at iteration  5\n",
      "Learned weights =  [5.58615181e-02 8.84895333e+01 7.75105684e+01]\n",
      "Iteration = 6\n",
      "Cost function =  3607602742514732.0\n",
      "Done with gradient descent at iteration  6\n",
      "Learned weights =  [6.28627548e-02 8.84895333e+01 7.75105684e+01]\n",
      "Done with gradient descent at iteration  6\n",
      "Learned weights =  [6.28627548e-02 8.97781857e+01 7.75105684e+01]\n",
      "Done with gradient descent at iteration  6\n",
      "Learned weights =  [6.28627548e-02 8.97781857e+01 7.83191298e+01]\n",
      "Iteration = 7\n",
      "Cost function =  3605886322161656.0\n",
      "Done with gradient descent at iteration  7\n",
      "Learned weights =  [6.97149675e-02 8.97781857e+01 7.83191298e+01]\n",
      "Done with gradient descent at iteration  7\n",
      "Learned weights =  [6.97149675e-02 9.04476279e+01 7.83191298e+01]\n",
      "Done with gradient descent at iteration  7\n",
      "Learned weights =  [6.97149675e-02 9.04476279e+01 7.86353320e+01]\n",
      "Iteration = 8\n",
      "Cost function =  3605474874533296.0\n",
      "Done with gradient descent at iteration  8\n",
      "Learned weights =  [7.64969334e-02 9.04476279e+01 7.86353320e+01]\n",
      "Done with gradient descent at iteration  8\n",
      "Learned weights =  [7.64969334e-02 9.08120373e+01 7.86353320e+01]\n",
      "Done with gradient descent at iteration  8\n",
      "Learned weights =  [7.64969334e-02 9.08120373e+01 7.87324664e+01]\n",
      "Iteration = 9\n",
      "Cost function =  3605365167765576.0\n",
      "Done with gradient descent at iteration  9\n",
      "Learned weights =  [8.32458386e-02 9.08120373e+01 7.87324664e+01]\n",
      "Done with gradient descent at iteration  9\n",
      "Learned weights =  [8.32458386e-02 9.10224203e+01 7.87324664e+01]\n",
      "Done with gradient descent at iteration  9\n",
      "Learned weights =  [8.32458386e-02 9.10224203e+01 7.87368616e+01]\n",
      "Iteration = 10\n",
      "Cost function =  3605329402184649.0\n",
      "Done with gradient descent at iteration  10\n",
      "Learned weights =  [8.99792254e-02 9.10224203e+01 7.87368616e+01]\n",
      "Done with gradient descent at iteration  10\n",
      "Learned weights =  [8.99792254e-02 9.11521624e+01 7.87368616e+01]\n",
      "Done with gradient descent at iteration  10\n",
      "Learned weights =  [8.99792254e-02 9.11521624e+01 7.87059866e+01]\n",
      "Done with gradient descent at iteration  11\n",
      "Learned weights =  [ 0.09670536 91.15216241 78.7059866 ]\n",
      "Done with gradient descent at iteration  11\n",
      "Learned weights =  [ 0.09670536 91.23755589 78.7059866 ]\n",
      "Done with gradient descent at iteration  11\n",
      "Learned weights =  [ 0.09670536 91.23755589 78.66523352]\n",
      "Done with gradient descent at iteration  12\n",
      "Learned weights =  [ 0.10342814 91.23755589 78.66523352]\n",
      "Done with gradient descent at iteration  12\n",
      "Learned weights =  [ 0.10342814 91.2970343  78.66523352]\n",
      "Done with gradient descent at iteration  12\n",
      "Learned weights =  [ 0.10342814 91.2970343  78.62520172]\n",
      "Done with gradient descent at iteration  13\n",
      "Learned weights =  [ 0.11014937 91.2970343  78.62520172]\n",
      "Done with gradient descent at iteration  13\n",
      "Learned weights =  [ 0.11014937 91.34032859 78.62520172]\n",
      "Done with gradient descent at iteration  13\n",
      "Learned weights =  [ 0.11014937 91.34032859 78.58978935]\n",
      "Done with gradient descent at iteration  14\n",
      "Learned weights =  [ 0.11686992 91.34032859 78.58978935]\n",
      "Done with gradient descent at iteration  14\n",
      "Learned weights =  [ 0.11686992 91.3728477  78.58978935]\n",
      "Done with gradient descent at iteration  14\n",
      "Learned weights =  [ 0.11686992 91.3728477  78.55995704]\n",
      "Done with gradient descent at iteration  15\n",
      "Learned weights =  [ 0.12359018 91.3728477  78.55995704]\n",
      "Done with gradient descent at iteration  15\n",
      "Learned weights =  [ 0.12359018 91.39779144 78.55995704]\n",
      "Done with gradient descent at iteration  15\n",
      "Learned weights =  [ 0.12359018 91.39779144 78.53545896]\n",
      "Done with gradient descent at iteration  16\n",
      "Learned weights =  [ 0.13031033 91.39779144 78.53545896]\n",
      "Done with gradient descent at iteration  16\n",
      "Learned weights =  [ 0.13031033 91.41718326 78.53545896]\n",
      "Done with gradient descent at iteration  16\n",
      "Learned weights =  [ 0.13031033 91.41718326 78.51562342]\n",
      "Done with gradient descent at iteration  17\n",
      "Learned weights =  [ 0.13703044 91.41718326 78.51562342]\n",
      "Done with gradient descent at iteration  17\n",
      "Learned weights =  [ 0.13703044 91.43238542 78.51562342]\n",
      "Done with gradient descent at iteration  17\n",
      "Learned weights =  [ 0.13703044 91.43238542 78.49969187]\n",
      "Done with gradient descent at iteration  18\n",
      "Learned weights =  [ 0.14375055 91.43238542 78.49969187]\n",
      "Done with gradient descent at iteration  18\n",
      "Learned weights =  [ 0.14375055 91.44436414 78.49969187]\n",
      "Done with gradient descent at iteration  18\n",
      "Learned weights =  [ 0.14375055 91.44436414 78.48695563]\n",
      "Done with gradient descent at iteration  19\n",
      "Learned weights =  [ 0.15047068 91.44436414 78.48695563]\n",
      "Done with gradient descent at iteration  19\n",
      "Learned weights =  [ 0.15047068 91.45383211 78.48695563]\n",
      "Done with gradient descent at iteration  19\n",
      "Learned weights =  [ 0.15047068 91.45383211 78.47680168]\n",
      "Iteration = 20\n",
      "Cost function =  3605294281022695.0\n",
      "Done with gradient descent at iteration  20\n",
      "Learned weights =  [ 0.15719082 91.45383211 78.47680168]\n",
      "Done with gradient descent at iteration  20\n",
      "Learned weights =  [ 0.15719082 91.46132946 78.47680168]\n",
      "Done with gradient descent at iteration  20\n",
      "Learned weights =  [ 0.15719082 91.46132946 78.46871952]\n",
      "Done with gradient descent at iteration  21\n",
      "Learned weights =  [ 0.16391098 91.46132946 78.46871952]\n",
      "Done with gradient descent at iteration  21\n",
      "Learned weights =  [ 0.16391098 91.46727291 78.46871952]\n",
      "Done with gradient descent at iteration  21\n",
      "Learned weights =  [ 0.16391098 91.46727291 78.46229255]\n",
      "Done with gradient descent at iteration  22\n",
      "Learned weights =  [ 0.17063115 91.46727291 78.46229255]\n",
      "Done with gradient descent at iteration  22\n",
      "Learned weights =  [ 0.17063115 91.47198762 78.46229255]\n",
      "Done with gradient descent at iteration  22\n",
      "Learned weights =  [ 0.17063115 91.47198762 78.45718466]\n",
      "Done with gradient descent at iteration  23\n",
      "Learned weights =  [ 0.17735133 91.47198762 78.45718466]\n",
      "Done with gradient descent at iteration  23\n",
      "Learned weights =  [ 0.17735133 91.47572905 78.45718466]\n",
      "Done with gradient descent at iteration  23\n",
      "Learned weights =  [ 0.17735133 91.47572905 78.45312645]\n",
      "Done with gradient descent at iteration  24\n",
      "Learned weights =  [ 0.18407153 91.47572905 78.45312645]\n",
      "Done with gradient descent at iteration  24\n",
      "Learned weights =  [ 0.18407153 91.47869879 78.45312645]\n",
      "Done with gradient descent at iteration  24\n",
      "Learned weights =  [ 0.18407153 91.47869879 78.44990282]\n",
      "Done with gradient descent at iteration  25\n",
      "Learned weights =  [ 0.19079173 91.47869879 78.44990282]\n",
      "Done with gradient descent at iteration  25\n",
      "Learned weights =  [ 0.19079173 91.4810563  78.44990282]\n",
      "Done with gradient descent at iteration  25\n",
      "Learned weights =  [ 0.19079173 91.4810563  78.4473424 ]\n",
      "Done with gradient descent at iteration  26\n",
      "Learned weights =  [ 0.19751193 91.4810563  78.4473424 ]\n",
      "Done with gradient descent at iteration  26\n",
      "Learned weights =  [ 0.19751193 91.48292791 78.4473424 ]\n",
      "Done with gradient descent at iteration  26\n",
      "Learned weights =  [ 0.19751193 91.48292791 78.44530885]\n",
      "Done with gradient descent at iteration  27\n",
      "Learned weights =  [ 0.20423214 91.48292791 78.44530885]\n",
      "Done with gradient descent at iteration  27\n",
      "Learned weights =  [ 0.20423214 91.4844138  78.44530885]\n",
      "Done with gradient descent at iteration  27\n",
      "Learned weights =  [ 0.20423214 91.4844138  78.44369379]\n",
      "Done with gradient descent at iteration  28\n",
      "Learned weights =  [ 0.21095236 91.4844138  78.44369379]\n",
      "Done with gradient descent at iteration  28\n",
      "Learned weights =  [ 0.21095236 91.48559346 78.44369379]\n",
      "Done with gradient descent at iteration  28\n",
      "Learned weights =  [ 0.21095236 91.48559346 78.44241108]\n",
      "Done with gradient descent at iteration  29\n",
      "Learned weights =  [ 0.21767258 91.48559346 78.44241108]\n",
      "Done with gradient descent at iteration  29\n",
      "Learned weights =  [ 0.21767258 91.48652998 78.44241108]\n",
      "Done with gradient descent at iteration  29\n",
      "Learned weights =  [ 0.21767258 91.48652998 78.44139232]\n",
      "Iteration = 30\n",
      "Cost function =  3605293537267100.0\n",
      "Done with gradient descent at iteration  30\n",
      "Learned weights =  [ 0.2243928  91.48652998 78.44139232]\n",
      "Done with gradient descent at iteration  30\n",
      "Learned weights =  [ 0.2243928  91.48727345 78.44139232]\n",
      "Done with gradient descent at iteration  30\n",
      "Learned weights =  [ 0.2243928  91.48727345 78.44058316]\n",
      "Done with gradient descent at iteration  31\n",
      "Learned weights =  [ 0.23111302 91.48727345 78.44058316]\n",
      "Done with gradient descent at iteration  31\n",
      "Learned weights =  [ 0.23111302 91.48786362 78.44058316]\n",
      "Done with gradient descent at iteration  31\n",
      "Learned weights =  [ 0.23111302 91.48786362 78.43994043]\n",
      "Done with gradient descent at iteration  32\n",
      "Learned weights =  [ 0.23783324 91.48786362 78.43994043]\n",
      "Done with gradient descent at iteration  32\n",
      "Learned weights =  [ 0.23783324 91.48833207 78.43994043]\n",
      "Done with gradient descent at iteration  32\n",
      "Learned weights =  [ 0.23783324 91.48833207 78.43942988]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  33\n",
      "Learned weights =  [ 0.24455347 91.48833207 78.43942988]\n",
      "Done with gradient descent at iteration  33\n",
      "Learned weights =  [ 0.24455347 91.48870387 78.43942988]\n",
      "Done with gradient descent at iteration  33\n",
      "Learned weights =  [ 0.24455347 91.48870387 78.43902427]\n",
      "Done with gradient descent at iteration  34\n",
      "Learned weights =  [ 0.25127369 91.48870387 78.43902427]\n",
      "Done with gradient descent at iteration  34\n",
      "Learned weights =  [ 0.25127369 91.48899892 78.43902427]\n",
      "Done with gradient descent at iteration  34\n",
      "Learned weights =  [ 0.25127369 91.48899892 78.43870201]\n",
      "Done with gradient descent at iteration  35\n",
      "Learned weights =  [ 0.25799392 91.48899892 78.43870201]\n",
      "Done with gradient descent at iteration  35\n",
      "Learned weights =  [ 0.25799392 91.48923302 78.43870201]\n",
      "Done with gradient descent at iteration  35\n",
      "Learned weights =  [ 0.25799392 91.48923302 78.43844592]\n",
      "Done with gradient descent at iteration  36\n",
      "Learned weights =  [ 0.26471415 91.48923302 78.43844592]\n",
      "Done with gradient descent at iteration  36\n",
      "Learned weights =  [ 0.26471415 91.48941873 78.43844592]\n",
      "Done with gradient descent at iteration  36\n",
      "Learned weights =  [ 0.26471415 91.48941873 78.43824238]\n",
      "Done with gradient descent at iteration  37\n",
      "Learned weights =  [ 0.27143437 91.48941873 78.43824238]\n",
      "Done with gradient descent at iteration  37\n",
      "Learned weights =  [ 0.27143437 91.48956601 78.43824238]\n",
      "Done with gradient descent at iteration  37\n",
      "Learned weights =  [ 0.27143437 91.48956601 78.43808057]\n",
      "Done with gradient descent at iteration  38\n",
      "Learned weights =  [ 0.2781546  91.48956601 78.43808057]\n",
      "Done with gradient descent at iteration  38\n",
      "Learned weights =  [ 0.2781546  91.48968277 78.43808057]\n",
      "Done with gradient descent at iteration  38\n",
      "Learned weights =  [ 0.2781546  91.48968277 78.4379519 ]\n",
      "Done with gradient descent at iteration  39\n",
      "Learned weights =  [ 0.28487483 91.48968277 78.4379519 ]\n",
      "Done with gradient descent at iteration  39\n",
      "Learned weights =  [ 0.28487483 91.48977531 78.4379519 ]\n",
      "Done with gradient descent at iteration  39\n",
      "Learned weights =  [ 0.28487483 91.48977531 78.43784954]\n",
      "Iteration = 40\n",
      "Cost function =  3605293082749905.0\n",
      "Done with gradient descent at iteration  40\n",
      "Learned weights =  [ 0.29159506 91.48977531 78.43784954]\n",
      "Done with gradient descent at iteration  40\n",
      "Learned weights =  [ 0.29159506 91.4898486  78.43784954]\n",
      "Done with gradient descent at iteration  40\n",
      "Learned weights =  [ 0.29159506 91.4898486  78.43776807]\n",
      "Done with gradient descent at iteration  41\n",
      "Learned weights =  [ 0.29831529 91.4898486  78.43776807]\n",
      "Done with gradient descent at iteration  41\n",
      "Learned weights =  [ 0.29831529 91.48990662 78.43776807]\n",
      "Done with gradient descent at iteration  41\n",
      "Learned weights =  [ 0.29831529 91.48990662 78.43770319]\n",
      "Done with gradient descent at iteration  42\n",
      "Learned weights =  [ 0.30503551 91.48990662 78.43770319]\n",
      "Done with gradient descent at iteration  42\n",
      "Learned weights =  [ 0.30503551 91.48995251 78.43770319]\n",
      "Done with gradient descent at iteration  42\n",
      "Learned weights =  [ 0.30503551 91.48995251 78.43765149]\n",
      "Done with gradient descent at iteration  43\n",
      "Learned weights =  [ 0.31175574 91.48995251 78.43765149]\n",
      "Done with gradient descent at iteration  43\n",
      "Learned weights =  [ 0.31175574 91.48998876 78.43765149]\n",
      "Done with gradient descent at iteration  43\n",
      "Learned weights =  [ 0.31175574 91.48998876 78.43761024]\n",
      "Done with gradient descent at iteration  44\n",
      "Learned weights =  [ 0.31847597 91.48998876 78.43761024]\n",
      "Done with gradient descent at iteration  44\n",
      "Learned weights =  [ 0.31847597 91.49001736 78.43761024]\n",
      "Done with gradient descent at iteration  44\n",
      "Learned weights =  [ 0.31847597 91.49001736 78.4375773 ]\n",
      "Done with gradient descent at iteration  45\n",
      "Learned weights =  [ 0.3251962  91.49001736 78.4375773 ]\n",
      "Done with gradient descent at iteration  45\n",
      "Learned weights =  [ 0.3251962  91.49003988 78.4375773 ]\n",
      "Done with gradient descent at iteration  45\n",
      "Learned weights =  [ 0.3251962  91.49003988 78.43755096]\n",
      "Done with gradient descent at iteration  46\n",
      "Learned weights =  [ 0.33191643 91.49003988 78.43755096]\n",
      "Done with gradient descent at iteration  46\n",
      "Learned weights =  [ 0.33191643 91.49005758 78.43755096]\n",
      "Done with gradient descent at iteration  46\n",
      "Learned weights =  [ 0.33191643 91.49005758 78.43752986]\n",
      "Done with gradient descent at iteration  47\n",
      "Learned weights =  [ 0.33863665 91.49005758 78.43752986]\n",
      "Done with gradient descent at iteration  47\n",
      "Learned weights =  [ 0.33863665 91.49007146 78.43752986]\n",
      "Done with gradient descent at iteration  47\n",
      "Learned weights =  [ 0.33863665 91.49007146 78.43751292]\n",
      "Done with gradient descent at iteration  48\n",
      "Learned weights =  [ 0.34535688 91.49007146 78.43751292]\n",
      "Done with gradient descent at iteration  48\n",
      "Learned weights =  [ 0.34535688 91.49008229 78.43751292]\n",
      "Done with gradient descent at iteration  48\n",
      "Learned weights =  [ 0.34535688 91.49008229 78.43749928]\n",
      "Done with gradient descent at iteration  49\n",
      "Learned weights =  [ 0.35207711 91.49008229 78.43749928]\n",
      "Done with gradient descent at iteration  49\n",
      "Learned weights =  [ 0.35207711 91.4900907  78.43749928]\n",
      "Done with gradient descent at iteration  49\n",
      "Learned weights =  [ 0.35207711 91.4900907  78.43748826]\n",
      "Iteration = 50\n",
      "Cost function =  3605292631106358.0\n",
      "Done with gradient descent at iteration  50\n",
      "Learned weights =  [ 0.35879734 91.4900907  78.43748826]\n",
      "Done with gradient descent at iteration  50\n",
      "Learned weights =  [ 0.35879734 91.4900972  78.43748826]\n",
      "Done with gradient descent at iteration  50\n",
      "Learned weights =  [ 0.35879734 91.4900972  78.43747932]\n",
      "Done with gradient descent at iteration  51\n",
      "Learned weights =  [ 0.36551757 91.4900972  78.43747932]\n",
      "Done with gradient descent at iteration  51\n",
      "Learned weights =  [ 0.36551757 91.49010218 78.43747932]\n",
      "Done with gradient descent at iteration  51\n",
      "Learned weights =  [ 0.36551757 91.49010218 78.43747204]\n",
      "Done with gradient descent at iteration  52\n",
      "Learned weights =  [ 0.37223779 91.49010218 78.43747204]\n",
      "Done with gradient descent at iteration  52\n",
      "Learned weights =  [ 0.37223779 91.49010594 78.43747204]\n",
      "Done with gradient descent at iteration  52\n",
      "Learned weights =  [ 0.37223779 91.49010594 78.43746607]\n",
      "Done with gradient descent at iteration  53\n",
      "Learned weights =  [ 0.37895802 91.49010594 78.43746607]\n",
      "Done with gradient descent at iteration  53\n",
      "Learned weights =  [ 0.37895802 91.49010875 78.43746607]\n",
      "Done with gradient descent at iteration  53\n",
      "Learned weights =  [ 0.37895802 91.49010875 78.43746114]\n",
      "Done with gradient descent at iteration  54\n",
      "Learned weights =  [ 0.38567825 91.49010875 78.43746114]\n",
      "Done with gradient descent at iteration  54\n",
      "Learned weights =  [ 0.38567825 91.49011079 78.43746114]\n",
      "Done with gradient descent at iteration  54\n",
      "Learned weights =  [ 0.38567825 91.49011079 78.43745704]\n",
      "Done with gradient descent at iteration  55\n",
      "Learned weights =  [ 0.39239848 91.49011079 78.43745704]\n",
      "Done with gradient descent at iteration  55\n",
      "Learned weights =  [ 0.39239848 91.49011223 78.43745704]\n",
      "Done with gradient descent at iteration  55\n",
      "Learned weights =  [ 0.39239848 91.49011223 78.4374536 ]\n",
      "Done with gradient descent at iteration  56\n",
      "Learned weights =  [ 0.3991187  91.49011223 78.4374536 ]\n",
      "Done with gradient descent at iteration  56\n",
      "Learned weights =  [ 0.3991187  91.49011319 78.4374536 ]\n",
      "Done with gradient descent at iteration  56\n",
      "Learned weights =  [ 0.3991187  91.49011319 78.43745068]\n",
      "Done with gradient descent at iteration  57\n",
      "Learned weights =  [ 0.40583893 91.49011319 78.43745068]\n",
      "Done with gradient descent at iteration  57\n",
      "Learned weights =  [ 0.40583893 91.49011376 78.43745068]\n",
      "Done with gradient descent at iteration  57\n",
      "Learned weights =  [ 0.40583893 91.49011376 78.43744818]\n",
      "Done with gradient descent at iteration  58\n",
      "Learned weights =  [ 0.41255916 91.49011376 78.43744818]\n",
      "Done with gradient descent at iteration  58\n",
      "Learned weights =  [ 0.41255916 91.49011403 78.43744818]\n",
      "Done with gradient descent at iteration  58\n",
      "Learned weights =  [ 0.41255916 91.49011403 78.437446  ]\n",
      "Done with gradient descent at iteration  59\n",
      "Learned weights =  [ 0.41927938 91.49011403 78.437446  ]\n",
      "Done with gradient descent at iteration  59\n",
      "Learned weights =  [ 0.41927938 91.49011406 78.437446  ]\n",
      "Done with gradient descent at iteration  59\n",
      "Learned weights =  [ 0.41927938 91.49011406 78.43744409]\n",
      "Iteration = 60\n",
      "Cost function =  3605292179491501.0\n",
      "Done with gradient descent at iteration  60\n",
      "Learned weights =  [ 0.42599961 91.49011406 78.43744409]\n",
      "Done with gradient descent at iteration  60\n",
      "Learned weights =  [ 0.42599961 91.4901139  78.43744409]\n",
      "Done with gradient descent at iteration  60\n",
      "Learned weights =  [ 0.42599961 91.4901139  78.43744238]\n",
      "Done with gradient descent at iteration  61\n",
      "Learned weights =  [ 0.43271984 91.4901139  78.43744238]\n",
      "Done with gradient descent at iteration  61\n",
      "Learned weights =  [ 0.43271984 91.49011359 78.43744238]\n",
      "Done with gradient descent at iteration  61\n",
      "Learned weights =  [ 0.43271984 91.49011359 78.43744084]\n",
      "Done with gradient descent at iteration  62\n",
      "Learned weights =  [ 0.43944006 91.49011359 78.43744084]\n",
      "Done with gradient descent at iteration  62\n",
      "Learned weights =  [ 0.43944006 91.49011316 78.43744084]\n",
      "Done with gradient descent at iteration  62\n",
      "Learned weights =  [ 0.43944006 91.49011316 78.43743943]\n",
      "Done with gradient descent at iteration  63\n",
      "Learned weights =  [ 0.44616029 91.49011316 78.43743943]\n",
      "Done with gradient descent at iteration  63\n",
      "Learned weights =  [ 0.44616029 91.49011263 78.43743943]\n",
      "Done with gradient descent at iteration  63\n",
      "Learned weights =  [ 0.44616029 91.49011263 78.43743813]\n",
      "Done with gradient descent at iteration  64\n",
      "Learned weights =  [ 0.45288052 91.49011263 78.43743813]\n",
      "Done with gradient descent at iteration  64\n",
      "Learned weights =  [ 0.45288052 91.49011203 78.43743813]\n",
      "Done with gradient descent at iteration  64\n",
      "Learned weights =  [ 0.45288052 91.49011203 78.4374369 ]\n",
      "Done with gradient descent at iteration  65\n",
      "Learned weights =  [ 0.45960074 91.49011203 78.4374369 ]\n",
      "Done with gradient descent at iteration  65\n",
      "Learned weights =  [ 0.45960074 91.49011137 78.4374369 ]\n",
      "Done with gradient descent at iteration  65\n",
      "Learned weights =  [ 0.45960074 91.49011137 78.43743574]\n",
      "Done with gradient descent at iteration  66\n",
      "Learned weights =  [ 0.46632097 91.49011137 78.43743574]\n",
      "Done with gradient descent at iteration  66\n",
      "Learned weights =  [ 0.46632097 91.49011065 78.43743574]\n",
      "Done with gradient descent at iteration  66\n",
      "Learned weights =  [ 0.46632097 91.49011065 78.43743464]\n",
      "Done with gradient descent at iteration  67\n",
      "Learned weights =  [ 0.47304119 91.49011065 78.43743464]\n",
      "Done with gradient descent at iteration  67\n",
      "Learned weights =  [ 0.47304119 91.4901099  78.43743464]\n",
      "Done with gradient descent at iteration  67\n",
      "Learned weights =  [ 0.47304119 91.4901099  78.43743357]\n",
      "Done with gradient descent at iteration  68\n",
      "Learned weights =  [ 0.47976142 91.4901099  78.43743357]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  68\n",
      "Learned weights =  [ 0.47976142 91.49010912 78.43743357]\n",
      "Done with gradient descent at iteration  68\n",
      "Learned weights =  [ 0.47976142 91.49010912 78.43743254]\n",
      "Done with gradient descent at iteration  69\n",
      "Learned weights =  [ 0.48648165 91.49010912 78.43743254]\n",
      "Done with gradient descent at iteration  69\n",
      "Learned weights =  [ 0.48648165 91.49010832 78.43743254]\n",
      "Done with gradient descent at iteration  69\n",
      "Learned weights =  [ 0.48648165 91.49010832 78.43743153]\n",
      "Iteration = 70\n",
      "Cost function =  3605291727877070.0\n",
      "Done with gradient descent at iteration  70\n",
      "Learned weights =  [ 0.49320187 91.49010832 78.43743153]\n",
      "Done with gradient descent at iteration  70\n",
      "Learned weights =  [ 0.49320187 91.49010749 78.43743153]\n",
      "Done with gradient descent at iteration  70\n",
      "Learned weights =  [ 0.49320187 91.49010749 78.43743055]\n",
      "Done with gradient descent at iteration  71\n",
      "Learned weights =  [ 0.4999221  91.49010749 78.43743055]\n",
      "Done with gradient descent at iteration  71\n",
      "Learned weights =  [ 0.4999221  91.49010666 78.43743055]\n",
      "Done with gradient descent at iteration  71\n",
      "Learned weights =  [ 0.4999221  91.49010666 78.43742958]\n",
      "Done with gradient descent at iteration  72\n",
      "Learned weights =  [ 0.50664232 91.49010666 78.43742958]\n",
      "Done with gradient descent at iteration  72\n",
      "Learned weights =  [ 0.50664232 91.4901058  78.43742958]\n",
      "Done with gradient descent at iteration  72\n",
      "Learned weights =  [ 0.50664232 91.4901058  78.43742862]\n",
      "Done with gradient descent at iteration  73\n",
      "Learned weights =  [ 0.51336255 91.4901058  78.43742862]\n",
      "Done with gradient descent at iteration  73\n",
      "Learned weights =  [ 0.51336255 91.49010494 78.43742862]\n",
      "Done with gradient descent at iteration  73\n",
      "Learned weights =  [ 0.51336255 91.49010494 78.43742767]\n",
      "Done with gradient descent at iteration  74\n",
      "Learned weights =  [ 0.52008277 91.49010494 78.43742767]\n",
      "Done with gradient descent at iteration  74\n",
      "Learned weights =  [ 0.52008277 91.49010408 78.43742767]\n",
      "Done with gradient descent at iteration  74\n",
      "Learned weights =  [ 0.52008277 91.49010408 78.43742674]\n",
      "Done with gradient descent at iteration  75\n",
      "Learned weights =  [ 0.526803   91.49010408 78.43742674]\n",
      "Done with gradient descent at iteration  75\n",
      "Learned weights =  [ 0.526803   91.4901032  78.43742674]\n",
      "Done with gradient descent at iteration  75\n",
      "Learned weights =  [ 0.526803   91.4901032  78.43742581]\n",
      "Done with gradient descent at iteration  76\n",
      "Learned weights =  [ 0.53352322 91.4901032  78.43742581]\n",
      "Done with gradient descent at iteration  76\n",
      "Learned weights =  [ 0.53352322 91.49010232 78.43742581]\n",
      "Done with gradient descent at iteration  76\n",
      "Learned weights =  [ 0.53352322 91.49010232 78.43742488]\n",
      "Done with gradient descent at iteration  77\n",
      "Learned weights =  [ 0.54024345 91.49010232 78.43742488]\n",
      "Done with gradient descent at iteration  77\n",
      "Learned weights =  [ 0.54024345 91.49010144 78.43742488]\n",
      "Done with gradient descent at iteration  77\n",
      "Learned weights =  [ 0.54024345 91.49010144 78.43742396]\n",
      "Done with gradient descent at iteration  78\n",
      "Learned weights =  [ 0.54696367 91.49010144 78.43742396]\n",
      "Done with gradient descent at iteration  78\n",
      "Learned weights =  [ 0.54696367 91.49010056 78.43742396]\n",
      "Done with gradient descent at iteration  78\n",
      "Learned weights =  [ 0.54696367 91.49010056 78.43742304]\n",
      "Done with gradient descent at iteration  79\n",
      "Learned weights =  [ 0.5536839  91.49010056 78.43742304]\n",
      "Done with gradient descent at iteration  79\n",
      "Learned weights =  [ 0.5536839  91.49009967 78.43742304]\n",
      "Done with gradient descent at iteration  79\n",
      "Learned weights =  [ 0.5536839  91.49009967 78.43742212]\n",
      "Iteration = 80\n",
      "Cost function =  3605291276262785.0\n",
      "Done with gradient descent at iteration  80\n",
      "Learned weights =  [ 0.56040412 91.49009967 78.43742212]\n",
      "Done with gradient descent at iteration  80\n",
      "Learned weights =  [ 0.56040412 91.49009878 78.43742212]\n",
      "Done with gradient descent at iteration  80\n",
      "Learned weights =  [ 0.56040412 91.49009878 78.43742121]\n",
      "Done with gradient descent at iteration  81\n",
      "Learned weights =  [ 0.56712435 91.49009878 78.43742121]\n",
      "Done with gradient descent at iteration  81\n",
      "Learned weights =  [ 0.56712435 91.49009789 78.43742121]\n",
      "Done with gradient descent at iteration  81\n",
      "Learned weights =  [ 0.56712435 91.49009789 78.4374203 ]\n",
      "Done with gradient descent at iteration  82\n",
      "Learned weights =  [ 0.57384457 91.49009789 78.4374203 ]\n",
      "Done with gradient descent at iteration  82\n",
      "Learned weights =  [ 0.57384457 91.490097   78.4374203 ]\n",
      "Done with gradient descent at iteration  82\n",
      "Learned weights =  [ 0.57384457 91.490097   78.43741939]\n",
      "Done with gradient descent at iteration  83\n",
      "Learned weights =  [ 0.5805648  91.490097   78.43741939]\n",
      "Done with gradient descent at iteration  83\n",
      "Learned weights =  [ 0.5805648  91.4900961  78.43741939]\n",
      "Done with gradient descent at iteration  83\n",
      "Learned weights =  [ 0.5805648  91.4900961  78.43741848]\n",
      "Done with gradient descent at iteration  84\n",
      "Learned weights =  [ 0.58728502 91.4900961  78.43741848]\n",
      "Done with gradient descent at iteration  84\n",
      "Learned weights =  [ 0.58728502 91.49009521 78.43741848]\n",
      "Done with gradient descent at iteration  84\n",
      "Learned weights =  [ 0.58728502 91.49009521 78.43741757]\n",
      "Done with gradient descent at iteration  85\n",
      "Learned weights =  [ 0.59400524 91.49009521 78.43741757]\n",
      "Done with gradient descent at iteration  85\n",
      "Learned weights =  [ 0.59400524 91.49009431 78.43741757]\n",
      "Done with gradient descent at iteration  85\n",
      "Learned weights =  [ 0.59400524 91.49009431 78.43741666]\n",
      "Done with gradient descent at iteration  86\n",
      "Learned weights =  [ 0.60072547 91.49009431 78.43741666]\n",
      "Done with gradient descent at iteration  86\n",
      "Learned weights =  [ 0.60072547 91.49009342 78.43741666]\n",
      "Done with gradient descent at iteration  86\n",
      "Learned weights =  [ 0.60072547 91.49009342 78.43741575]\n",
      "Done with gradient descent at iteration  87\n",
      "Learned weights =  [ 0.60744569 91.49009342 78.43741575]\n",
      "Done with gradient descent at iteration  87\n",
      "Learned weights =  [ 0.60744569 91.49009252 78.43741575]\n",
      "Done with gradient descent at iteration  87\n",
      "Learned weights =  [ 0.60744569 91.49009252 78.43741484]\n",
      "Done with gradient descent at iteration  88\n",
      "Learned weights =  [ 0.61416592 91.49009252 78.43741484]\n",
      "Done with gradient descent at iteration  88\n",
      "Learned weights =  [ 0.61416592 91.49009163 78.43741484]\n",
      "Done with gradient descent at iteration  88\n",
      "Learned weights =  [ 0.61416592 91.49009163 78.43741394]\n",
      "Done with gradient descent at iteration  89\n",
      "Learned weights =  [ 0.62088614 91.49009163 78.43741394]\n",
      "Done with gradient descent at iteration  89\n",
      "Learned weights =  [ 0.62088614 91.49009073 78.43741394]\n",
      "Done with gradient descent at iteration  89\n",
      "Learned weights =  [ 0.62088614 91.49009073 78.43741303]\n",
      "Iteration = 90\n",
      "Cost function =  3605290824648642.5\n",
      "Done with gradient descent at iteration  90\n",
      "Learned weights =  [ 0.62760636 91.49009073 78.43741303]\n",
      "Done with gradient descent at iteration  90\n",
      "Learned weights =  [ 0.62760636 91.49008984 78.43741303]\n",
      "Done with gradient descent at iteration  90\n",
      "Learned weights =  [ 0.62760636 91.49008984 78.43741212]\n",
      "Done with gradient descent at iteration  91\n",
      "Learned weights =  [ 0.63432659 91.49008984 78.43741212]\n",
      "Done with gradient descent at iteration  91\n",
      "Learned weights =  [ 0.63432659 91.49008894 78.43741212]\n",
      "Done with gradient descent at iteration  91\n",
      "Learned weights =  [ 0.63432659 91.49008894 78.43741121]\n",
      "Done with gradient descent at iteration  92\n",
      "Learned weights =  [ 0.64104681 91.49008894 78.43741121]\n",
      "Done with gradient descent at iteration  92\n",
      "Learned weights =  [ 0.64104681 91.49008804 78.43741121]\n",
      "Done with gradient descent at iteration  92\n",
      "Learned weights =  [ 0.64104681 91.49008804 78.43741031]\n",
      "Done with gradient descent at iteration  93\n",
      "Learned weights =  [ 0.64776703 91.49008804 78.43741031]\n",
      "Done with gradient descent at iteration  93\n",
      "Learned weights =  [ 0.64776703 91.49008715 78.43741031]\n",
      "Done with gradient descent at iteration  93\n",
      "Learned weights =  [ 0.64776703 91.49008715 78.4374094 ]\n",
      "Done with gradient descent at iteration  94\n",
      "Learned weights =  [ 0.65448726 91.49008715 78.4374094 ]\n",
      "Done with gradient descent at iteration  94\n",
      "Learned weights =  [ 0.65448726 91.49008625 78.4374094 ]\n",
      "Done with gradient descent at iteration  94\n",
      "Learned weights =  [ 0.65448726 91.49008625 78.4374085 ]\n",
      "Done with gradient descent at iteration  95\n",
      "Learned weights =  [ 0.66120748 91.49008625 78.4374085 ]\n",
      "Done with gradient descent at iteration  95\n",
      "Learned weights =  [ 0.66120748 91.49008535 78.4374085 ]\n",
      "Done with gradient descent at iteration  95\n",
      "Learned weights =  [ 0.66120748 91.49008535 78.43740759]\n",
      "Done with gradient descent at iteration  96\n",
      "Learned weights =  [ 0.6679277  91.49008535 78.43740759]\n",
      "Done with gradient descent at iteration  96\n",
      "Learned weights =  [ 0.6679277  91.49008446 78.43740759]\n",
      "Done with gradient descent at iteration  96\n",
      "Learned weights =  [ 0.6679277  91.49008446 78.43740668]\n",
      "Done with gradient descent at iteration  97\n",
      "Learned weights =  [ 0.67464793 91.49008446 78.43740668]\n",
      "Done with gradient descent at iteration  97\n",
      "Learned weights =  [ 0.67464793 91.49008356 78.43740668]\n",
      "Done with gradient descent at iteration  97\n",
      "Learned weights =  [ 0.67464793 91.49008356 78.43740578]\n",
      "Done with gradient descent at iteration  98\n",
      "Learned weights =  [ 0.68136815 91.49008356 78.43740578]\n",
      "Done with gradient descent at iteration  98\n",
      "Learned weights =  [ 0.68136815 91.49008266 78.43740578]\n",
      "Done with gradient descent at iteration  98\n",
      "Learned weights =  [ 0.68136815 91.49008266 78.43740487]\n",
      "Done with gradient descent at iteration  99\n",
      "Learned weights =  [ 0.68808837 91.49008266 78.43740487]\n",
      "Done with gradient descent at iteration  99\n",
      "Learned weights =  [ 0.68808837 91.49008177 78.43740487]\n",
      "Done with gradient descent at iteration  99\n",
      "Learned weights =  [ 0.68808837 91.49008177 78.43740397]\n",
      "Iteration = 100\n",
      "Cost function =  3605290373034644.0\n",
      "Done with gradient descent at iteration  100\n",
      "Learned weights =  [ 0.69480859 91.49008177 78.43740397]\n",
      "Done with gradient descent at iteration  100\n",
      "Learned weights =  [ 0.69480859 91.49008087 78.43740397]\n",
      "Done with gradient descent at iteration  100\n",
      "Learned weights =  [ 0.69480859 91.49008087 78.43740306]\n",
      "Done with gradient descent at iteration  101\n",
      "Learned weights =  [ 0.70152882 91.49008087 78.43740306]\n",
      "Done with gradient descent at iteration  101\n",
      "Learned weights =  [ 0.70152882 91.49007997 78.43740306]\n",
      "Done with gradient descent at iteration  101\n",
      "Learned weights =  [ 0.70152882 91.49007997 78.43740215]\n",
      "Done with gradient descent at iteration  102\n",
      "Learned weights =  [ 0.70824904 91.49007997 78.43740215]\n",
      "Done with gradient descent at iteration  102\n",
      "Learned weights =  [ 0.70824904 91.49007907 78.43740215]\n",
      "Done with gradient descent at iteration  102\n",
      "Learned weights =  [ 0.70824904 91.49007907 78.43740125]\n",
      "Done with gradient descent at iteration  103\n",
      "Learned weights =  [ 0.71496926 91.49007907 78.43740125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  103\n",
      "Learned weights =  [ 0.71496926 91.49007818 78.43740125]\n",
      "Done with gradient descent at iteration  103\n",
      "Learned weights =  [ 0.71496926 91.49007818 78.43740034]\n",
      "Done with gradient descent at iteration  104\n",
      "Learned weights =  [ 0.72168948 91.49007818 78.43740034]\n",
      "Done with gradient descent at iteration  104\n",
      "Learned weights =  [ 0.72168948 91.49007728 78.43740034]\n",
      "Done with gradient descent at iteration  104\n",
      "Learned weights =  [ 0.72168948 91.49007728 78.43739943]\n",
      "Done with gradient descent at iteration  105\n",
      "Learned weights =  [ 0.7284097  91.49007728 78.43739943]\n",
      "Done with gradient descent at iteration  105\n",
      "Learned weights =  [ 0.7284097  91.49007638 78.43739943]\n",
      "Done with gradient descent at iteration  105\n",
      "Learned weights =  [ 0.7284097  91.49007638 78.43739853]\n",
      "Done with gradient descent at iteration  106\n",
      "Learned weights =  [ 0.73512993 91.49007638 78.43739853]\n",
      "Done with gradient descent at iteration  106\n",
      "Learned weights =  [ 0.73512993 91.49007549 78.43739853]\n",
      "Done with gradient descent at iteration  106\n",
      "Learned weights =  [ 0.73512993 91.49007549 78.43739762]\n",
      "Done with gradient descent at iteration  107\n",
      "Learned weights =  [ 0.74185015 91.49007549 78.43739762]\n",
      "Done with gradient descent at iteration  107\n",
      "Learned weights =  [ 0.74185015 91.49007459 78.43739762]\n",
      "Done with gradient descent at iteration  107\n",
      "Learned weights =  [ 0.74185015 91.49007459 78.43739672]\n",
      "Done with gradient descent at iteration  108\n",
      "Learned weights =  [ 0.74857037 91.49007459 78.43739672]\n",
      "Done with gradient descent at iteration  108\n",
      "Learned weights =  [ 0.74857037 91.49007369 78.43739672]\n",
      "Done with gradient descent at iteration  108\n",
      "Learned weights =  [ 0.74857037 91.49007369 78.43739581]\n",
      "Done with gradient descent at iteration  109\n",
      "Learned weights =  [ 0.75529059 91.49007369 78.43739581]\n",
      "Done with gradient descent at iteration  109\n",
      "Learned weights =  [ 0.75529059 91.4900728  78.43739581]\n",
      "Done with gradient descent at iteration  109\n",
      "Learned weights =  [ 0.75529059 91.4900728  78.4373949 ]\n",
      "Done with gradient descent at iteration  110\n",
      "Learned weights =  [ 0.76201081 91.4900728  78.4373949 ]\n",
      "Done with gradient descent at iteration  110\n",
      "Learned weights =  [ 0.76201081 91.4900719  78.4373949 ]\n",
      "Done with gradient descent at iteration  110\n",
      "Learned weights =  [ 0.76201081 91.4900719  78.437394  ]\n",
      "Done with gradient descent at iteration  111\n",
      "Learned weights =  [ 0.76873103 91.4900719  78.437394  ]\n",
      "Done with gradient descent at iteration  111\n",
      "Learned weights =  [ 0.76873103 91.490071   78.437394  ]\n",
      "Done with gradient descent at iteration  111\n",
      "Learned weights =  [ 0.76873103 91.490071   78.43739309]\n",
      "Done with gradient descent at iteration  112\n",
      "Learned weights =  [ 0.77545126 91.490071   78.43739309]\n",
      "Done with gradient descent at iteration  112\n",
      "Learned weights =  [ 0.77545126 91.49007011 78.43739309]\n",
      "Done with gradient descent at iteration  112\n",
      "Learned weights =  [ 0.77545126 91.49007011 78.43739219]\n",
      "Done with gradient descent at iteration  113\n",
      "Learned weights =  [ 0.78217148 91.49007011 78.43739219]\n",
      "Done with gradient descent at iteration  113\n",
      "Learned weights =  [ 0.78217148 91.49006921 78.43739219]\n",
      "Done with gradient descent at iteration  113\n",
      "Learned weights =  [ 0.78217148 91.49006921 78.43739128]\n",
      "Done with gradient descent at iteration  114\n",
      "Learned weights =  [ 0.7888917  91.49006921 78.43739128]\n",
      "Done with gradient descent at iteration  114\n",
      "Learned weights =  [ 0.7888917  91.49006831 78.43739128]\n",
      "Done with gradient descent at iteration  114\n",
      "Learned weights =  [ 0.7888917  91.49006831 78.43739038]\n",
      "Done with gradient descent at iteration  115\n",
      "Learned weights =  [ 0.79561192 91.49006831 78.43739038]\n",
      "Done with gradient descent at iteration  115\n",
      "Learned weights =  [ 0.79561192 91.49006741 78.43739038]\n",
      "Done with gradient descent at iteration  115\n",
      "Learned weights =  [ 0.79561192 91.49006741 78.43738947]\n",
      "Done with gradient descent at iteration  116\n",
      "Learned weights =  [ 0.80233214 91.49006741 78.43738947]\n",
      "Done with gradient descent at iteration  116\n",
      "Learned weights =  [ 0.80233214 91.49006652 78.43738947]\n",
      "Done with gradient descent at iteration  116\n",
      "Learned weights =  [ 0.80233214 91.49006652 78.43738856]\n",
      "Done with gradient descent at iteration  117\n",
      "Learned weights =  [ 0.80905236 91.49006652 78.43738856]\n",
      "Done with gradient descent at iteration  117\n",
      "Learned weights =  [ 0.80905236 91.49006562 78.43738856]\n",
      "Done with gradient descent at iteration  117\n",
      "Learned weights =  [ 0.80905236 91.49006562 78.43738766]\n",
      "Done with gradient descent at iteration  118\n",
      "Learned weights =  [ 0.81577258 91.49006562 78.43738766]\n",
      "Done with gradient descent at iteration  118\n",
      "Learned weights =  [ 0.81577258 91.49006472 78.43738766]\n",
      "Done with gradient descent at iteration  118\n",
      "Learned weights =  [ 0.81577258 91.49006472 78.43738675]\n",
      "Done with gradient descent at iteration  119\n",
      "Learned weights =  [ 0.8224928  91.49006472 78.43738675]\n",
      "Done with gradient descent at iteration  119\n",
      "Learned weights =  [ 0.8224928  91.49006383 78.43738675]\n",
      "Done with gradient descent at iteration  119\n",
      "Learned weights =  [ 0.8224928  91.49006383 78.43738585]\n",
      "Done with gradient descent at iteration  120\n",
      "Learned weights =  [ 0.82921302 91.49006383 78.43738585]\n",
      "Done with gradient descent at iteration  120\n",
      "Learned weights =  [ 0.82921302 91.49006293 78.43738585]\n",
      "Done with gradient descent at iteration  120\n",
      "Learned weights =  [ 0.82921302 91.49006293 78.43738494]\n",
      "Done with gradient descent at iteration  121\n",
      "Learned weights =  [ 0.83593324 91.49006293 78.43738494]\n",
      "Done with gradient descent at iteration  121\n",
      "Learned weights =  [ 0.83593324 91.49006203 78.43738494]\n",
      "Done with gradient descent at iteration  121\n",
      "Learned weights =  [ 0.83593324 91.49006203 78.43738403]\n",
      "Done with gradient descent at iteration  122\n",
      "Learned weights =  [ 0.84265346 91.49006203 78.43738403]\n",
      "Done with gradient descent at iteration  122\n",
      "Learned weights =  [ 0.84265346 91.49006114 78.43738403]\n",
      "Done with gradient descent at iteration  122\n",
      "Learned weights =  [ 0.84265346 91.49006114 78.43738313]\n",
      "Done with gradient descent at iteration  123\n",
      "Learned weights =  [ 0.84937368 91.49006114 78.43738313]\n",
      "Done with gradient descent at iteration  123\n",
      "Learned weights =  [ 0.84937368 91.49006024 78.43738313]\n",
      "Done with gradient descent at iteration  123\n",
      "Learned weights =  [ 0.84937368 91.49006024 78.43738222]\n",
      "Done with gradient descent at iteration  124\n",
      "Learned weights =  [ 0.8560939  91.49006024 78.43738222]\n",
      "Done with gradient descent at iteration  124\n",
      "Learned weights =  [ 0.8560939  91.49005934 78.43738222]\n",
      "Done with gradient descent at iteration  124\n",
      "Learned weights =  [ 0.8560939  91.49005934 78.43738132]\n",
      "Done with gradient descent at iteration  125\n",
      "Learned weights =  [ 0.86281412 91.49005934 78.43738132]\n",
      "Done with gradient descent at iteration  125\n",
      "Learned weights =  [ 0.86281412 91.49005844 78.43738132]\n",
      "Done with gradient descent at iteration  125\n",
      "Learned weights =  [ 0.86281412 91.49005844 78.43738041]\n",
      "Done with gradient descent at iteration  126\n",
      "Learned weights =  [ 0.86953434 91.49005844 78.43738041]\n",
      "Done with gradient descent at iteration  126\n",
      "Learned weights =  [ 0.86953434 91.49005755 78.43738041]\n",
      "Done with gradient descent at iteration  126\n",
      "Learned weights =  [ 0.86953434 91.49005755 78.4373795 ]\n",
      "Done with gradient descent at iteration  127\n",
      "Learned weights =  [ 0.87625456 91.49005755 78.4373795 ]\n",
      "Done with gradient descent at iteration  127\n",
      "Learned weights =  [ 0.87625456 91.49005665 78.4373795 ]\n",
      "Done with gradient descent at iteration  127\n",
      "Learned weights =  [ 0.87625456 91.49005665 78.4373786 ]\n",
      "Done with gradient descent at iteration  128\n",
      "Learned weights =  [ 0.88297478 91.49005665 78.4373786 ]\n",
      "Done with gradient descent at iteration  128\n",
      "Learned weights =  [ 0.88297478 91.49005575 78.4373786 ]\n",
      "Done with gradient descent at iteration  128\n",
      "Learned weights =  [ 0.88297478 91.49005575 78.43737769]\n",
      "Done with gradient descent at iteration  129\n",
      "Learned weights =  [ 0.889695   91.49005575 78.43737769]\n",
      "Done with gradient descent at iteration  129\n",
      "Learned weights =  [ 0.889695   91.49005486 78.43737769]\n",
      "Done with gradient descent at iteration  129\n",
      "Learned weights =  [ 0.889695   91.49005486 78.43737679]\n",
      "Done with gradient descent at iteration  130\n",
      "Learned weights =  [ 0.89641522 91.49005486 78.43737679]\n",
      "Done with gradient descent at iteration  130\n",
      "Learned weights =  [ 0.89641522 91.49005396 78.43737679]\n",
      "Done with gradient descent at iteration  130\n",
      "Learned weights =  [ 0.89641522 91.49005396 78.43737588]\n",
      "Done with gradient descent at iteration  131\n",
      "Learned weights =  [ 0.90313544 91.49005396 78.43737588]\n",
      "Done with gradient descent at iteration  131\n",
      "Learned weights =  [ 0.90313544 91.49005306 78.43737588]\n",
      "Done with gradient descent at iteration  131\n",
      "Learned weights =  [ 0.90313544 91.49005306 78.43737497]\n",
      "Done with gradient descent at iteration  132\n",
      "Learned weights =  [ 0.90985566 91.49005306 78.43737497]\n",
      "Done with gradient descent at iteration  132\n",
      "Learned weights =  [ 0.90985566 91.49005217 78.43737497]\n",
      "Done with gradient descent at iteration  132\n",
      "Learned weights =  [ 0.90985566 91.49005217 78.43737407]\n",
      "Done with gradient descent at iteration  133\n",
      "Learned weights =  [ 0.91657588 91.49005217 78.43737407]\n",
      "Done with gradient descent at iteration  133\n",
      "Learned weights =  [ 0.91657588 91.49005127 78.43737407]\n",
      "Done with gradient descent at iteration  133\n",
      "Learned weights =  [ 0.91657588 91.49005127 78.43737316]\n",
      "Done with gradient descent at iteration  134\n",
      "Learned weights =  [ 0.9232961  91.49005127 78.43737316]\n",
      "Done with gradient descent at iteration  134\n",
      "Learned weights =  [ 0.9232961  91.49005037 78.43737316]\n",
      "Done with gradient descent at iteration  134\n",
      "Learned weights =  [ 0.9232961  91.49005037 78.43737226]\n",
      "Done with gradient descent at iteration  135\n",
      "Learned weights =  [ 0.93001632 91.49005037 78.43737226]\n",
      "Done with gradient descent at iteration  135\n",
      "Learned weights =  [ 0.93001632 91.49004948 78.43737226]\n",
      "Done with gradient descent at iteration  135\n",
      "Learned weights =  [ 0.93001632 91.49004948 78.43737135]\n",
      "Done with gradient descent at iteration  136\n",
      "Learned weights =  [ 0.93673653 91.49004948 78.43737135]\n",
      "Done with gradient descent at iteration  136\n",
      "Learned weights =  [ 0.93673653 91.49004858 78.43737135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  136\n",
      "Learned weights =  [ 0.93673653 91.49004858 78.43737044]\n",
      "Done with gradient descent at iteration  137\n",
      "Learned weights =  [ 0.94345675 91.49004858 78.43737044]\n",
      "Done with gradient descent at iteration  137\n",
      "Learned weights =  [ 0.94345675 91.49004768 78.43737044]\n",
      "Done with gradient descent at iteration  137\n",
      "Learned weights =  [ 0.94345675 91.49004768 78.43736954]\n",
      "Done with gradient descent at iteration  138\n",
      "Learned weights =  [ 0.95017697 91.49004768 78.43736954]\n",
      "Done with gradient descent at iteration  138\n",
      "Learned weights =  [ 0.95017697 91.49004678 78.43736954]\n",
      "Done with gradient descent at iteration  138\n",
      "Learned weights =  [ 0.95017697 91.49004678 78.43736863]\n",
      "Done with gradient descent at iteration  139\n",
      "Learned weights =  [ 0.95689719 91.49004678 78.43736863]\n",
      "Done with gradient descent at iteration  139\n",
      "Learned weights =  [ 0.95689719 91.49004589 78.43736863]\n",
      "Done with gradient descent at iteration  139\n",
      "Learned weights =  [ 0.95689719 91.49004589 78.43736773]\n",
      "Done with gradient descent at iteration  140\n",
      "Learned weights =  [ 0.96361741 91.49004589 78.43736773]\n",
      "Done with gradient descent at iteration  140\n",
      "Learned weights =  [ 0.96361741 91.49004499 78.43736773]\n",
      "Done with gradient descent at iteration  140\n",
      "Learned weights =  [ 0.96361741 91.49004499 78.43736682]\n",
      "Done with gradient descent at iteration  141\n",
      "Learned weights =  [ 0.97033763 91.49004499 78.43736682]\n",
      "Done with gradient descent at iteration  141\n",
      "Learned weights =  [ 0.97033763 91.49004409 78.43736682]\n",
      "Done with gradient descent at iteration  141\n",
      "Learned weights =  [ 0.97033763 91.49004409 78.43736591]\n",
      "Done with gradient descent at iteration  142\n",
      "Learned weights =  [ 0.97705784 91.49004409 78.43736591]\n",
      "Done with gradient descent at iteration  142\n",
      "Learned weights =  [ 0.97705784 91.4900432  78.43736591]\n",
      "Done with gradient descent at iteration  142\n",
      "Learned weights =  [ 0.97705784 91.4900432  78.43736501]\n",
      "Done with gradient descent at iteration  143\n",
      "Learned weights =  [ 0.98377806 91.4900432  78.43736501]\n",
      "Done with gradient descent at iteration  143\n",
      "Learned weights =  [ 0.98377806 91.4900423  78.43736501]\n",
      "Done with gradient descent at iteration  143\n",
      "Learned weights =  [ 0.98377806 91.4900423  78.4373641 ]\n",
      "Done with gradient descent at iteration  144\n",
      "Learned weights =  [ 0.99049828 91.4900423  78.4373641 ]\n",
      "Done with gradient descent at iteration  144\n",
      "Learned weights =  [ 0.99049828 91.4900414  78.4373641 ]\n",
      "Done with gradient descent at iteration  144\n",
      "Learned weights =  [ 0.99049828 91.4900414  78.4373632 ]\n",
      "Done with gradient descent at iteration  145\n",
      "Learned weights =  [ 0.9972185 91.4900414 78.4373632]\n",
      "Done with gradient descent at iteration  145\n",
      "Learned weights =  [ 0.9972185  91.49004051 78.4373632 ]\n",
      "Done with gradient descent at iteration  145\n",
      "Learned weights =  [ 0.9972185  91.49004051 78.43736229]\n",
      "Done with gradient descent at iteration  146\n",
      "Learned weights =  [ 1.00393872 91.49004051 78.43736229]\n",
      "Done with gradient descent at iteration  146\n",
      "Learned weights =  [ 1.00393872 91.49003961 78.43736229]\n",
      "Done with gradient descent at iteration  146\n",
      "Learned weights =  [ 1.00393872 91.49003961 78.43736138]\n",
      "Done with gradient descent at iteration  147\n",
      "Learned weights =  [ 1.01065893 91.49003961 78.43736138]\n",
      "Done with gradient descent at iteration  147\n",
      "Learned weights =  [ 1.01065893 91.49003871 78.43736138]\n",
      "Done with gradient descent at iteration  147\n",
      "Learned weights =  [ 1.01065893 91.49003871 78.43736048]\n",
      "Done with gradient descent at iteration  148\n",
      "Learned weights =  [ 1.01737915 91.49003871 78.43736048]\n",
      "Done with gradient descent at iteration  148\n",
      "Learned weights =  [ 1.01737915 91.49003781 78.43736048]\n",
      "Done with gradient descent at iteration  148\n",
      "Learned weights =  [ 1.01737915 91.49003781 78.43735957]\n",
      "Done with gradient descent at iteration  149\n",
      "Learned weights =  [ 1.02409937 91.49003781 78.43735957]\n",
      "Done with gradient descent at iteration  149\n",
      "Learned weights =  [ 1.02409937 91.49003692 78.43735957]\n",
      "Done with gradient descent at iteration  149\n",
      "Learned weights =  [ 1.02409937 91.49003692 78.43735867]\n",
      "Done with gradient descent at iteration  150\n",
      "Learned weights =  [ 1.03081958 91.49003692 78.43735867]\n",
      "Done with gradient descent at iteration  150\n",
      "Learned weights =  [ 1.03081958 91.49003602 78.43735867]\n",
      "Done with gradient descent at iteration  150\n",
      "Learned weights =  [ 1.03081958 91.49003602 78.43735776]\n",
      "Done with gradient descent at iteration  151\n",
      "Learned weights =  [ 1.0375398  91.49003602 78.43735776]\n",
      "Done with gradient descent at iteration  151\n",
      "Learned weights =  [ 1.0375398  91.49003512 78.43735776]\n",
      "Done with gradient descent at iteration  151\n",
      "Learned weights =  [ 1.0375398  91.49003512 78.43735685]\n",
      "Done with gradient descent at iteration  152\n",
      "Learned weights =  [ 1.04426002 91.49003512 78.43735685]\n",
      "Done with gradient descent at iteration  152\n",
      "Learned weights =  [ 1.04426002 91.49003423 78.43735685]\n",
      "Done with gradient descent at iteration  152\n",
      "Learned weights =  [ 1.04426002 91.49003423 78.43735595]\n",
      "Done with gradient descent at iteration  153\n",
      "Learned weights =  [ 1.05098024 91.49003423 78.43735595]\n",
      "Done with gradient descent at iteration  153\n",
      "Learned weights =  [ 1.05098024 91.49003333 78.43735595]\n",
      "Done with gradient descent at iteration  153\n",
      "Learned weights =  [ 1.05098024 91.49003333 78.43735504]\n",
      "Done with gradient descent at iteration  154\n",
      "Learned weights =  [ 1.05770045 91.49003333 78.43735504]\n",
      "Done with gradient descent at iteration  154\n",
      "Learned weights =  [ 1.05770045 91.49003243 78.43735504]\n",
      "Done with gradient descent at iteration  154\n",
      "Learned weights =  [ 1.05770045 91.49003243 78.43735414]\n",
      "Done with gradient descent at iteration  155\n",
      "Learned weights =  [ 1.06442067 91.49003243 78.43735414]\n",
      "Done with gradient descent at iteration  155\n",
      "Learned weights =  [ 1.06442067 91.49003154 78.43735414]\n",
      "Done with gradient descent at iteration  155\n",
      "Learned weights =  [ 1.06442067 91.49003154 78.43735323]\n",
      "Done with gradient descent at iteration  156\n",
      "Learned weights =  [ 1.07114089 91.49003154 78.43735323]\n",
      "Done with gradient descent at iteration  156\n",
      "Learned weights =  [ 1.07114089 91.49003064 78.43735323]\n",
      "Done with gradient descent at iteration  156\n",
      "Learned weights =  [ 1.07114089 91.49003064 78.43735232]\n",
      "Done with gradient descent at iteration  157\n",
      "Learned weights =  [ 1.0778611  91.49003064 78.43735232]\n",
      "Done with gradient descent at iteration  157\n",
      "Learned weights =  [ 1.0778611  91.49002974 78.43735232]\n",
      "Done with gradient descent at iteration  157\n",
      "Learned weights =  [ 1.0778611  91.49002974 78.43735142]\n",
      "Done with gradient descent at iteration  158\n",
      "Learned weights =  [ 1.08458132 91.49002974 78.43735142]\n",
      "Done with gradient descent at iteration  158\n",
      "Learned weights =  [ 1.08458132 91.49002885 78.43735142]\n",
      "Done with gradient descent at iteration  158\n",
      "Learned weights =  [ 1.08458132 91.49002885 78.43735051]\n",
      "Done with gradient descent at iteration  159\n",
      "Learned weights =  [ 1.09130154 91.49002885 78.43735051]\n",
      "Done with gradient descent at iteration  159\n",
      "Learned weights =  [ 1.09130154 91.49002795 78.43735051]\n",
      "Done with gradient descent at iteration  159\n",
      "Learned weights =  [ 1.09130154 91.49002795 78.43734961]\n",
      "Done with gradient descent at iteration  160\n",
      "Learned weights =  [ 1.09802175 91.49002795 78.43734961]\n",
      "Done with gradient descent at iteration  160\n",
      "Learned weights =  [ 1.09802175 91.49002705 78.43734961]\n",
      "Done with gradient descent at iteration  160\n",
      "Learned weights =  [ 1.09802175 91.49002705 78.4373487 ]\n",
      "Done with gradient descent at iteration  161\n",
      "Learned weights =  [ 1.10474197 91.49002705 78.4373487 ]\n",
      "Done with gradient descent at iteration  161\n",
      "Learned weights =  [ 1.10474197 91.49002615 78.4373487 ]\n",
      "Done with gradient descent at iteration  161\n",
      "Learned weights =  [ 1.10474197 91.49002615 78.43734779]\n",
      "Done with gradient descent at iteration  162\n",
      "Learned weights =  [ 1.11146218 91.49002615 78.43734779]\n",
      "Done with gradient descent at iteration  162\n",
      "Learned weights =  [ 1.11146218 91.49002526 78.43734779]\n",
      "Done with gradient descent at iteration  162\n",
      "Learned weights =  [ 1.11146218 91.49002526 78.43734689]\n",
      "Done with gradient descent at iteration  163\n",
      "Learned weights =  [ 1.1181824  91.49002526 78.43734689]\n",
      "Done with gradient descent at iteration  163\n",
      "Learned weights =  [ 1.1181824  91.49002436 78.43734689]\n",
      "Done with gradient descent at iteration  163\n",
      "Learned weights =  [ 1.1181824  91.49002436 78.43734598]\n",
      "Done with gradient descent at iteration  164\n",
      "Learned weights =  [ 1.12490261 91.49002436 78.43734598]\n",
      "Done with gradient descent at iteration  164\n",
      "Learned weights =  [ 1.12490261 91.49002346 78.43734598]\n",
      "Done with gradient descent at iteration  164\n",
      "Learned weights =  [ 1.12490261 91.49002346 78.43734508]\n",
      "Done with gradient descent at iteration  165\n",
      "Learned weights =  [ 1.13162283 91.49002346 78.43734508]\n",
      "Done with gradient descent at iteration  165\n",
      "Learned weights =  [ 1.13162283 91.49002257 78.43734508]\n",
      "Done with gradient descent at iteration  165\n",
      "Learned weights =  [ 1.13162283 91.49002257 78.43734417]\n",
      "Done with gradient descent at iteration  166\n",
      "Learned weights =  [ 1.13834305 91.49002257 78.43734417]\n",
      "Done with gradient descent at iteration  166\n",
      "Learned weights =  [ 1.13834305 91.49002167 78.43734417]\n",
      "Done with gradient descent at iteration  166\n",
      "Learned weights =  [ 1.13834305 91.49002167 78.43734326]\n",
      "Done with gradient descent at iteration  167\n",
      "Learned weights =  [ 1.14506326 91.49002167 78.43734326]\n",
      "Done with gradient descent at iteration  167\n",
      "Learned weights =  [ 1.14506326 91.49002077 78.43734326]\n",
      "Done with gradient descent at iteration  167\n",
      "Learned weights =  [ 1.14506326 91.49002077 78.43734236]\n",
      "Done with gradient descent at iteration  168\n",
      "Learned weights =  [ 1.15178348 91.49002077 78.43734236]\n",
      "Done with gradient descent at iteration  168\n",
      "Learned weights =  [ 1.15178348 91.49001988 78.43734236]\n",
      "Done with gradient descent at iteration  168\n",
      "Learned weights =  [ 1.15178348 91.49001988 78.43734145]\n",
      "Done with gradient descent at iteration  169\n",
      "Learned weights =  [ 1.15850369 91.49001988 78.43734145]\n",
      "Done with gradient descent at iteration  169\n",
      "Learned weights =  [ 1.15850369 91.49001898 78.43734145]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  169\n",
      "Learned weights =  [ 1.15850369 91.49001898 78.43734055]\n",
      "Done with gradient descent at iteration  170\n",
      "Learned weights =  [ 1.16522391 91.49001898 78.43734055]\n",
      "Done with gradient descent at iteration  170\n",
      "Learned weights =  [ 1.16522391 91.49001808 78.43734055]\n",
      "Done with gradient descent at iteration  170\n",
      "Learned weights =  [ 1.16522391 91.49001808 78.43733964]\n",
      "Done with gradient descent at iteration  171\n",
      "Learned weights =  [ 1.17194412 91.49001808 78.43733964]\n",
      "Done with gradient descent at iteration  171\n",
      "Learned weights =  [ 1.17194412 91.49001718 78.43733964]\n",
      "Done with gradient descent at iteration  171\n",
      "Learned weights =  [ 1.17194412 91.49001718 78.43733873]\n",
      "Done with gradient descent at iteration  172\n",
      "Learned weights =  [ 1.17866434 91.49001718 78.43733873]\n",
      "Done with gradient descent at iteration  172\n",
      "Learned weights =  [ 1.17866434 91.49001629 78.43733873]\n",
      "Done with gradient descent at iteration  172\n",
      "Learned weights =  [ 1.17866434 91.49001629 78.43733783]\n",
      "Done with gradient descent at iteration  173\n",
      "Learned weights =  [ 1.18538455 91.49001629 78.43733783]\n",
      "Done with gradient descent at iteration  173\n",
      "Learned weights =  [ 1.18538455 91.49001539 78.43733783]\n",
      "Done with gradient descent at iteration  173\n",
      "Learned weights =  [ 1.18538455 91.49001539 78.43733692]\n",
      "Done with gradient descent at iteration  174\n",
      "Learned weights =  [ 1.19210477 91.49001539 78.43733692]\n",
      "Done with gradient descent at iteration  174\n",
      "Learned weights =  [ 1.19210477 91.49001449 78.43733692]\n",
      "Done with gradient descent at iteration  174\n",
      "Learned weights =  [ 1.19210477 91.49001449 78.43733602]\n",
      "Done with gradient descent at iteration  175\n",
      "Learned weights =  [ 1.19882498 91.49001449 78.43733602]\n",
      "Done with gradient descent at iteration  175\n",
      "Learned weights =  [ 1.19882498 91.4900136  78.43733602]\n",
      "Done with gradient descent at iteration  175\n",
      "Learned weights =  [ 1.19882498 91.4900136  78.43733511]\n",
      "Done with gradient descent at iteration  176\n",
      "Learned weights =  [ 1.2055452  91.4900136  78.43733511]\n",
      "Done with gradient descent at iteration  176\n",
      "Learned weights =  [ 1.2055452  91.4900127  78.43733511]\n",
      "Done with gradient descent at iteration  176\n",
      "Learned weights =  [ 1.2055452 91.4900127 78.4373342]\n",
      "Done with gradient descent at iteration  177\n",
      "Learned weights =  [ 1.21226541 91.4900127  78.4373342 ]\n",
      "Done with gradient descent at iteration  177\n",
      "Learned weights =  [ 1.21226541 91.4900118  78.4373342 ]\n",
      "Done with gradient descent at iteration  177\n",
      "Learned weights =  [ 1.21226541 91.4900118  78.4373333 ]\n",
      "Done with gradient descent at iteration  178\n",
      "Learned weights =  [ 1.21898562 91.4900118  78.4373333 ]\n",
      "Done with gradient descent at iteration  178\n",
      "Learned weights =  [ 1.21898562 91.49001091 78.4373333 ]\n",
      "Done with gradient descent at iteration  178\n",
      "Learned weights =  [ 1.21898562 91.49001091 78.43733239]\n",
      "Done with gradient descent at iteration  179\n",
      "Learned weights =  [ 1.22570584 91.49001091 78.43733239]\n",
      "Done with gradient descent at iteration  179\n",
      "Learned weights =  [ 1.22570584 91.49001001 78.43733239]\n",
      "Done with gradient descent at iteration  179\n",
      "Learned weights =  [ 1.22570584 91.49001001 78.43733149]\n",
      "Done with gradient descent at iteration  180\n",
      "Learned weights =  [ 1.23242605 91.49001001 78.43733149]\n",
      "Done with gradient descent at iteration  180\n",
      "Learned weights =  [ 1.23242605 91.49000911 78.43733149]\n",
      "Done with gradient descent at iteration  180\n",
      "Learned weights =  [ 1.23242605 91.49000911 78.43733058]\n",
      "Done with gradient descent at iteration  181\n",
      "Learned weights =  [ 1.23914627 91.49000911 78.43733058]\n",
      "Done with gradient descent at iteration  181\n",
      "Learned weights =  [ 1.23914627 91.49000822 78.43733058]\n",
      "Done with gradient descent at iteration  181\n",
      "Learned weights =  [ 1.23914627 91.49000822 78.43732967]\n",
      "Done with gradient descent at iteration  182\n",
      "Learned weights =  [ 1.24586648 91.49000822 78.43732967]\n",
      "Done with gradient descent at iteration  182\n",
      "Learned weights =  [ 1.24586648 91.49000732 78.43732967]\n",
      "Done with gradient descent at iteration  182\n",
      "Learned weights =  [ 1.24586648 91.49000732 78.43732877]\n",
      "Done with gradient descent at iteration  183\n",
      "Learned weights =  [ 1.25258669 91.49000732 78.43732877]\n",
      "Done with gradient descent at iteration  183\n",
      "Learned weights =  [ 1.25258669 91.49000642 78.43732877]\n",
      "Done with gradient descent at iteration  183\n",
      "Learned weights =  [ 1.25258669 91.49000642 78.43732786]\n",
      "Done with gradient descent at iteration  184\n",
      "Learned weights =  [ 1.25930691 91.49000642 78.43732786]\n",
      "Done with gradient descent at iteration  184\n",
      "Learned weights =  [ 1.25930691 91.49000552 78.43732786]\n",
      "Done with gradient descent at iteration  184\n",
      "Learned weights =  [ 1.25930691 91.49000552 78.43732696]\n",
      "Done with gradient descent at iteration  185\n",
      "Learned weights =  [ 1.26602712 91.49000552 78.43732696]\n",
      "Done with gradient descent at iteration  185\n",
      "Learned weights =  [ 1.26602712 91.49000463 78.43732696]\n",
      "Done with gradient descent at iteration  185\n",
      "Learned weights =  [ 1.26602712 91.49000463 78.43732605]\n",
      "Done with gradient descent at iteration  186\n",
      "Learned weights =  [ 1.27274733 91.49000463 78.43732605]\n",
      "Done with gradient descent at iteration  186\n",
      "Learned weights =  [ 1.27274733 91.49000373 78.43732605]\n",
      "Done with gradient descent at iteration  186\n",
      "Learned weights =  [ 1.27274733 91.49000373 78.43732514]\n",
      "Done with gradient descent at iteration  187\n",
      "Learned weights =  [ 1.27946755 91.49000373 78.43732514]\n",
      "Done with gradient descent at iteration  187\n",
      "Learned weights =  [ 1.27946755 91.49000283 78.43732514]\n",
      "Done with gradient descent at iteration  187\n",
      "Learned weights =  [ 1.27946755 91.49000283 78.43732424]\n",
      "Done with gradient descent at iteration  188\n",
      "Learned weights =  [ 1.28618776 91.49000283 78.43732424]\n",
      "Done with gradient descent at iteration  188\n",
      "Learned weights =  [ 1.28618776 91.49000194 78.43732424]\n",
      "Done with gradient descent at iteration  188\n",
      "Learned weights =  [ 1.28618776 91.49000194 78.43732333]\n",
      "Done with gradient descent at iteration  189\n",
      "Learned weights =  [ 1.29290797 91.49000194 78.43732333]\n",
      "Done with gradient descent at iteration  189\n",
      "Learned weights =  [ 1.29290797 91.49000104 78.43732333]\n",
      "Done with gradient descent at iteration  189\n",
      "Learned weights =  [ 1.29290797 91.49000104 78.43732243]\n",
      "Done with gradient descent at iteration  190\n",
      "Learned weights =  [ 1.29962819 91.49000104 78.43732243]\n",
      "Done with gradient descent at iteration  190\n",
      "Learned weights =  [ 1.29962819 91.49000014 78.43732243]\n",
      "Done with gradient descent at iteration  190\n",
      "Learned weights =  [ 1.29962819 91.49000014 78.43732152]\n",
      "Done with gradient descent at iteration  191\n",
      "Learned weights =  [ 1.3063484  91.49000014 78.43732152]\n",
      "Done with gradient descent at iteration  191\n",
      "Learned weights =  [ 1.3063484  91.48999925 78.43732152]\n",
      "Done with gradient descent at iteration  191\n",
      "Learned weights =  [ 1.3063484  91.48999925 78.43732061]\n",
      "Done with gradient descent at iteration  192\n",
      "Learned weights =  [ 1.31306861 91.48999925 78.43732061]\n",
      "Done with gradient descent at iteration  192\n",
      "Learned weights =  [ 1.31306861 91.48999835 78.43732061]\n",
      "Done with gradient descent at iteration  192\n",
      "Learned weights =  [ 1.31306861 91.48999835 78.43731971]\n",
      "Done with gradient descent at iteration  193\n",
      "Learned weights =  [ 1.31978882 91.48999835 78.43731971]\n",
      "Done with gradient descent at iteration  193\n",
      "Learned weights =  [ 1.31978882 91.48999745 78.43731971]\n",
      "Done with gradient descent at iteration  193\n",
      "Learned weights =  [ 1.31978882 91.48999745 78.4373188 ]\n",
      "Done with gradient descent at iteration  194\n",
      "Learned weights =  [ 1.32650904 91.48999745 78.4373188 ]\n",
      "Done with gradient descent at iteration  194\n",
      "Learned weights =  [ 1.32650904 91.48999655 78.4373188 ]\n",
      "Done with gradient descent at iteration  194\n",
      "Learned weights =  [ 1.32650904 91.48999655 78.4373179 ]\n",
      "Done with gradient descent at iteration  195\n",
      "Learned weights =  [ 1.33322925 91.48999655 78.4373179 ]\n",
      "Done with gradient descent at iteration  195\n",
      "Learned weights =  [ 1.33322925 91.48999566 78.4373179 ]\n",
      "Done with gradient descent at iteration  195\n",
      "Learned weights =  [ 1.33322925 91.48999566 78.43731699]\n",
      "Done with gradient descent at iteration  196\n",
      "Learned weights =  [ 1.33994946 91.48999566 78.43731699]\n",
      "Done with gradient descent at iteration  196\n",
      "Learned weights =  [ 1.33994946 91.48999476 78.43731699]\n",
      "Done with gradient descent at iteration  196\n",
      "Learned weights =  [ 1.33994946 91.48999476 78.43731608]\n",
      "Done with gradient descent at iteration  197\n",
      "Learned weights =  [ 1.34666967 91.48999476 78.43731608]\n",
      "Done with gradient descent at iteration  197\n",
      "Learned weights =  [ 1.34666967 91.48999386 78.43731608]\n",
      "Done with gradient descent at iteration  197\n",
      "Learned weights =  [ 1.34666967 91.48999386 78.43731518]\n",
      "Done with gradient descent at iteration  198\n",
      "Learned weights =  [ 1.35338989 91.48999386 78.43731518]\n",
      "Done with gradient descent at iteration  198\n",
      "Learned weights =  [ 1.35338989 91.48999297 78.43731518]\n",
      "Done with gradient descent at iteration  198\n",
      "Learned weights =  [ 1.35338989 91.48999297 78.43731427]\n",
      "Done with gradient descent at iteration  199\n",
      "Learned weights =  [ 1.3601101  91.48999297 78.43731427]\n",
      "Done with gradient descent at iteration  199\n",
      "Learned weights =  [ 1.3601101  91.48999207 78.43731427]\n",
      "Done with gradient descent at iteration  199\n",
      "Learned weights =  [ 1.3601101  91.48999207 78.43731337]\n",
      "Iteration = 200\n",
      "Cost function =  3605285856902500.0\n",
      "Done with gradient descent at iteration  200\n",
      "Learned weights =  [ 1.36683031 91.48999207 78.43731337]\n",
      "Done with gradient descent at iteration  200\n",
      "Learned weights =  [ 1.36683031 91.48999117 78.43731337]\n",
      "Done with gradient descent at iteration  200\n",
      "Learned weights =  [ 1.36683031 91.48999117 78.43731246]\n",
      "Done with gradient descent at iteration  201\n",
      "Learned weights =  [ 1.37355052 91.48999117 78.43731246]\n",
      "Done with gradient descent at iteration  201\n",
      "Learned weights =  [ 1.37355052 91.48999028 78.43731246]\n",
      "Done with gradient descent at iteration  201\n",
      "Learned weights =  [ 1.37355052 91.48999028 78.43731155]\n",
      "Done with gradient descent at iteration  202\n",
      "Learned weights =  [ 1.38027073 91.48999028 78.43731155]\n",
      "Done with gradient descent at iteration  202\n",
      "Learned weights =  [ 1.38027073 91.48998938 78.43731155]\n",
      "Done with gradient descent at iteration  202\n",
      "Learned weights =  [ 1.38027073 91.48998938 78.43731065]\n",
      "Done with gradient descent at iteration  203\n",
      "Learned weights =  [ 1.38699095 91.48998938 78.43731065]\n",
      "Done with gradient descent at iteration  203\n",
      "Learned weights =  [ 1.38699095 91.48998848 78.43731065]\n",
      "Done with gradient descent at iteration  203\n",
      "Learned weights =  [ 1.38699095 91.48998848 78.43730974]\n",
      "Done with gradient descent at iteration  204\n",
      "Learned weights =  [ 1.39371116 91.48998848 78.43730974]\n",
      "Done with gradient descent at iteration  204\n",
      "Learned weights =  [ 1.39371116 91.48998759 78.43730974]\n",
      "Done with gradient descent at iteration  204\n",
      "Learned weights =  [ 1.39371116 91.48998759 78.43730884]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  205\n",
      "Learned weights =  [ 1.40043137 91.48998759 78.43730884]\n",
      "Done with gradient descent at iteration  205\n",
      "Learned weights =  [ 1.40043137 91.48998669 78.43730884]\n",
      "Done with gradient descent at iteration  205\n",
      "Learned weights =  [ 1.40043137 91.48998669 78.43730793]\n",
      "Done with gradient descent at iteration  206\n",
      "Learned weights =  [ 1.40715158 91.48998669 78.43730793]\n",
      "Done with gradient descent at iteration  206\n",
      "Learned weights =  [ 1.40715158 91.48998579 78.43730793]\n",
      "Done with gradient descent at iteration  206\n",
      "Learned weights =  [ 1.40715158 91.48998579 78.43730702]\n",
      "Done with gradient descent at iteration  207\n",
      "Learned weights =  [ 1.41387179 91.48998579 78.43730702]\n",
      "Done with gradient descent at iteration  207\n",
      "Learned weights =  [ 1.41387179 91.48998489 78.43730702]\n",
      "Done with gradient descent at iteration  207\n",
      "Learned weights =  [ 1.41387179 91.48998489 78.43730612]\n",
      "Done with gradient descent at iteration  208\n",
      "Learned weights =  [ 1.420592   91.48998489 78.43730612]\n",
      "Done with gradient descent at iteration  208\n",
      "Learned weights =  [ 1.420592   91.489984   78.43730612]\n",
      "Done with gradient descent at iteration  208\n",
      "Learned weights =  [ 1.420592   91.489984   78.43730521]\n",
      "Done with gradient descent at iteration  209\n",
      "Learned weights =  [ 1.42731221 91.489984   78.43730521]\n",
      "Done with gradient descent at iteration  209\n",
      "Learned weights =  [ 1.42731221 91.4899831  78.43730521]\n",
      "Done with gradient descent at iteration  209\n",
      "Learned weights =  [ 1.42731221 91.4899831  78.43730431]\n",
      "Done with gradient descent at iteration  210\n",
      "Learned weights =  [ 1.43403242 91.4899831  78.43730431]\n",
      "Done with gradient descent at iteration  210\n",
      "Learned weights =  [ 1.43403242 91.4899822  78.43730431]\n",
      "Done with gradient descent at iteration  210\n",
      "Learned weights =  [ 1.43403242 91.4899822  78.4373034 ]\n",
      "Done with gradient descent at iteration  211\n",
      "Learned weights =  [ 1.44075263 91.4899822  78.4373034 ]\n",
      "Done with gradient descent at iteration  211\n",
      "Learned weights =  [ 1.44075263 91.48998131 78.4373034 ]\n",
      "Done with gradient descent at iteration  211\n",
      "Learned weights =  [ 1.44075263 91.48998131 78.4373025 ]\n",
      "Done with gradient descent at iteration  212\n",
      "Learned weights =  [ 1.44747284 91.48998131 78.4373025 ]\n",
      "Done with gradient descent at iteration  212\n",
      "Learned weights =  [ 1.44747284 91.48998041 78.4373025 ]\n",
      "Done with gradient descent at iteration  212\n",
      "Learned weights =  [ 1.44747284 91.48998041 78.43730159]\n",
      "Done with gradient descent at iteration  213\n",
      "Learned weights =  [ 1.45419306 91.48998041 78.43730159]\n",
      "Done with gradient descent at iteration  213\n",
      "Learned weights =  [ 1.45419306 91.48997951 78.43730159]\n",
      "Done with gradient descent at iteration  213\n",
      "Learned weights =  [ 1.45419306 91.48997951 78.43730068]\n",
      "Done with gradient descent at iteration  214\n",
      "Learned weights =  [ 1.46091327 91.48997951 78.43730068]\n",
      "Done with gradient descent at iteration  214\n",
      "Learned weights =  [ 1.46091327 91.48997862 78.43730068]\n",
      "Done with gradient descent at iteration  214\n",
      "Learned weights =  [ 1.46091327 91.48997862 78.43729978]\n",
      "Done with gradient descent at iteration  215\n",
      "Learned weights =  [ 1.46763348 91.48997862 78.43729978]\n",
      "Done with gradient descent at iteration  215\n",
      "Learned weights =  [ 1.46763348 91.48997772 78.43729978]\n",
      "Done with gradient descent at iteration  215\n",
      "Learned weights =  [ 1.46763348 91.48997772 78.43729887]\n",
      "Done with gradient descent at iteration  216\n",
      "Learned weights =  [ 1.47435369 91.48997772 78.43729887]\n",
      "Done with gradient descent at iteration  216\n",
      "Learned weights =  [ 1.47435369 91.48997682 78.43729887]\n",
      "Done with gradient descent at iteration  216\n",
      "Learned weights =  [ 1.47435369 91.48997682 78.43729797]\n",
      "Done with gradient descent at iteration  217\n",
      "Learned weights =  [ 1.4810739  91.48997682 78.43729797]\n",
      "Done with gradient descent at iteration  217\n",
      "Learned weights =  [ 1.4810739  91.48997592 78.43729797]\n",
      "Done with gradient descent at iteration  217\n",
      "Learned weights =  [ 1.4810739  91.48997592 78.43729706]\n",
      "Done with gradient descent at iteration  218\n",
      "Learned weights =  [ 1.48779411 91.48997592 78.43729706]\n",
      "Done with gradient descent at iteration  218\n",
      "Learned weights =  [ 1.48779411 91.48997503 78.43729706]\n",
      "Done with gradient descent at iteration  218\n",
      "Learned weights =  [ 1.48779411 91.48997503 78.43729615]\n",
      "Done with gradient descent at iteration  219\n",
      "Learned weights =  [ 1.49451432 91.48997503 78.43729615]\n",
      "Done with gradient descent at iteration  219\n",
      "Learned weights =  [ 1.49451432 91.48997413 78.43729615]\n",
      "Done with gradient descent at iteration  219\n",
      "Learned weights =  [ 1.49451432 91.48997413 78.43729525]\n",
      "Done with gradient descent at iteration  220\n",
      "Learned weights =  [ 1.50123453 91.48997413 78.43729525]\n",
      "Done with gradient descent at iteration  220\n",
      "Learned weights =  [ 1.50123453 91.48997323 78.43729525]\n",
      "Done with gradient descent at iteration  220\n",
      "Learned weights =  [ 1.50123453 91.48997323 78.43729434]\n",
      "Done with gradient descent at iteration  221\n",
      "Learned weights =  [ 1.50795474 91.48997323 78.43729434]\n",
      "Done with gradient descent at iteration  221\n",
      "Learned weights =  [ 1.50795474 91.48997234 78.43729434]\n",
      "Done with gradient descent at iteration  221\n",
      "Learned weights =  [ 1.50795474 91.48997234 78.43729344]\n",
      "Done with gradient descent at iteration  222\n",
      "Learned weights =  [ 1.51467495 91.48997234 78.43729344]\n",
      "Done with gradient descent at iteration  222\n",
      "Learned weights =  [ 1.51467495 91.48997144 78.43729344]\n",
      "Done with gradient descent at iteration  222\n",
      "Learned weights =  [ 1.51467495 91.48997144 78.43729253]\n",
      "Done with gradient descent at iteration  223\n",
      "Learned weights =  [ 1.52139515 91.48997144 78.43729253]\n",
      "Done with gradient descent at iteration  223\n",
      "Learned weights =  [ 1.52139515 91.48997054 78.43729253]\n",
      "Done with gradient descent at iteration  223\n",
      "Learned weights =  [ 1.52139515 91.48997054 78.43729162]\n",
      "Done with gradient descent at iteration  224\n",
      "Learned weights =  [ 1.52811536 91.48997054 78.43729162]\n",
      "Done with gradient descent at iteration  224\n",
      "Learned weights =  [ 1.52811536 91.48996965 78.43729162]\n",
      "Done with gradient descent at iteration  224\n",
      "Learned weights =  [ 1.52811536 91.48996965 78.43729072]\n",
      "Done with gradient descent at iteration  225\n",
      "Learned weights =  [ 1.53483557 91.48996965 78.43729072]\n",
      "Done with gradient descent at iteration  225\n",
      "Learned weights =  [ 1.53483557 91.48996875 78.43729072]\n",
      "Done with gradient descent at iteration  225\n",
      "Learned weights =  [ 1.53483557 91.48996875 78.43728981]\n",
      "Done with gradient descent at iteration  226\n",
      "Learned weights =  [ 1.54155578 91.48996875 78.43728981]\n",
      "Done with gradient descent at iteration  226\n",
      "Learned weights =  [ 1.54155578 91.48996785 78.43728981]\n",
      "Done with gradient descent at iteration  226\n",
      "Learned weights =  [ 1.54155578 91.48996785 78.43728891]\n",
      "Done with gradient descent at iteration  227\n",
      "Learned weights =  [ 1.54827599 91.48996785 78.43728891]\n",
      "Done with gradient descent at iteration  227\n",
      "Learned weights =  [ 1.54827599 91.48996696 78.43728891]\n",
      "Done with gradient descent at iteration  227\n",
      "Learned weights =  [ 1.54827599 91.48996696 78.437288  ]\n",
      "Done with gradient descent at iteration  228\n",
      "Learned weights =  [ 1.5549962  91.48996696 78.437288  ]\n",
      "Done with gradient descent at iteration  228\n",
      "Learned weights =  [ 1.5549962  91.48996606 78.437288  ]\n",
      "Done with gradient descent at iteration  228\n",
      "Learned weights =  [ 1.5549962  91.48996606 78.43728709]\n",
      "Done with gradient descent at iteration  229\n",
      "Learned weights =  [ 1.56171641 91.48996606 78.43728709]\n",
      "Done with gradient descent at iteration  229\n",
      "Learned weights =  [ 1.56171641 91.48996516 78.43728709]\n",
      "Done with gradient descent at iteration  229\n",
      "Learned weights =  [ 1.56171641 91.48996516 78.43728619]\n",
      "Done with gradient descent at iteration  230\n",
      "Learned weights =  [ 1.56843662 91.48996516 78.43728619]\n",
      "Done with gradient descent at iteration  230\n",
      "Learned weights =  [ 1.56843662 91.48996426 78.43728619]\n",
      "Done with gradient descent at iteration  230\n",
      "Learned weights =  [ 1.56843662 91.48996426 78.43728528]\n",
      "Done with gradient descent at iteration  231\n",
      "Learned weights =  [ 1.57515683 91.48996426 78.43728528]\n",
      "Done with gradient descent at iteration  231\n",
      "Learned weights =  [ 1.57515683 91.48996337 78.43728528]\n",
      "Done with gradient descent at iteration  231\n",
      "Learned weights =  [ 1.57515683 91.48996337 78.43728438]\n",
      "Done with gradient descent at iteration  232\n",
      "Learned weights =  [ 1.58187704 91.48996337 78.43728438]\n",
      "Done with gradient descent at iteration  232\n",
      "Learned weights =  [ 1.58187704 91.48996247 78.43728438]\n",
      "Done with gradient descent at iteration  232\n",
      "Learned weights =  [ 1.58187704 91.48996247 78.43728347]\n",
      "Done with gradient descent at iteration  233\n",
      "Learned weights =  [ 1.58859724 91.48996247 78.43728347]\n",
      "Done with gradient descent at iteration  233\n",
      "Learned weights =  [ 1.58859724 91.48996157 78.43728347]\n",
      "Done with gradient descent at iteration  233\n",
      "Learned weights =  [ 1.58859724 91.48996157 78.43728256]\n",
      "Done with gradient descent at iteration  234\n",
      "Learned weights =  [ 1.59531745 91.48996157 78.43728256]\n",
      "Done with gradient descent at iteration  234\n",
      "Learned weights =  [ 1.59531745 91.48996068 78.43728256]\n",
      "Done with gradient descent at iteration  234\n",
      "Learned weights =  [ 1.59531745 91.48996068 78.43728166]\n",
      "Done with gradient descent at iteration  235\n",
      "Learned weights =  [ 1.60203766 91.48996068 78.43728166]\n",
      "Done with gradient descent at iteration  235\n",
      "Learned weights =  [ 1.60203766 91.48995978 78.43728166]\n",
      "Done with gradient descent at iteration  235\n",
      "Learned weights =  [ 1.60203766 91.48995978 78.43728075]\n",
      "Done with gradient descent at iteration  236\n",
      "Learned weights =  [ 1.60875787 91.48995978 78.43728075]\n",
      "Done with gradient descent at iteration  236\n",
      "Learned weights =  [ 1.60875787 91.48995888 78.43728075]\n",
      "Done with gradient descent at iteration  236\n",
      "Learned weights =  [ 1.60875787 91.48995888 78.43727985]\n",
      "Done with gradient descent at iteration  237\n",
      "Learned weights =  [ 1.61547808 91.48995888 78.43727985]\n",
      "Done with gradient descent at iteration  237\n",
      "Learned weights =  [ 1.61547808 91.48995799 78.43727985]\n",
      "Done with gradient descent at iteration  237\n",
      "Learned weights =  [ 1.61547808 91.48995799 78.43727894]\n",
      "Done with gradient descent at iteration  238\n",
      "Learned weights =  [ 1.62219828 91.48995799 78.43727894]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  238\n",
      "Learned weights =  [ 1.62219828 91.48995709 78.43727894]\n",
      "Done with gradient descent at iteration  238\n",
      "Learned weights =  [ 1.62219828 91.48995709 78.43727803]\n",
      "Done with gradient descent at iteration  239\n",
      "Learned weights =  [ 1.62891849 91.48995709 78.43727803]\n",
      "Done with gradient descent at iteration  239\n",
      "Learned weights =  [ 1.62891849 91.48995619 78.43727803]\n",
      "Done with gradient descent at iteration  239\n",
      "Learned weights =  [ 1.62891849 91.48995619 78.43727713]\n",
      "Done with gradient descent at iteration  240\n",
      "Learned weights =  [ 1.6356387  91.48995619 78.43727713]\n",
      "Done with gradient descent at iteration  240\n",
      "Learned weights =  [ 1.6356387  91.48995529 78.43727713]\n",
      "Done with gradient descent at iteration  240\n",
      "Learned weights =  [ 1.6356387  91.48995529 78.43727622]\n",
      "Done with gradient descent at iteration  241\n",
      "Learned weights =  [ 1.64235891 91.48995529 78.43727622]\n",
      "Done with gradient descent at iteration  241\n",
      "Learned weights =  [ 1.64235891 91.4899544  78.43727622]\n",
      "Done with gradient descent at iteration  241\n",
      "Learned weights =  [ 1.64235891 91.4899544  78.43727532]\n",
      "Done with gradient descent at iteration  242\n",
      "Learned weights =  [ 1.64907911 91.4899544  78.43727532]\n",
      "Done with gradient descent at iteration  242\n",
      "Learned weights =  [ 1.64907911 91.4899535  78.43727532]\n",
      "Done with gradient descent at iteration  242\n",
      "Learned weights =  [ 1.64907911 91.4899535  78.43727441]\n",
      "Done with gradient descent at iteration  243\n",
      "Learned weights =  [ 1.65579932 91.4899535  78.43727441]\n",
      "Done with gradient descent at iteration  243\n",
      "Learned weights =  [ 1.65579932 91.4899526  78.43727441]\n",
      "Done with gradient descent at iteration  243\n",
      "Learned weights =  [ 1.65579932 91.4899526  78.4372735 ]\n",
      "Done with gradient descent at iteration  244\n",
      "Learned weights =  [ 1.66251953 91.4899526  78.4372735 ]\n",
      "Done with gradient descent at iteration  244\n",
      "Learned weights =  [ 1.66251953 91.48995171 78.4372735 ]\n",
      "Done with gradient descent at iteration  244\n",
      "Learned weights =  [ 1.66251953 91.48995171 78.4372726 ]\n",
      "Done with gradient descent at iteration  245\n",
      "Learned weights =  [ 1.66923974 91.48995171 78.4372726 ]\n",
      "Done with gradient descent at iteration  245\n",
      "Learned weights =  [ 1.66923974 91.48995081 78.4372726 ]\n",
      "Done with gradient descent at iteration  245\n",
      "Learned weights =  [ 1.66923974 91.48995081 78.43727169]\n",
      "Done with gradient descent at iteration  246\n",
      "Learned weights =  [ 1.67595994 91.48995081 78.43727169]\n",
      "Done with gradient descent at iteration  246\n",
      "Learned weights =  [ 1.67595994 91.48994991 78.43727169]\n",
      "Done with gradient descent at iteration  246\n",
      "Learned weights =  [ 1.67595994 91.48994991 78.43727079]\n",
      "Done with gradient descent at iteration  247\n",
      "Learned weights =  [ 1.68268015 91.48994991 78.43727079]\n",
      "Done with gradient descent at iteration  247\n",
      "Learned weights =  [ 1.68268015 91.48994902 78.43727079]\n",
      "Done with gradient descent at iteration  247\n",
      "Learned weights =  [ 1.68268015 91.48994902 78.43726988]\n",
      "Done with gradient descent at iteration  248\n",
      "Learned weights =  [ 1.68940036 91.48994902 78.43726988]\n",
      "Done with gradient descent at iteration  248\n",
      "Learned weights =  [ 1.68940036 91.48994812 78.43726988]\n",
      "Done with gradient descent at iteration  248\n",
      "Learned weights =  [ 1.68940036 91.48994812 78.43726897]\n",
      "Done with gradient descent at iteration  249\n",
      "Learned weights =  [ 1.69612056 91.48994812 78.43726897]\n",
      "Done with gradient descent at iteration  249\n",
      "Learned weights =  [ 1.69612056 91.48994722 78.43726897]\n",
      "Done with gradient descent at iteration  249\n",
      "Learned weights =  [ 1.69612056 91.48994722 78.43726807]\n",
      "Done with gradient descent at iteration  250\n",
      "Learned weights =  [ 1.70284077 91.48994722 78.43726807]\n",
      "Done with gradient descent at iteration  250\n",
      "Learned weights =  [ 1.70284077 91.48994633 78.43726807]\n",
      "Done with gradient descent at iteration  250\n",
      "Learned weights =  [ 1.70284077 91.48994633 78.43726716]\n",
      "Done with gradient descent at iteration  251\n",
      "Learned weights =  [ 1.70956098 91.48994633 78.43726716]\n",
      "Done with gradient descent at iteration  251\n",
      "Learned weights =  [ 1.70956098 91.48994543 78.43726716]\n",
      "Done with gradient descent at iteration  251\n",
      "Learned weights =  [ 1.70956098 91.48994543 78.43726626]\n",
      "Done with gradient descent at iteration  252\n",
      "Learned weights =  [ 1.71628118 91.48994543 78.43726626]\n",
      "Done with gradient descent at iteration  252\n",
      "Learned weights =  [ 1.71628118 91.48994453 78.43726626]\n",
      "Done with gradient descent at iteration  252\n",
      "Learned weights =  [ 1.71628118 91.48994453 78.43726535]\n",
      "Done with gradient descent at iteration  253\n",
      "Learned weights =  [ 1.72300139 91.48994453 78.43726535]\n",
      "Done with gradient descent at iteration  253\n",
      "Learned weights =  [ 1.72300139 91.48994363 78.43726535]\n",
      "Done with gradient descent at iteration  253\n",
      "Learned weights =  [ 1.72300139 91.48994363 78.43726444]\n",
      "Done with gradient descent at iteration  254\n",
      "Learned weights =  [ 1.7297216  91.48994363 78.43726444]\n",
      "Done with gradient descent at iteration  254\n",
      "Learned weights =  [ 1.7297216  91.48994274 78.43726444]\n",
      "Done with gradient descent at iteration  254\n",
      "Learned weights =  [ 1.7297216  91.48994274 78.43726354]\n",
      "Done with gradient descent at iteration  255\n",
      "Learned weights =  [ 1.7364418  91.48994274 78.43726354]\n",
      "Done with gradient descent at iteration  255\n",
      "Learned weights =  [ 1.7364418  91.48994184 78.43726354]\n",
      "Done with gradient descent at iteration  255\n",
      "Learned weights =  [ 1.7364418  91.48994184 78.43726263]\n",
      "Done with gradient descent at iteration  256\n",
      "Learned weights =  [ 1.74316201 91.48994184 78.43726263]\n",
      "Done with gradient descent at iteration  256\n",
      "Learned weights =  [ 1.74316201 91.48994094 78.43726263]\n",
      "Done with gradient descent at iteration  256\n",
      "Learned weights =  [ 1.74316201 91.48994094 78.43726173]\n",
      "Done with gradient descent at iteration  257\n",
      "Learned weights =  [ 1.74988221 91.48994094 78.43726173]\n",
      "Done with gradient descent at iteration  257\n",
      "Learned weights =  [ 1.74988221 91.48994005 78.43726173]\n",
      "Done with gradient descent at iteration  257\n",
      "Learned weights =  [ 1.74988221 91.48994005 78.43726082]\n",
      "Done with gradient descent at iteration  258\n",
      "Learned weights =  [ 1.75660242 91.48994005 78.43726082]\n",
      "Done with gradient descent at iteration  258\n",
      "Learned weights =  [ 1.75660242 91.48993915 78.43726082]\n",
      "Done with gradient descent at iteration  258\n",
      "Learned weights =  [ 1.75660242 91.48993915 78.43725991]\n",
      "Done with gradient descent at iteration  259\n",
      "Learned weights =  [ 1.76332262 91.48993915 78.43725991]\n",
      "Done with gradient descent at iteration  259\n",
      "Learned weights =  [ 1.76332262 91.48993825 78.43725991]\n",
      "Done with gradient descent at iteration  259\n",
      "Learned weights =  [ 1.76332262 91.48993825 78.43725901]\n",
      "Done with gradient descent at iteration  260\n",
      "Learned weights =  [ 1.77004283 91.48993825 78.43725901]\n",
      "Done with gradient descent at iteration  260\n",
      "Learned weights =  [ 1.77004283 91.48993736 78.43725901]\n",
      "Done with gradient descent at iteration  260\n",
      "Learned weights =  [ 1.77004283 91.48993736 78.4372581 ]\n",
      "Done with gradient descent at iteration  261\n",
      "Learned weights =  [ 1.77676304 91.48993736 78.4372581 ]\n",
      "Done with gradient descent at iteration  261\n",
      "Learned weights =  [ 1.77676304 91.48993646 78.4372581 ]\n",
      "Done with gradient descent at iteration  261\n",
      "Learned weights =  [ 1.77676304 91.48993646 78.4372572 ]\n",
      "Done with gradient descent at iteration  262\n",
      "Learned weights =  [ 1.78348324 91.48993646 78.4372572 ]\n",
      "Done with gradient descent at iteration  262\n",
      "Learned weights =  [ 1.78348324 91.48993556 78.4372572 ]\n",
      "Done with gradient descent at iteration  262\n",
      "Learned weights =  [ 1.78348324 91.48993556 78.43725629]\n",
      "Done with gradient descent at iteration  263\n",
      "Learned weights =  [ 1.79020345 91.48993556 78.43725629]\n",
      "Done with gradient descent at iteration  263\n",
      "Learned weights =  [ 1.79020345 91.48993466 78.43725629]\n",
      "Done with gradient descent at iteration  263\n",
      "Learned weights =  [ 1.79020345 91.48993466 78.43725538]\n",
      "Done with gradient descent at iteration  264\n",
      "Learned weights =  [ 1.79692365 91.48993466 78.43725538]\n",
      "Done with gradient descent at iteration  264\n",
      "Learned weights =  [ 1.79692365 91.48993377 78.43725538]\n",
      "Done with gradient descent at iteration  264\n",
      "Learned weights =  [ 1.79692365 91.48993377 78.43725448]\n",
      "Done with gradient descent at iteration  265\n",
      "Learned weights =  [ 1.80364386 91.48993377 78.43725448]\n",
      "Done with gradient descent at iteration  265\n",
      "Learned weights =  [ 1.80364386 91.48993287 78.43725448]\n",
      "Done with gradient descent at iteration  265\n",
      "Learned weights =  [ 1.80364386 91.48993287 78.43725357]\n",
      "Done with gradient descent at iteration  266\n",
      "Learned weights =  [ 1.81036406 91.48993287 78.43725357]\n",
      "Done with gradient descent at iteration  266\n",
      "Learned weights =  [ 1.81036406 91.48993197 78.43725357]\n",
      "Done with gradient descent at iteration  266\n",
      "Learned weights =  [ 1.81036406 91.48993197 78.43725267]\n",
      "Done with gradient descent at iteration  267\n",
      "Learned weights =  [ 1.81708427 91.48993197 78.43725267]\n",
      "Done with gradient descent at iteration  267\n",
      "Learned weights =  [ 1.81708427 91.48993108 78.43725267]\n",
      "Done with gradient descent at iteration  267\n",
      "Learned weights =  [ 1.81708427 91.48993108 78.43725176]\n",
      "Done with gradient descent at iteration  268\n",
      "Learned weights =  [ 1.82380447 91.48993108 78.43725176]\n",
      "Done with gradient descent at iteration  268\n",
      "Learned weights =  [ 1.82380447 91.48993018 78.43725176]\n",
      "Done with gradient descent at iteration  268\n",
      "Learned weights =  [ 1.82380447 91.48993018 78.43725085]\n",
      "Done with gradient descent at iteration  269\n",
      "Learned weights =  [ 1.83052468 91.48993018 78.43725085]\n",
      "Done with gradient descent at iteration  269\n",
      "Learned weights =  [ 1.83052468 91.48992928 78.43725085]\n",
      "Done with gradient descent at iteration  269\n",
      "Learned weights =  [ 1.83052468 91.48992928 78.43724995]\n",
      "Done with gradient descent at iteration  270\n",
      "Learned weights =  [ 1.83724488 91.48992928 78.43724995]\n",
      "Done with gradient descent at iteration  270\n",
      "Learned weights =  [ 1.83724488 91.48992839 78.43724995]\n",
      "Done with gradient descent at iteration  270\n",
      "Learned weights =  [ 1.83724488 91.48992839 78.43724904]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  271\n",
      "Learned weights =  [ 1.84396508 91.48992839 78.43724904]\n",
      "Done with gradient descent at iteration  271\n",
      "Learned weights =  [ 1.84396508 91.48992749 78.43724904]\n",
      "Done with gradient descent at iteration  271\n",
      "Learned weights =  [ 1.84396508 91.48992749 78.43724814]\n",
      "Done with gradient descent at iteration  272\n",
      "Learned weights =  [ 1.85068529 91.48992749 78.43724814]\n",
      "Done with gradient descent at iteration  272\n",
      "Learned weights =  [ 1.85068529 91.48992659 78.43724814]\n",
      "Done with gradient descent at iteration  272\n",
      "Learned weights =  [ 1.85068529 91.48992659 78.43724723]\n",
      "Done with gradient descent at iteration  273\n",
      "Learned weights =  [ 1.85740549 91.48992659 78.43724723]\n",
      "Done with gradient descent at iteration  273\n",
      "Learned weights =  [ 1.85740549 91.48992569 78.43724723]\n",
      "Done with gradient descent at iteration  273\n",
      "Learned weights =  [ 1.85740549 91.48992569 78.43724632]\n",
      "Done with gradient descent at iteration  274\n",
      "Learned weights =  [ 1.8641257  91.48992569 78.43724632]\n",
      "Done with gradient descent at iteration  274\n",
      "Learned weights =  [ 1.8641257  91.4899248  78.43724632]\n",
      "Done with gradient descent at iteration  274\n",
      "Learned weights =  [ 1.8641257  91.4899248  78.43724542]\n",
      "Done with gradient descent at iteration  275\n",
      "Learned weights =  [ 1.8708459  91.4899248  78.43724542]\n",
      "Done with gradient descent at iteration  275\n",
      "Learned weights =  [ 1.8708459  91.4899239  78.43724542]\n",
      "Done with gradient descent at iteration  275\n",
      "Learned weights =  [ 1.8708459  91.4899239  78.43724451]\n",
      "Done with gradient descent at iteration  276\n",
      "Learned weights =  [ 1.8775661  91.4899239  78.43724451]\n",
      "Done with gradient descent at iteration  276\n",
      "Learned weights =  [ 1.8775661  91.489923   78.43724451]\n",
      "Done with gradient descent at iteration  276\n",
      "Learned weights =  [ 1.8775661  91.489923   78.43724361]\n",
      "Done with gradient descent at iteration  277\n",
      "Learned weights =  [ 1.88428631 91.489923   78.43724361]\n",
      "Done with gradient descent at iteration  277\n",
      "Learned weights =  [ 1.88428631 91.48992211 78.43724361]\n",
      "Done with gradient descent at iteration  277\n",
      "Learned weights =  [ 1.88428631 91.48992211 78.4372427 ]\n",
      "Done with gradient descent at iteration  278\n",
      "Learned weights =  [ 1.89100651 91.48992211 78.4372427 ]\n",
      "Done with gradient descent at iteration  278\n",
      "Learned weights =  [ 1.89100651 91.48992121 78.4372427 ]\n",
      "Done with gradient descent at iteration  278\n",
      "Learned weights =  [ 1.89100651 91.48992121 78.43724179]\n",
      "Done with gradient descent at iteration  279\n",
      "Learned weights =  [ 1.89772672 91.48992121 78.43724179]\n",
      "Done with gradient descent at iteration  279\n",
      "Learned weights =  [ 1.89772672 91.48992031 78.43724179]\n",
      "Done with gradient descent at iteration  279\n",
      "Learned weights =  [ 1.89772672 91.48992031 78.43724089]\n",
      "Done with gradient descent at iteration  280\n",
      "Learned weights =  [ 1.90444692 91.48992031 78.43724089]\n",
      "Done with gradient descent at iteration  280\n",
      "Learned weights =  [ 1.90444692 91.48991942 78.43724089]\n",
      "Done with gradient descent at iteration  280\n",
      "Learned weights =  [ 1.90444692 91.48991942 78.43723998]\n",
      "Done with gradient descent at iteration  281\n",
      "Learned weights =  [ 1.91116712 91.48991942 78.43723998]\n",
      "Done with gradient descent at iteration  281\n",
      "Learned weights =  [ 1.91116712 91.48991852 78.43723998]\n",
      "Done with gradient descent at iteration  281\n",
      "Learned weights =  [ 1.91116712 91.48991852 78.43723908]\n",
      "Done with gradient descent at iteration  282\n",
      "Learned weights =  [ 1.91788733 91.48991852 78.43723908]\n",
      "Done with gradient descent at iteration  282\n",
      "Learned weights =  [ 1.91788733 91.48991762 78.43723908]\n",
      "Done with gradient descent at iteration  282\n",
      "Learned weights =  [ 1.91788733 91.48991762 78.43723817]\n",
      "Done with gradient descent at iteration  283\n",
      "Learned weights =  [ 1.92460753 91.48991762 78.43723817]\n",
      "Done with gradient descent at iteration  283\n",
      "Learned weights =  [ 1.92460753 91.48991673 78.43723817]\n",
      "Done with gradient descent at iteration  283\n",
      "Learned weights =  [ 1.92460753 91.48991673 78.43723726]\n",
      "Done with gradient descent at iteration  284\n",
      "Learned weights =  [ 1.93132773 91.48991673 78.43723726]\n",
      "Done with gradient descent at iteration  284\n",
      "Learned weights =  [ 1.93132773 91.48991583 78.43723726]\n",
      "Done with gradient descent at iteration  284\n",
      "Learned weights =  [ 1.93132773 91.48991583 78.43723636]\n",
      "Done with gradient descent at iteration  285\n",
      "Learned weights =  [ 1.93804793 91.48991583 78.43723636]\n",
      "Done with gradient descent at iteration  285\n",
      "Learned weights =  [ 1.93804793 91.48991493 78.43723636]\n",
      "Done with gradient descent at iteration  285\n",
      "Learned weights =  [ 1.93804793 91.48991493 78.43723545]\n",
      "Done with gradient descent at iteration  286\n",
      "Learned weights =  [ 1.94476814 91.48991493 78.43723545]\n",
      "Done with gradient descent at iteration  286\n",
      "Learned weights =  [ 1.94476814 91.48991403 78.43723545]\n",
      "Done with gradient descent at iteration  286\n",
      "Learned weights =  [ 1.94476814 91.48991403 78.43723455]\n",
      "Done with gradient descent at iteration  287\n",
      "Learned weights =  [ 1.95148834 91.48991403 78.43723455]\n",
      "Done with gradient descent at iteration  287\n",
      "Learned weights =  [ 1.95148834 91.48991314 78.43723455]\n",
      "Done with gradient descent at iteration  287\n",
      "Learned weights =  [ 1.95148834 91.48991314 78.43723364]\n",
      "Done with gradient descent at iteration  288\n",
      "Learned weights =  [ 1.95820854 91.48991314 78.43723364]\n",
      "Done with gradient descent at iteration  288\n",
      "Learned weights =  [ 1.95820854 91.48991224 78.43723364]\n",
      "Done with gradient descent at iteration  288\n",
      "Learned weights =  [ 1.95820854 91.48991224 78.43723273]\n",
      "Done with gradient descent at iteration  289\n",
      "Learned weights =  [ 1.96492874 91.48991224 78.43723273]\n",
      "Done with gradient descent at iteration  289\n",
      "Learned weights =  [ 1.96492874 91.48991134 78.43723273]\n",
      "Done with gradient descent at iteration  289\n",
      "Learned weights =  [ 1.96492874 91.48991134 78.43723183]\n",
      "Done with gradient descent at iteration  290\n",
      "Learned weights =  [ 1.97164895 91.48991134 78.43723183]\n",
      "Done with gradient descent at iteration  290\n",
      "Learned weights =  [ 1.97164895 91.48991045 78.43723183]\n",
      "Done with gradient descent at iteration  290\n",
      "Learned weights =  [ 1.97164895 91.48991045 78.43723092]\n",
      "Done with gradient descent at iteration  291\n",
      "Learned weights =  [ 1.97836915 91.48991045 78.43723092]\n",
      "Done with gradient descent at iteration  291\n",
      "Learned weights =  [ 1.97836915 91.48990955 78.43723092]\n",
      "Done with gradient descent at iteration  291\n",
      "Learned weights =  [ 1.97836915 91.48990955 78.43723002]\n",
      "Done with gradient descent at iteration  292\n",
      "Learned weights =  [ 1.98508935 91.48990955 78.43723002]\n",
      "Done with gradient descent at iteration  292\n",
      "Learned weights =  [ 1.98508935 91.48990865 78.43723002]\n",
      "Done with gradient descent at iteration  292\n",
      "Learned weights =  [ 1.98508935 91.48990865 78.43722911]\n",
      "Done with gradient descent at iteration  293\n",
      "Learned weights =  [ 1.99180955 91.48990865 78.43722911]\n",
      "Done with gradient descent at iteration  293\n",
      "Learned weights =  [ 1.99180955 91.48990776 78.43722911]\n",
      "Done with gradient descent at iteration  293\n",
      "Learned weights =  [ 1.99180955 91.48990776 78.4372282 ]\n",
      "Done with gradient descent at iteration  294\n",
      "Learned weights =  [ 1.99852976 91.48990776 78.4372282 ]\n",
      "Done with gradient descent at iteration  294\n",
      "Learned weights =  [ 1.99852976 91.48990686 78.4372282 ]\n",
      "Done with gradient descent at iteration  294\n",
      "Learned weights =  [ 1.99852976 91.48990686 78.4372273 ]\n",
      "Done with gradient descent at iteration  295\n",
      "Learned weights =  [ 2.00524996 91.48990686 78.4372273 ]\n",
      "Done with gradient descent at iteration  295\n",
      "Learned weights =  [ 2.00524996 91.48990596 78.4372273 ]\n",
      "Done with gradient descent at iteration  295\n",
      "Learned weights =  [ 2.00524996 91.48990596 78.43722639]\n",
      "Done with gradient descent at iteration  296\n",
      "Learned weights =  [ 2.01197016 91.48990596 78.43722639]\n",
      "Done with gradient descent at iteration  296\n",
      "Learned weights =  [ 2.01197016 91.48990506 78.43722639]\n",
      "Done with gradient descent at iteration  296\n",
      "Learned weights =  [ 2.01197016 91.48990506 78.43722549]\n",
      "Done with gradient descent at iteration  297\n",
      "Learned weights =  [ 2.01869036 91.48990506 78.43722549]\n",
      "Done with gradient descent at iteration  297\n",
      "Learned weights =  [ 2.01869036 91.48990417 78.43722549]\n",
      "Done with gradient descent at iteration  297\n",
      "Learned weights =  [ 2.01869036 91.48990417 78.43722458]\n",
      "Done with gradient descent at iteration  298\n",
      "Learned weights =  [ 2.02541056 91.48990417 78.43722458]\n",
      "Done with gradient descent at iteration  298\n",
      "Learned weights =  [ 2.02541056 91.48990327 78.43722458]\n",
      "Done with gradient descent at iteration  298\n",
      "Learned weights =  [ 2.02541056 91.48990327 78.43722368]\n",
      "Done with gradient descent at iteration  299\n",
      "Learned weights =  [ 2.03213076 91.48990327 78.43722368]\n",
      "Done with gradient descent at iteration  299\n",
      "Learned weights =  [ 2.03213076 91.48990237 78.43722368]\n",
      "Done with gradient descent at iteration  299\n",
      "Learned weights =  [ 2.03213076 91.48990237 78.43722277]\n",
      "Iteration = 300\n",
      "Cost function =  3605281340784635.0\n",
      "Done with gradient descent at iteration  300\n",
      "Learned weights =  [ 2.03885096 91.48990237 78.43722277]\n",
      "Done with gradient descent at iteration  300\n",
      "Learned weights =  [ 2.03885096 91.48990148 78.43722277]\n",
      "Done with gradient descent at iteration  300\n",
      "Learned weights =  [ 2.03885096 91.48990148 78.43722186]\n",
      "Done with gradient descent at iteration  301\n",
      "Learned weights =  [ 2.04557117 91.48990148 78.43722186]\n",
      "Done with gradient descent at iteration  301\n",
      "Learned weights =  [ 2.04557117 91.48990058 78.43722186]\n",
      "Done with gradient descent at iteration  301\n",
      "Learned weights =  [ 2.04557117 91.48990058 78.43722096]\n",
      "Done with gradient descent at iteration  302\n",
      "Learned weights =  [ 2.05229137 91.48990058 78.43722096]\n",
      "Done with gradient descent at iteration  302\n",
      "Learned weights =  [ 2.05229137 91.48989968 78.43722096]\n",
      "Done with gradient descent at iteration  302\n",
      "Learned weights =  [ 2.05229137 91.48989968 78.43722005]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  303\n",
      "Learned weights =  [ 2.05901157 91.48989968 78.43722005]\n",
      "Done with gradient descent at iteration  303\n",
      "Learned weights =  [ 2.05901157 91.48989879 78.43722005]\n",
      "Done with gradient descent at iteration  303\n",
      "Learned weights =  [ 2.05901157 91.48989879 78.43721915]\n",
      "Done with gradient descent at iteration  304\n",
      "Learned weights =  [ 2.06573177 91.48989879 78.43721915]\n",
      "Done with gradient descent at iteration  304\n",
      "Learned weights =  [ 2.06573177 91.48989789 78.43721915]\n",
      "Done with gradient descent at iteration  304\n",
      "Learned weights =  [ 2.06573177 91.48989789 78.43721824]\n",
      "Done with gradient descent at iteration  305\n",
      "Learned weights =  [ 2.07245197 91.48989789 78.43721824]\n",
      "Done with gradient descent at iteration  305\n",
      "Learned weights =  [ 2.07245197 91.48989699 78.43721824]\n",
      "Done with gradient descent at iteration  305\n",
      "Learned weights =  [ 2.07245197 91.48989699 78.43721733]\n",
      "Done with gradient descent at iteration  306\n",
      "Learned weights =  [ 2.07917217 91.48989699 78.43721733]\n",
      "Done with gradient descent at iteration  306\n",
      "Learned weights =  [ 2.07917217 91.4898961  78.43721733]\n",
      "Done with gradient descent at iteration  306\n",
      "Learned weights =  [ 2.07917217 91.4898961  78.43721643]\n",
      "Done with gradient descent at iteration  307\n",
      "Learned weights =  [ 2.08589237 91.4898961  78.43721643]\n",
      "Done with gradient descent at iteration  307\n",
      "Learned weights =  [ 2.08589237 91.4898952  78.43721643]\n",
      "Done with gradient descent at iteration  307\n",
      "Learned weights =  [ 2.08589237 91.4898952  78.43721552]\n",
      "Done with gradient descent at iteration  308\n",
      "Learned weights =  [ 2.09261257 91.4898952  78.43721552]\n",
      "Done with gradient descent at iteration  308\n",
      "Learned weights =  [ 2.09261257 91.4898943  78.43721552]\n",
      "Done with gradient descent at iteration  308\n",
      "Learned weights =  [ 2.09261257 91.4898943  78.43721462]\n",
      "Done with gradient descent at iteration  309\n",
      "Learned weights =  [ 2.09933277 91.4898943  78.43721462]\n",
      "Done with gradient descent at iteration  309\n",
      "Learned weights =  [ 2.09933277 91.4898934  78.43721462]\n",
      "Done with gradient descent at iteration  309\n",
      "Learned weights =  [ 2.09933277 91.4898934  78.43721371]\n",
      "Done with gradient descent at iteration  310\n",
      "Learned weights =  [ 2.10605297 91.4898934  78.43721371]\n",
      "Done with gradient descent at iteration  310\n",
      "Learned weights =  [ 2.10605297 91.48989251 78.43721371]\n",
      "Done with gradient descent at iteration  310\n",
      "Learned weights =  [ 2.10605297 91.48989251 78.4372128 ]\n",
      "Done with gradient descent at iteration  311\n",
      "Learned weights =  [ 2.11277317 91.48989251 78.4372128 ]\n",
      "Done with gradient descent at iteration  311\n",
      "Learned weights =  [ 2.11277317 91.48989161 78.4372128 ]\n",
      "Done with gradient descent at iteration  311\n",
      "Learned weights =  [ 2.11277317 91.48989161 78.4372119 ]\n",
      "Done with gradient descent at iteration  312\n",
      "Learned weights =  [ 2.11949337 91.48989161 78.4372119 ]\n",
      "Done with gradient descent at iteration  312\n",
      "Learned weights =  [ 2.11949337 91.48989071 78.4372119 ]\n",
      "Done with gradient descent at iteration  312\n",
      "Learned weights =  [ 2.11949337 91.48989071 78.43721099]\n",
      "Done with gradient descent at iteration  313\n",
      "Learned weights =  [ 2.12621357 91.48989071 78.43721099]\n",
      "Done with gradient descent at iteration  313\n",
      "Learned weights =  [ 2.12621357 91.48988982 78.43721099]\n",
      "Done with gradient descent at iteration  313\n",
      "Learned weights =  [ 2.12621357 91.48988982 78.43721009]\n",
      "Done with gradient descent at iteration  314\n",
      "Learned weights =  [ 2.13293377 91.48988982 78.43721009]\n",
      "Done with gradient descent at iteration  314\n",
      "Learned weights =  [ 2.13293377 91.48988892 78.43721009]\n",
      "Done with gradient descent at iteration  314\n",
      "Learned weights =  [ 2.13293377 91.48988892 78.43720918]\n",
      "Done with gradient descent at iteration  315\n",
      "Learned weights =  [ 2.13965397 91.48988892 78.43720918]\n",
      "Done with gradient descent at iteration  315\n",
      "Learned weights =  [ 2.13965397 91.48988802 78.43720918]\n",
      "Done with gradient descent at iteration  315\n",
      "Learned weights =  [ 2.13965397 91.48988802 78.43720827]\n",
      "Done with gradient descent at iteration  316\n",
      "Learned weights =  [ 2.14637417 91.48988802 78.43720827]\n",
      "Done with gradient descent at iteration  316\n",
      "Learned weights =  [ 2.14637417 91.48988713 78.43720827]\n",
      "Done with gradient descent at iteration  316\n",
      "Learned weights =  [ 2.14637417 91.48988713 78.43720737]\n",
      "Done with gradient descent at iteration  317\n",
      "Learned weights =  [ 2.15309437 91.48988713 78.43720737]\n",
      "Done with gradient descent at iteration  317\n",
      "Learned weights =  [ 2.15309437 91.48988623 78.43720737]\n",
      "Done with gradient descent at iteration  317\n",
      "Learned weights =  [ 2.15309437 91.48988623 78.43720646]\n",
      "Done with gradient descent at iteration  318\n",
      "Learned weights =  [ 2.15981457 91.48988623 78.43720646]\n",
      "Done with gradient descent at iteration  318\n",
      "Learned weights =  [ 2.15981457 91.48988533 78.43720646]\n",
      "Done with gradient descent at iteration  318\n",
      "Learned weights =  [ 2.15981457 91.48988533 78.43720556]\n",
      "Done with gradient descent at iteration  319\n",
      "Learned weights =  [ 2.16653477 91.48988533 78.43720556]\n",
      "Done with gradient descent at iteration  319\n",
      "Learned weights =  [ 2.16653477 91.48988443 78.43720556]\n",
      "Done with gradient descent at iteration  319\n",
      "Learned weights =  [ 2.16653477 91.48988443 78.43720465]\n",
      "Done with gradient descent at iteration  320\n",
      "Learned weights =  [ 2.17325497 91.48988443 78.43720465]\n",
      "Done with gradient descent at iteration  320\n",
      "Learned weights =  [ 2.17325497 91.48988354 78.43720465]\n",
      "Done with gradient descent at iteration  320\n",
      "Learned weights =  [ 2.17325497 91.48988354 78.43720374]\n",
      "Done with gradient descent at iteration  321\n",
      "Learned weights =  [ 2.17997517 91.48988354 78.43720374]\n",
      "Done with gradient descent at iteration  321\n",
      "Learned weights =  [ 2.17997517 91.48988264 78.43720374]\n",
      "Done with gradient descent at iteration  321\n",
      "Learned weights =  [ 2.17997517 91.48988264 78.43720284]\n",
      "Done with gradient descent at iteration  322\n",
      "Learned weights =  [ 2.18669537 91.48988264 78.43720284]\n",
      "Done with gradient descent at iteration  322\n",
      "Learned weights =  [ 2.18669537 91.48988174 78.43720284]\n",
      "Done with gradient descent at iteration  322\n",
      "Learned weights =  [ 2.18669537 91.48988174 78.43720193]\n",
      "Done with gradient descent at iteration  323\n",
      "Learned weights =  [ 2.19341556 91.48988174 78.43720193]\n",
      "Done with gradient descent at iteration  323\n",
      "Learned weights =  [ 2.19341556 91.48988085 78.43720193]\n",
      "Done with gradient descent at iteration  323\n",
      "Learned weights =  [ 2.19341556 91.48988085 78.43720103]\n",
      "Done with gradient descent at iteration  324\n",
      "Learned weights =  [ 2.20013576 91.48988085 78.43720103]\n",
      "Done with gradient descent at iteration  324\n",
      "Learned weights =  [ 2.20013576 91.48987995 78.43720103]\n",
      "Done with gradient descent at iteration  324\n",
      "Learned weights =  [ 2.20013576 91.48987995 78.43720012]\n",
      "Done with gradient descent at iteration  325\n",
      "Learned weights =  [ 2.20685596 91.48987995 78.43720012]\n",
      "Done with gradient descent at iteration  325\n",
      "Learned weights =  [ 2.20685596 91.48987905 78.43720012]\n",
      "Done with gradient descent at iteration  325\n",
      "Learned weights =  [ 2.20685596 91.48987905 78.43719921]\n",
      "Done with gradient descent at iteration  326\n",
      "Learned weights =  [ 2.21357616 91.48987905 78.43719921]\n",
      "Done with gradient descent at iteration  326\n",
      "Learned weights =  [ 2.21357616 91.48987816 78.43719921]\n",
      "Done with gradient descent at iteration  326\n",
      "Learned weights =  [ 2.21357616 91.48987816 78.43719831]\n",
      "Done with gradient descent at iteration  327\n",
      "Learned weights =  [ 2.22029636 91.48987816 78.43719831]\n",
      "Done with gradient descent at iteration  327\n",
      "Learned weights =  [ 2.22029636 91.48987726 78.43719831]\n",
      "Done with gradient descent at iteration  327\n",
      "Learned weights =  [ 2.22029636 91.48987726 78.4371974 ]\n",
      "Done with gradient descent at iteration  328\n",
      "Learned weights =  [ 2.22701656 91.48987726 78.4371974 ]\n",
      "Done with gradient descent at iteration  328\n",
      "Learned weights =  [ 2.22701656 91.48987636 78.4371974 ]\n",
      "Done with gradient descent at iteration  328\n",
      "Learned weights =  [ 2.22701656 91.48987636 78.4371965 ]\n",
      "Done with gradient descent at iteration  329\n",
      "Learned weights =  [ 2.23373676 91.48987636 78.4371965 ]\n",
      "Done with gradient descent at iteration  329\n",
      "Learned weights =  [ 2.23373676 91.48987547 78.4371965 ]\n",
      "Done with gradient descent at iteration  329\n",
      "Learned weights =  [ 2.23373676 91.48987547 78.43719559]\n",
      "Done with gradient descent at iteration  330\n",
      "Learned weights =  [ 2.24045695 91.48987547 78.43719559]\n",
      "Done with gradient descent at iteration  330\n",
      "Learned weights =  [ 2.24045695 91.48987457 78.43719559]\n",
      "Done with gradient descent at iteration  330\n",
      "Learned weights =  [ 2.24045695 91.48987457 78.43719468]\n",
      "Done with gradient descent at iteration  331\n",
      "Learned weights =  [ 2.24717715 91.48987457 78.43719468]\n",
      "Done with gradient descent at iteration  331\n",
      "Learned weights =  [ 2.24717715 91.48987367 78.43719468]\n",
      "Done with gradient descent at iteration  331\n",
      "Learned weights =  [ 2.24717715 91.48987367 78.43719378]\n",
      "Done with gradient descent at iteration  332\n",
      "Learned weights =  [ 2.25389735 91.48987367 78.43719378]\n",
      "Done with gradient descent at iteration  332\n",
      "Learned weights =  [ 2.25389735 91.48987277 78.43719378]\n",
      "Done with gradient descent at iteration  332\n",
      "Learned weights =  [ 2.25389735 91.48987277 78.43719287]\n",
      "Done with gradient descent at iteration  333\n",
      "Learned weights =  [ 2.26061755 91.48987277 78.43719287]\n",
      "Done with gradient descent at iteration  333\n",
      "Learned weights =  [ 2.26061755 91.48987188 78.43719287]\n",
      "Done with gradient descent at iteration  333\n",
      "Learned weights =  [ 2.26061755 91.48987188 78.43719197]\n",
      "Done with gradient descent at iteration  334\n",
      "Learned weights =  [ 2.26733774 91.48987188 78.43719197]\n",
      "Done with gradient descent at iteration  334\n",
      "Learned weights =  [ 2.26733774 91.48987098 78.43719197]\n",
      "Done with gradient descent at iteration  334\n",
      "Learned weights =  [ 2.26733774 91.48987098 78.43719106]\n",
      "Done with gradient descent at iteration  335\n",
      "Learned weights =  [ 2.27405794 91.48987098 78.43719106]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  335\n",
      "Learned weights =  [ 2.27405794 91.48987008 78.43719106]\n",
      "Done with gradient descent at iteration  335\n",
      "Learned weights =  [ 2.27405794 91.48987008 78.43719015]\n",
      "Done with gradient descent at iteration  336\n",
      "Learned weights =  [ 2.28077814 91.48987008 78.43719015]\n",
      "Done with gradient descent at iteration  336\n",
      "Learned weights =  [ 2.28077814 91.48986919 78.43719015]\n",
      "Done with gradient descent at iteration  336\n",
      "Learned weights =  [ 2.28077814 91.48986919 78.43718925]\n",
      "Done with gradient descent at iteration  337\n",
      "Learned weights =  [ 2.28749834 91.48986919 78.43718925]\n",
      "Done with gradient descent at iteration  337\n",
      "Learned weights =  [ 2.28749834 91.48986829 78.43718925]\n",
      "Done with gradient descent at iteration  337\n",
      "Learned weights =  [ 2.28749834 91.48986829 78.43718834]\n",
      "Done with gradient descent at iteration  338\n",
      "Learned weights =  [ 2.29421853 91.48986829 78.43718834]\n",
      "Done with gradient descent at iteration  338\n",
      "Learned weights =  [ 2.29421853 91.48986739 78.43718834]\n",
      "Done with gradient descent at iteration  338\n",
      "Learned weights =  [ 2.29421853 91.48986739 78.43718744]\n",
      "Done with gradient descent at iteration  339\n",
      "Learned weights =  [ 2.30093873 91.48986739 78.43718744]\n",
      "Done with gradient descent at iteration  339\n",
      "Learned weights =  [ 2.30093873 91.4898665  78.43718744]\n",
      "Done with gradient descent at iteration  339\n",
      "Learned weights =  [ 2.30093873 91.4898665  78.43718653]\n",
      "Done with gradient descent at iteration  340\n",
      "Learned weights =  [ 2.30765893 91.4898665  78.43718653]\n",
      "Done with gradient descent at iteration  340\n",
      "Learned weights =  [ 2.30765893 91.4898656  78.43718653]\n",
      "Done with gradient descent at iteration  340\n",
      "Learned weights =  [ 2.30765893 91.4898656  78.43718562]\n",
      "Done with gradient descent at iteration  341\n",
      "Learned weights =  [ 2.31437913 91.4898656  78.43718562]\n",
      "Done with gradient descent at iteration  341\n",
      "Learned weights =  [ 2.31437913 91.4898647  78.43718562]\n",
      "Done with gradient descent at iteration  341\n",
      "Learned weights =  [ 2.31437913 91.4898647  78.43718472]\n",
      "Done with gradient descent at iteration  342\n",
      "Learned weights =  [ 2.32109932 91.4898647  78.43718472]\n",
      "Done with gradient descent at iteration  342\n",
      "Learned weights =  [ 2.32109932 91.48986381 78.43718472]\n",
      "Done with gradient descent at iteration  342\n",
      "Learned weights =  [ 2.32109932 91.48986381 78.43718381]\n",
      "Done with gradient descent at iteration  343\n",
      "Learned weights =  [ 2.32781952 91.48986381 78.43718381]\n",
      "Done with gradient descent at iteration  343\n",
      "Learned weights =  [ 2.32781952 91.48986291 78.43718381]\n",
      "Done with gradient descent at iteration  343\n",
      "Learned weights =  [ 2.32781952 91.48986291 78.43718291]\n",
      "Done with gradient descent at iteration  344\n",
      "Learned weights =  [ 2.33453972 91.48986291 78.43718291]\n",
      "Done with gradient descent at iteration  344\n",
      "Learned weights =  [ 2.33453972 91.48986201 78.43718291]\n",
      "Done with gradient descent at iteration  344\n",
      "Learned weights =  [ 2.33453972 91.48986201 78.437182  ]\n",
      "Done with gradient descent at iteration  345\n",
      "Learned weights =  [ 2.34125991 91.48986201 78.437182  ]\n",
      "Done with gradient descent at iteration  345\n",
      "Learned weights =  [ 2.34125991 91.48986111 78.437182  ]\n",
      "Done with gradient descent at iteration  345\n",
      "Learned weights =  [ 2.34125991 91.48986111 78.43718109]\n",
      "Done with gradient descent at iteration  346\n",
      "Learned weights =  [ 2.34798011 91.48986111 78.43718109]\n",
      "Done with gradient descent at iteration  346\n",
      "Learned weights =  [ 2.34798011 91.48986022 78.43718109]\n",
      "Done with gradient descent at iteration  346\n",
      "Learned weights =  [ 2.34798011 91.48986022 78.43718019]\n",
      "Done with gradient descent at iteration  347\n",
      "Learned weights =  [ 2.3547003  91.48986022 78.43718019]\n",
      "Done with gradient descent at iteration  347\n",
      "Learned weights =  [ 2.3547003  91.48985932 78.43718019]\n",
      "Done with gradient descent at iteration  347\n",
      "Learned weights =  [ 2.3547003  91.48985932 78.43717928]\n",
      "Done with gradient descent at iteration  348\n",
      "Learned weights =  [ 2.3614205  91.48985932 78.43717928]\n",
      "Done with gradient descent at iteration  348\n",
      "Learned weights =  [ 2.3614205  91.48985842 78.43717928]\n",
      "Done with gradient descent at iteration  348\n",
      "Learned weights =  [ 2.3614205  91.48985842 78.43717838]\n",
      "Done with gradient descent at iteration  349\n",
      "Learned weights =  [ 2.3681407  91.48985842 78.43717838]\n",
      "Done with gradient descent at iteration  349\n",
      "Learned weights =  [ 2.3681407  91.48985753 78.43717838]\n",
      "Done with gradient descent at iteration  349\n",
      "Learned weights =  [ 2.3681407  91.48985753 78.43717747]\n",
      "Done with gradient descent at iteration  350\n",
      "Learned weights =  [ 2.37486089 91.48985753 78.43717747]\n",
      "Done with gradient descent at iteration  350\n",
      "Learned weights =  [ 2.37486089 91.48985663 78.43717747]\n",
      "Done with gradient descent at iteration  350\n",
      "Learned weights =  [ 2.37486089 91.48985663 78.43717656]\n",
      "Done with gradient descent at iteration  351\n",
      "Learned weights =  [ 2.38158109 91.48985663 78.43717656]\n",
      "Done with gradient descent at iteration  351\n",
      "Learned weights =  [ 2.38158109 91.48985573 78.43717656]\n",
      "Done with gradient descent at iteration  351\n",
      "Learned weights =  [ 2.38158109 91.48985573 78.43717566]\n",
      "Done with gradient descent at iteration  352\n",
      "Learned weights =  [ 2.38830128 91.48985573 78.43717566]\n",
      "Done with gradient descent at iteration  352\n",
      "Learned weights =  [ 2.38830128 91.48985484 78.43717566]\n",
      "Done with gradient descent at iteration  352\n",
      "Learned weights =  [ 2.38830128 91.48985484 78.43717475]\n",
      "Done with gradient descent at iteration  353\n",
      "Learned weights =  [ 2.39502148 91.48985484 78.43717475]\n",
      "Done with gradient descent at iteration  353\n",
      "Learned weights =  [ 2.39502148 91.48985394 78.43717475]\n",
      "Done with gradient descent at iteration  353\n",
      "Learned weights =  [ 2.39502148 91.48985394 78.43717385]\n",
      "Done with gradient descent at iteration  354\n",
      "Learned weights =  [ 2.40174168 91.48985394 78.43717385]\n",
      "Done with gradient descent at iteration  354\n",
      "Learned weights =  [ 2.40174168 91.48985304 78.43717385]\n",
      "Done with gradient descent at iteration  354\n",
      "Learned weights =  [ 2.40174168 91.48985304 78.43717294]\n",
      "Done with gradient descent at iteration  355\n",
      "Learned weights =  [ 2.40846187 91.48985304 78.43717294]\n",
      "Done with gradient descent at iteration  355\n",
      "Learned weights =  [ 2.40846187 91.48985214 78.43717294]\n",
      "Done with gradient descent at iteration  355\n",
      "Learned weights =  [ 2.40846187 91.48985214 78.43717203]\n",
      "Done with gradient descent at iteration  356\n",
      "Learned weights =  [ 2.41518207 91.48985214 78.43717203]\n",
      "Done with gradient descent at iteration  356\n",
      "Learned weights =  [ 2.41518207 91.48985125 78.43717203]\n",
      "Done with gradient descent at iteration  356\n",
      "Learned weights =  [ 2.41518207 91.48985125 78.43717113]\n",
      "Done with gradient descent at iteration  357\n",
      "Learned weights =  [ 2.42190226 91.48985125 78.43717113]\n",
      "Done with gradient descent at iteration  357\n",
      "Learned weights =  [ 2.42190226 91.48985035 78.43717113]\n",
      "Done with gradient descent at iteration  357\n",
      "Learned weights =  [ 2.42190226 91.48985035 78.43717022]\n",
      "Done with gradient descent at iteration  358\n",
      "Learned weights =  [ 2.42862246 91.48985035 78.43717022]\n",
      "Done with gradient descent at iteration  358\n",
      "Learned weights =  [ 2.42862246 91.48984945 78.43717022]\n",
      "Done with gradient descent at iteration  358\n",
      "Learned weights =  [ 2.42862246 91.48984945 78.43716932]\n",
      "Done with gradient descent at iteration  359\n",
      "Learned weights =  [ 2.43534265 91.48984945 78.43716932]\n",
      "Done with gradient descent at iteration  359\n",
      "Learned weights =  [ 2.43534265 91.48984856 78.43716932]\n",
      "Done with gradient descent at iteration  359\n",
      "Learned weights =  [ 2.43534265 91.48984856 78.43716841]\n",
      "Done with gradient descent at iteration  360\n",
      "Learned weights =  [ 2.44206285 91.48984856 78.43716841]\n",
      "Done with gradient descent at iteration  360\n",
      "Learned weights =  [ 2.44206285 91.48984766 78.43716841]\n",
      "Done with gradient descent at iteration  360\n",
      "Learned weights =  [ 2.44206285 91.48984766 78.4371675 ]\n",
      "Done with gradient descent at iteration  361\n",
      "Learned weights =  [ 2.44878304 91.48984766 78.4371675 ]\n",
      "Done with gradient descent at iteration  361\n",
      "Learned weights =  [ 2.44878304 91.48984676 78.4371675 ]\n",
      "Done with gradient descent at iteration  361\n",
      "Learned weights =  [ 2.44878304 91.48984676 78.4371666 ]\n",
      "Done with gradient descent at iteration  362\n",
      "Learned weights =  [ 2.45550324 91.48984676 78.4371666 ]\n",
      "Done with gradient descent at iteration  362\n",
      "Learned weights =  [ 2.45550324 91.48984587 78.4371666 ]\n",
      "Done with gradient descent at iteration  362\n",
      "Learned weights =  [ 2.45550324 91.48984587 78.43716569]\n",
      "Done with gradient descent at iteration  363\n",
      "Learned weights =  [ 2.46222343 91.48984587 78.43716569]\n",
      "Done with gradient descent at iteration  363\n",
      "Learned weights =  [ 2.46222343 91.48984497 78.43716569]\n",
      "Done with gradient descent at iteration  363\n",
      "Learned weights =  [ 2.46222343 91.48984497 78.43716479]\n",
      "Done with gradient descent at iteration  364\n",
      "Learned weights =  [ 2.46894363 91.48984497 78.43716479]\n",
      "Done with gradient descent at iteration  364\n",
      "Learned weights =  [ 2.46894363 91.48984407 78.43716479]\n",
      "Done with gradient descent at iteration  364\n",
      "Learned weights =  [ 2.46894363 91.48984407 78.43716388]\n",
      "Done with gradient descent at iteration  365\n",
      "Learned weights =  [ 2.47566382 91.48984407 78.43716388]\n",
      "Done with gradient descent at iteration  365\n",
      "Learned weights =  [ 2.47566382 91.48984318 78.43716388]\n",
      "Done with gradient descent at iteration  365\n",
      "Learned weights =  [ 2.47566382 91.48984318 78.43716297]\n",
      "Done with gradient descent at iteration  366\n",
      "Learned weights =  [ 2.48238401 91.48984318 78.43716297]\n",
      "Done with gradient descent at iteration  366\n",
      "Learned weights =  [ 2.48238401 91.48984228 78.43716297]\n",
      "Done with gradient descent at iteration  366\n",
      "Learned weights =  [ 2.48238401 91.48984228 78.43716207]\n",
      "Done with gradient descent at iteration  367\n",
      "Learned weights =  [ 2.48910421 91.48984228 78.43716207]\n",
      "Done with gradient descent at iteration  367\n",
      "Learned weights =  [ 2.48910421 91.48984138 78.43716207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  367\n",
      "Learned weights =  [ 2.48910421 91.48984138 78.43716116]\n",
      "Done with gradient descent at iteration  368\n",
      "Learned weights =  [ 2.4958244  91.48984138 78.43716116]\n",
      "Done with gradient descent at iteration  368\n",
      "Learned weights =  [ 2.4958244  91.48984048 78.43716116]\n",
      "Done with gradient descent at iteration  368\n",
      "Learned weights =  [ 2.4958244  91.48984048 78.43716026]\n",
      "Done with gradient descent at iteration  369\n",
      "Learned weights =  [ 2.5025446  91.48984048 78.43716026]\n",
      "Done with gradient descent at iteration  369\n",
      "Learned weights =  [ 2.5025446  91.48983959 78.43716026]\n",
      "Done with gradient descent at iteration  369\n",
      "Learned weights =  [ 2.5025446  91.48983959 78.43715935]\n",
      "Done with gradient descent at iteration  370\n",
      "Learned weights =  [ 2.50926479 91.48983959 78.43715935]\n",
      "Done with gradient descent at iteration  370\n",
      "Learned weights =  [ 2.50926479 91.48983869 78.43715935]\n",
      "Done with gradient descent at iteration  370\n",
      "Learned weights =  [ 2.50926479 91.48983869 78.43715844]\n",
      "Done with gradient descent at iteration  371\n",
      "Learned weights =  [ 2.51598498 91.48983869 78.43715844]\n",
      "Done with gradient descent at iteration  371\n",
      "Learned weights =  [ 2.51598498 91.48983779 78.43715844]\n",
      "Done with gradient descent at iteration  371\n",
      "Learned weights =  [ 2.51598498 91.48983779 78.43715754]\n",
      "Done with gradient descent at iteration  372\n",
      "Learned weights =  [ 2.52270518 91.48983779 78.43715754]\n",
      "Done with gradient descent at iteration  372\n",
      "Learned weights =  [ 2.52270518 91.4898369  78.43715754]\n",
      "Done with gradient descent at iteration  372\n",
      "Learned weights =  [ 2.52270518 91.4898369  78.43715663]\n",
      "Done with gradient descent at iteration  373\n",
      "Learned weights =  [ 2.52942537 91.4898369  78.43715663]\n",
      "Done with gradient descent at iteration  373\n",
      "Learned weights =  [ 2.52942537 91.489836   78.43715663]\n",
      "Done with gradient descent at iteration  373\n",
      "Learned weights =  [ 2.52942537 91.489836   78.43715573]\n",
      "Done with gradient descent at iteration  374\n",
      "Learned weights =  [ 2.53614556 91.489836   78.43715573]\n",
      "Done with gradient descent at iteration  374\n",
      "Learned weights =  [ 2.53614556 91.4898351  78.43715573]\n",
      "Done with gradient descent at iteration  374\n",
      "Learned weights =  [ 2.53614556 91.4898351  78.43715482]\n",
      "Done with gradient descent at iteration  375\n",
      "Learned weights =  [ 2.54286576 91.4898351  78.43715482]\n",
      "Done with gradient descent at iteration  375\n",
      "Learned weights =  [ 2.54286576 91.48983421 78.43715482]\n",
      "Done with gradient descent at iteration  375\n",
      "Learned weights =  [ 2.54286576 91.48983421 78.43715391]\n",
      "Done with gradient descent at iteration  376\n",
      "Learned weights =  [ 2.54958595 91.48983421 78.43715391]\n",
      "Done with gradient descent at iteration  376\n",
      "Learned weights =  [ 2.54958595 91.48983331 78.43715391]\n",
      "Done with gradient descent at iteration  376\n",
      "Learned weights =  [ 2.54958595 91.48983331 78.43715301]\n",
      "Done with gradient descent at iteration  377\n",
      "Learned weights =  [ 2.55630614 91.48983331 78.43715301]\n",
      "Done with gradient descent at iteration  377\n",
      "Learned weights =  [ 2.55630614 91.48983241 78.43715301]\n",
      "Done with gradient descent at iteration  377\n",
      "Learned weights =  [ 2.55630614 91.48983241 78.4371521 ]\n",
      "Done with gradient descent at iteration  378\n",
      "Learned weights =  [ 2.56302634 91.48983241 78.4371521 ]\n",
      "Done with gradient descent at iteration  378\n",
      "Learned weights =  [ 2.56302634 91.48983151 78.4371521 ]\n",
      "Done with gradient descent at iteration  378\n",
      "Learned weights =  [ 2.56302634 91.48983151 78.4371512 ]\n",
      "Done with gradient descent at iteration  379\n",
      "Learned weights =  [ 2.56974653 91.48983151 78.4371512 ]\n",
      "Done with gradient descent at iteration  379\n",
      "Learned weights =  [ 2.56974653 91.48983062 78.4371512 ]\n",
      "Done with gradient descent at iteration  379\n",
      "Learned weights =  [ 2.56974653 91.48983062 78.43715029]\n",
      "Done with gradient descent at iteration  380\n",
      "Learned weights =  [ 2.57646672 91.48983062 78.43715029]\n",
      "Done with gradient descent at iteration  380\n",
      "Learned weights =  [ 2.57646672 91.48982972 78.43715029]\n",
      "Done with gradient descent at iteration  380\n",
      "Learned weights =  [ 2.57646672 91.48982972 78.43714938]\n",
      "Done with gradient descent at iteration  381\n",
      "Learned weights =  [ 2.58318692 91.48982972 78.43714938]\n",
      "Done with gradient descent at iteration  381\n",
      "Learned weights =  [ 2.58318692 91.48982882 78.43714938]\n",
      "Done with gradient descent at iteration  381\n",
      "Learned weights =  [ 2.58318692 91.48982882 78.43714848]\n",
      "Done with gradient descent at iteration  382\n",
      "Learned weights =  [ 2.58990711 91.48982882 78.43714848]\n",
      "Done with gradient descent at iteration  382\n",
      "Learned weights =  [ 2.58990711 91.48982793 78.43714848]\n",
      "Done with gradient descent at iteration  382\n",
      "Learned weights =  [ 2.58990711 91.48982793 78.43714757]\n",
      "Done with gradient descent at iteration  383\n",
      "Learned weights =  [ 2.5966273  91.48982793 78.43714757]\n",
      "Done with gradient descent at iteration  383\n",
      "Learned weights =  [ 2.5966273  91.48982703 78.43714757]\n",
      "Done with gradient descent at iteration  383\n",
      "Learned weights =  [ 2.5966273  91.48982703 78.43714667]\n",
      "Done with gradient descent at iteration  384\n",
      "Learned weights =  [ 2.60334749 91.48982703 78.43714667]\n",
      "Done with gradient descent at iteration  384\n",
      "Learned weights =  [ 2.60334749 91.48982613 78.43714667]\n",
      "Done with gradient descent at iteration  384\n",
      "Learned weights =  [ 2.60334749 91.48982613 78.43714576]\n",
      "Done with gradient descent at iteration  385\n",
      "Learned weights =  [ 2.61006769 91.48982613 78.43714576]\n",
      "Done with gradient descent at iteration  385\n",
      "Learned weights =  [ 2.61006769 91.48982524 78.43714576]\n",
      "Done with gradient descent at iteration  385\n",
      "Learned weights =  [ 2.61006769 91.48982524 78.43714486]\n",
      "Done with gradient descent at iteration  386\n",
      "Learned weights =  [ 2.61678788 91.48982524 78.43714486]\n",
      "Done with gradient descent at iteration  386\n",
      "Learned weights =  [ 2.61678788 91.48982434 78.43714486]\n",
      "Done with gradient descent at iteration  386\n",
      "Learned weights =  [ 2.61678788 91.48982434 78.43714395]\n",
      "Done with gradient descent at iteration  387\n",
      "Learned weights =  [ 2.62350807 91.48982434 78.43714395]\n",
      "Done with gradient descent at iteration  387\n",
      "Learned weights =  [ 2.62350807 91.48982344 78.43714395]\n",
      "Done with gradient descent at iteration  387\n",
      "Learned weights =  [ 2.62350807 91.48982344 78.43714304]\n",
      "Done with gradient descent at iteration  388\n",
      "Learned weights =  [ 2.63022826 91.48982344 78.43714304]\n",
      "Done with gradient descent at iteration  388\n",
      "Learned weights =  [ 2.63022826 91.48982255 78.43714304]\n",
      "Done with gradient descent at iteration  388\n",
      "Learned weights =  [ 2.63022826 91.48982255 78.43714214]\n",
      "Done with gradient descent at iteration  389\n",
      "Learned weights =  [ 2.63694845 91.48982255 78.43714214]\n",
      "Done with gradient descent at iteration  389\n",
      "Learned weights =  [ 2.63694845 91.48982165 78.43714214]\n",
      "Done with gradient descent at iteration  389\n",
      "Learned weights =  [ 2.63694845 91.48982165 78.43714123]\n",
      "Done with gradient descent at iteration  390\n",
      "Learned weights =  [ 2.64366864 91.48982165 78.43714123]\n",
      "Done with gradient descent at iteration  390\n",
      "Learned weights =  [ 2.64366864 91.48982075 78.43714123]\n",
      "Done with gradient descent at iteration  390\n",
      "Learned weights =  [ 2.64366864 91.48982075 78.43714033]\n",
      "Done with gradient descent at iteration  391\n",
      "Learned weights =  [ 2.65038884 91.48982075 78.43714033]\n",
      "Done with gradient descent at iteration  391\n",
      "Learned weights =  [ 2.65038884 91.48981985 78.43714033]\n",
      "Done with gradient descent at iteration  391\n",
      "Learned weights =  [ 2.65038884 91.48981985 78.43713942]\n",
      "Done with gradient descent at iteration  392\n",
      "Learned weights =  [ 2.65710903 91.48981985 78.43713942]\n",
      "Done with gradient descent at iteration  392\n",
      "Learned weights =  [ 2.65710903 91.48981896 78.43713942]\n",
      "Done with gradient descent at iteration  392\n",
      "Learned weights =  [ 2.65710903 91.48981896 78.43713851]\n",
      "Done with gradient descent at iteration  393\n",
      "Learned weights =  [ 2.66382922 91.48981896 78.43713851]\n",
      "Done with gradient descent at iteration  393\n",
      "Learned weights =  [ 2.66382922 91.48981806 78.43713851]\n",
      "Done with gradient descent at iteration  393\n",
      "Learned weights =  [ 2.66382922 91.48981806 78.43713761]\n",
      "Done with gradient descent at iteration  394\n",
      "Learned weights =  [ 2.67054941 91.48981806 78.43713761]\n",
      "Done with gradient descent at iteration  394\n",
      "Learned weights =  [ 2.67054941 91.48981716 78.43713761]\n",
      "Done with gradient descent at iteration  394\n",
      "Learned weights =  [ 2.67054941 91.48981716 78.4371367 ]\n",
      "Done with gradient descent at iteration  395\n",
      "Learned weights =  [ 2.6772696  91.48981716 78.4371367 ]\n",
      "Done with gradient descent at iteration  395\n",
      "Learned weights =  [ 2.6772696  91.48981627 78.4371367 ]\n",
      "Done with gradient descent at iteration  395\n",
      "Learned weights =  [ 2.6772696  91.48981627 78.4371358 ]\n",
      "Done with gradient descent at iteration  396\n",
      "Learned weights =  [ 2.68398979 91.48981627 78.4371358 ]\n",
      "Done with gradient descent at iteration  396\n",
      "Learned weights =  [ 2.68398979 91.48981537 78.4371358 ]\n",
      "Done with gradient descent at iteration  396\n",
      "Learned weights =  [ 2.68398979 91.48981537 78.43713489]\n",
      "Done with gradient descent at iteration  397\n",
      "Learned weights =  [ 2.69070998 91.48981537 78.43713489]\n",
      "Done with gradient descent at iteration  397\n",
      "Learned weights =  [ 2.69070998 91.48981447 78.43713489]\n",
      "Done with gradient descent at iteration  397\n",
      "Learned weights =  [ 2.69070998 91.48981447 78.43713398]\n",
      "Done with gradient descent at iteration  398\n",
      "Learned weights =  [ 2.69743017 91.48981447 78.43713398]\n",
      "Done with gradient descent at iteration  398\n",
      "Learned weights =  [ 2.69743017 91.48981358 78.43713398]\n",
      "Done with gradient descent at iteration  398\n",
      "Learned weights =  [ 2.69743017 91.48981358 78.43713308]\n",
      "Done with gradient descent at iteration  399\n",
      "Learned weights =  [ 2.70415037 91.48981358 78.43713308]\n",
      "Done with gradient descent at iteration  399\n",
      "Learned weights =  [ 2.70415037 91.48981268 78.43713308]\n",
      "Done with gradient descent at iteration  399\n",
      "Learned weights =  [ 2.70415037 91.48981268 78.43713217]\n",
      "Iteration = 400\n",
      "Cost function =  3605276824681046.5\n",
      "Done with gradient descent at iteration  400\n",
      "Learned weights =  [ 2.71087056 91.48981268 78.43713217]\n",
      "Done with gradient descent at iteration  400\n",
      "Learned weights =  [ 2.71087056 91.48981178 78.43713217]\n",
      "Done with gradient descent at iteration  400\n",
      "Learned weights =  [ 2.71087056 91.48981178 78.43713127]\n",
      "Done with gradient descent at iteration  401\n",
      "Learned weights =  [ 2.71759075 91.48981178 78.43713127]\n",
      "Done with gradient descent at iteration  401\n",
      "Learned weights =  [ 2.71759075 91.48981088 78.43713127]\n",
      "Done with gradient descent at iteration  401\n",
      "Learned weights =  [ 2.71759075 91.48981088 78.43713036]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  402\n",
      "Learned weights =  [ 2.72431094 91.48981088 78.43713036]\n",
      "Done with gradient descent at iteration  402\n",
      "Learned weights =  [ 2.72431094 91.48980999 78.43713036]\n",
      "Done with gradient descent at iteration  402\n",
      "Learned weights =  [ 2.72431094 91.48980999 78.43712945]\n",
      "Done with gradient descent at iteration  403\n",
      "Learned weights =  [ 2.73103113 91.48980999 78.43712945]\n",
      "Done with gradient descent at iteration  403\n",
      "Learned weights =  [ 2.73103113 91.48980909 78.43712945]\n",
      "Done with gradient descent at iteration  403\n",
      "Learned weights =  [ 2.73103113 91.48980909 78.43712855]\n",
      "Done with gradient descent at iteration  404\n",
      "Learned weights =  [ 2.73775132 91.48980909 78.43712855]\n",
      "Done with gradient descent at iteration  404\n",
      "Learned weights =  [ 2.73775132 91.48980819 78.43712855]\n",
      "Done with gradient descent at iteration  404\n",
      "Learned weights =  [ 2.73775132 91.48980819 78.43712764]\n",
      "Done with gradient descent at iteration  405\n",
      "Learned weights =  [ 2.74447151 91.48980819 78.43712764]\n",
      "Done with gradient descent at iteration  405\n",
      "Learned weights =  [ 2.74447151 91.4898073  78.43712764]\n",
      "Done with gradient descent at iteration  405\n",
      "Learned weights =  [ 2.74447151 91.4898073  78.43712674]\n",
      "Done with gradient descent at iteration  406\n",
      "Learned weights =  [ 2.7511917  91.4898073  78.43712674]\n",
      "Done with gradient descent at iteration  406\n",
      "Learned weights =  [ 2.7511917  91.4898064  78.43712674]\n",
      "Done with gradient descent at iteration  406\n",
      "Learned weights =  [ 2.7511917  91.4898064  78.43712583]\n",
      "Done with gradient descent at iteration  407\n",
      "Learned weights =  [ 2.75791189 91.4898064  78.43712583]\n",
      "Done with gradient descent at iteration  407\n",
      "Learned weights =  [ 2.75791189 91.4898055  78.43712583]\n",
      "Done with gradient descent at iteration  407\n",
      "Learned weights =  [ 2.75791189 91.4898055  78.43712492]\n",
      "Done with gradient descent at iteration  408\n",
      "Learned weights =  [ 2.76463208 91.4898055  78.43712492]\n",
      "Done with gradient descent at iteration  408\n",
      "Learned weights =  [ 2.76463208 91.48980461 78.43712492]\n",
      "Done with gradient descent at iteration  408\n",
      "Learned weights =  [ 2.76463208 91.48980461 78.43712402]\n",
      "Done with gradient descent at iteration  409\n",
      "Learned weights =  [ 2.77135227 91.48980461 78.43712402]\n",
      "Done with gradient descent at iteration  409\n",
      "Learned weights =  [ 2.77135227 91.48980371 78.43712402]\n",
      "Done with gradient descent at iteration  409\n",
      "Learned weights =  [ 2.77135227 91.48980371 78.43712311]\n",
      "Done with gradient descent at iteration  410\n",
      "Learned weights =  [ 2.77807246 91.48980371 78.43712311]\n",
      "Done with gradient descent at iteration  410\n",
      "Learned weights =  [ 2.77807246 91.48980281 78.43712311]\n",
      "Done with gradient descent at iteration  410\n",
      "Learned weights =  [ 2.77807246 91.48980281 78.43712221]\n",
      "Done with gradient descent at iteration  411\n",
      "Learned weights =  [ 2.78479265 91.48980281 78.43712221]\n",
      "Done with gradient descent at iteration  411\n",
      "Learned weights =  [ 2.78479265 91.48980192 78.43712221]\n",
      "Done with gradient descent at iteration  411\n",
      "Learned weights =  [ 2.78479265 91.48980192 78.4371213 ]\n",
      "Done with gradient descent at iteration  412\n",
      "Learned weights =  [ 2.79151284 91.48980192 78.4371213 ]\n",
      "Done with gradient descent at iteration  412\n",
      "Learned weights =  [ 2.79151284 91.48980102 78.4371213 ]\n",
      "Done with gradient descent at iteration  412\n",
      "Learned weights =  [ 2.79151284 91.48980102 78.43712039]\n",
      "Done with gradient descent at iteration  413\n",
      "Learned weights =  [ 2.79823303 91.48980102 78.43712039]\n",
      "Done with gradient descent at iteration  413\n",
      "Learned weights =  [ 2.79823303 91.48980012 78.43712039]\n",
      "Done with gradient descent at iteration  413\n",
      "Learned weights =  [ 2.79823303 91.48980012 78.43711949]\n",
      "Done with gradient descent at iteration  414\n",
      "Learned weights =  [ 2.80495321 91.48980012 78.43711949]\n",
      "Done with gradient descent at iteration  414\n",
      "Learned weights =  [ 2.80495321 91.48979922 78.43711949]\n",
      "Done with gradient descent at iteration  414\n",
      "Learned weights =  [ 2.80495321 91.48979922 78.43711858]\n",
      "Done with gradient descent at iteration  415\n",
      "Learned weights =  [ 2.8116734  91.48979922 78.43711858]\n",
      "Done with gradient descent at iteration  415\n",
      "Learned weights =  [ 2.8116734  91.48979833 78.43711858]\n",
      "Done with gradient descent at iteration  415\n",
      "Learned weights =  [ 2.8116734  91.48979833 78.43711768]\n",
      "Done with gradient descent at iteration  416\n",
      "Learned weights =  [ 2.81839359 91.48979833 78.43711768]\n",
      "Done with gradient descent at iteration  416\n",
      "Learned weights =  [ 2.81839359 91.48979743 78.43711768]\n",
      "Done with gradient descent at iteration  416\n",
      "Learned weights =  [ 2.81839359 91.48979743 78.43711677]\n",
      "Done with gradient descent at iteration  417\n",
      "Learned weights =  [ 2.82511378 91.48979743 78.43711677]\n",
      "Done with gradient descent at iteration  417\n",
      "Learned weights =  [ 2.82511378 91.48979653 78.43711677]\n",
      "Done with gradient descent at iteration  417\n",
      "Learned weights =  [ 2.82511378 91.48979653 78.43711586]\n",
      "Done with gradient descent at iteration  418\n",
      "Learned weights =  [ 2.83183397 91.48979653 78.43711586]\n",
      "Done with gradient descent at iteration  418\n",
      "Learned weights =  [ 2.83183397 91.48979564 78.43711586]\n",
      "Done with gradient descent at iteration  418\n",
      "Learned weights =  [ 2.83183397 91.48979564 78.43711496]\n",
      "Done with gradient descent at iteration  419\n",
      "Learned weights =  [ 2.83855416 91.48979564 78.43711496]\n",
      "Done with gradient descent at iteration  419\n",
      "Learned weights =  [ 2.83855416 91.48979474 78.43711496]\n",
      "Done with gradient descent at iteration  419\n",
      "Learned weights =  [ 2.83855416 91.48979474 78.43711405]\n",
      "Done with gradient descent at iteration  420\n",
      "Learned weights =  [ 2.84527435 91.48979474 78.43711405]\n",
      "Done with gradient descent at iteration  420\n",
      "Learned weights =  [ 2.84527435 91.48979384 78.43711405]\n",
      "Done with gradient descent at iteration  420\n",
      "Learned weights =  [ 2.84527435 91.48979384 78.43711315]\n",
      "Done with gradient descent at iteration  421\n",
      "Learned weights =  [ 2.85199454 91.48979384 78.43711315]\n",
      "Done with gradient descent at iteration  421\n",
      "Learned weights =  [ 2.85199454 91.48979295 78.43711315]\n",
      "Done with gradient descent at iteration  421\n",
      "Learned weights =  [ 2.85199454 91.48979295 78.43711224]\n",
      "Done with gradient descent at iteration  422\n",
      "Learned weights =  [ 2.85871472 91.48979295 78.43711224]\n",
      "Done with gradient descent at iteration  422\n",
      "Learned weights =  [ 2.85871472 91.48979205 78.43711224]\n",
      "Done with gradient descent at iteration  422\n",
      "Learned weights =  [ 2.85871472 91.48979205 78.43711133]\n",
      "Done with gradient descent at iteration  423\n",
      "Learned weights =  [ 2.86543491 91.48979205 78.43711133]\n",
      "Done with gradient descent at iteration  423\n",
      "Learned weights =  [ 2.86543491 91.48979115 78.43711133]\n",
      "Done with gradient descent at iteration  423\n",
      "Learned weights =  [ 2.86543491 91.48979115 78.43711043]\n",
      "Done with gradient descent at iteration  424\n",
      "Learned weights =  [ 2.8721551  91.48979115 78.43711043]\n",
      "Done with gradient descent at iteration  424\n",
      "Learned weights =  [ 2.8721551  91.48979025 78.43711043]\n",
      "Done with gradient descent at iteration  424\n",
      "Learned weights =  [ 2.8721551  91.48979025 78.43710952]\n",
      "Done with gradient descent at iteration  425\n",
      "Learned weights =  [ 2.87887529 91.48979025 78.43710952]\n",
      "Done with gradient descent at iteration  425\n",
      "Learned weights =  [ 2.87887529 91.48978936 78.43710952]\n",
      "Done with gradient descent at iteration  425\n",
      "Learned weights =  [ 2.87887529 91.48978936 78.43710862]\n",
      "Done with gradient descent at iteration  426\n",
      "Learned weights =  [ 2.88559548 91.48978936 78.43710862]\n",
      "Done with gradient descent at iteration  426\n",
      "Learned weights =  [ 2.88559548 91.48978846 78.43710862]\n",
      "Done with gradient descent at iteration  426\n",
      "Learned weights =  [ 2.88559548 91.48978846 78.43710771]\n",
      "Done with gradient descent at iteration  427\n",
      "Learned weights =  [ 2.89231566 91.48978846 78.43710771]\n",
      "Done with gradient descent at iteration  427\n",
      "Learned weights =  [ 2.89231566 91.48978756 78.43710771]\n",
      "Done with gradient descent at iteration  427\n",
      "Learned weights =  [ 2.89231566 91.48978756 78.4371068 ]\n",
      "Done with gradient descent at iteration  428\n",
      "Learned weights =  [ 2.89903585 91.48978756 78.4371068 ]\n",
      "Done with gradient descent at iteration  428\n",
      "Learned weights =  [ 2.89903585 91.48978667 78.4371068 ]\n",
      "Done with gradient descent at iteration  428\n",
      "Learned weights =  [ 2.89903585 91.48978667 78.4371059 ]\n",
      "Done with gradient descent at iteration  429\n",
      "Learned weights =  [ 2.90575604 91.48978667 78.4371059 ]\n",
      "Done with gradient descent at iteration  429\n",
      "Learned weights =  [ 2.90575604 91.48978577 78.4371059 ]\n",
      "Done with gradient descent at iteration  429\n",
      "Learned weights =  [ 2.90575604 91.48978577 78.43710499]\n",
      "Done with gradient descent at iteration  430\n",
      "Learned weights =  [ 2.91247623 91.48978577 78.43710499]\n",
      "Done with gradient descent at iteration  430\n",
      "Learned weights =  [ 2.91247623 91.48978487 78.43710499]\n",
      "Done with gradient descent at iteration  430\n",
      "Learned weights =  [ 2.91247623 91.48978487 78.43710409]\n",
      "Done with gradient descent at iteration  431\n",
      "Learned weights =  [ 2.91919641 91.48978487 78.43710409]\n",
      "Done with gradient descent at iteration  431\n",
      "Learned weights =  [ 2.91919641 91.48978398 78.43710409]\n",
      "Done with gradient descent at iteration  431\n",
      "Learned weights =  [ 2.91919641 91.48978398 78.43710318]\n",
      "Done with gradient descent at iteration  432\n",
      "Learned weights =  [ 2.9259166  91.48978398 78.43710318]\n",
      "Done with gradient descent at iteration  432\n",
      "Learned weights =  [ 2.9259166  91.48978308 78.43710318]\n",
      "Done with gradient descent at iteration  432\n",
      "Learned weights =  [ 2.9259166  91.48978308 78.43710227]\n",
      "Done with gradient descent at iteration  433\n",
      "Learned weights =  [ 2.93263679 91.48978308 78.43710227]\n",
      "Done with gradient descent at iteration  433\n",
      "Learned weights =  [ 2.93263679 91.48978218 78.43710227]\n",
      "Done with gradient descent at iteration  433\n",
      "Learned weights =  [ 2.93263679 91.48978218 78.43710137]\n",
      "Done with gradient descent at iteration  434\n",
      "Learned weights =  [ 2.93935698 91.48978218 78.43710137]\n",
      "Done with gradient descent at iteration  434\n",
      "Learned weights =  [ 2.93935698 91.48978129 78.43710137]\n",
      "Done with gradient descent at iteration  434\n",
      "Learned weights =  [ 2.93935698 91.48978129 78.43710046]\n",
      "Done with gradient descent at iteration  435\n",
      "Learned weights =  [ 2.94607716 91.48978129 78.43710046]\n",
      "Done with gradient descent at iteration  435\n",
      "Learned weights =  [ 2.94607716 91.48978039 78.43710046]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  435\n",
      "Learned weights =  [ 2.94607716 91.48978039 78.43709956]\n",
      "Done with gradient descent at iteration  436\n",
      "Learned weights =  [ 2.95279735 91.48978039 78.43709956]\n",
      "Done with gradient descent at iteration  436\n",
      "Learned weights =  [ 2.95279735 91.48977949 78.43709956]\n",
      "Done with gradient descent at iteration  436\n",
      "Learned weights =  [ 2.95279735 91.48977949 78.43709865]\n",
      "Done with gradient descent at iteration  437\n",
      "Learned weights =  [ 2.95951754 91.48977949 78.43709865]\n",
      "Done with gradient descent at iteration  437\n",
      "Learned weights =  [ 2.95951754 91.48977859 78.43709865]\n",
      "Done with gradient descent at iteration  437\n",
      "Learned weights =  [ 2.95951754 91.48977859 78.43709774]\n",
      "Done with gradient descent at iteration  438\n",
      "Learned weights =  [ 2.96623772 91.48977859 78.43709774]\n",
      "Done with gradient descent at iteration  438\n",
      "Learned weights =  [ 2.96623772 91.4897777  78.43709774]\n",
      "Done with gradient descent at iteration  438\n",
      "Learned weights =  [ 2.96623772 91.4897777  78.43709684]\n",
      "Done with gradient descent at iteration  439\n",
      "Learned weights =  [ 2.97295791 91.4897777  78.43709684]\n",
      "Done with gradient descent at iteration  439\n",
      "Learned weights =  [ 2.97295791 91.4897768  78.43709684]\n",
      "Done with gradient descent at iteration  439\n",
      "Learned weights =  [ 2.97295791 91.4897768  78.43709593]\n",
      "Done with gradient descent at iteration  440\n",
      "Learned weights =  [ 2.9796781  91.4897768  78.43709593]\n",
      "Done with gradient descent at iteration  440\n",
      "Learned weights =  [ 2.9796781  91.4897759  78.43709593]\n",
      "Done with gradient descent at iteration  440\n",
      "Learned weights =  [ 2.9796781  91.4897759  78.43709503]\n",
      "Done with gradient descent at iteration  441\n",
      "Learned weights =  [ 2.98639828 91.4897759  78.43709503]\n",
      "Done with gradient descent at iteration  441\n",
      "Learned weights =  [ 2.98639828 91.48977501 78.43709503]\n",
      "Done with gradient descent at iteration  441\n",
      "Learned weights =  [ 2.98639828 91.48977501 78.43709412]\n",
      "Done with gradient descent at iteration  442\n",
      "Learned weights =  [ 2.99311847 91.48977501 78.43709412]\n",
      "Done with gradient descent at iteration  442\n",
      "Learned weights =  [ 2.99311847 91.48977411 78.43709412]\n",
      "Done with gradient descent at iteration  442\n",
      "Learned weights =  [ 2.99311847 91.48977411 78.43709321]\n",
      "Done with gradient descent at iteration  443\n",
      "Learned weights =  [ 2.99983865 91.48977411 78.43709321]\n",
      "Done with gradient descent at iteration  443\n",
      "Learned weights =  [ 2.99983865 91.48977321 78.43709321]\n",
      "Done with gradient descent at iteration  443\n",
      "Learned weights =  [ 2.99983865 91.48977321 78.43709231]\n",
      "Done with gradient descent at iteration  444\n",
      "Learned weights =  [ 3.00655884 91.48977321 78.43709231]\n",
      "Done with gradient descent at iteration  444\n",
      "Learned weights =  [ 3.00655884 91.48977232 78.43709231]\n",
      "Done with gradient descent at iteration  444\n",
      "Learned weights =  [ 3.00655884 91.48977232 78.4370914 ]\n",
      "Done with gradient descent at iteration  445\n",
      "Learned weights =  [ 3.01327903 91.48977232 78.4370914 ]\n",
      "Done with gradient descent at iteration  445\n",
      "Learned weights =  [ 3.01327903 91.48977142 78.4370914 ]\n",
      "Done with gradient descent at iteration  445\n",
      "Learned weights =  [ 3.01327903 91.48977142 78.4370905 ]\n",
      "Done with gradient descent at iteration  446\n",
      "Learned weights =  [ 3.01999921 91.48977142 78.4370905 ]\n",
      "Done with gradient descent at iteration  446\n",
      "Learned weights =  [ 3.01999921 91.48977052 78.4370905 ]\n",
      "Done with gradient descent at iteration  446\n",
      "Learned weights =  [ 3.01999921 91.48977052 78.43708959]\n",
      "Done with gradient descent at iteration  447\n",
      "Learned weights =  [ 3.0267194  91.48977052 78.43708959]\n",
      "Done with gradient descent at iteration  447\n",
      "Learned weights =  [ 3.0267194  91.48976962 78.43708959]\n",
      "Done with gradient descent at iteration  447\n",
      "Learned weights =  [ 3.0267194  91.48976962 78.43708868]\n",
      "Done with gradient descent at iteration  448\n",
      "Learned weights =  [ 3.03343958 91.48976962 78.43708868]\n",
      "Done with gradient descent at iteration  448\n",
      "Learned weights =  [ 3.03343958 91.48976873 78.43708868]\n",
      "Done with gradient descent at iteration  448\n",
      "Learned weights =  [ 3.03343958 91.48976873 78.43708778]\n",
      "Done with gradient descent at iteration  449\n",
      "Learned weights =  [ 3.04015977 91.48976873 78.43708778]\n",
      "Done with gradient descent at iteration  449\n",
      "Learned weights =  [ 3.04015977 91.48976783 78.43708778]\n",
      "Done with gradient descent at iteration  449\n",
      "Learned weights =  [ 3.04015977 91.48976783 78.43708687]\n",
      "Done with gradient descent at iteration  450\n",
      "Learned weights =  [ 3.04687995 91.48976783 78.43708687]\n",
      "Done with gradient descent at iteration  450\n",
      "Learned weights =  [ 3.04687995 91.48976693 78.43708687]\n",
      "Done with gradient descent at iteration  450\n",
      "Learned weights =  [ 3.04687995 91.48976693 78.43708597]\n",
      "Done with gradient descent at iteration  451\n",
      "Learned weights =  [ 3.05360014 91.48976693 78.43708597]\n",
      "Done with gradient descent at iteration  451\n",
      "Learned weights =  [ 3.05360014 91.48976604 78.43708597]\n",
      "Done with gradient descent at iteration  451\n",
      "Learned weights =  [ 3.05360014 91.48976604 78.43708506]\n",
      "Done with gradient descent at iteration  452\n",
      "Learned weights =  [ 3.06032032 91.48976604 78.43708506]\n",
      "Done with gradient descent at iteration  452\n",
      "Learned weights =  [ 3.06032032 91.48976514 78.43708506]\n",
      "Done with gradient descent at iteration  452\n",
      "Learned weights =  [ 3.06032032 91.48976514 78.43708415]\n",
      "Done with gradient descent at iteration  453\n",
      "Learned weights =  [ 3.06704051 91.48976514 78.43708415]\n",
      "Done with gradient descent at iteration  453\n",
      "Learned weights =  [ 3.06704051 91.48976424 78.43708415]\n",
      "Done with gradient descent at iteration  453\n",
      "Learned weights =  [ 3.06704051 91.48976424 78.43708325]\n",
      "Done with gradient descent at iteration  454\n",
      "Learned weights =  [ 3.07376069 91.48976424 78.43708325]\n",
      "Done with gradient descent at iteration  454\n",
      "Learned weights =  [ 3.07376069 91.48976335 78.43708325]\n",
      "Done with gradient descent at iteration  454\n",
      "Learned weights =  [ 3.07376069 91.48976335 78.43708234]\n",
      "Done with gradient descent at iteration  455\n",
      "Learned weights =  [ 3.08048088 91.48976335 78.43708234]\n",
      "Done with gradient descent at iteration  455\n",
      "Learned weights =  [ 3.08048088 91.48976245 78.43708234]\n",
      "Done with gradient descent at iteration  455\n",
      "Learned weights =  [ 3.08048088 91.48976245 78.43708144]\n",
      "Done with gradient descent at iteration  456\n",
      "Learned weights =  [ 3.08720106 91.48976245 78.43708144]\n",
      "Done with gradient descent at iteration  456\n",
      "Learned weights =  [ 3.08720106 91.48976155 78.43708144]\n",
      "Done with gradient descent at iteration  456\n",
      "Learned weights =  [ 3.08720106 91.48976155 78.43708053]\n",
      "Done with gradient descent at iteration  457\n",
      "Learned weights =  [ 3.09392125 91.48976155 78.43708053]\n",
      "Done with gradient descent at iteration  457\n",
      "Learned weights =  [ 3.09392125 91.48976066 78.43708053]\n",
      "Done with gradient descent at iteration  457\n",
      "Learned weights =  [ 3.09392125 91.48976066 78.43707962]\n",
      "Done with gradient descent at iteration  458\n",
      "Learned weights =  [ 3.10064143 91.48976066 78.43707962]\n",
      "Done with gradient descent at iteration  458\n",
      "Learned weights =  [ 3.10064143 91.48975976 78.43707962]\n",
      "Done with gradient descent at iteration  458\n",
      "Learned weights =  [ 3.10064143 91.48975976 78.43707872]\n",
      "Done with gradient descent at iteration  459\n",
      "Learned weights =  [ 3.10736162 91.48975976 78.43707872]\n",
      "Done with gradient descent at iteration  459\n",
      "Learned weights =  [ 3.10736162 91.48975886 78.43707872]\n",
      "Done with gradient descent at iteration  459\n",
      "Learned weights =  [ 3.10736162 91.48975886 78.43707781]\n",
      "Done with gradient descent at iteration  460\n",
      "Learned weights =  [ 3.1140818  91.48975886 78.43707781]\n",
      "Done with gradient descent at iteration  460\n",
      "Learned weights =  [ 3.1140818  91.48975796 78.43707781]\n",
      "Done with gradient descent at iteration  460\n",
      "Learned weights =  [ 3.1140818  91.48975796 78.43707691]\n",
      "Done with gradient descent at iteration  461\n",
      "Learned weights =  [ 3.12080199 91.48975796 78.43707691]\n",
      "Done with gradient descent at iteration  461\n",
      "Learned weights =  [ 3.12080199 91.48975707 78.43707691]\n",
      "Done with gradient descent at iteration  461\n",
      "Learned weights =  [ 3.12080199 91.48975707 78.437076  ]\n",
      "Done with gradient descent at iteration  462\n",
      "Learned weights =  [ 3.12752217 91.48975707 78.437076  ]\n",
      "Done with gradient descent at iteration  462\n",
      "Learned weights =  [ 3.12752217 91.48975617 78.437076  ]\n",
      "Done with gradient descent at iteration  462\n",
      "Learned weights =  [ 3.12752217 91.48975617 78.43707509]\n",
      "Done with gradient descent at iteration  463\n",
      "Learned weights =  [ 3.13424235 91.48975617 78.43707509]\n",
      "Done with gradient descent at iteration  463\n",
      "Learned weights =  [ 3.13424235 91.48975527 78.43707509]\n",
      "Done with gradient descent at iteration  463\n",
      "Learned weights =  [ 3.13424235 91.48975527 78.43707419]\n",
      "Done with gradient descent at iteration  464\n",
      "Learned weights =  [ 3.14096254 91.48975527 78.43707419]\n",
      "Done with gradient descent at iteration  464\n",
      "Learned weights =  [ 3.14096254 91.48975438 78.43707419]\n",
      "Done with gradient descent at iteration  464\n",
      "Learned weights =  [ 3.14096254 91.48975438 78.43707328]\n",
      "Done with gradient descent at iteration  465\n",
      "Learned weights =  [ 3.14768272 91.48975438 78.43707328]\n",
      "Done with gradient descent at iteration  465\n",
      "Learned weights =  [ 3.14768272 91.48975348 78.43707328]\n",
      "Done with gradient descent at iteration  465\n",
      "Learned weights =  [ 3.14768272 91.48975348 78.43707238]\n",
      "Done with gradient descent at iteration  466\n",
      "Learned weights =  [ 3.15440291 91.48975348 78.43707238]\n",
      "Done with gradient descent at iteration  466\n",
      "Learned weights =  [ 3.15440291 91.48975258 78.43707238]\n",
      "Done with gradient descent at iteration  466\n",
      "Learned weights =  [ 3.15440291 91.48975258 78.43707147]\n",
      "Done with gradient descent at iteration  467\n",
      "Learned weights =  [ 3.16112309 91.48975258 78.43707147]\n",
      "Done with gradient descent at iteration  467\n",
      "Learned weights =  [ 3.16112309 91.48975169 78.43707147]\n",
      "Done with gradient descent at iteration  467\n",
      "Learned weights =  [ 3.16112309 91.48975169 78.43707057]\n",
      "Done with gradient descent at iteration  468\n",
      "Learned weights =  [ 3.16784327 91.48975169 78.43707057]\n",
      "Done with gradient descent at iteration  468\n",
      "Learned weights =  [ 3.16784327 91.48975079 78.43707057]\n",
      "Done with gradient descent at iteration  468\n",
      "Learned weights =  [ 3.16784327 91.48975079 78.43706966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  469\n",
      "Learned weights =  [ 3.17456346 91.48975079 78.43706966]\n",
      "Done with gradient descent at iteration  469\n",
      "Learned weights =  [ 3.17456346 91.48974989 78.43706966]\n",
      "Done with gradient descent at iteration  469\n",
      "Learned weights =  [ 3.17456346 91.48974989 78.43706875]\n",
      "Done with gradient descent at iteration  470\n",
      "Learned weights =  [ 3.18128364 91.48974989 78.43706875]\n",
      "Done with gradient descent at iteration  470\n",
      "Learned weights =  [ 3.18128364 91.48974899 78.43706875]\n",
      "Done with gradient descent at iteration  470\n",
      "Learned weights =  [ 3.18128364 91.48974899 78.43706785]\n",
      "Done with gradient descent at iteration  471\n",
      "Learned weights =  [ 3.18800382 91.48974899 78.43706785]\n",
      "Done with gradient descent at iteration  471\n",
      "Learned weights =  [ 3.18800382 91.4897481  78.43706785]\n",
      "Done with gradient descent at iteration  471\n",
      "Learned weights =  [ 3.18800382 91.4897481  78.43706694]\n",
      "Done with gradient descent at iteration  472\n",
      "Learned weights =  [ 3.194724   91.4897481  78.43706694]\n",
      "Done with gradient descent at iteration  472\n",
      "Learned weights =  [ 3.194724   91.4897472  78.43706694]\n",
      "Done with gradient descent at iteration  472\n",
      "Learned weights =  [ 3.194724   91.4897472  78.43706604]\n",
      "Done with gradient descent at iteration  473\n",
      "Learned weights =  [ 3.20144419 91.4897472  78.43706604]\n",
      "Done with gradient descent at iteration  473\n",
      "Learned weights =  [ 3.20144419 91.4897463  78.43706604]\n",
      "Done with gradient descent at iteration  473\n",
      "Learned weights =  [ 3.20144419 91.4897463  78.43706513]\n",
      "Done with gradient descent at iteration  474\n",
      "Learned weights =  [ 3.20816437 91.4897463  78.43706513]\n",
      "Done with gradient descent at iteration  474\n",
      "Learned weights =  [ 3.20816437 91.48974541 78.43706513]\n",
      "Done with gradient descent at iteration  474\n",
      "Learned weights =  [ 3.20816437 91.48974541 78.43706422]\n",
      "Done with gradient descent at iteration  475\n",
      "Learned weights =  [ 3.21488455 91.48974541 78.43706422]\n",
      "Done with gradient descent at iteration  475\n",
      "Learned weights =  [ 3.21488455 91.48974451 78.43706422]\n",
      "Done with gradient descent at iteration  475\n",
      "Learned weights =  [ 3.21488455 91.48974451 78.43706332]\n",
      "Done with gradient descent at iteration  476\n",
      "Learned weights =  [ 3.22160474 91.48974451 78.43706332]\n",
      "Done with gradient descent at iteration  476\n",
      "Learned weights =  [ 3.22160474 91.48974361 78.43706332]\n",
      "Done with gradient descent at iteration  476\n",
      "Learned weights =  [ 3.22160474 91.48974361 78.43706241]\n",
      "Done with gradient descent at iteration  477\n",
      "Learned weights =  [ 3.22832492 91.48974361 78.43706241]\n",
      "Done with gradient descent at iteration  477\n",
      "Learned weights =  [ 3.22832492 91.48974272 78.43706241]\n",
      "Done with gradient descent at iteration  477\n",
      "Learned weights =  [ 3.22832492 91.48974272 78.43706151]\n",
      "Done with gradient descent at iteration  478\n",
      "Learned weights =  [ 3.2350451  91.48974272 78.43706151]\n",
      "Done with gradient descent at iteration  478\n",
      "Learned weights =  [ 3.2350451  91.48974182 78.43706151]\n",
      "Done with gradient descent at iteration  478\n",
      "Learned weights =  [ 3.2350451  91.48974182 78.4370606 ]\n",
      "Done with gradient descent at iteration  479\n",
      "Learned weights =  [ 3.24176528 91.48974182 78.4370606 ]\n",
      "Done with gradient descent at iteration  479\n",
      "Learned weights =  [ 3.24176528 91.48974092 78.4370606 ]\n",
      "Done with gradient descent at iteration  479\n",
      "Learned weights =  [ 3.24176528 91.48974092 78.43705969]\n",
      "Done with gradient descent at iteration  480\n",
      "Learned weights =  [ 3.24848547 91.48974092 78.43705969]\n",
      "Done with gradient descent at iteration  480\n",
      "Learned weights =  [ 3.24848547 91.48974003 78.43705969]\n",
      "Done with gradient descent at iteration  480\n",
      "Learned weights =  [ 3.24848547 91.48974003 78.43705879]\n",
      "Done with gradient descent at iteration  481\n",
      "Learned weights =  [ 3.25520565 91.48974003 78.43705879]\n",
      "Done with gradient descent at iteration  481\n",
      "Learned weights =  [ 3.25520565 91.48973913 78.43705879]\n",
      "Done with gradient descent at iteration  481\n",
      "Learned weights =  [ 3.25520565 91.48973913 78.43705788]\n",
      "Done with gradient descent at iteration  482\n",
      "Learned weights =  [ 3.26192583 91.48973913 78.43705788]\n",
      "Done with gradient descent at iteration  482\n",
      "Learned weights =  [ 3.26192583 91.48973823 78.43705788]\n",
      "Done with gradient descent at iteration  482\n",
      "Learned weights =  [ 3.26192583 91.48973823 78.43705698]\n",
      "Done with gradient descent at iteration  483\n",
      "Learned weights =  [ 3.26864601 91.48973823 78.43705698]\n",
      "Done with gradient descent at iteration  483\n",
      "Learned weights =  [ 3.26864601 91.48973733 78.43705698]\n",
      "Done with gradient descent at iteration  483\n",
      "Learned weights =  [ 3.26864601 91.48973733 78.43705607]\n",
      "Done with gradient descent at iteration  484\n",
      "Learned weights =  [ 3.27536619 91.48973733 78.43705607]\n",
      "Done with gradient descent at iteration  484\n",
      "Learned weights =  [ 3.27536619 91.48973644 78.43705607]\n",
      "Done with gradient descent at iteration  484\n",
      "Learned weights =  [ 3.27536619 91.48973644 78.43705516]\n",
      "Done with gradient descent at iteration  485\n",
      "Learned weights =  [ 3.28208637 91.48973644 78.43705516]\n",
      "Done with gradient descent at iteration  485\n",
      "Learned weights =  [ 3.28208637 91.48973554 78.43705516]\n",
      "Done with gradient descent at iteration  485\n",
      "Learned weights =  [ 3.28208637 91.48973554 78.43705426]\n",
      "Done with gradient descent at iteration  486\n",
      "Learned weights =  [ 3.28880656 91.48973554 78.43705426]\n",
      "Done with gradient descent at iteration  486\n",
      "Learned weights =  [ 3.28880656 91.48973464 78.43705426]\n",
      "Done with gradient descent at iteration  486\n",
      "Learned weights =  [ 3.28880656 91.48973464 78.43705335]\n",
      "Done with gradient descent at iteration  487\n",
      "Learned weights =  [ 3.29552674 91.48973464 78.43705335]\n",
      "Done with gradient descent at iteration  487\n",
      "Learned weights =  [ 3.29552674 91.48973375 78.43705335]\n",
      "Done with gradient descent at iteration  487\n",
      "Learned weights =  [ 3.29552674 91.48973375 78.43705245]\n",
      "Done with gradient descent at iteration  488\n",
      "Learned weights =  [ 3.30224692 91.48973375 78.43705245]\n",
      "Done with gradient descent at iteration  488\n",
      "Learned weights =  [ 3.30224692 91.48973285 78.43705245]\n",
      "Done with gradient descent at iteration  488\n",
      "Learned weights =  [ 3.30224692 91.48973285 78.43705154]\n",
      "Done with gradient descent at iteration  489\n",
      "Learned weights =  [ 3.3089671  91.48973285 78.43705154]\n",
      "Done with gradient descent at iteration  489\n",
      "Learned weights =  [ 3.3089671  91.48973195 78.43705154]\n",
      "Done with gradient descent at iteration  489\n",
      "Learned weights =  [ 3.3089671  91.48973195 78.43705063]\n",
      "Done with gradient descent at iteration  490\n",
      "Learned weights =  [ 3.31568728 91.48973195 78.43705063]\n",
      "Done with gradient descent at iteration  490\n",
      "Learned weights =  [ 3.31568728 91.48973106 78.43705063]\n",
      "Done with gradient descent at iteration  490\n",
      "Learned weights =  [ 3.31568728 91.48973106 78.43704973]\n",
      "Done with gradient descent at iteration  491\n",
      "Learned weights =  [ 3.32240746 91.48973106 78.43704973]\n",
      "Done with gradient descent at iteration  491\n",
      "Learned weights =  [ 3.32240746 91.48973016 78.43704973]\n",
      "Done with gradient descent at iteration  491\n",
      "Learned weights =  [ 3.32240746 91.48973016 78.43704882]\n",
      "Done with gradient descent at iteration  492\n",
      "Learned weights =  [ 3.32912764 91.48973016 78.43704882]\n",
      "Done with gradient descent at iteration  492\n",
      "Learned weights =  [ 3.32912764 91.48972926 78.43704882]\n",
      "Done with gradient descent at iteration  492\n",
      "Learned weights =  [ 3.32912764 91.48972926 78.43704792]\n",
      "Done with gradient descent at iteration  493\n",
      "Learned weights =  [ 3.33584782 91.48972926 78.43704792]\n",
      "Done with gradient descent at iteration  493\n",
      "Learned weights =  [ 3.33584782 91.48972836 78.43704792]\n",
      "Done with gradient descent at iteration  493\n",
      "Learned weights =  [ 3.33584782 91.48972836 78.43704701]\n",
      "Done with gradient descent at iteration  494\n",
      "Learned weights =  [ 3.342568   91.48972836 78.43704701]\n",
      "Done with gradient descent at iteration  494\n",
      "Learned weights =  [ 3.342568   91.48972747 78.43704701]\n",
      "Done with gradient descent at iteration  494\n",
      "Learned weights =  [ 3.342568   91.48972747 78.4370461 ]\n",
      "Done with gradient descent at iteration  495\n",
      "Learned weights =  [ 3.34928818 91.48972747 78.4370461 ]\n",
      "Done with gradient descent at iteration  495\n",
      "Learned weights =  [ 3.34928818 91.48972657 78.4370461 ]\n",
      "Done with gradient descent at iteration  495\n",
      "Learned weights =  [ 3.34928818 91.48972657 78.4370452 ]\n",
      "Done with gradient descent at iteration  496\n",
      "Learned weights =  [ 3.35600837 91.48972657 78.4370452 ]\n",
      "Done with gradient descent at iteration  496\n",
      "Learned weights =  [ 3.35600837 91.48972567 78.4370452 ]\n",
      "Done with gradient descent at iteration  496\n",
      "Learned weights =  [ 3.35600837 91.48972567 78.43704429]\n",
      "Done with gradient descent at iteration  497\n",
      "Learned weights =  [ 3.36272855 91.48972567 78.43704429]\n",
      "Done with gradient descent at iteration  497\n",
      "Learned weights =  [ 3.36272855 91.48972478 78.43704429]\n",
      "Done with gradient descent at iteration  497\n",
      "Learned weights =  [ 3.36272855 91.48972478 78.43704339]\n",
      "Done with gradient descent at iteration  498\n",
      "Learned weights =  [ 3.36944873 91.48972478 78.43704339]\n",
      "Done with gradient descent at iteration  498\n",
      "Learned weights =  [ 3.36944873 91.48972388 78.43704339]\n",
      "Done with gradient descent at iteration  498\n",
      "Learned weights =  [ 3.36944873 91.48972388 78.43704248]\n",
      "Done with gradient descent at iteration  499\n",
      "Learned weights =  [ 3.37616891 91.48972388 78.43704248]\n",
      "Done with gradient descent at iteration  499\n",
      "Learned weights =  [ 3.37616891 91.48972298 78.43704248]\n",
      "Done with gradient descent at iteration  499\n",
      "Learned weights =  [ 3.37616891 91.48972298 78.43704157]\n",
      "Iteration = 500\n",
      "Cost function =  3605272308591734.0\n",
      "Done with gradient descent at iteration  500\n",
      "Learned weights =  [ 3.38288909 91.48972298 78.43704157]\n",
      "Done with gradient descent at iteration  500\n",
      "Learned weights =  [ 3.38288909 91.48972209 78.43704157]\n",
      "Done with gradient descent at iteration  500\n",
      "Learned weights =  [ 3.38288909 91.48972209 78.43704067]\n",
      "Done with gradient descent at iteration  501\n",
      "Learned weights =  [ 3.38960927 91.48972209 78.43704067]\n",
      "Done with gradient descent at iteration  501\n",
      "Learned weights =  [ 3.38960927 91.48972119 78.43704067]\n",
      "Done with gradient descent at iteration  501\n",
      "Learned weights =  [ 3.38960927 91.48972119 78.43703976]\n",
      "Done with gradient descent at iteration  502\n",
      "Learned weights =  [ 3.39632945 91.48972119 78.43703976]\n",
      "Done with gradient descent at iteration  502\n",
      "Learned weights =  [ 3.39632945 91.48972029 78.43703976]\n",
      "Done with gradient descent at iteration  502\n",
      "Learned weights =  [ 3.39632945 91.48972029 78.43703886]\n",
      "Done with gradient descent at iteration  503\n",
      "Learned weights =  [ 3.40304963 91.48972029 78.43703886]\n",
      "Done with gradient descent at iteration  503\n",
      "Learned weights =  [ 3.40304963 91.4897194  78.43703886]\n",
      "Done with gradient descent at iteration  503\n",
      "Learned weights =  [ 3.40304963 91.4897194  78.43703795]\n",
      "Done with gradient descent at iteration  504\n",
      "Learned weights =  [ 3.40976981 91.4897194  78.43703795]\n",
      "Done with gradient descent at iteration  504\n",
      "Learned weights =  [ 3.40976981 91.4897185  78.43703795]\n",
      "Done with gradient descent at iteration  504\n",
      "Learned weights =  [ 3.40976981 91.4897185  78.43703704]\n",
      "Done with gradient descent at iteration  505\n",
      "Learned weights =  [ 3.41648998 91.4897185  78.43703704]\n",
      "Done with gradient descent at iteration  505\n",
      "Learned weights =  [ 3.41648998 91.4897176  78.43703704]\n",
      "Done with gradient descent at iteration  505\n",
      "Learned weights =  [ 3.41648998 91.4897176  78.43703614]\n",
      "Done with gradient descent at iteration  506\n",
      "Learned weights =  [ 3.42321016 91.4897176  78.43703614]\n",
      "Done with gradient descent at iteration  506\n",
      "Learned weights =  [ 3.42321016 91.4897167  78.43703614]\n",
      "Done with gradient descent at iteration  506\n",
      "Learned weights =  [ 3.42321016 91.4897167  78.43703523]\n",
      "Done with gradient descent at iteration  507\n",
      "Learned weights =  [ 3.42993034 91.4897167  78.43703523]\n",
      "Done with gradient descent at iteration  507\n",
      "Learned weights =  [ 3.42993034 91.48971581 78.43703523]\n",
      "Done with gradient descent at iteration  507\n",
      "Learned weights =  [ 3.42993034 91.48971581 78.43703433]\n",
      "Done with gradient descent at iteration  508\n",
      "Learned weights =  [ 3.43665052 91.48971581 78.43703433]\n",
      "Done with gradient descent at iteration  508\n",
      "Learned weights =  [ 3.43665052 91.48971491 78.43703433]\n",
      "Done with gradient descent at iteration  508\n",
      "Learned weights =  [ 3.43665052 91.48971491 78.43703342]\n",
      "Done with gradient descent at iteration  509\n",
      "Learned weights =  [ 3.4433707  91.48971491 78.43703342]\n",
      "Done with gradient descent at iteration  509\n",
      "Learned weights =  [ 3.4433707  91.48971401 78.43703342]\n",
      "Done with gradient descent at iteration  509\n",
      "Learned weights =  [ 3.4433707  91.48971401 78.43703251]\n",
      "Done with gradient descent at iteration  510\n",
      "Learned weights =  [ 3.45009088 91.48971401 78.43703251]\n",
      "Done with gradient descent at iteration  510\n",
      "Learned weights =  [ 3.45009088 91.48971312 78.43703251]\n",
      "Done with gradient descent at iteration  510\n",
      "Learned weights =  [ 3.45009088 91.48971312 78.43703161]\n",
      "Done with gradient descent at iteration  511\n",
      "Learned weights =  [ 3.45681106 91.48971312 78.43703161]\n",
      "Done with gradient descent at iteration  511\n",
      "Learned weights =  [ 3.45681106 91.48971222 78.43703161]\n",
      "Done with gradient descent at iteration  511\n",
      "Learned weights =  [ 3.45681106 91.48971222 78.4370307 ]\n",
      "Done with gradient descent at iteration  512\n",
      "Learned weights =  [ 3.46353124 91.48971222 78.4370307 ]\n",
      "Done with gradient descent at iteration  512\n",
      "Learned weights =  [ 3.46353124 91.48971132 78.4370307 ]\n",
      "Done with gradient descent at iteration  512\n",
      "Learned weights =  [ 3.46353124 91.48971132 78.4370298 ]\n",
      "Done with gradient descent at iteration  513\n",
      "Learned weights =  [ 3.47025142 91.48971132 78.4370298 ]\n",
      "Done with gradient descent at iteration  513\n",
      "Learned weights =  [ 3.47025142 91.48971043 78.4370298 ]\n",
      "Done with gradient descent at iteration  513\n",
      "Learned weights =  [ 3.47025142 91.48971043 78.43702889]\n",
      "Done with gradient descent at iteration  514\n",
      "Learned weights =  [ 3.4769716  91.48971043 78.43702889]\n",
      "Done with gradient descent at iteration  514\n",
      "Learned weights =  [ 3.4769716  91.48970953 78.43702889]\n",
      "Done with gradient descent at iteration  514\n",
      "Learned weights =  [ 3.4769716  91.48970953 78.43702798]\n",
      "Done with gradient descent at iteration  515\n",
      "Learned weights =  [ 3.48369177 91.48970953 78.43702798]\n",
      "Done with gradient descent at iteration  515\n",
      "Learned weights =  [ 3.48369177 91.48970863 78.43702798]\n",
      "Done with gradient descent at iteration  515\n",
      "Learned weights =  [ 3.48369177 91.48970863 78.43702708]\n",
      "Done with gradient descent at iteration  516\n",
      "Learned weights =  [ 3.49041195 91.48970863 78.43702708]\n",
      "Done with gradient descent at iteration  516\n",
      "Learned weights =  [ 3.49041195 91.48970773 78.43702708]\n",
      "Done with gradient descent at iteration  516\n",
      "Learned weights =  [ 3.49041195 91.48970773 78.43702617]\n",
      "Done with gradient descent at iteration  517\n",
      "Learned weights =  [ 3.49713213 91.48970773 78.43702617]\n",
      "Done with gradient descent at iteration  517\n",
      "Learned weights =  [ 3.49713213 91.48970684 78.43702617]\n",
      "Done with gradient descent at iteration  517\n",
      "Learned weights =  [ 3.49713213 91.48970684 78.43702527]\n",
      "Done with gradient descent at iteration  518\n",
      "Learned weights =  [ 3.50385231 91.48970684 78.43702527]\n",
      "Done with gradient descent at iteration  518\n",
      "Learned weights =  [ 3.50385231 91.48970594 78.43702527]\n",
      "Done with gradient descent at iteration  518\n",
      "Learned weights =  [ 3.50385231 91.48970594 78.43702436]\n",
      "Done with gradient descent at iteration  519\n",
      "Learned weights =  [ 3.51057249 91.48970594 78.43702436]\n",
      "Done with gradient descent at iteration  519\n",
      "Learned weights =  [ 3.51057249 91.48970504 78.43702436]\n",
      "Done with gradient descent at iteration  519\n",
      "Learned weights =  [ 3.51057249 91.48970504 78.43702345]\n",
      "Done with gradient descent at iteration  520\n",
      "Learned weights =  [ 3.51729266 91.48970504 78.43702345]\n",
      "Done with gradient descent at iteration  520\n",
      "Learned weights =  [ 3.51729266 91.48970415 78.43702345]\n",
      "Done with gradient descent at iteration  520\n",
      "Learned weights =  [ 3.51729266 91.48970415 78.43702255]\n",
      "Done with gradient descent at iteration  521\n",
      "Learned weights =  [ 3.52401284 91.48970415 78.43702255]\n",
      "Done with gradient descent at iteration  521\n",
      "Learned weights =  [ 3.52401284 91.48970325 78.43702255]\n",
      "Done with gradient descent at iteration  521\n",
      "Learned weights =  [ 3.52401284 91.48970325 78.43702164]\n",
      "Done with gradient descent at iteration  522\n",
      "Learned weights =  [ 3.53073302 91.48970325 78.43702164]\n",
      "Done with gradient descent at iteration  522\n",
      "Learned weights =  [ 3.53073302 91.48970235 78.43702164]\n",
      "Done with gradient descent at iteration  522\n",
      "Learned weights =  [ 3.53073302 91.48970235 78.43702074]\n",
      "Done with gradient descent at iteration  523\n",
      "Learned weights =  [ 3.5374532  91.48970235 78.43702074]\n",
      "Done with gradient descent at iteration  523\n",
      "Learned weights =  [ 3.5374532  91.48970146 78.43702074]\n",
      "Done with gradient descent at iteration  523\n",
      "Learned weights =  [ 3.5374532  91.48970146 78.43701983]\n",
      "Done with gradient descent at iteration  524\n",
      "Learned weights =  [ 3.54417338 91.48970146 78.43701983]\n",
      "Done with gradient descent at iteration  524\n",
      "Learned weights =  [ 3.54417338 91.48970056 78.43701983]\n",
      "Done with gradient descent at iteration  524\n",
      "Learned weights =  [ 3.54417338 91.48970056 78.43701892]\n",
      "Done with gradient descent at iteration  525\n",
      "Learned weights =  [ 3.55089355 91.48970056 78.43701892]\n",
      "Done with gradient descent at iteration  525\n",
      "Learned weights =  [ 3.55089355 91.48969966 78.43701892]\n",
      "Done with gradient descent at iteration  525\n",
      "Learned weights =  [ 3.55089355 91.48969966 78.43701802]\n",
      "Done with gradient descent at iteration  526\n",
      "Learned weights =  [ 3.55761373 91.48969966 78.43701802]\n",
      "Done with gradient descent at iteration  526\n",
      "Learned weights =  [ 3.55761373 91.48969877 78.43701802]\n",
      "Done with gradient descent at iteration  526\n",
      "Learned weights =  [ 3.55761373 91.48969877 78.43701711]\n",
      "Done with gradient descent at iteration  527\n",
      "Learned weights =  [ 3.56433391 91.48969877 78.43701711]\n",
      "Done with gradient descent at iteration  527\n",
      "Learned weights =  [ 3.56433391 91.48969787 78.43701711]\n",
      "Done with gradient descent at iteration  527\n",
      "Learned weights =  [ 3.56433391 91.48969787 78.43701621]\n",
      "Done with gradient descent at iteration  528\n",
      "Learned weights =  [ 3.57105408 91.48969787 78.43701621]\n",
      "Done with gradient descent at iteration  528\n",
      "Learned weights =  [ 3.57105408 91.48969697 78.43701621]\n",
      "Done with gradient descent at iteration  528\n",
      "Learned weights =  [ 3.57105408 91.48969697 78.4370153 ]\n",
      "Done with gradient descent at iteration  529\n",
      "Learned weights =  [ 3.57777426 91.48969697 78.4370153 ]\n",
      "Done with gradient descent at iteration  529\n",
      "Learned weights =  [ 3.57777426 91.48969607 78.4370153 ]\n",
      "Done with gradient descent at iteration  529\n",
      "Learned weights =  [ 3.57777426 91.48969607 78.43701439]\n",
      "Done with gradient descent at iteration  530\n",
      "Learned weights =  [ 3.58449444 91.48969607 78.43701439]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  530\n",
      "Learned weights =  [ 3.58449444 91.48969518 78.43701439]\n",
      "Done with gradient descent at iteration  530\n",
      "Learned weights =  [ 3.58449444 91.48969518 78.43701349]\n",
      "Done with gradient descent at iteration  531\n",
      "Learned weights =  [ 3.59121461 91.48969518 78.43701349]\n",
      "Done with gradient descent at iteration  531\n",
      "Learned weights =  [ 3.59121461 91.48969428 78.43701349]\n",
      "Done with gradient descent at iteration  531\n",
      "Learned weights =  [ 3.59121461 91.48969428 78.43701258]\n",
      "Done with gradient descent at iteration  532\n",
      "Learned weights =  [ 3.59793479 91.48969428 78.43701258]\n",
      "Done with gradient descent at iteration  532\n",
      "Learned weights =  [ 3.59793479 91.48969338 78.43701258]\n",
      "Done with gradient descent at iteration  532\n",
      "Learned weights =  [ 3.59793479 91.48969338 78.43701168]\n",
      "Done with gradient descent at iteration  533\n",
      "Learned weights =  [ 3.60465497 91.48969338 78.43701168]\n",
      "Done with gradient descent at iteration  533\n",
      "Learned weights =  [ 3.60465497 91.48969249 78.43701168]\n",
      "Done with gradient descent at iteration  533\n",
      "Learned weights =  [ 3.60465497 91.48969249 78.43701077]\n",
      "Done with gradient descent at iteration  534\n",
      "Learned weights =  [ 3.61137514 91.48969249 78.43701077]\n",
      "Done with gradient descent at iteration  534\n",
      "Learned weights =  [ 3.61137514 91.48969159 78.43701077]\n",
      "Done with gradient descent at iteration  534\n",
      "Learned weights =  [ 3.61137514 91.48969159 78.43700986]\n",
      "Done with gradient descent at iteration  535\n",
      "Learned weights =  [ 3.61809532 91.48969159 78.43700986]\n",
      "Done with gradient descent at iteration  535\n",
      "Learned weights =  [ 3.61809532 91.48969069 78.43700986]\n",
      "Done with gradient descent at iteration  535\n",
      "Learned weights =  [ 3.61809532 91.48969069 78.43700896]\n",
      "Done with gradient descent at iteration  536\n",
      "Learned weights =  [ 3.6248155  91.48969069 78.43700896]\n",
      "Done with gradient descent at iteration  536\n",
      "Learned weights =  [ 3.6248155  91.4896898  78.43700896]\n",
      "Done with gradient descent at iteration  536\n",
      "Learned weights =  [ 3.6248155  91.4896898  78.43700805]\n",
      "Done with gradient descent at iteration  537\n",
      "Learned weights =  [ 3.63153567 91.4896898  78.43700805]\n",
      "Done with gradient descent at iteration  537\n",
      "Learned weights =  [ 3.63153567 91.4896889  78.43700805]\n",
      "Done with gradient descent at iteration  537\n",
      "Learned weights =  [ 3.63153567 91.4896889  78.43700715]\n",
      "Done with gradient descent at iteration  538\n",
      "Learned weights =  [ 3.63825585 91.4896889  78.43700715]\n",
      "Done with gradient descent at iteration  538\n",
      "Learned weights =  [ 3.63825585 91.489688   78.43700715]\n",
      "Done with gradient descent at iteration  538\n",
      "Learned weights =  [ 3.63825585 91.489688   78.43700624]\n",
      "Done with gradient descent at iteration  539\n",
      "Learned weights =  [ 3.64497602 91.489688   78.43700624]\n",
      "Done with gradient descent at iteration  539\n",
      "Learned weights =  [ 3.64497602 91.4896871  78.43700624]\n",
      "Done with gradient descent at iteration  539\n",
      "Learned weights =  [ 3.64497602 91.4896871  78.43700533]\n",
      "Done with gradient descent at iteration  540\n",
      "Learned weights =  [ 3.6516962  91.4896871  78.43700533]\n",
      "Done with gradient descent at iteration  540\n",
      "Learned weights =  [ 3.6516962  91.48968621 78.43700533]\n",
      "Done with gradient descent at iteration  540\n",
      "Learned weights =  [ 3.6516962  91.48968621 78.43700443]\n",
      "Done with gradient descent at iteration  541\n",
      "Learned weights =  [ 3.65841638 91.48968621 78.43700443]\n",
      "Done with gradient descent at iteration  541\n",
      "Learned weights =  [ 3.65841638 91.48968531 78.43700443]\n",
      "Done with gradient descent at iteration  541\n",
      "Learned weights =  [ 3.65841638 91.48968531 78.43700352]\n",
      "Done with gradient descent at iteration  542\n",
      "Learned weights =  [ 3.66513655 91.48968531 78.43700352]\n",
      "Done with gradient descent at iteration  542\n",
      "Learned weights =  [ 3.66513655 91.48968441 78.43700352]\n",
      "Done with gradient descent at iteration  542\n",
      "Learned weights =  [ 3.66513655 91.48968441 78.43700262]\n",
      "Done with gradient descent at iteration  543\n",
      "Learned weights =  [ 3.67185673 91.48968441 78.43700262]\n",
      "Done with gradient descent at iteration  543\n",
      "Learned weights =  [ 3.67185673 91.48968352 78.43700262]\n",
      "Done with gradient descent at iteration  543\n",
      "Learned weights =  [ 3.67185673 91.48968352 78.43700171]\n",
      "Done with gradient descent at iteration  544\n",
      "Learned weights =  [ 3.6785769  91.48968352 78.43700171]\n",
      "Done with gradient descent at iteration  544\n",
      "Learned weights =  [ 3.6785769  91.48968262 78.43700171]\n",
      "Done with gradient descent at iteration  544\n",
      "Learned weights =  [ 3.6785769  91.48968262 78.43700081]\n",
      "Done with gradient descent at iteration  545\n",
      "Learned weights =  [ 3.68529708 91.48968262 78.43700081]\n",
      "Done with gradient descent at iteration  545\n",
      "Learned weights =  [ 3.68529708 91.48968172 78.43700081]\n",
      "Done with gradient descent at iteration  545\n",
      "Learned weights =  [ 3.68529708 91.48968172 78.4369999 ]\n",
      "Done with gradient descent at iteration  546\n",
      "Learned weights =  [ 3.69201725 91.48968172 78.4369999 ]\n",
      "Done with gradient descent at iteration  546\n",
      "Learned weights =  [ 3.69201725 91.48968083 78.4369999 ]\n",
      "Done with gradient descent at iteration  546\n",
      "Learned weights =  [ 3.69201725 91.48968083 78.43699899]\n",
      "Done with gradient descent at iteration  547\n",
      "Learned weights =  [ 3.69873743 91.48968083 78.43699899]\n",
      "Done with gradient descent at iteration  547\n",
      "Learned weights =  [ 3.69873743 91.48967993 78.43699899]\n",
      "Done with gradient descent at iteration  547\n",
      "Learned weights =  [ 3.69873743 91.48967993 78.43699809]\n",
      "Done with gradient descent at iteration  548\n",
      "Learned weights =  [ 3.7054576  91.48967993 78.43699809]\n",
      "Done with gradient descent at iteration  548\n",
      "Learned weights =  [ 3.7054576  91.48967903 78.43699809]\n",
      "Done with gradient descent at iteration  548\n",
      "Learned weights =  [ 3.7054576  91.48967903 78.43699718]\n",
      "Done with gradient descent at iteration  549\n",
      "Learned weights =  [ 3.71217778 91.48967903 78.43699718]\n",
      "Done with gradient descent at iteration  549\n",
      "Learned weights =  [ 3.71217778 91.48967814 78.43699718]\n",
      "Done with gradient descent at iteration  549\n",
      "Learned weights =  [ 3.71217778 91.48967814 78.43699628]\n",
      "Done with gradient descent at iteration  550\n",
      "Learned weights =  [ 3.71889795 91.48967814 78.43699628]\n",
      "Done with gradient descent at iteration  550\n",
      "Learned weights =  [ 3.71889795 91.48967724 78.43699628]\n",
      "Done with gradient descent at iteration  550\n",
      "Learned weights =  [ 3.71889795 91.48967724 78.43699537]\n",
      "Done with gradient descent at iteration  551\n",
      "Learned weights =  [ 3.72561813 91.48967724 78.43699537]\n",
      "Done with gradient descent at iteration  551\n",
      "Learned weights =  [ 3.72561813 91.48967634 78.43699537]\n",
      "Done with gradient descent at iteration  551\n",
      "Learned weights =  [ 3.72561813 91.48967634 78.43699446]\n",
      "Done with gradient descent at iteration  552\n",
      "Learned weights =  [ 3.7323383  91.48967634 78.43699446]\n",
      "Done with gradient descent at iteration  552\n",
      "Learned weights =  [ 3.7323383  91.48967544 78.43699446]\n",
      "Done with gradient descent at iteration  552\n",
      "Learned weights =  [ 3.7323383  91.48967544 78.43699356]\n",
      "Done with gradient descent at iteration  553\n",
      "Learned weights =  [ 3.73905848 91.48967544 78.43699356]\n",
      "Done with gradient descent at iteration  553\n",
      "Learned weights =  [ 3.73905848 91.48967455 78.43699356]\n",
      "Done with gradient descent at iteration  553\n",
      "Learned weights =  [ 3.73905848 91.48967455 78.43699265]\n",
      "Done with gradient descent at iteration  554\n",
      "Learned weights =  [ 3.74577865 91.48967455 78.43699265]\n",
      "Done with gradient descent at iteration  554\n",
      "Learned weights =  [ 3.74577865 91.48967365 78.43699265]\n",
      "Done with gradient descent at iteration  554\n",
      "Learned weights =  [ 3.74577865 91.48967365 78.43699175]\n",
      "Done with gradient descent at iteration  555\n",
      "Learned weights =  [ 3.75249882 91.48967365 78.43699175]\n",
      "Done with gradient descent at iteration  555\n",
      "Learned weights =  [ 3.75249882 91.48967275 78.43699175]\n",
      "Done with gradient descent at iteration  555\n",
      "Learned weights =  [ 3.75249882 91.48967275 78.43699084]\n",
      "Done with gradient descent at iteration  556\n",
      "Learned weights =  [ 3.759219   91.48967275 78.43699084]\n",
      "Done with gradient descent at iteration  556\n",
      "Learned weights =  [ 3.759219   91.48967186 78.43699084]\n",
      "Done with gradient descent at iteration  556\n",
      "Learned weights =  [ 3.759219   91.48967186 78.43698993]\n",
      "Done with gradient descent at iteration  557\n",
      "Learned weights =  [ 3.76593917 91.48967186 78.43698993]\n",
      "Done with gradient descent at iteration  557\n",
      "Learned weights =  [ 3.76593917 91.48967096 78.43698993]\n",
      "Done with gradient descent at iteration  557\n",
      "Learned weights =  [ 3.76593917 91.48967096 78.43698903]\n",
      "Done with gradient descent at iteration  558\n",
      "Learned weights =  [ 3.77265935 91.48967096 78.43698903]\n",
      "Done with gradient descent at iteration  558\n",
      "Learned weights =  [ 3.77265935 91.48967006 78.43698903]\n",
      "Done with gradient descent at iteration  558\n",
      "Learned weights =  [ 3.77265935 91.48967006 78.43698812]\n",
      "Done with gradient descent at iteration  559\n",
      "Learned weights =  [ 3.77937952 91.48967006 78.43698812]\n",
      "Done with gradient descent at iteration  559\n",
      "Learned weights =  [ 3.77937952 91.48966917 78.43698812]\n",
      "Done with gradient descent at iteration  559\n",
      "Learned weights =  [ 3.77937952 91.48966917 78.43698722]\n",
      "Done with gradient descent at iteration  560\n",
      "Learned weights =  [ 3.78609969 91.48966917 78.43698722]\n",
      "Done with gradient descent at iteration  560\n",
      "Learned weights =  [ 3.78609969 91.48966827 78.43698722]\n",
      "Done with gradient descent at iteration  560\n",
      "Learned weights =  [ 3.78609969 91.48966827 78.43698631]\n",
      "Done with gradient descent at iteration  561\n",
      "Learned weights =  [ 3.79281987 91.48966827 78.43698631]\n",
      "Done with gradient descent at iteration  561\n",
      "Learned weights =  [ 3.79281987 91.48966737 78.43698631]\n",
      "Done with gradient descent at iteration  561\n",
      "Learned weights =  [ 3.79281987 91.48966737 78.4369854 ]\n",
      "Done with gradient descent at iteration  562\n",
      "Learned weights =  [ 3.79954004 91.48966737 78.4369854 ]\n",
      "Done with gradient descent at iteration  562\n",
      "Learned weights =  [ 3.79954004 91.48966648 78.4369854 ]\n",
      "Done with gradient descent at iteration  562\n",
      "Learned weights =  [ 3.79954004 91.48966648 78.4369845 ]\n",
      "Done with gradient descent at iteration  563\n",
      "Learned weights =  [ 3.80626021 91.48966648 78.4369845 ]\n",
      "Done with gradient descent at iteration  563\n",
      "Learned weights =  [ 3.80626021 91.48966558 78.4369845 ]\n",
      "Done with gradient descent at iteration  563\n",
      "Learned weights =  [ 3.80626021 91.48966558 78.43698359]\n",
      "Done with gradient descent at iteration  564\n",
      "Learned weights =  [ 3.81298039 91.48966558 78.43698359]\n",
      "Done with gradient descent at iteration  564\n",
      "Learned weights =  [ 3.81298039 91.48966468 78.43698359]\n",
      "Done with gradient descent at iteration  564\n",
      "Learned weights =  [ 3.81298039 91.48966468 78.43698269]\n",
      "Done with gradient descent at iteration  565\n",
      "Learned weights =  [ 3.81970056 91.48966468 78.43698269]\n",
      "Done with gradient descent at iteration  565\n",
      "Learned weights =  [ 3.81970056 91.48966378 78.43698269]\n",
      "Done with gradient descent at iteration  565\n",
      "Learned weights =  [ 3.81970056 91.48966378 78.43698178]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  566\n",
      "Learned weights =  [ 3.82642073 91.48966378 78.43698178]\n",
      "Done with gradient descent at iteration  566\n",
      "Learned weights =  [ 3.82642073 91.48966289 78.43698178]\n",
      "Done with gradient descent at iteration  566\n",
      "Learned weights =  [ 3.82642073 91.48966289 78.43698087]\n",
      "Done with gradient descent at iteration  567\n",
      "Learned weights =  [ 3.83314091 91.48966289 78.43698087]\n",
      "Done with gradient descent at iteration  567\n",
      "Learned weights =  [ 3.83314091 91.48966199 78.43698087]\n",
      "Done with gradient descent at iteration  567\n",
      "Learned weights =  [ 3.83314091 91.48966199 78.43697997]\n",
      "Done with gradient descent at iteration  568\n",
      "Learned weights =  [ 3.83986108 91.48966199 78.43697997]\n",
      "Done with gradient descent at iteration  568\n",
      "Learned weights =  [ 3.83986108 91.48966109 78.43697997]\n",
      "Done with gradient descent at iteration  568\n",
      "Learned weights =  [ 3.83986108 91.48966109 78.43697906]\n",
      "Done with gradient descent at iteration  569\n",
      "Learned weights =  [ 3.84658125 91.48966109 78.43697906]\n",
      "Done with gradient descent at iteration  569\n",
      "Learned weights =  [ 3.84658125 91.4896602  78.43697906]\n",
      "Done with gradient descent at iteration  569\n",
      "Learned weights =  [ 3.84658125 91.4896602  78.43697816]\n",
      "Done with gradient descent at iteration  570\n",
      "Learned weights =  [ 3.85330142 91.4896602  78.43697816]\n",
      "Done with gradient descent at iteration  570\n",
      "Learned weights =  [ 3.85330142 91.4896593  78.43697816]\n",
      "Done with gradient descent at iteration  570\n",
      "Learned weights =  [ 3.85330142 91.4896593  78.43697725]\n",
      "Done with gradient descent at iteration  571\n",
      "Learned weights =  [ 3.8600216  91.4896593  78.43697725]\n",
      "Done with gradient descent at iteration  571\n",
      "Learned weights =  [ 3.8600216  91.4896584  78.43697725]\n",
      "Done with gradient descent at iteration  571\n",
      "Learned weights =  [ 3.8600216  91.4896584  78.43697634]\n",
      "Done with gradient descent at iteration  572\n",
      "Learned weights =  [ 3.86674177 91.4896584  78.43697634]\n",
      "Done with gradient descent at iteration  572\n",
      "Learned weights =  [ 3.86674177 91.48965751 78.43697634]\n",
      "Done with gradient descent at iteration  572\n",
      "Learned weights =  [ 3.86674177 91.48965751 78.43697544]\n",
      "Done with gradient descent at iteration  573\n",
      "Learned weights =  [ 3.87346194 91.48965751 78.43697544]\n",
      "Done with gradient descent at iteration  573\n",
      "Learned weights =  [ 3.87346194 91.48965661 78.43697544]\n",
      "Done with gradient descent at iteration  573\n",
      "Learned weights =  [ 3.87346194 91.48965661 78.43697453]\n",
      "Done with gradient descent at iteration  574\n",
      "Learned weights =  [ 3.88018211 91.48965661 78.43697453]\n",
      "Done with gradient descent at iteration  574\n",
      "Learned weights =  [ 3.88018211 91.48965571 78.43697453]\n",
      "Done with gradient descent at iteration  574\n",
      "Learned weights =  [ 3.88018211 91.48965571 78.43697363]\n",
      "Done with gradient descent at iteration  575\n",
      "Learned weights =  [ 3.88690229 91.48965571 78.43697363]\n",
      "Done with gradient descent at iteration  575\n",
      "Learned weights =  [ 3.88690229 91.48965481 78.43697363]\n",
      "Done with gradient descent at iteration  575\n",
      "Learned weights =  [ 3.88690229 91.48965481 78.43697272]\n",
      "Done with gradient descent at iteration  576\n",
      "Learned weights =  [ 3.89362246 91.48965481 78.43697272]\n",
      "Done with gradient descent at iteration  576\n",
      "Learned weights =  [ 3.89362246 91.48965392 78.43697272]\n",
      "Done with gradient descent at iteration  576\n",
      "Learned weights =  [ 3.89362246 91.48965392 78.43697181]\n",
      "Done with gradient descent at iteration  577\n",
      "Learned weights =  [ 3.90034263 91.48965392 78.43697181]\n",
      "Done with gradient descent at iteration  577\n",
      "Learned weights =  [ 3.90034263 91.48965302 78.43697181]\n",
      "Done with gradient descent at iteration  577\n",
      "Learned weights =  [ 3.90034263 91.48965302 78.43697091]\n",
      "Done with gradient descent at iteration  578\n",
      "Learned weights =  [ 3.9070628  91.48965302 78.43697091]\n",
      "Done with gradient descent at iteration  578\n",
      "Learned weights =  [ 3.9070628  91.48965212 78.43697091]\n",
      "Done with gradient descent at iteration  578\n",
      "Learned weights =  [ 3.9070628  91.48965212 78.43697   ]\n",
      "Done with gradient descent at iteration  579\n",
      "Learned weights =  [ 3.91378297 91.48965212 78.43697   ]\n",
      "Done with gradient descent at iteration  579\n",
      "Learned weights =  [ 3.91378297 91.48965123 78.43697   ]\n",
      "Done with gradient descent at iteration  579\n",
      "Learned weights =  [ 3.91378297 91.48965123 78.4369691 ]\n",
      "Done with gradient descent at iteration  580\n",
      "Learned weights =  [ 3.92050314 91.48965123 78.4369691 ]\n",
      "Done with gradient descent at iteration  580\n",
      "Learned weights =  [ 3.92050314 91.48965033 78.4369691 ]\n",
      "Done with gradient descent at iteration  580\n",
      "Learned weights =  [ 3.92050314 91.48965033 78.43696819]\n",
      "Done with gradient descent at iteration  581\n",
      "Learned weights =  [ 3.92722332 91.48965033 78.43696819]\n",
      "Done with gradient descent at iteration  581\n",
      "Learned weights =  [ 3.92722332 91.48964943 78.43696819]\n",
      "Done with gradient descent at iteration  581\n",
      "Learned weights =  [ 3.92722332 91.48964943 78.43696728]\n",
      "Done with gradient descent at iteration  582\n",
      "Learned weights =  [ 3.93394349 91.48964943 78.43696728]\n",
      "Done with gradient descent at iteration  582\n",
      "Learned weights =  [ 3.93394349 91.48964854 78.43696728]\n",
      "Done with gradient descent at iteration  582\n",
      "Learned weights =  [ 3.93394349 91.48964854 78.43696638]\n",
      "Done with gradient descent at iteration  583\n",
      "Learned weights =  [ 3.94066366 91.48964854 78.43696638]\n",
      "Done with gradient descent at iteration  583\n",
      "Learned weights =  [ 3.94066366 91.48964764 78.43696638]\n",
      "Done with gradient descent at iteration  583\n",
      "Learned weights =  [ 3.94066366 91.48964764 78.43696547]\n",
      "Done with gradient descent at iteration  584\n",
      "Learned weights =  [ 3.94738383 91.48964764 78.43696547]\n",
      "Done with gradient descent at iteration  584\n",
      "Learned weights =  [ 3.94738383 91.48964674 78.43696547]\n",
      "Done with gradient descent at iteration  584\n",
      "Learned weights =  [ 3.94738383 91.48964674 78.43696457]\n",
      "Done with gradient descent at iteration  585\n",
      "Learned weights =  [ 3.954104   91.48964674 78.43696457]\n",
      "Done with gradient descent at iteration  585\n",
      "Learned weights =  [ 3.954104   91.48964585 78.43696457]\n",
      "Done with gradient descent at iteration  585\n",
      "Learned weights =  [ 3.954104   91.48964585 78.43696366]\n",
      "Done with gradient descent at iteration  586\n",
      "Learned weights =  [ 3.96082417 91.48964585 78.43696366]\n",
      "Done with gradient descent at iteration  586\n",
      "Learned weights =  [ 3.96082417 91.48964495 78.43696366]\n",
      "Done with gradient descent at iteration  586\n",
      "Learned weights =  [ 3.96082417 91.48964495 78.43696275]\n",
      "Done with gradient descent at iteration  587\n",
      "Learned weights =  [ 3.96754434 91.48964495 78.43696275]\n",
      "Done with gradient descent at iteration  587\n",
      "Learned weights =  [ 3.96754434 91.48964405 78.43696275]\n",
      "Done with gradient descent at iteration  587\n",
      "Learned weights =  [ 3.96754434 91.48964405 78.43696185]\n",
      "Done with gradient descent at iteration  588\n",
      "Learned weights =  [ 3.97426451 91.48964405 78.43696185]\n",
      "Done with gradient descent at iteration  588\n",
      "Learned weights =  [ 3.97426451 91.48964315 78.43696185]\n",
      "Done with gradient descent at iteration  588\n",
      "Learned weights =  [ 3.97426451 91.48964315 78.43696094]\n",
      "Done with gradient descent at iteration  589\n",
      "Learned weights =  [ 3.98098468 91.48964315 78.43696094]\n",
      "Done with gradient descent at iteration  589\n",
      "Learned weights =  [ 3.98098468 91.48964226 78.43696094]\n",
      "Done with gradient descent at iteration  589\n",
      "Learned weights =  [ 3.98098468 91.48964226 78.43696004]\n",
      "Done with gradient descent at iteration  590\n",
      "Learned weights =  [ 3.98770485 91.48964226 78.43696004]\n",
      "Done with gradient descent at iteration  590\n",
      "Learned weights =  [ 3.98770485 91.48964136 78.43696004]\n",
      "Done with gradient descent at iteration  590\n",
      "Learned weights =  [ 3.98770485 91.48964136 78.43695913]\n",
      "Done with gradient descent at iteration  591\n",
      "Learned weights =  [ 3.99442502 91.48964136 78.43695913]\n",
      "Done with gradient descent at iteration  591\n",
      "Learned weights =  [ 3.99442502 91.48964046 78.43695913]\n",
      "Done with gradient descent at iteration  591\n",
      "Learned weights =  [ 3.99442502 91.48964046 78.43695822]\n",
      "Done with gradient descent at iteration  592\n",
      "Learned weights =  [ 4.0011452  91.48964046 78.43695822]\n",
      "Done with gradient descent at iteration  592\n",
      "Learned weights =  [ 4.0011452  91.48963957 78.43695822]\n",
      "Done with gradient descent at iteration  592\n",
      "Learned weights =  [ 4.0011452  91.48963957 78.43695732]\n",
      "Done with gradient descent at iteration  593\n",
      "Learned weights =  [ 4.00786537 91.48963957 78.43695732]\n",
      "Done with gradient descent at iteration  593\n",
      "Learned weights =  [ 4.00786537 91.48963867 78.43695732]\n",
      "Done with gradient descent at iteration  593\n",
      "Learned weights =  [ 4.00786537 91.48963867 78.43695641]\n",
      "Done with gradient descent at iteration  594\n",
      "Learned weights =  [ 4.01458554 91.48963867 78.43695641]\n",
      "Done with gradient descent at iteration  594\n",
      "Learned weights =  [ 4.01458554 91.48963777 78.43695641]\n",
      "Done with gradient descent at iteration  594\n",
      "Learned weights =  [ 4.01458554 91.48963777 78.43695551]\n",
      "Done with gradient descent at iteration  595\n",
      "Learned weights =  [ 4.02130571 91.48963777 78.43695551]\n",
      "Done with gradient descent at iteration  595\n",
      "Learned weights =  [ 4.02130571 91.48963688 78.43695551]\n",
      "Done with gradient descent at iteration  595\n",
      "Learned weights =  [ 4.02130571 91.48963688 78.4369546 ]\n",
      "Done with gradient descent at iteration  596\n",
      "Learned weights =  [ 4.02802588 91.48963688 78.4369546 ]\n",
      "Done with gradient descent at iteration  596\n",
      "Learned weights =  [ 4.02802588 91.48963598 78.4369546 ]\n",
      "Done with gradient descent at iteration  596\n",
      "Learned weights =  [ 4.02802588 91.48963598 78.43695369]\n",
      "Done with gradient descent at iteration  597\n",
      "Learned weights =  [ 4.03474604 91.48963598 78.43695369]\n",
      "Done with gradient descent at iteration  597\n",
      "Learned weights =  [ 4.03474604 91.48963508 78.43695369]\n",
      "Done with gradient descent at iteration  597\n",
      "Learned weights =  [ 4.03474604 91.48963508 78.43695279]\n",
      "Done with gradient descent at iteration  598\n",
      "Learned weights =  [ 4.04146621 91.48963508 78.43695279]\n",
      "Done with gradient descent at iteration  598\n",
      "Learned weights =  [ 4.04146621 91.48963418 78.43695279]\n",
      "Done with gradient descent at iteration  598\n",
      "Learned weights =  [ 4.04146621 91.48963418 78.43695188]\n",
      "Done with gradient descent at iteration  599\n",
      "Learned weights =  [ 4.04818638 91.48963418 78.43695188]\n",
      "Done with gradient descent at iteration  599\n",
      "Learned weights =  [ 4.04818638 91.48963329 78.43695188]\n",
      "Done with gradient descent at iteration  599\n",
      "Learned weights =  [ 4.04818638 91.48963329 78.43695098]\n",
      "Iteration = 600\n",
      "Cost function =  3605267792516699.5\n",
      "Done with gradient descent at iteration  600\n",
      "Learned weights =  [ 4.05490655 91.48963329 78.43695098]\n",
      "Done with gradient descent at iteration  600\n",
      "Learned weights =  [ 4.05490655 91.48963239 78.43695098]\n",
      "Done with gradient descent at iteration  600\n",
      "Learned weights =  [ 4.05490655 91.48963239 78.43695007]\n",
      "Done with gradient descent at iteration  601\n",
      "Learned weights =  [ 4.06162672 91.48963239 78.43695007]\n",
      "Done with gradient descent at iteration  601\n",
      "Learned weights =  [ 4.06162672 91.48963149 78.43695007]\n",
      "Done with gradient descent at iteration  601\n",
      "Learned weights =  [ 4.06162672 91.48963149 78.43694916]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  602\n",
      "Learned weights =  [ 4.06834689 91.48963149 78.43694916]\n",
      "Done with gradient descent at iteration  602\n",
      "Learned weights =  [ 4.06834689 91.4896306  78.43694916]\n",
      "Done with gradient descent at iteration  602\n",
      "Learned weights =  [ 4.06834689 91.4896306  78.43694826]\n",
      "Done with gradient descent at iteration  603\n",
      "Learned weights =  [ 4.07506706 91.4896306  78.43694826]\n",
      "Done with gradient descent at iteration  603\n",
      "Learned weights =  [ 4.07506706 91.4896297  78.43694826]\n",
      "Done with gradient descent at iteration  603\n",
      "Learned weights =  [ 4.07506706 91.4896297  78.43694735]\n",
      "Done with gradient descent at iteration  604\n",
      "Learned weights =  [ 4.08178723 91.4896297  78.43694735]\n",
      "Done with gradient descent at iteration  604\n",
      "Learned weights =  [ 4.08178723 91.4896288  78.43694735]\n",
      "Done with gradient descent at iteration  604\n",
      "Learned weights =  [ 4.08178723 91.4896288  78.43694645]\n",
      "Done with gradient descent at iteration  605\n",
      "Learned weights =  [ 4.0885074  91.4896288  78.43694645]\n",
      "Done with gradient descent at iteration  605\n",
      "Learned weights =  [ 4.0885074  91.48962791 78.43694645]\n",
      "Done with gradient descent at iteration  605\n",
      "Learned weights =  [ 4.0885074  91.48962791 78.43694554]\n",
      "Done with gradient descent at iteration  606\n",
      "Learned weights =  [ 4.09522757 91.48962791 78.43694554]\n",
      "Done with gradient descent at iteration  606\n",
      "Learned weights =  [ 4.09522757 91.48962701 78.43694554]\n",
      "Done with gradient descent at iteration  606\n",
      "Learned weights =  [ 4.09522757 91.48962701 78.43694463]\n",
      "Done with gradient descent at iteration  607\n",
      "Learned weights =  [ 4.10194774 91.48962701 78.43694463]\n",
      "Done with gradient descent at iteration  607\n",
      "Learned weights =  [ 4.10194774 91.48962611 78.43694463]\n",
      "Done with gradient descent at iteration  607\n",
      "Learned weights =  [ 4.10194774 91.48962611 78.43694373]\n",
      "Done with gradient descent at iteration  608\n",
      "Learned weights =  [ 4.1086679  91.48962611 78.43694373]\n",
      "Done with gradient descent at iteration  608\n",
      "Learned weights =  [ 4.1086679  91.48962522 78.43694373]\n",
      "Done with gradient descent at iteration  608\n",
      "Learned weights =  [ 4.1086679  91.48962522 78.43694282]\n",
      "Done with gradient descent at iteration  609\n",
      "Learned weights =  [ 4.11538807 91.48962522 78.43694282]\n",
      "Done with gradient descent at iteration  609\n",
      "Learned weights =  [ 4.11538807 91.48962432 78.43694282]\n",
      "Done with gradient descent at iteration  609\n",
      "Learned weights =  [ 4.11538807 91.48962432 78.43694192]\n",
      "Done with gradient descent at iteration  610\n",
      "Learned weights =  [ 4.12210824 91.48962432 78.43694192]\n",
      "Done with gradient descent at iteration  610\n",
      "Learned weights =  [ 4.12210824 91.48962342 78.43694192]\n",
      "Done with gradient descent at iteration  610\n",
      "Learned weights =  [ 4.12210824 91.48962342 78.43694101]\n",
      "Done with gradient descent at iteration  611\n",
      "Learned weights =  [ 4.12882841 91.48962342 78.43694101]\n",
      "Done with gradient descent at iteration  611\n",
      "Learned weights =  [ 4.12882841 91.48962252 78.43694101]\n",
      "Done with gradient descent at iteration  611\n",
      "Learned weights =  [ 4.12882841 91.48962252 78.4369401 ]\n",
      "Done with gradient descent at iteration  612\n",
      "Learned weights =  [ 4.13554858 91.48962252 78.4369401 ]\n",
      "Done with gradient descent at iteration  612\n",
      "Learned weights =  [ 4.13554858 91.48962163 78.4369401 ]\n",
      "Done with gradient descent at iteration  612\n",
      "Learned weights =  [ 4.13554858 91.48962163 78.4369392 ]\n",
      "Done with gradient descent at iteration  613\n",
      "Learned weights =  [ 4.14226875 91.48962163 78.4369392 ]\n",
      "Done with gradient descent at iteration  613\n",
      "Learned weights =  [ 4.14226875 91.48962073 78.4369392 ]\n",
      "Done with gradient descent at iteration  613\n",
      "Learned weights =  [ 4.14226875 91.48962073 78.43693829]\n",
      "Done with gradient descent at iteration  614\n",
      "Learned weights =  [ 4.14898891 91.48962073 78.43693829]\n",
      "Done with gradient descent at iteration  614\n",
      "Learned weights =  [ 4.14898891 91.48961983 78.43693829]\n",
      "Done with gradient descent at iteration  614\n",
      "Learned weights =  [ 4.14898891 91.48961983 78.43693739]\n",
      "Done with gradient descent at iteration  615\n",
      "Learned weights =  [ 4.15570908 91.48961983 78.43693739]\n",
      "Done with gradient descent at iteration  615\n",
      "Learned weights =  [ 4.15570908 91.48961894 78.43693739]\n",
      "Done with gradient descent at iteration  615\n",
      "Learned weights =  [ 4.15570908 91.48961894 78.43693648]\n",
      "Done with gradient descent at iteration  616\n",
      "Learned weights =  [ 4.16242925 91.48961894 78.43693648]\n",
      "Done with gradient descent at iteration  616\n",
      "Learned weights =  [ 4.16242925 91.48961804 78.43693648]\n",
      "Done with gradient descent at iteration  616\n",
      "Learned weights =  [ 4.16242925 91.48961804 78.43693558]\n",
      "Done with gradient descent at iteration  617\n",
      "Learned weights =  [ 4.16914942 91.48961804 78.43693558]\n",
      "Done with gradient descent at iteration  617\n",
      "Learned weights =  [ 4.16914942 91.48961714 78.43693558]\n",
      "Done with gradient descent at iteration  617\n",
      "Learned weights =  [ 4.16914942 91.48961714 78.43693467]\n",
      "Done with gradient descent at iteration  618\n",
      "Learned weights =  [ 4.17586958 91.48961714 78.43693467]\n",
      "Done with gradient descent at iteration  618\n",
      "Learned weights =  [ 4.17586958 91.48961625 78.43693467]\n",
      "Done with gradient descent at iteration  618\n",
      "Learned weights =  [ 4.17586958 91.48961625 78.43693376]\n",
      "Done with gradient descent at iteration  619\n",
      "Learned weights =  [ 4.18258975 91.48961625 78.43693376]\n",
      "Done with gradient descent at iteration  619\n",
      "Learned weights =  [ 4.18258975 91.48961535 78.43693376]\n",
      "Done with gradient descent at iteration  619\n",
      "Learned weights =  [ 4.18258975 91.48961535 78.43693286]\n",
      "Done with gradient descent at iteration  620\n",
      "Learned weights =  [ 4.18930992 91.48961535 78.43693286]\n",
      "Done with gradient descent at iteration  620\n",
      "Learned weights =  [ 4.18930992 91.48961445 78.43693286]\n",
      "Done with gradient descent at iteration  620\n",
      "Learned weights =  [ 4.18930992 91.48961445 78.43693195]\n",
      "Done with gradient descent at iteration  621\n",
      "Learned weights =  [ 4.19603009 91.48961445 78.43693195]\n",
      "Done with gradient descent at iteration  621\n",
      "Learned weights =  [ 4.19603009 91.48961355 78.43693195]\n",
      "Done with gradient descent at iteration  621\n",
      "Learned weights =  [ 4.19603009 91.48961355 78.43693105]\n",
      "Done with gradient descent at iteration  622\n",
      "Learned weights =  [ 4.20275025 91.48961355 78.43693105]\n",
      "Done with gradient descent at iteration  622\n",
      "Learned weights =  [ 4.20275025 91.48961266 78.43693105]\n",
      "Done with gradient descent at iteration  622\n",
      "Learned weights =  [ 4.20275025 91.48961266 78.43693014]\n",
      "Done with gradient descent at iteration  623\n",
      "Learned weights =  [ 4.20947042 91.48961266 78.43693014]\n",
      "Done with gradient descent at iteration  623\n",
      "Learned weights =  [ 4.20947042 91.48961176 78.43693014]\n",
      "Done with gradient descent at iteration  623\n",
      "Learned weights =  [ 4.20947042 91.48961176 78.43692923]\n",
      "Done with gradient descent at iteration  624\n",
      "Learned weights =  [ 4.21619059 91.48961176 78.43692923]\n",
      "Done with gradient descent at iteration  624\n",
      "Learned weights =  [ 4.21619059 91.48961086 78.43692923]\n",
      "Done with gradient descent at iteration  624\n",
      "Learned weights =  [ 4.21619059 91.48961086 78.43692833]\n",
      "Done with gradient descent at iteration  625\n",
      "Learned weights =  [ 4.22291075 91.48961086 78.43692833]\n",
      "Done with gradient descent at iteration  625\n",
      "Learned weights =  [ 4.22291075 91.48960997 78.43692833]\n",
      "Done with gradient descent at iteration  625\n",
      "Learned weights =  [ 4.22291075 91.48960997 78.43692742]\n",
      "Done with gradient descent at iteration  626\n",
      "Learned weights =  [ 4.22963092 91.48960997 78.43692742]\n",
      "Done with gradient descent at iteration  626\n",
      "Learned weights =  [ 4.22963092 91.48960907 78.43692742]\n",
      "Done with gradient descent at iteration  626\n",
      "Learned weights =  [ 4.22963092 91.48960907 78.43692652]\n",
      "Done with gradient descent at iteration  627\n",
      "Learned weights =  [ 4.23635109 91.48960907 78.43692652]\n",
      "Done with gradient descent at iteration  627\n",
      "Learned weights =  [ 4.23635109 91.48960817 78.43692652]\n",
      "Done with gradient descent at iteration  627\n",
      "Learned weights =  [ 4.23635109 91.48960817 78.43692561]\n",
      "Done with gradient descent at iteration  628\n",
      "Learned weights =  [ 4.24307125 91.48960817 78.43692561]\n",
      "Done with gradient descent at iteration  628\n",
      "Learned weights =  [ 4.24307125 91.48960728 78.43692561]\n",
      "Done with gradient descent at iteration  628\n",
      "Learned weights =  [ 4.24307125 91.48960728 78.4369247 ]\n",
      "Done with gradient descent at iteration  629\n",
      "Learned weights =  [ 4.24979142 91.48960728 78.4369247 ]\n",
      "Done with gradient descent at iteration  629\n",
      "Learned weights =  [ 4.24979142 91.48960638 78.4369247 ]\n",
      "Done with gradient descent at iteration  629\n",
      "Learned weights =  [ 4.24979142 91.48960638 78.4369238 ]\n",
      "Done with gradient descent at iteration  630\n",
      "Learned weights =  [ 4.25651159 91.48960638 78.4369238 ]\n",
      "Done with gradient descent at iteration  630\n",
      "Learned weights =  [ 4.25651159 91.48960548 78.4369238 ]\n",
      "Done with gradient descent at iteration  630\n",
      "Learned weights =  [ 4.25651159 91.48960548 78.43692289]\n",
      "Done with gradient descent at iteration  631\n",
      "Learned weights =  [ 4.26323175 91.48960548 78.43692289]\n",
      "Done with gradient descent at iteration  631\n",
      "Learned weights =  [ 4.26323175 91.48960459 78.43692289]\n",
      "Done with gradient descent at iteration  631\n",
      "Learned weights =  [ 4.26323175 91.48960459 78.43692199]\n",
      "Done with gradient descent at iteration  632\n",
      "Learned weights =  [ 4.26995192 91.48960459 78.43692199]\n",
      "Done with gradient descent at iteration  632\n",
      "Learned weights =  [ 4.26995192 91.48960369 78.43692199]\n",
      "Done with gradient descent at iteration  632\n",
      "Learned weights =  [ 4.26995192 91.48960369 78.43692108]\n",
      "Done with gradient descent at iteration  633\n",
      "Learned weights =  [ 4.27667208 91.48960369 78.43692108]\n",
      "Done with gradient descent at iteration  633\n",
      "Learned weights =  [ 4.27667208 91.48960279 78.43692108]\n",
      "Done with gradient descent at iteration  633\n",
      "Learned weights =  [ 4.27667208 91.48960279 78.43692017]\n",
      "Done with gradient descent at iteration  634\n",
      "Learned weights =  [ 4.28339225 91.48960279 78.43692017]\n",
      "Done with gradient descent at iteration  634\n",
      "Learned weights =  [ 4.28339225 91.48960189 78.43692017]\n",
      "Done with gradient descent at iteration  634\n",
      "Learned weights =  [ 4.28339225 91.48960189 78.43691927]\n",
      "Done with gradient descent at iteration  635\n",
      "Learned weights =  [ 4.29011242 91.48960189 78.43691927]\n",
      "Done with gradient descent at iteration  635\n",
      "Learned weights =  [ 4.29011242 91.489601   78.43691927]\n",
      "Done with gradient descent at iteration  635\n",
      "Learned weights =  [ 4.29011242 91.489601   78.43691836]\n",
      "Done with gradient descent at iteration  636\n",
      "Learned weights =  [ 4.29683258 91.489601   78.43691836]\n",
      "Done with gradient descent at iteration  636\n",
      "Learned weights =  [ 4.29683258 91.4896001  78.43691836]\n",
      "Done with gradient descent at iteration  636\n",
      "Learned weights =  [ 4.29683258 91.4896001  78.43691746]\n",
      "Done with gradient descent at iteration  637\n",
      "Learned weights =  [ 4.30355275 91.4896001  78.43691746]\n",
      "Done with gradient descent at iteration  637\n",
      "Learned weights =  [ 4.30355275 91.4895992  78.43691746]\n",
      "Done with gradient descent at iteration  637\n",
      "Learned weights =  [ 4.30355275 91.4895992  78.43691655]\n",
      "Done with gradient descent at iteration  638\n",
      "Learned weights =  [ 4.31027291 91.4895992  78.43691655]\n",
      "Done with gradient descent at iteration  638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learned weights =  [ 4.31027291 91.48959831 78.43691655]\n",
      "Done with gradient descent at iteration  638\n",
      "Learned weights =  [ 4.31027291 91.48959831 78.43691564]\n",
      "Done with gradient descent at iteration  639\n",
      "Learned weights =  [ 4.31699308 91.48959831 78.43691564]\n",
      "Done with gradient descent at iteration  639\n",
      "Learned weights =  [ 4.31699308 91.48959741 78.43691564]\n",
      "Done with gradient descent at iteration  639\n",
      "Learned weights =  [ 4.31699308 91.48959741 78.43691474]\n",
      "Done with gradient descent at iteration  640\n",
      "Learned weights =  [ 4.32371324 91.48959741 78.43691474]\n",
      "Done with gradient descent at iteration  640\n",
      "Learned weights =  [ 4.32371324 91.48959651 78.43691474]\n",
      "Done with gradient descent at iteration  640\n",
      "Learned weights =  [ 4.32371324 91.48959651 78.43691383]\n",
      "Done with gradient descent at iteration  641\n",
      "Learned weights =  [ 4.33043341 91.48959651 78.43691383]\n",
      "Done with gradient descent at iteration  641\n",
      "Learned weights =  [ 4.33043341 91.48959562 78.43691383]\n",
      "Done with gradient descent at iteration  641\n",
      "Learned weights =  [ 4.33043341 91.48959562 78.43691293]\n",
      "Done with gradient descent at iteration  642\n",
      "Learned weights =  [ 4.33715357 91.48959562 78.43691293]\n",
      "Done with gradient descent at iteration  642\n",
      "Learned weights =  [ 4.33715357 91.48959472 78.43691293]\n",
      "Done with gradient descent at iteration  642\n",
      "Learned weights =  [ 4.33715357 91.48959472 78.43691202]\n",
      "Done with gradient descent at iteration  643\n",
      "Learned weights =  [ 4.34387374 91.48959472 78.43691202]\n",
      "Done with gradient descent at iteration  643\n",
      "Learned weights =  [ 4.34387374 91.48959382 78.43691202]\n",
      "Done with gradient descent at iteration  643\n",
      "Learned weights =  [ 4.34387374 91.48959382 78.43691111]\n",
      "Done with gradient descent at iteration  644\n",
      "Learned weights =  [ 4.3505939  91.48959382 78.43691111]\n",
      "Done with gradient descent at iteration  644\n",
      "Learned weights =  [ 4.3505939  91.48959292 78.43691111]\n",
      "Done with gradient descent at iteration  644\n",
      "Learned weights =  [ 4.3505939  91.48959292 78.43691021]\n",
      "Done with gradient descent at iteration  645\n",
      "Learned weights =  [ 4.35731407 91.48959292 78.43691021]\n",
      "Done with gradient descent at iteration  645\n",
      "Learned weights =  [ 4.35731407 91.48959203 78.43691021]\n",
      "Done with gradient descent at iteration  645\n",
      "Learned weights =  [ 4.35731407 91.48959203 78.4369093 ]\n",
      "Done with gradient descent at iteration  646\n",
      "Learned weights =  [ 4.36403423 91.48959203 78.4369093 ]\n",
      "Done with gradient descent at iteration  646\n",
      "Learned weights =  [ 4.36403423 91.48959113 78.4369093 ]\n",
      "Done with gradient descent at iteration  646\n",
      "Learned weights =  [ 4.36403423 91.48959113 78.4369084 ]\n",
      "Done with gradient descent at iteration  647\n",
      "Learned weights =  [ 4.3707544  91.48959113 78.4369084 ]\n",
      "Done with gradient descent at iteration  647\n",
      "Learned weights =  [ 4.3707544  91.48959023 78.4369084 ]\n",
      "Done with gradient descent at iteration  647\n",
      "Learned weights =  [ 4.3707544  91.48959023 78.43690749]\n",
      "Done with gradient descent at iteration  648\n",
      "Learned weights =  [ 4.37747456 91.48959023 78.43690749]\n",
      "Done with gradient descent at iteration  648\n",
      "Learned weights =  [ 4.37747456 91.48958934 78.43690749]\n",
      "Done with gradient descent at iteration  648\n",
      "Learned weights =  [ 4.37747456 91.48958934 78.43690658]\n",
      "Done with gradient descent at iteration  649\n",
      "Learned weights =  [ 4.38419472 91.48958934 78.43690658]\n",
      "Done with gradient descent at iteration  649\n",
      "Learned weights =  [ 4.38419472 91.48958844 78.43690658]\n",
      "Done with gradient descent at iteration  649\n",
      "Learned weights =  [ 4.38419472 91.48958844 78.43690568]\n",
      "Done with gradient descent at iteration  650\n",
      "Learned weights =  [ 4.39091489 91.48958844 78.43690568]\n",
      "Done with gradient descent at iteration  650\n",
      "Learned weights =  [ 4.39091489 91.48958754 78.43690568]\n",
      "Done with gradient descent at iteration  650\n",
      "Learned weights =  [ 4.39091489 91.48958754 78.43690477]\n",
      "Done with gradient descent at iteration  651\n",
      "Learned weights =  [ 4.39763505 91.48958754 78.43690477]\n",
      "Done with gradient descent at iteration  651\n",
      "Learned weights =  [ 4.39763505 91.48958665 78.43690477]\n",
      "Done with gradient descent at iteration  651\n",
      "Learned weights =  [ 4.39763505 91.48958665 78.43690387]\n",
      "Done with gradient descent at iteration  652\n",
      "Learned weights =  [ 4.40435522 91.48958665 78.43690387]\n",
      "Done with gradient descent at iteration  652\n",
      "Learned weights =  [ 4.40435522 91.48958575 78.43690387]\n",
      "Done with gradient descent at iteration  652\n",
      "Learned weights =  [ 4.40435522 91.48958575 78.43690296]\n",
      "Done with gradient descent at iteration  653\n",
      "Learned weights =  [ 4.41107538 91.48958575 78.43690296]\n",
      "Done with gradient descent at iteration  653\n",
      "Learned weights =  [ 4.41107538 91.48958485 78.43690296]\n",
      "Done with gradient descent at iteration  653\n",
      "Learned weights =  [ 4.41107538 91.48958485 78.43690205]\n",
      "Done with gradient descent at iteration  654\n",
      "Learned weights =  [ 4.41779554 91.48958485 78.43690205]\n",
      "Done with gradient descent at iteration  654\n",
      "Learned weights =  [ 4.41779554 91.48958396 78.43690205]\n",
      "Done with gradient descent at iteration  654\n",
      "Learned weights =  [ 4.41779554 91.48958396 78.43690115]\n",
      "Done with gradient descent at iteration  655\n",
      "Learned weights =  [ 4.42451571 91.48958396 78.43690115]\n",
      "Done with gradient descent at iteration  655\n",
      "Learned weights =  [ 4.42451571 91.48958306 78.43690115]\n",
      "Done with gradient descent at iteration  655\n",
      "Learned weights =  [ 4.42451571 91.48958306 78.43690024]\n",
      "Done with gradient descent at iteration  656\n",
      "Learned weights =  [ 4.43123587 91.48958306 78.43690024]\n",
      "Done with gradient descent at iteration  656\n",
      "Learned weights =  [ 4.43123587 91.48958216 78.43690024]\n",
      "Done with gradient descent at iteration  656\n",
      "Learned weights =  [ 4.43123587 91.48958216 78.43689934]\n",
      "Done with gradient descent at iteration  657\n",
      "Learned weights =  [ 4.43795603 91.48958216 78.43689934]\n",
      "Done with gradient descent at iteration  657\n",
      "Learned weights =  [ 4.43795603 91.48958126 78.43689934]\n",
      "Done with gradient descent at iteration  657\n",
      "Learned weights =  [ 4.43795603 91.48958126 78.43689843]\n",
      "Done with gradient descent at iteration  658\n",
      "Learned weights =  [ 4.4446762  91.48958126 78.43689843]\n",
      "Done with gradient descent at iteration  658\n",
      "Learned weights =  [ 4.4446762  91.48958037 78.43689843]\n",
      "Done with gradient descent at iteration  658\n",
      "Learned weights =  [ 4.4446762  91.48958037 78.43689752]\n",
      "Done with gradient descent at iteration  659\n",
      "Learned weights =  [ 4.45139636 91.48958037 78.43689752]\n",
      "Done with gradient descent at iteration  659\n",
      "Learned weights =  [ 4.45139636 91.48957947 78.43689752]\n",
      "Done with gradient descent at iteration  659\n",
      "Learned weights =  [ 4.45139636 91.48957947 78.43689662]\n",
      "Done with gradient descent at iteration  660\n",
      "Learned weights =  [ 4.45811652 91.48957947 78.43689662]\n",
      "Done with gradient descent at iteration  660\n",
      "Learned weights =  [ 4.45811652 91.48957857 78.43689662]\n",
      "Done with gradient descent at iteration  660\n",
      "Learned weights =  [ 4.45811652 91.48957857 78.43689571]\n",
      "Done with gradient descent at iteration  661\n",
      "Learned weights =  [ 4.46483669 91.48957857 78.43689571]\n",
      "Done with gradient descent at iteration  661\n",
      "Learned weights =  [ 4.46483669 91.48957768 78.43689571]\n",
      "Done with gradient descent at iteration  661\n",
      "Learned weights =  [ 4.46483669 91.48957768 78.43689481]\n",
      "Done with gradient descent at iteration  662\n",
      "Learned weights =  [ 4.47155685 91.48957768 78.43689481]\n",
      "Done with gradient descent at iteration  662\n",
      "Learned weights =  [ 4.47155685 91.48957678 78.43689481]\n",
      "Done with gradient descent at iteration  662\n",
      "Learned weights =  [ 4.47155685 91.48957678 78.4368939 ]\n",
      "Done with gradient descent at iteration  663\n",
      "Learned weights =  [ 4.47827701 91.48957678 78.4368939 ]\n",
      "Done with gradient descent at iteration  663\n",
      "Learned weights =  [ 4.47827701 91.48957588 78.4368939 ]\n",
      "Done with gradient descent at iteration  663\n",
      "Learned weights =  [ 4.47827701 91.48957588 78.43689299]\n",
      "Done with gradient descent at iteration  664\n",
      "Learned weights =  [ 4.48499718 91.48957588 78.43689299]\n",
      "Done with gradient descent at iteration  664\n",
      "Learned weights =  [ 4.48499718 91.48957499 78.43689299]\n",
      "Done with gradient descent at iteration  664\n",
      "Learned weights =  [ 4.48499718 91.48957499 78.43689209]\n",
      "Done with gradient descent at iteration  665\n",
      "Learned weights =  [ 4.49171734 91.48957499 78.43689209]\n",
      "Done with gradient descent at iteration  665\n",
      "Learned weights =  [ 4.49171734 91.48957409 78.43689209]\n",
      "Done with gradient descent at iteration  665\n",
      "Learned weights =  [ 4.49171734 91.48957409 78.43689118]\n",
      "Done with gradient descent at iteration  666\n",
      "Learned weights =  [ 4.4984375  91.48957409 78.43689118]\n",
      "Done with gradient descent at iteration  666\n",
      "Learned weights =  [ 4.4984375  91.48957319 78.43689118]\n",
      "Done with gradient descent at iteration  666\n",
      "Learned weights =  [ 4.4984375  91.48957319 78.43689028]\n",
      "Done with gradient descent at iteration  667\n",
      "Learned weights =  [ 4.50515766 91.48957319 78.43689028]\n",
      "Done with gradient descent at iteration  667\n",
      "Learned weights =  [ 4.50515766 91.4895723  78.43689028]\n",
      "Done with gradient descent at iteration  667\n",
      "Learned weights =  [ 4.50515766 91.4895723  78.43688937]\n",
      "Done with gradient descent at iteration  668\n",
      "Learned weights =  [ 4.51187782 91.4895723  78.43688937]\n",
      "Done with gradient descent at iteration  668\n",
      "Learned weights =  [ 4.51187782 91.4895714  78.43688937]\n",
      "Done with gradient descent at iteration  668\n",
      "Learned weights =  [ 4.51187782 91.4895714  78.43688846]\n",
      "Done with gradient descent at iteration  669\n",
      "Learned weights =  [ 4.51859799 91.4895714  78.43688846]\n",
      "Done with gradient descent at iteration  669\n",
      "Learned weights =  [ 4.51859799 91.4895705  78.43688846]\n",
      "Done with gradient descent at iteration  669\n",
      "Learned weights =  [ 4.51859799 91.4895705  78.43688756]\n",
      "Done with gradient descent at iteration  670\n",
      "Learned weights =  [ 4.52531815 91.4895705  78.43688756]\n",
      "Done with gradient descent at iteration  670\n",
      "Learned weights =  [ 4.52531815 91.4895696  78.43688756]\n",
      "Done with gradient descent at iteration  670\n",
      "Learned weights =  [ 4.52531815 91.4895696  78.43688665]\n",
      "Done with gradient descent at iteration  671\n",
      "Learned weights =  [ 4.53203831 91.4895696  78.43688665]\n",
      "Done with gradient descent at iteration  671\n",
      "Learned weights =  [ 4.53203831 91.48956871 78.43688665]\n",
      "Done with gradient descent at iteration  671\n",
      "Learned weights =  [ 4.53203831 91.48956871 78.43688575]\n",
      "Done with gradient descent at iteration  672\n",
      "Learned weights =  [ 4.53875847 91.48956871 78.43688575]\n",
      "Done with gradient descent at iteration  672\n",
      "Learned weights =  [ 4.53875847 91.48956781 78.43688575]\n",
      "Done with gradient descent at iteration  672\n",
      "Learned weights =  [ 4.53875847 91.48956781 78.43688484]\n",
      "Done with gradient descent at iteration  673\n",
      "Learned weights =  [ 4.54547863 91.48956781 78.43688484]\n",
      "Done with gradient descent at iteration  673\n",
      "Learned weights =  [ 4.54547863 91.48956691 78.43688484]\n",
      "Done with gradient descent at iteration  673\n",
      "Learned weights =  [ 4.54547863 91.48956691 78.43688393]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  674\n",
      "Learned weights =  [ 4.5521988  91.48956691 78.43688393]\n",
      "Done with gradient descent at iteration  674\n",
      "Learned weights =  [ 4.5521988  91.48956602 78.43688393]\n",
      "Done with gradient descent at iteration  674\n",
      "Learned weights =  [ 4.5521988  91.48956602 78.43688303]\n",
      "Done with gradient descent at iteration  675\n",
      "Learned weights =  [ 4.55891896 91.48956602 78.43688303]\n",
      "Done with gradient descent at iteration  675\n",
      "Learned weights =  [ 4.55891896 91.48956512 78.43688303]\n",
      "Done with gradient descent at iteration  675\n",
      "Learned weights =  [ 4.55891896 91.48956512 78.43688212]\n",
      "Done with gradient descent at iteration  676\n",
      "Learned weights =  [ 4.56563912 91.48956512 78.43688212]\n",
      "Done with gradient descent at iteration  676\n",
      "Learned weights =  [ 4.56563912 91.48956422 78.43688212]\n",
      "Done with gradient descent at iteration  676\n",
      "Learned weights =  [ 4.56563912 91.48956422 78.43688122]\n",
      "Done with gradient descent at iteration  677\n",
      "Learned weights =  [ 4.57235928 91.48956422 78.43688122]\n",
      "Done with gradient descent at iteration  677\n",
      "Learned weights =  [ 4.57235928 91.48956333 78.43688122]\n",
      "Done with gradient descent at iteration  677\n",
      "Learned weights =  [ 4.57235928 91.48956333 78.43688031]\n",
      "Done with gradient descent at iteration  678\n",
      "Learned weights =  [ 4.57907944 91.48956333 78.43688031]\n",
      "Done with gradient descent at iteration  678\n",
      "Learned weights =  [ 4.57907944 91.48956243 78.43688031]\n",
      "Done with gradient descent at iteration  678\n",
      "Learned weights =  [ 4.57907944 91.48956243 78.4368794 ]\n",
      "Done with gradient descent at iteration  679\n",
      "Learned weights =  [ 4.5857996  91.48956243 78.4368794 ]\n",
      "Done with gradient descent at iteration  679\n",
      "Learned weights =  [ 4.5857996  91.48956153 78.4368794 ]\n",
      "Done with gradient descent at iteration  679\n",
      "Learned weights =  [ 4.5857996  91.48956153 78.4368785 ]\n",
      "Done with gradient descent at iteration  680\n",
      "Learned weights =  [ 4.59251976 91.48956153 78.4368785 ]\n",
      "Done with gradient descent at iteration  680\n",
      "Learned weights =  [ 4.59251976 91.48956063 78.4368785 ]\n",
      "Done with gradient descent at iteration  680\n",
      "Learned weights =  [ 4.59251976 91.48956063 78.43687759]\n",
      "Done with gradient descent at iteration  681\n",
      "Learned weights =  [ 4.59923992 91.48956063 78.43687759]\n",
      "Done with gradient descent at iteration  681\n",
      "Learned weights =  [ 4.59923992 91.48955974 78.43687759]\n",
      "Done with gradient descent at iteration  681\n",
      "Learned weights =  [ 4.59923992 91.48955974 78.43687669]\n",
      "Done with gradient descent at iteration  682\n",
      "Learned weights =  [ 4.60596008 91.48955974 78.43687669]\n",
      "Done with gradient descent at iteration  682\n",
      "Learned weights =  [ 4.60596008 91.48955884 78.43687669]\n",
      "Done with gradient descent at iteration  682\n",
      "Learned weights =  [ 4.60596008 91.48955884 78.43687578]\n",
      "Done with gradient descent at iteration  683\n",
      "Learned weights =  [ 4.61268024 91.48955884 78.43687578]\n",
      "Done with gradient descent at iteration  683\n",
      "Learned weights =  [ 4.61268024 91.48955794 78.43687578]\n",
      "Done with gradient descent at iteration  683\n",
      "Learned weights =  [ 4.61268024 91.48955794 78.43687487]\n",
      "Done with gradient descent at iteration  684\n",
      "Learned weights =  [ 4.61940041 91.48955794 78.43687487]\n",
      "Done with gradient descent at iteration  684\n",
      "Learned weights =  [ 4.61940041 91.48955705 78.43687487]\n",
      "Done with gradient descent at iteration  684\n",
      "Learned weights =  [ 4.61940041 91.48955705 78.43687397]\n",
      "Done with gradient descent at iteration  685\n",
      "Learned weights =  [ 4.62612057 91.48955705 78.43687397]\n",
      "Done with gradient descent at iteration  685\n",
      "Learned weights =  [ 4.62612057 91.48955615 78.43687397]\n",
      "Done with gradient descent at iteration  685\n",
      "Learned weights =  [ 4.62612057 91.48955615 78.43687306]\n",
      "Done with gradient descent at iteration  686\n",
      "Learned weights =  [ 4.63284073 91.48955615 78.43687306]\n",
      "Done with gradient descent at iteration  686\n",
      "Learned weights =  [ 4.63284073 91.48955525 78.43687306]\n",
      "Done with gradient descent at iteration  686\n",
      "Learned weights =  [ 4.63284073 91.48955525 78.43687216]\n",
      "Done with gradient descent at iteration  687\n",
      "Learned weights =  [ 4.63956089 91.48955525 78.43687216]\n",
      "Done with gradient descent at iteration  687\n",
      "Learned weights =  [ 4.63956089 91.48955436 78.43687216]\n",
      "Done with gradient descent at iteration  687\n",
      "Learned weights =  [ 4.63956089 91.48955436 78.43687125]\n",
      "Done with gradient descent at iteration  688\n",
      "Learned weights =  [ 4.64628105 91.48955436 78.43687125]\n",
      "Done with gradient descent at iteration  688\n",
      "Learned weights =  [ 4.64628105 91.48955346 78.43687125]\n",
      "Done with gradient descent at iteration  688\n",
      "Learned weights =  [ 4.64628105 91.48955346 78.43687035]\n",
      "Done with gradient descent at iteration  689\n",
      "Learned weights =  [ 4.65300121 91.48955346 78.43687035]\n",
      "Done with gradient descent at iteration  689\n",
      "Learned weights =  [ 4.65300121 91.48955256 78.43687035]\n",
      "Done with gradient descent at iteration  689\n",
      "Learned weights =  [ 4.65300121 91.48955256 78.43686944]\n",
      "Done with gradient descent at iteration  690\n",
      "Learned weights =  [ 4.65972137 91.48955256 78.43686944]\n",
      "Done with gradient descent at iteration  690\n",
      "Learned weights =  [ 4.65972137 91.48955167 78.43686944]\n",
      "Done with gradient descent at iteration  690\n",
      "Learned weights =  [ 4.65972137 91.48955167 78.43686853]\n",
      "Done with gradient descent at iteration  691\n",
      "Learned weights =  [ 4.66644153 91.48955167 78.43686853]\n",
      "Done with gradient descent at iteration  691\n",
      "Learned weights =  [ 4.66644153 91.48955077 78.43686853]\n",
      "Done with gradient descent at iteration  691\n",
      "Learned weights =  [ 4.66644153 91.48955077 78.43686763]\n",
      "Done with gradient descent at iteration  692\n",
      "Learned weights =  [ 4.67316169 91.48955077 78.43686763]\n",
      "Done with gradient descent at iteration  692\n",
      "Learned weights =  [ 4.67316169 91.48954987 78.43686763]\n",
      "Done with gradient descent at iteration  692\n",
      "Learned weights =  [ 4.67316169 91.48954987 78.43686672]\n",
      "Done with gradient descent at iteration  693\n",
      "Learned weights =  [ 4.67988184 91.48954987 78.43686672]\n",
      "Done with gradient descent at iteration  693\n",
      "Learned weights =  [ 4.67988184 91.48954897 78.43686672]\n",
      "Done with gradient descent at iteration  693\n",
      "Learned weights =  [ 4.67988184 91.48954897 78.43686582]\n",
      "Done with gradient descent at iteration  694\n",
      "Learned weights =  [ 4.686602   91.48954897 78.43686582]\n",
      "Done with gradient descent at iteration  694\n",
      "Learned weights =  [ 4.686602   91.48954808 78.43686582]\n",
      "Done with gradient descent at iteration  694\n",
      "Learned weights =  [ 4.686602   91.48954808 78.43686491]\n",
      "Done with gradient descent at iteration  695\n",
      "Learned weights =  [ 4.69332216 91.48954808 78.43686491]\n",
      "Done with gradient descent at iteration  695\n",
      "Learned weights =  [ 4.69332216 91.48954718 78.43686491]\n",
      "Done with gradient descent at iteration  695\n",
      "Learned weights =  [ 4.69332216 91.48954718 78.436864  ]\n",
      "Done with gradient descent at iteration  696\n",
      "Learned weights =  [ 4.70004232 91.48954718 78.436864  ]\n",
      "Done with gradient descent at iteration  696\n",
      "Learned weights =  [ 4.70004232 91.48954628 78.436864  ]\n",
      "Done with gradient descent at iteration  696\n",
      "Learned weights =  [ 4.70004232 91.48954628 78.4368631 ]\n",
      "Done with gradient descent at iteration  697\n",
      "Learned weights =  [ 4.70676248 91.48954628 78.4368631 ]\n",
      "Done with gradient descent at iteration  697\n",
      "Learned weights =  [ 4.70676248 91.48954539 78.4368631 ]\n",
      "Done with gradient descent at iteration  697\n",
      "Learned weights =  [ 4.70676248 91.48954539 78.43686219]\n",
      "Done with gradient descent at iteration  698\n",
      "Learned weights =  [ 4.71348264 91.48954539 78.43686219]\n",
      "Done with gradient descent at iteration  698\n",
      "Learned weights =  [ 4.71348264 91.48954449 78.43686219]\n",
      "Done with gradient descent at iteration  698\n",
      "Learned weights =  [ 4.71348264 91.48954449 78.43686129]\n",
      "Done with gradient descent at iteration  699\n",
      "Learned weights =  [ 4.7202028  91.48954449 78.43686129]\n",
      "Done with gradient descent at iteration  699\n",
      "Learned weights =  [ 4.7202028  91.48954359 78.43686129]\n",
      "Done with gradient descent at iteration  699\n",
      "Learned weights =  [ 4.7202028  91.48954359 78.43686038]\n",
      "Iteration = 700\n",
      "Cost function =  3605263276455942.0\n",
      "Done with gradient descent at iteration  700\n",
      "Learned weights =  [ 4.72692296 91.48954359 78.43686038]\n",
      "Done with gradient descent at iteration  700\n",
      "Learned weights =  [ 4.72692296 91.4895427  78.43686038]\n",
      "Done with gradient descent at iteration  700\n",
      "Learned weights =  [ 4.72692296 91.4895427  78.43685947]\n",
      "Done with gradient descent at iteration  701\n",
      "Learned weights =  [ 4.73364312 91.4895427  78.43685947]\n",
      "Done with gradient descent at iteration  701\n",
      "Learned weights =  [ 4.73364312 91.4895418  78.43685947]\n",
      "Done with gradient descent at iteration  701\n",
      "Learned weights =  [ 4.73364312 91.4895418  78.43685857]\n",
      "Done with gradient descent at iteration  702\n",
      "Learned weights =  [ 4.74036328 91.4895418  78.43685857]\n",
      "Done with gradient descent at iteration  702\n",
      "Learned weights =  [ 4.74036328 91.4895409  78.43685857]\n",
      "Done with gradient descent at iteration  702\n",
      "Learned weights =  [ 4.74036328 91.4895409  78.43685766]\n",
      "Done with gradient descent at iteration  703\n",
      "Learned weights =  [ 4.74708343 91.4895409  78.43685766]\n",
      "Done with gradient descent at iteration  703\n",
      "Learned weights =  [ 4.74708343 91.48954    78.43685766]\n",
      "Done with gradient descent at iteration  703\n",
      "Learned weights =  [ 4.74708343 91.48954    78.43685676]\n",
      "Done with gradient descent at iteration  704\n",
      "Learned weights =  [ 4.75380359 91.48954    78.43685676]\n",
      "Done with gradient descent at iteration  704\n",
      "Learned weights =  [ 4.75380359 91.48953911 78.43685676]\n",
      "Done with gradient descent at iteration  704\n",
      "Learned weights =  [ 4.75380359 91.48953911 78.43685585]\n",
      "Done with gradient descent at iteration  705\n",
      "Learned weights =  [ 4.76052375 91.48953911 78.43685585]\n",
      "Done with gradient descent at iteration  705\n",
      "Learned weights =  [ 4.76052375 91.48953821 78.43685585]\n",
      "Done with gradient descent at iteration  705\n",
      "Learned weights =  [ 4.76052375 91.48953821 78.43685494]\n",
      "Done with gradient descent at iteration  706\n",
      "Learned weights =  [ 4.76724391 91.48953821 78.43685494]\n",
      "Done with gradient descent at iteration  706\n",
      "Learned weights =  [ 4.76724391 91.48953731 78.43685494]\n",
      "Done with gradient descent at iteration  706\n",
      "Learned weights =  [ 4.76724391 91.48953731 78.43685404]\n",
      "Done with gradient descent at iteration  707\n",
      "Learned weights =  [ 4.77396407 91.48953731 78.43685404]\n",
      "Done with gradient descent at iteration  707\n",
      "Learned weights =  [ 4.77396407 91.48953642 78.43685404]\n",
      "Done with gradient descent at iteration  707\n",
      "Learned weights =  [ 4.77396407 91.48953642 78.43685313]\n",
      "Done with gradient descent at iteration  708\n",
      "Learned weights =  [ 4.78068423 91.48953642 78.43685313]\n",
      "Done with gradient descent at iteration  708\n",
      "Learned weights =  [ 4.78068423 91.48953552 78.43685313]\n",
      "Done with gradient descent at iteration  708\n",
      "Learned weights =  [ 4.78068423 91.48953552 78.43685223]\n",
      "Done with gradient descent at iteration  709\n",
      "Learned weights =  [ 4.78740438 91.48953552 78.43685223]\n",
      "Done with gradient descent at iteration  709\n",
      "Learned weights =  [ 4.78740438 91.48953462 78.43685223]\n",
      "Done with gradient descent at iteration  709\n",
      "Learned weights =  [ 4.78740438 91.48953462 78.43685132]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  710\n",
      "Learned weights =  [ 4.79412454 91.48953462 78.43685132]\n",
      "Done with gradient descent at iteration  710\n",
      "Learned weights =  [ 4.79412454 91.48953373 78.43685132]\n",
      "Done with gradient descent at iteration  710\n",
      "Learned weights =  [ 4.79412454 91.48953373 78.43685041]\n",
      "Done with gradient descent at iteration  711\n",
      "Learned weights =  [ 4.8008447  91.48953373 78.43685041]\n",
      "Done with gradient descent at iteration  711\n",
      "Learned weights =  [ 4.8008447  91.48953283 78.43685041]\n",
      "Done with gradient descent at iteration  711\n",
      "Learned weights =  [ 4.8008447  91.48953283 78.43684951]\n",
      "Done with gradient descent at iteration  712\n",
      "Learned weights =  [ 4.80756486 91.48953283 78.43684951]\n",
      "Done with gradient descent at iteration  712\n",
      "Learned weights =  [ 4.80756486 91.48953193 78.43684951]\n",
      "Done with gradient descent at iteration  712\n",
      "Learned weights =  [ 4.80756486 91.48953193 78.4368486 ]\n",
      "Done with gradient descent at iteration  713\n",
      "Learned weights =  [ 4.81428501 91.48953193 78.4368486 ]\n",
      "Done with gradient descent at iteration  713\n",
      "Learned weights =  [ 4.81428501 91.48953104 78.4368486 ]\n",
      "Done with gradient descent at iteration  713\n",
      "Learned weights =  [ 4.81428501 91.48953104 78.4368477 ]\n",
      "Done with gradient descent at iteration  714\n",
      "Learned weights =  [ 4.82100517 91.48953104 78.4368477 ]\n",
      "Done with gradient descent at iteration  714\n",
      "Learned weights =  [ 4.82100517 91.48953014 78.4368477 ]\n",
      "Done with gradient descent at iteration  714\n",
      "Learned weights =  [ 4.82100517 91.48953014 78.43684679]\n",
      "Done with gradient descent at iteration  715\n",
      "Learned weights =  [ 4.82772533 91.48953014 78.43684679]\n",
      "Done with gradient descent at iteration  715\n",
      "Learned weights =  [ 4.82772533 91.48952924 78.43684679]\n",
      "Done with gradient descent at iteration  715\n",
      "Learned weights =  [ 4.82772533 91.48952924 78.43684588]\n",
      "Done with gradient descent at iteration  716\n",
      "Learned weights =  [ 4.83444548 91.48952924 78.43684588]\n",
      "Done with gradient descent at iteration  716\n",
      "Learned weights =  [ 4.83444548 91.48952834 78.43684588]\n",
      "Done with gradient descent at iteration  716\n",
      "Learned weights =  [ 4.83444548 91.48952834 78.43684498]\n",
      "Done with gradient descent at iteration  717\n",
      "Learned weights =  [ 4.84116564 91.48952834 78.43684498]\n",
      "Done with gradient descent at iteration  717\n",
      "Learned weights =  [ 4.84116564 91.48952745 78.43684498]\n",
      "Done with gradient descent at iteration  717\n",
      "Learned weights =  [ 4.84116564 91.48952745 78.43684407]\n",
      "Done with gradient descent at iteration  718\n",
      "Learned weights =  [ 4.8478858  91.48952745 78.43684407]\n",
      "Done with gradient descent at iteration  718\n",
      "Learned weights =  [ 4.8478858  91.48952655 78.43684407]\n",
      "Done with gradient descent at iteration  718\n",
      "Learned weights =  [ 4.8478858  91.48952655 78.43684317]\n",
      "Done with gradient descent at iteration  719\n",
      "Learned weights =  [ 4.85460596 91.48952655 78.43684317]\n",
      "Done with gradient descent at iteration  719\n",
      "Learned weights =  [ 4.85460596 91.48952565 78.43684317]\n",
      "Done with gradient descent at iteration  719\n",
      "Learned weights =  [ 4.85460596 91.48952565 78.43684226]\n",
      "Done with gradient descent at iteration  720\n",
      "Learned weights =  [ 4.86132611 91.48952565 78.43684226]\n",
      "Done with gradient descent at iteration  720\n",
      "Learned weights =  [ 4.86132611 91.48952476 78.43684226]\n",
      "Done with gradient descent at iteration  720\n",
      "Learned weights =  [ 4.86132611 91.48952476 78.43684135]\n",
      "Done with gradient descent at iteration  721\n",
      "Learned weights =  [ 4.86804627 91.48952476 78.43684135]\n",
      "Done with gradient descent at iteration  721\n",
      "Learned weights =  [ 4.86804627 91.48952386 78.43684135]\n",
      "Done with gradient descent at iteration  721\n",
      "Learned weights =  [ 4.86804627 91.48952386 78.43684045]\n",
      "Done with gradient descent at iteration  722\n",
      "Learned weights =  [ 4.87476643 91.48952386 78.43684045]\n",
      "Done with gradient descent at iteration  722\n",
      "Learned weights =  [ 4.87476643 91.48952296 78.43684045]\n",
      "Done with gradient descent at iteration  722\n",
      "Learned weights =  [ 4.87476643 91.48952296 78.43683954]\n",
      "Done with gradient descent at iteration  723\n",
      "Learned weights =  [ 4.88148658 91.48952296 78.43683954]\n",
      "Done with gradient descent at iteration  723\n",
      "Learned weights =  [ 4.88148658 91.48952207 78.43683954]\n",
      "Done with gradient descent at iteration  723\n",
      "Learned weights =  [ 4.88148658 91.48952207 78.43683864]\n",
      "Done with gradient descent at iteration  724\n",
      "Learned weights =  [ 4.88820674 91.48952207 78.43683864]\n",
      "Done with gradient descent at iteration  724\n",
      "Learned weights =  [ 4.88820674 91.48952117 78.43683864]\n",
      "Done with gradient descent at iteration  724\n",
      "Learned weights =  [ 4.88820674 91.48952117 78.43683773]\n",
      "Done with gradient descent at iteration  725\n",
      "Learned weights =  [ 4.89492689 91.48952117 78.43683773]\n",
      "Done with gradient descent at iteration  725\n",
      "Learned weights =  [ 4.89492689 91.48952027 78.43683773]\n",
      "Done with gradient descent at iteration  725\n",
      "Learned weights =  [ 4.89492689 91.48952027 78.43683682]\n",
      "Done with gradient descent at iteration  726\n",
      "Learned weights =  [ 4.90164705 91.48952027 78.43683682]\n",
      "Done with gradient descent at iteration  726\n",
      "Learned weights =  [ 4.90164705 91.48951938 78.43683682]\n",
      "Done with gradient descent at iteration  726\n",
      "Learned weights =  [ 4.90164705 91.48951938 78.43683592]\n",
      "Done with gradient descent at iteration  727\n",
      "Learned weights =  [ 4.90836721 91.48951938 78.43683592]\n",
      "Done with gradient descent at iteration  727\n",
      "Learned weights =  [ 4.90836721 91.48951848 78.43683592]\n",
      "Done with gradient descent at iteration  727\n",
      "Learned weights =  [ 4.90836721 91.48951848 78.43683501]\n",
      "Done with gradient descent at iteration  728\n",
      "Learned weights =  [ 4.91508736 91.48951848 78.43683501]\n",
      "Done with gradient descent at iteration  728\n",
      "Learned weights =  [ 4.91508736 91.48951758 78.43683501]\n",
      "Done with gradient descent at iteration  728\n",
      "Learned weights =  [ 4.91508736 91.48951758 78.43683411]\n",
      "Done with gradient descent at iteration  729\n",
      "Learned weights =  [ 4.92180752 91.48951758 78.43683411]\n",
      "Done with gradient descent at iteration  729\n",
      "Learned weights =  [ 4.92180752 91.48951668 78.43683411]\n",
      "Done with gradient descent at iteration  729\n",
      "Learned weights =  [ 4.92180752 91.48951668 78.4368332 ]\n",
      "Done with gradient descent at iteration  730\n",
      "Learned weights =  [ 4.92852767 91.48951668 78.4368332 ]\n",
      "Done with gradient descent at iteration  730\n",
      "Learned weights =  [ 4.92852767 91.48951579 78.4368332 ]\n",
      "Done with gradient descent at iteration  730\n",
      "Learned weights =  [ 4.92852767 91.48951579 78.43683229]\n",
      "Done with gradient descent at iteration  731\n",
      "Learned weights =  [ 4.93524783 91.48951579 78.43683229]\n",
      "Done with gradient descent at iteration  731\n",
      "Learned weights =  [ 4.93524783 91.48951489 78.43683229]\n",
      "Done with gradient descent at iteration  731\n",
      "Learned weights =  [ 4.93524783 91.48951489 78.43683139]\n",
      "Done with gradient descent at iteration  732\n",
      "Learned weights =  [ 4.94196798 91.48951489 78.43683139]\n",
      "Done with gradient descent at iteration  732\n",
      "Learned weights =  [ 4.94196798 91.48951399 78.43683139]\n",
      "Done with gradient descent at iteration  732\n",
      "Learned weights =  [ 4.94196798 91.48951399 78.43683048]\n",
      "Done with gradient descent at iteration  733\n",
      "Learned weights =  [ 4.94868814 91.48951399 78.43683048]\n",
      "Done with gradient descent at iteration  733\n",
      "Learned weights =  [ 4.94868814 91.4895131  78.43683048]\n",
      "Done with gradient descent at iteration  733\n",
      "Learned weights =  [ 4.94868814 91.4895131  78.43682958]\n",
      "Done with gradient descent at iteration  734\n",
      "Learned weights =  [ 4.95540829 91.4895131  78.43682958]\n",
      "Done with gradient descent at iteration  734\n",
      "Learned weights =  [ 4.95540829 91.4895122  78.43682958]\n",
      "Done with gradient descent at iteration  734\n",
      "Learned weights =  [ 4.95540829 91.4895122  78.43682867]\n",
      "Done with gradient descent at iteration  735\n",
      "Learned weights =  [ 4.96212845 91.4895122  78.43682867]\n",
      "Done with gradient descent at iteration  735\n",
      "Learned weights =  [ 4.96212845 91.4895113  78.43682867]\n",
      "Done with gradient descent at iteration  735\n",
      "Learned weights =  [ 4.96212845 91.4895113  78.43682776]\n",
      "Done with gradient descent at iteration  736\n",
      "Learned weights =  [ 4.9688486  91.4895113  78.43682776]\n",
      "Done with gradient descent at iteration  736\n",
      "Learned weights =  [ 4.9688486  91.48951041 78.43682776]\n",
      "Done with gradient descent at iteration  736\n",
      "Learned weights =  [ 4.9688486  91.48951041 78.43682686]\n",
      "Done with gradient descent at iteration  737\n",
      "Learned weights =  [ 4.97556876 91.48951041 78.43682686]\n",
      "Done with gradient descent at iteration  737\n",
      "Learned weights =  [ 4.97556876 91.48950951 78.43682686]\n",
      "Done with gradient descent at iteration  737\n",
      "Learned weights =  [ 4.97556876 91.48950951 78.43682595]\n",
      "Done with gradient descent at iteration  738\n",
      "Learned weights =  [ 4.98228891 91.48950951 78.43682595]\n",
      "Done with gradient descent at iteration  738\n",
      "Learned weights =  [ 4.98228891 91.48950861 78.43682595]\n",
      "Done with gradient descent at iteration  738\n",
      "Learned weights =  [ 4.98228891 91.48950861 78.43682505]\n",
      "Done with gradient descent at iteration  739\n",
      "Learned weights =  [ 4.98900907 91.48950861 78.43682505]\n",
      "Done with gradient descent at iteration  739\n",
      "Learned weights =  [ 4.98900907 91.48950771 78.43682505]\n",
      "Done with gradient descent at iteration  739\n",
      "Learned weights =  [ 4.98900907 91.48950771 78.43682414]\n",
      "Done with gradient descent at iteration  740\n",
      "Learned weights =  [ 4.99572922 91.48950771 78.43682414]\n",
      "Done with gradient descent at iteration  740\n",
      "Learned weights =  [ 4.99572922 91.48950682 78.43682414]\n",
      "Done with gradient descent at iteration  740\n",
      "Learned weights =  [ 4.99572922 91.48950682 78.43682323]\n",
      "Done with gradient descent at iteration  741\n",
      "Learned weights =  [ 5.00244938 91.48950682 78.43682323]\n",
      "Done with gradient descent at iteration  741\n",
      "Learned weights =  [ 5.00244938 91.48950592 78.43682323]\n",
      "Done with gradient descent at iteration  741\n",
      "Learned weights =  [ 5.00244938 91.48950592 78.43682233]\n",
      "Done with gradient descent at iteration  742\n",
      "Learned weights =  [ 5.00916953 91.48950592 78.43682233]\n",
      "Done with gradient descent at iteration  742\n",
      "Learned weights =  [ 5.00916953 91.48950502 78.43682233]\n",
      "Done with gradient descent at iteration  742\n",
      "Learned weights =  [ 5.00916953 91.48950502 78.43682142]\n",
      "Done with gradient descent at iteration  743\n",
      "Learned weights =  [ 5.01588969 91.48950502 78.43682142]\n",
      "Done with gradient descent at iteration  743\n",
      "Learned weights =  [ 5.01588969 91.48950413 78.43682142]\n",
      "Done with gradient descent at iteration  743\n",
      "Learned weights =  [ 5.01588969 91.48950413 78.43682052]\n",
      "Done with gradient descent at iteration  744\n",
      "Learned weights =  [ 5.02260984 91.48950413 78.43682052]\n",
      "Done with gradient descent at iteration  744\n",
      "Learned weights =  [ 5.02260984 91.48950323 78.43682052]\n",
      "Done with gradient descent at iteration  744\n",
      "Learned weights =  [ 5.02260984 91.48950323 78.43681961]\n",
      "Done with gradient descent at iteration  745\n",
      "Learned weights =  [ 5.02932999 91.48950323 78.43681961]\n",
      "Done with gradient descent at iteration  745\n",
      "Learned weights =  [ 5.02932999 91.48950233 78.43681961]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  745\n",
      "Learned weights =  [ 5.02932999 91.48950233 78.4368187 ]\n",
      "Done with gradient descent at iteration  746\n",
      "Learned weights =  [ 5.03605015 91.48950233 78.4368187 ]\n",
      "Done with gradient descent at iteration  746\n",
      "Learned weights =  [ 5.03605015 91.48950144 78.4368187 ]\n",
      "Done with gradient descent at iteration  746\n",
      "Learned weights =  [ 5.03605015 91.48950144 78.4368178 ]\n",
      "Done with gradient descent at iteration  747\n",
      "Learned weights =  [ 5.0427703  91.48950144 78.4368178 ]\n",
      "Done with gradient descent at iteration  747\n",
      "Learned weights =  [ 5.0427703  91.48950054 78.4368178 ]\n",
      "Done with gradient descent at iteration  747\n",
      "Learned weights =  [ 5.0427703  91.48950054 78.43681689]\n",
      "Done with gradient descent at iteration  748\n",
      "Learned weights =  [ 5.04949046 91.48950054 78.43681689]\n",
      "Done with gradient descent at iteration  748\n",
      "Learned weights =  [ 5.04949046 91.48949964 78.43681689]\n",
      "Done with gradient descent at iteration  748\n",
      "Learned weights =  [ 5.04949046 91.48949964 78.43681599]\n",
      "Done with gradient descent at iteration  749\n",
      "Learned weights =  [ 5.05621061 91.48949964 78.43681599]\n",
      "Done with gradient descent at iteration  749\n",
      "Learned weights =  [ 5.05621061 91.48949875 78.43681599]\n",
      "Done with gradient descent at iteration  749\n",
      "Learned weights =  [ 5.05621061 91.48949875 78.43681508]\n",
      "Done with gradient descent at iteration  750\n",
      "Learned weights =  [ 5.06293076 91.48949875 78.43681508]\n",
      "Done with gradient descent at iteration  750\n",
      "Learned weights =  [ 5.06293076 91.48949785 78.43681508]\n",
      "Done with gradient descent at iteration  750\n",
      "Learned weights =  [ 5.06293076 91.48949785 78.43681417]\n",
      "Done with gradient descent at iteration  751\n",
      "Learned weights =  [ 5.06965092 91.48949785 78.43681417]\n",
      "Done with gradient descent at iteration  751\n",
      "Learned weights =  [ 5.06965092 91.48949695 78.43681417]\n",
      "Done with gradient descent at iteration  751\n",
      "Learned weights =  [ 5.06965092 91.48949695 78.43681327]\n",
      "Done with gradient descent at iteration  752\n",
      "Learned weights =  [ 5.07637107 91.48949695 78.43681327]\n",
      "Done with gradient descent at iteration  752\n",
      "Learned weights =  [ 5.07637107 91.48949605 78.43681327]\n",
      "Done with gradient descent at iteration  752\n",
      "Learned weights =  [ 5.07637107 91.48949605 78.43681236]\n",
      "Done with gradient descent at iteration  753\n",
      "Learned weights =  [ 5.08309122 91.48949605 78.43681236]\n",
      "Done with gradient descent at iteration  753\n",
      "Learned weights =  [ 5.08309122 91.48949516 78.43681236]\n",
      "Done with gradient descent at iteration  753\n",
      "Learned weights =  [ 5.08309122 91.48949516 78.43681146]\n",
      "Done with gradient descent at iteration  754\n",
      "Learned weights =  [ 5.08981138 91.48949516 78.43681146]\n",
      "Done with gradient descent at iteration  754\n",
      "Learned weights =  [ 5.08981138 91.48949426 78.43681146]\n",
      "Done with gradient descent at iteration  754\n",
      "Learned weights =  [ 5.08981138 91.48949426 78.43681055]\n",
      "Done with gradient descent at iteration  755\n",
      "Learned weights =  [ 5.09653153 91.48949426 78.43681055]\n",
      "Done with gradient descent at iteration  755\n",
      "Learned weights =  [ 5.09653153 91.48949336 78.43681055]\n",
      "Done with gradient descent at iteration  755\n",
      "Learned weights =  [ 5.09653153 91.48949336 78.43680965]\n",
      "Done with gradient descent at iteration  756\n",
      "Learned weights =  [ 5.10325168 91.48949336 78.43680965]\n",
      "Done with gradient descent at iteration  756\n",
      "Learned weights =  [ 5.10325168 91.48949247 78.43680965]\n",
      "Done with gradient descent at iteration  756\n",
      "Learned weights =  [ 5.10325168 91.48949247 78.43680874]\n",
      "Done with gradient descent at iteration  757\n",
      "Learned weights =  [ 5.10997183 91.48949247 78.43680874]\n",
      "Done with gradient descent at iteration  757\n",
      "Learned weights =  [ 5.10997183 91.48949157 78.43680874]\n",
      "Done with gradient descent at iteration  757\n",
      "Learned weights =  [ 5.10997183 91.48949157 78.43680783]\n",
      "Done with gradient descent at iteration  758\n",
      "Learned weights =  [ 5.11669199 91.48949157 78.43680783]\n",
      "Done with gradient descent at iteration  758\n",
      "Learned weights =  [ 5.11669199 91.48949067 78.43680783]\n",
      "Done with gradient descent at iteration  758\n",
      "Learned weights =  [ 5.11669199 91.48949067 78.43680693]\n",
      "Done with gradient descent at iteration  759\n",
      "Learned weights =  [ 5.12341214 91.48949067 78.43680693]\n",
      "Done with gradient descent at iteration  759\n",
      "Learned weights =  [ 5.12341214 91.48948978 78.43680693]\n",
      "Done with gradient descent at iteration  759\n",
      "Learned weights =  [ 5.12341214 91.48948978 78.43680602]\n",
      "Done with gradient descent at iteration  760\n",
      "Learned weights =  [ 5.13013229 91.48948978 78.43680602]\n",
      "Done with gradient descent at iteration  760\n",
      "Learned weights =  [ 5.13013229 91.48948888 78.43680602]\n",
      "Done with gradient descent at iteration  760\n",
      "Learned weights =  [ 5.13013229 91.48948888 78.43680512]\n",
      "Done with gradient descent at iteration  761\n",
      "Learned weights =  [ 5.13685244 91.48948888 78.43680512]\n",
      "Done with gradient descent at iteration  761\n",
      "Learned weights =  [ 5.13685244 91.48948798 78.43680512]\n",
      "Done with gradient descent at iteration  761\n",
      "Learned weights =  [ 5.13685244 91.48948798 78.43680421]\n",
      "Done with gradient descent at iteration  762\n",
      "Learned weights =  [ 5.1435726  91.48948798 78.43680421]\n",
      "Done with gradient descent at iteration  762\n",
      "Learned weights =  [ 5.1435726  91.48948708 78.43680421]\n",
      "Done with gradient descent at iteration  762\n",
      "Learned weights =  [ 5.1435726  91.48948708 78.4368033 ]\n",
      "Done with gradient descent at iteration  763\n",
      "Learned weights =  [ 5.15029275 91.48948708 78.4368033 ]\n",
      "Done with gradient descent at iteration  763\n",
      "Learned weights =  [ 5.15029275 91.48948619 78.4368033 ]\n",
      "Done with gradient descent at iteration  763\n",
      "Learned weights =  [ 5.15029275 91.48948619 78.4368024 ]\n",
      "Done with gradient descent at iteration  764\n",
      "Learned weights =  [ 5.1570129  91.48948619 78.4368024 ]\n",
      "Done with gradient descent at iteration  764\n",
      "Learned weights =  [ 5.1570129  91.48948529 78.4368024 ]\n",
      "Done with gradient descent at iteration  764\n",
      "Learned weights =  [ 5.1570129  91.48948529 78.43680149]\n",
      "Done with gradient descent at iteration  765\n",
      "Learned weights =  [ 5.16373305 91.48948529 78.43680149]\n",
      "Done with gradient descent at iteration  765\n",
      "Learned weights =  [ 5.16373305 91.48948439 78.43680149]\n",
      "Done with gradient descent at iteration  765\n",
      "Learned weights =  [ 5.16373305 91.48948439 78.43680059]\n",
      "Done with gradient descent at iteration  766\n",
      "Learned weights =  [ 5.1704532  91.48948439 78.43680059]\n",
      "Done with gradient descent at iteration  766\n",
      "Learned weights =  [ 5.1704532  91.4894835  78.43680059]\n",
      "Done with gradient descent at iteration  766\n",
      "Learned weights =  [ 5.1704532  91.4894835  78.43679968]\n",
      "Done with gradient descent at iteration  767\n",
      "Learned weights =  [ 5.17717336 91.4894835  78.43679968]\n",
      "Done with gradient descent at iteration  767\n",
      "Learned weights =  [ 5.17717336 91.4894826  78.43679968]\n",
      "Done with gradient descent at iteration  767\n",
      "Learned weights =  [ 5.17717336 91.4894826  78.43679877]\n",
      "Done with gradient descent at iteration  768\n",
      "Learned weights =  [ 5.18389351 91.4894826  78.43679877]\n",
      "Done with gradient descent at iteration  768\n",
      "Learned weights =  [ 5.18389351 91.4894817  78.43679877]\n",
      "Done with gradient descent at iteration  768\n",
      "Learned weights =  [ 5.18389351 91.4894817  78.43679787]\n",
      "Done with gradient descent at iteration  769\n",
      "Learned weights =  [ 5.19061366 91.4894817  78.43679787]\n",
      "Done with gradient descent at iteration  769\n",
      "Learned weights =  [ 5.19061366 91.48948081 78.43679787]\n",
      "Done with gradient descent at iteration  769\n",
      "Learned weights =  [ 5.19061366 91.48948081 78.43679696]\n",
      "Done with gradient descent at iteration  770\n",
      "Learned weights =  [ 5.19733381 91.48948081 78.43679696]\n",
      "Done with gradient descent at iteration  770\n",
      "Learned weights =  [ 5.19733381 91.48947991 78.43679696]\n",
      "Done with gradient descent at iteration  770\n",
      "Learned weights =  [ 5.19733381 91.48947991 78.43679606]\n",
      "Done with gradient descent at iteration  771\n",
      "Learned weights =  [ 5.20405396 91.48947991 78.43679606]\n",
      "Done with gradient descent at iteration  771\n",
      "Learned weights =  [ 5.20405396 91.48947901 78.43679606]\n",
      "Done with gradient descent at iteration  771\n",
      "Learned weights =  [ 5.20405396 91.48947901 78.43679515]\n",
      "Done with gradient descent at iteration  772\n",
      "Learned weights =  [ 5.21077411 91.48947901 78.43679515]\n",
      "Done with gradient descent at iteration  772\n",
      "Learned weights =  [ 5.21077411 91.48947812 78.43679515]\n",
      "Done with gradient descent at iteration  772\n",
      "Learned weights =  [ 5.21077411 91.48947812 78.43679424]\n",
      "Done with gradient descent at iteration  773\n",
      "Learned weights =  [ 5.21749426 91.48947812 78.43679424]\n",
      "Done with gradient descent at iteration  773\n",
      "Learned weights =  [ 5.21749426 91.48947722 78.43679424]\n",
      "Done with gradient descent at iteration  773\n",
      "Learned weights =  [ 5.21749426 91.48947722 78.43679334]\n",
      "Done with gradient descent at iteration  774\n",
      "Learned weights =  [ 5.22421441 91.48947722 78.43679334]\n",
      "Done with gradient descent at iteration  774\n",
      "Learned weights =  [ 5.22421441 91.48947632 78.43679334]\n",
      "Done with gradient descent at iteration  774\n",
      "Learned weights =  [ 5.22421441 91.48947632 78.43679243]\n",
      "Done with gradient descent at iteration  775\n",
      "Learned weights =  [ 5.23093457 91.48947632 78.43679243]\n",
      "Done with gradient descent at iteration  775\n",
      "Learned weights =  [ 5.23093457 91.48947542 78.43679243]\n",
      "Done with gradient descent at iteration  775\n",
      "Learned weights =  [ 5.23093457 91.48947542 78.43679153]\n",
      "Done with gradient descent at iteration  776\n",
      "Learned weights =  [ 5.23765472 91.48947542 78.43679153]\n",
      "Done with gradient descent at iteration  776\n",
      "Learned weights =  [ 5.23765472 91.48947453 78.43679153]\n",
      "Done with gradient descent at iteration  776\n",
      "Learned weights =  [ 5.23765472 91.48947453 78.43679062]\n",
      "Done with gradient descent at iteration  777\n",
      "Learned weights =  [ 5.24437487 91.48947453 78.43679062]\n",
      "Done with gradient descent at iteration  777\n",
      "Learned weights =  [ 5.24437487 91.48947363 78.43679062]\n",
      "Done with gradient descent at iteration  777\n",
      "Learned weights =  [ 5.24437487 91.48947363 78.43678971]\n",
      "Done with gradient descent at iteration  778\n",
      "Learned weights =  [ 5.25109502 91.48947363 78.43678971]\n",
      "Done with gradient descent at iteration  778\n",
      "Learned weights =  [ 5.25109502 91.48947273 78.43678971]\n",
      "Done with gradient descent at iteration  778\n",
      "Learned weights =  [ 5.25109502 91.48947273 78.43678881]\n",
      "Done with gradient descent at iteration  779\n",
      "Learned weights =  [ 5.25781517 91.48947273 78.43678881]\n",
      "Done with gradient descent at iteration  779\n",
      "Learned weights =  [ 5.25781517 91.48947184 78.43678881]\n",
      "Done with gradient descent at iteration  779\n",
      "Learned weights =  [ 5.25781517 91.48947184 78.4367879 ]\n",
      "Done with gradient descent at iteration  780\n",
      "Learned weights =  [ 5.26453532 91.48947184 78.4367879 ]\n",
      "Done with gradient descent at iteration  780\n",
      "Learned weights =  [ 5.26453532 91.48947094 78.4367879 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  780\n",
      "Learned weights =  [ 5.26453532 91.48947094 78.436787  ]\n",
      "Done with gradient descent at iteration  781\n",
      "Learned weights =  [ 5.27125547 91.48947094 78.436787  ]\n",
      "Done with gradient descent at iteration  781\n",
      "Learned weights =  [ 5.27125547 91.48947004 78.436787  ]\n",
      "Done with gradient descent at iteration  781\n",
      "Learned weights =  [ 5.27125547 91.48947004 78.43678609]\n",
      "Done with gradient descent at iteration  782\n",
      "Learned weights =  [ 5.27797562 91.48947004 78.43678609]\n",
      "Done with gradient descent at iteration  782\n",
      "Learned weights =  [ 5.27797562 91.48946915 78.43678609]\n",
      "Done with gradient descent at iteration  782\n",
      "Learned weights =  [ 5.27797562 91.48946915 78.43678518]\n",
      "Done with gradient descent at iteration  783\n",
      "Learned weights =  [ 5.28469577 91.48946915 78.43678518]\n",
      "Done with gradient descent at iteration  783\n",
      "Learned weights =  [ 5.28469577 91.48946825 78.43678518]\n",
      "Done with gradient descent at iteration  783\n",
      "Learned weights =  [ 5.28469577 91.48946825 78.43678428]\n",
      "Done with gradient descent at iteration  784\n",
      "Learned weights =  [ 5.29141592 91.48946825 78.43678428]\n",
      "Done with gradient descent at iteration  784\n",
      "Learned weights =  [ 5.29141592 91.48946735 78.43678428]\n",
      "Done with gradient descent at iteration  784\n",
      "Learned weights =  [ 5.29141592 91.48946735 78.43678337]\n",
      "Done with gradient descent at iteration  785\n",
      "Learned weights =  [ 5.29813607 91.48946735 78.43678337]\n",
      "Done with gradient descent at iteration  785\n",
      "Learned weights =  [ 5.29813607 91.48946645 78.43678337]\n",
      "Done with gradient descent at iteration  785\n",
      "Learned weights =  [ 5.29813607 91.48946645 78.43678247]\n",
      "Done with gradient descent at iteration  786\n",
      "Learned weights =  [ 5.30485622 91.48946645 78.43678247]\n",
      "Done with gradient descent at iteration  786\n",
      "Learned weights =  [ 5.30485622 91.48946556 78.43678247]\n",
      "Done with gradient descent at iteration  786\n",
      "Learned weights =  [ 5.30485622 91.48946556 78.43678156]\n",
      "Done with gradient descent at iteration  787\n",
      "Learned weights =  [ 5.31157637 91.48946556 78.43678156]\n",
      "Done with gradient descent at iteration  787\n",
      "Learned weights =  [ 5.31157637 91.48946466 78.43678156]\n",
      "Done with gradient descent at iteration  787\n",
      "Learned weights =  [ 5.31157637 91.48946466 78.43678065]\n",
      "Done with gradient descent at iteration  788\n",
      "Learned weights =  [ 5.31829652 91.48946466 78.43678065]\n",
      "Done with gradient descent at iteration  788\n",
      "Learned weights =  [ 5.31829652 91.48946376 78.43678065]\n",
      "Done with gradient descent at iteration  788\n",
      "Learned weights =  [ 5.31829652 91.48946376 78.43677975]\n",
      "Done with gradient descent at iteration  789\n",
      "Learned weights =  [ 5.32501667 91.48946376 78.43677975]\n",
      "Done with gradient descent at iteration  789\n",
      "Learned weights =  [ 5.32501667 91.48946287 78.43677975]\n",
      "Done with gradient descent at iteration  789\n",
      "Learned weights =  [ 5.32501667 91.48946287 78.43677884]\n",
      "Done with gradient descent at iteration  790\n",
      "Learned weights =  [ 5.33173682 91.48946287 78.43677884]\n",
      "Done with gradient descent at iteration  790\n",
      "Learned weights =  [ 5.33173682 91.48946197 78.43677884]\n",
      "Done with gradient descent at iteration  790\n",
      "Learned weights =  [ 5.33173682 91.48946197 78.43677794]\n",
      "Done with gradient descent at iteration  791\n",
      "Learned weights =  [ 5.33845696 91.48946197 78.43677794]\n",
      "Done with gradient descent at iteration  791\n",
      "Learned weights =  [ 5.33845696 91.48946107 78.43677794]\n",
      "Done with gradient descent at iteration  791\n",
      "Learned weights =  [ 5.33845696 91.48946107 78.43677703]\n",
      "Done with gradient descent at iteration  792\n",
      "Learned weights =  [ 5.34517711 91.48946107 78.43677703]\n",
      "Done with gradient descent at iteration  792\n",
      "Learned weights =  [ 5.34517711 91.48946018 78.43677703]\n",
      "Done with gradient descent at iteration  792\n",
      "Learned weights =  [ 5.34517711 91.48946018 78.43677612]\n",
      "Done with gradient descent at iteration  793\n",
      "Learned weights =  [ 5.35189726 91.48946018 78.43677612]\n",
      "Done with gradient descent at iteration  793\n",
      "Learned weights =  [ 5.35189726 91.48945928 78.43677612]\n",
      "Done with gradient descent at iteration  793\n",
      "Learned weights =  [ 5.35189726 91.48945928 78.43677522]\n",
      "Done with gradient descent at iteration  794\n",
      "Learned weights =  [ 5.35861741 91.48945928 78.43677522]\n",
      "Done with gradient descent at iteration  794\n",
      "Learned weights =  [ 5.35861741 91.48945838 78.43677522]\n",
      "Done with gradient descent at iteration  794\n",
      "Learned weights =  [ 5.35861741 91.48945838 78.43677431]\n",
      "Done with gradient descent at iteration  795\n",
      "Learned weights =  [ 5.36533756 91.48945838 78.43677431]\n",
      "Done with gradient descent at iteration  795\n",
      "Learned weights =  [ 5.36533756 91.48945749 78.43677431]\n",
      "Done with gradient descent at iteration  795\n",
      "Learned weights =  [ 5.36533756 91.48945749 78.43677341]\n",
      "Done with gradient descent at iteration  796\n",
      "Learned weights =  [ 5.37205771 91.48945749 78.43677341]\n",
      "Done with gradient descent at iteration  796\n",
      "Learned weights =  [ 5.37205771 91.48945659 78.43677341]\n",
      "Done with gradient descent at iteration  796\n",
      "Learned weights =  [ 5.37205771 91.48945659 78.4367725 ]\n",
      "Done with gradient descent at iteration  797\n",
      "Learned weights =  [ 5.37877786 91.48945659 78.4367725 ]\n",
      "Done with gradient descent at iteration  797\n",
      "Learned weights =  [ 5.37877786 91.48945569 78.4367725 ]\n",
      "Done with gradient descent at iteration  797\n",
      "Learned weights =  [ 5.37877786 91.48945569 78.43677159]\n",
      "Done with gradient descent at iteration  798\n",
      "Learned weights =  [ 5.38549801 91.48945569 78.43677159]\n",
      "Done with gradient descent at iteration  798\n",
      "Learned weights =  [ 5.38549801 91.48945479 78.43677159]\n",
      "Done with gradient descent at iteration  798\n",
      "Learned weights =  [ 5.38549801 91.48945479 78.43677069]\n",
      "Done with gradient descent at iteration  799\n",
      "Learned weights =  [ 5.39221815 91.48945479 78.43677069]\n",
      "Done with gradient descent at iteration  799\n",
      "Learned weights =  [ 5.39221815 91.4894539  78.43677069]\n",
      "Done with gradient descent at iteration  799\n",
      "Learned weights =  [ 5.39221815 91.4894539  78.43676978]\n",
      "Iteration = 800\n",
      "Cost function =  3605258760409460.0\n",
      "Done with gradient descent at iteration  800\n",
      "Learned weights =  [ 5.3989383  91.4894539  78.43676978]\n",
      "Done with gradient descent at iteration  800\n",
      "Learned weights =  [ 5.3989383  91.489453   78.43676978]\n",
      "Done with gradient descent at iteration  800\n",
      "Learned weights =  [ 5.3989383  91.489453   78.43676888]\n",
      "Done with gradient descent at iteration  801\n",
      "Learned weights =  [ 5.40565845 91.489453   78.43676888]\n",
      "Done with gradient descent at iteration  801\n",
      "Learned weights =  [ 5.40565845 91.4894521  78.43676888]\n",
      "Done with gradient descent at iteration  801\n",
      "Learned weights =  [ 5.40565845 91.4894521  78.43676797]\n",
      "Done with gradient descent at iteration  802\n",
      "Learned weights =  [ 5.4123786  91.4894521  78.43676797]\n",
      "Done with gradient descent at iteration  802\n",
      "Learned weights =  [ 5.4123786  91.48945121 78.43676797]\n",
      "Done with gradient descent at iteration  802\n",
      "Learned weights =  [ 5.4123786  91.48945121 78.43676706]\n",
      "Done with gradient descent at iteration  803\n",
      "Learned weights =  [ 5.41909875 91.48945121 78.43676706]\n",
      "Done with gradient descent at iteration  803\n",
      "Learned weights =  [ 5.41909875 91.48945031 78.43676706]\n",
      "Done with gradient descent at iteration  803\n",
      "Learned weights =  [ 5.41909875 91.48945031 78.43676616]\n",
      "Done with gradient descent at iteration  804\n",
      "Learned weights =  [ 5.42581889 91.48945031 78.43676616]\n",
      "Done with gradient descent at iteration  804\n",
      "Learned weights =  [ 5.42581889 91.48944941 78.43676616]\n",
      "Done with gradient descent at iteration  804\n",
      "Learned weights =  [ 5.42581889 91.48944941 78.43676525]\n",
      "Done with gradient descent at iteration  805\n",
      "Learned weights =  [ 5.43253904 91.48944941 78.43676525]\n",
      "Done with gradient descent at iteration  805\n",
      "Learned weights =  [ 5.43253904 91.48944852 78.43676525]\n",
      "Done with gradient descent at iteration  805\n",
      "Learned weights =  [ 5.43253904 91.48944852 78.43676435]\n",
      "Done with gradient descent at iteration  806\n",
      "Learned weights =  [ 5.43925919 91.48944852 78.43676435]\n",
      "Done with gradient descent at iteration  806\n",
      "Learned weights =  [ 5.43925919 91.48944762 78.43676435]\n",
      "Done with gradient descent at iteration  806\n",
      "Learned weights =  [ 5.43925919 91.48944762 78.43676344]\n",
      "Done with gradient descent at iteration  807\n",
      "Learned weights =  [ 5.44597934 91.48944762 78.43676344]\n",
      "Done with gradient descent at iteration  807\n",
      "Learned weights =  [ 5.44597934 91.48944672 78.43676344]\n",
      "Done with gradient descent at iteration  807\n",
      "Learned weights =  [ 5.44597934 91.48944672 78.43676253]\n",
      "Done with gradient descent at iteration  808\n",
      "Learned weights =  [ 5.45269948 91.48944672 78.43676253]\n",
      "Done with gradient descent at iteration  808\n",
      "Learned weights =  [ 5.45269948 91.48944583 78.43676253]\n",
      "Done with gradient descent at iteration  808\n",
      "Learned weights =  [ 5.45269948 91.48944583 78.43676163]\n",
      "Done with gradient descent at iteration  809\n",
      "Learned weights =  [ 5.45941963 91.48944583 78.43676163]\n",
      "Done with gradient descent at iteration  809\n",
      "Learned weights =  [ 5.45941963 91.48944493 78.43676163]\n",
      "Done with gradient descent at iteration  809\n",
      "Learned weights =  [ 5.45941963 91.48944493 78.43676072]\n",
      "Done with gradient descent at iteration  810\n",
      "Learned weights =  [ 5.46613978 91.48944493 78.43676072]\n",
      "Done with gradient descent at iteration  810\n",
      "Learned weights =  [ 5.46613978 91.48944403 78.43676072]\n",
      "Done with gradient descent at iteration  810\n",
      "Learned weights =  [ 5.46613978 91.48944403 78.43675982]\n",
      "Done with gradient descent at iteration  811\n",
      "Learned weights =  [ 5.47285992 91.48944403 78.43675982]\n",
      "Done with gradient descent at iteration  811\n",
      "Learned weights =  [ 5.47285992 91.48944313 78.43675982]\n",
      "Done with gradient descent at iteration  811\n",
      "Learned weights =  [ 5.47285992 91.48944313 78.43675891]\n",
      "Done with gradient descent at iteration  812\n",
      "Learned weights =  [ 5.47958007 91.48944313 78.43675891]\n",
      "Done with gradient descent at iteration  812\n",
      "Learned weights =  [ 5.47958007 91.48944224 78.43675891]\n",
      "Done with gradient descent at iteration  812\n",
      "Learned weights =  [ 5.47958007 91.48944224 78.436758  ]\n",
      "Done with gradient descent at iteration  813\n",
      "Learned weights =  [ 5.48630022 91.48944224 78.436758  ]\n",
      "Done with gradient descent at iteration  813\n",
      "Learned weights =  [ 5.48630022 91.48944134 78.436758  ]\n",
      "Done with gradient descent at iteration  813\n",
      "Learned weights =  [ 5.48630022 91.48944134 78.4367571 ]\n",
      "Done with gradient descent at iteration  814\n",
      "Learned weights =  [ 5.49302036 91.48944134 78.4367571 ]\n",
      "Done with gradient descent at iteration  814\n",
      "Learned weights =  [ 5.49302036 91.48944044 78.4367571 ]\n",
      "Done with gradient descent at iteration  814\n",
      "Learned weights =  [ 5.49302036 91.48944044 78.43675619]\n",
      "Done with gradient descent at iteration  815\n",
      "Learned weights =  [ 5.49974051 91.48944044 78.43675619]\n",
      "Done with gradient descent at iteration  815\n",
      "Learned weights =  [ 5.49974051 91.48943955 78.43675619]\n",
      "Done with gradient descent at iteration  815\n",
      "Learned weights =  [ 5.49974051 91.48943955 78.43675529]\n",
      "Done with gradient descent at iteration  816\n",
      "Learned weights =  [ 5.50646066 91.48943955 78.43675529]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  816\n",
      "Learned weights =  [ 5.50646066 91.48943865 78.43675529]\n",
      "Done with gradient descent at iteration  816\n",
      "Learned weights =  [ 5.50646066 91.48943865 78.43675438]\n",
      "Done with gradient descent at iteration  817\n",
      "Learned weights =  [ 5.5131808  91.48943865 78.43675438]\n",
      "Done with gradient descent at iteration  817\n",
      "Learned weights =  [ 5.5131808  91.48943775 78.43675438]\n",
      "Done with gradient descent at iteration  817\n",
      "Learned weights =  [ 5.5131808  91.48943775 78.43675347]\n",
      "Done with gradient descent at iteration  818\n",
      "Learned weights =  [ 5.51990095 91.48943775 78.43675347]\n",
      "Done with gradient descent at iteration  818\n",
      "Learned weights =  [ 5.51990095 91.48943686 78.43675347]\n",
      "Done with gradient descent at iteration  818\n",
      "Learned weights =  [ 5.51990095 91.48943686 78.43675257]\n",
      "Done with gradient descent at iteration  819\n",
      "Learned weights =  [ 5.5266211  91.48943686 78.43675257]\n",
      "Done with gradient descent at iteration  819\n",
      "Learned weights =  [ 5.5266211  91.48943596 78.43675257]\n",
      "Done with gradient descent at iteration  819\n",
      "Learned weights =  [ 5.5266211  91.48943596 78.43675166]\n",
      "Done with gradient descent at iteration  820\n",
      "Learned weights =  [ 5.53334124 91.48943596 78.43675166]\n",
      "Done with gradient descent at iteration  820\n",
      "Learned weights =  [ 5.53334124 91.48943506 78.43675166]\n",
      "Done with gradient descent at iteration  820\n",
      "Learned weights =  [ 5.53334124 91.48943506 78.43675076]\n",
      "Done with gradient descent at iteration  821\n",
      "Learned weights =  [ 5.54006139 91.48943506 78.43675076]\n",
      "Done with gradient descent at iteration  821\n",
      "Learned weights =  [ 5.54006139 91.48943416 78.43675076]\n",
      "Done with gradient descent at iteration  821\n",
      "Learned weights =  [ 5.54006139 91.48943416 78.43674985]\n",
      "Done with gradient descent at iteration  822\n",
      "Learned weights =  [ 5.54678153 91.48943416 78.43674985]\n",
      "Done with gradient descent at iteration  822\n",
      "Learned weights =  [ 5.54678153 91.48943327 78.43674985]\n",
      "Done with gradient descent at iteration  822\n",
      "Learned weights =  [ 5.54678153 91.48943327 78.43674895]\n",
      "Done with gradient descent at iteration  823\n",
      "Learned weights =  [ 5.55350168 91.48943327 78.43674895]\n",
      "Done with gradient descent at iteration  823\n",
      "Learned weights =  [ 5.55350168 91.48943237 78.43674895]\n",
      "Done with gradient descent at iteration  823\n",
      "Learned weights =  [ 5.55350168 91.48943237 78.43674804]\n",
      "Done with gradient descent at iteration  824\n",
      "Learned weights =  [ 5.56022183 91.48943237 78.43674804]\n",
      "Done with gradient descent at iteration  824\n",
      "Learned weights =  [ 5.56022183 91.48943147 78.43674804]\n",
      "Done with gradient descent at iteration  824\n",
      "Learned weights =  [ 5.56022183 91.48943147 78.43674713]\n",
      "Done with gradient descent at iteration  825\n",
      "Learned weights =  [ 5.56694197 91.48943147 78.43674713]\n",
      "Done with gradient descent at iteration  825\n",
      "Learned weights =  [ 5.56694197 91.48943058 78.43674713]\n",
      "Done with gradient descent at iteration  825\n",
      "Learned weights =  [ 5.56694197 91.48943058 78.43674623]\n",
      "Done with gradient descent at iteration  826\n",
      "Learned weights =  [ 5.57366212 91.48943058 78.43674623]\n",
      "Done with gradient descent at iteration  826\n",
      "Learned weights =  [ 5.57366212 91.48942968 78.43674623]\n",
      "Done with gradient descent at iteration  826\n",
      "Learned weights =  [ 5.57366212 91.48942968 78.43674532]\n",
      "Done with gradient descent at iteration  827\n",
      "Learned weights =  [ 5.58038226 91.48942968 78.43674532]\n",
      "Done with gradient descent at iteration  827\n",
      "Learned weights =  [ 5.58038226 91.48942878 78.43674532]\n",
      "Done with gradient descent at iteration  827\n",
      "Learned weights =  [ 5.58038226 91.48942878 78.43674442]\n",
      "Done with gradient descent at iteration  828\n",
      "Learned weights =  [ 5.58710241 91.48942878 78.43674442]\n",
      "Done with gradient descent at iteration  828\n",
      "Learned weights =  [ 5.58710241 91.48942789 78.43674442]\n",
      "Done with gradient descent at iteration  828\n",
      "Learned weights =  [ 5.58710241 91.48942789 78.43674351]\n",
      "Done with gradient descent at iteration  829\n",
      "Learned weights =  [ 5.59382255 91.48942789 78.43674351]\n",
      "Done with gradient descent at iteration  829\n",
      "Learned weights =  [ 5.59382255 91.48942699 78.43674351]\n",
      "Done with gradient descent at iteration  829\n",
      "Learned weights =  [ 5.59382255 91.48942699 78.4367426 ]\n",
      "Done with gradient descent at iteration  830\n",
      "Learned weights =  [ 5.6005427  91.48942699 78.4367426 ]\n",
      "Done with gradient descent at iteration  830\n",
      "Learned weights =  [ 5.6005427  91.48942609 78.4367426 ]\n",
      "Done with gradient descent at iteration  830\n",
      "Learned weights =  [ 5.6005427  91.48942609 78.4367417 ]\n",
      "Done with gradient descent at iteration  831\n",
      "Learned weights =  [ 5.60726284 91.48942609 78.4367417 ]\n",
      "Done with gradient descent at iteration  831\n",
      "Learned weights =  [ 5.60726284 91.4894252  78.4367417 ]\n",
      "Done with gradient descent at iteration  831\n",
      "Learned weights =  [ 5.60726284 91.4894252  78.43674079]\n",
      "Done with gradient descent at iteration  832\n",
      "Learned weights =  [ 5.61398299 91.4894252  78.43674079]\n",
      "Done with gradient descent at iteration  832\n",
      "Learned weights =  [ 5.61398299 91.4894243  78.43674079]\n",
      "Done with gradient descent at iteration  832\n",
      "Learned weights =  [ 5.61398299 91.4894243  78.43673989]\n",
      "Done with gradient descent at iteration  833\n",
      "Learned weights =  [ 5.62070313 91.4894243  78.43673989]\n",
      "Done with gradient descent at iteration  833\n",
      "Learned weights =  [ 5.62070313 91.4894234  78.43673989]\n",
      "Done with gradient descent at iteration  833\n",
      "Learned weights =  [ 5.62070313 91.4894234  78.43673898]\n",
      "Done with gradient descent at iteration  834\n",
      "Learned weights =  [ 5.62742328 91.4894234  78.43673898]\n",
      "Done with gradient descent at iteration  834\n",
      "Learned weights =  [ 5.62742328 91.4894225  78.43673898]\n",
      "Done with gradient descent at iteration  834\n",
      "Learned weights =  [ 5.62742328 91.4894225  78.43673807]\n",
      "Done with gradient descent at iteration  835\n",
      "Learned weights =  [ 5.63414342 91.4894225  78.43673807]\n",
      "Done with gradient descent at iteration  835\n",
      "Learned weights =  [ 5.63414342 91.48942161 78.43673807]\n",
      "Done with gradient descent at iteration  835\n",
      "Learned weights =  [ 5.63414342 91.48942161 78.43673717]\n",
      "Done with gradient descent at iteration  836\n",
      "Learned weights =  [ 5.64086356 91.48942161 78.43673717]\n",
      "Done with gradient descent at iteration  836\n",
      "Learned weights =  [ 5.64086356 91.48942071 78.43673717]\n",
      "Done with gradient descent at iteration  836\n",
      "Learned weights =  [ 5.64086356 91.48942071 78.43673626]\n",
      "Done with gradient descent at iteration  837\n",
      "Learned weights =  [ 5.64758371 91.48942071 78.43673626]\n",
      "Done with gradient descent at iteration  837\n",
      "Learned weights =  [ 5.64758371 91.48941981 78.43673626]\n",
      "Done with gradient descent at iteration  837\n",
      "Learned weights =  [ 5.64758371 91.48941981 78.43673536]\n",
      "Done with gradient descent at iteration  838\n",
      "Learned weights =  [ 5.65430385 91.48941981 78.43673536]\n",
      "Done with gradient descent at iteration  838\n",
      "Learned weights =  [ 5.65430385 91.48941892 78.43673536]\n",
      "Done with gradient descent at iteration  838\n",
      "Learned weights =  [ 5.65430385 91.48941892 78.43673445]\n",
      "Done with gradient descent at iteration  839\n",
      "Learned weights =  [ 5.661024   91.48941892 78.43673445]\n",
      "Done with gradient descent at iteration  839\n",
      "Learned weights =  [ 5.661024   91.48941802 78.43673445]\n",
      "Done with gradient descent at iteration  839\n",
      "Learned weights =  [ 5.661024   91.48941802 78.43673354]\n",
      "Done with gradient descent at iteration  840\n",
      "Learned weights =  [ 5.66774414 91.48941802 78.43673354]\n",
      "Done with gradient descent at iteration  840\n",
      "Learned weights =  [ 5.66774414 91.48941712 78.43673354]\n",
      "Done with gradient descent at iteration  840\n",
      "Learned weights =  [ 5.66774414 91.48941712 78.43673264]\n",
      "Done with gradient descent at iteration  841\n",
      "Learned weights =  [ 5.67446429 91.48941712 78.43673264]\n",
      "Done with gradient descent at iteration  841\n",
      "Learned weights =  [ 5.67446429 91.48941623 78.43673264]\n",
      "Done with gradient descent at iteration  841\n",
      "Learned weights =  [ 5.67446429 91.48941623 78.43673173]\n",
      "Done with gradient descent at iteration  842\n",
      "Learned weights =  [ 5.68118443 91.48941623 78.43673173]\n",
      "Done with gradient descent at iteration  842\n",
      "Learned weights =  [ 5.68118443 91.48941533 78.43673173]\n",
      "Done with gradient descent at iteration  842\n",
      "Learned weights =  [ 5.68118443 91.48941533 78.43673083]\n",
      "Done with gradient descent at iteration  843\n",
      "Learned weights =  [ 5.68790457 91.48941533 78.43673083]\n",
      "Done with gradient descent at iteration  843\n",
      "Learned weights =  [ 5.68790457 91.48941443 78.43673083]\n",
      "Done with gradient descent at iteration  843\n",
      "Learned weights =  [ 5.68790457 91.48941443 78.43672992]\n",
      "Done with gradient descent at iteration  844\n",
      "Learned weights =  [ 5.69462472 91.48941443 78.43672992]\n",
      "Done with gradient descent at iteration  844\n",
      "Learned weights =  [ 5.69462472 91.48941353 78.43672992]\n",
      "Done with gradient descent at iteration  844\n",
      "Learned weights =  [ 5.69462472 91.48941353 78.43672901]\n",
      "Done with gradient descent at iteration  845\n",
      "Learned weights =  [ 5.70134486 91.48941353 78.43672901]\n",
      "Done with gradient descent at iteration  845\n",
      "Learned weights =  [ 5.70134486 91.48941264 78.43672901]\n",
      "Done with gradient descent at iteration  845\n",
      "Learned weights =  [ 5.70134486 91.48941264 78.43672811]\n",
      "Done with gradient descent at iteration  846\n",
      "Learned weights =  [ 5.708065   91.48941264 78.43672811]\n",
      "Done with gradient descent at iteration  846\n",
      "Learned weights =  [ 5.708065   91.48941174 78.43672811]\n",
      "Done with gradient descent at iteration  846\n",
      "Learned weights =  [ 5.708065   91.48941174 78.4367272 ]\n",
      "Done with gradient descent at iteration  847\n",
      "Learned weights =  [ 5.71478515 91.48941174 78.4367272 ]\n",
      "Done with gradient descent at iteration  847\n",
      "Learned weights =  [ 5.71478515 91.48941084 78.4367272 ]\n",
      "Done with gradient descent at iteration  847\n",
      "Learned weights =  [ 5.71478515 91.48941084 78.4367263 ]\n",
      "Done with gradient descent at iteration  848\n",
      "Learned weights =  [ 5.72150529 91.48941084 78.4367263 ]\n",
      "Done with gradient descent at iteration  848\n",
      "Learned weights =  [ 5.72150529 91.48940995 78.4367263 ]\n",
      "Done with gradient descent at iteration  848\n",
      "Learned weights =  [ 5.72150529 91.48940995 78.43672539]\n",
      "Done with gradient descent at iteration  849\n",
      "Learned weights =  [ 5.72822543 91.48940995 78.43672539]\n",
      "Done with gradient descent at iteration  849\n",
      "Learned weights =  [ 5.72822543 91.48940905 78.43672539]\n",
      "Done with gradient descent at iteration  849\n",
      "Learned weights =  [ 5.72822543 91.48940905 78.43672448]\n",
      "Done with gradient descent at iteration  850\n",
      "Learned weights =  [ 5.73494557 91.48940905 78.43672448]\n",
      "Done with gradient descent at iteration  850\n",
      "Learned weights =  [ 5.73494557 91.48940815 78.43672448]\n",
      "Done with gradient descent at iteration  850\n",
      "Learned weights =  [ 5.73494557 91.48940815 78.43672358]\n",
      "Done with gradient descent at iteration  851\n",
      "Learned weights =  [ 5.74166572 91.48940815 78.43672358]\n",
      "Done with gradient descent at iteration  851\n",
      "Learned weights =  [ 5.74166572 91.48940726 78.43672358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  851\n",
      "Learned weights =  [ 5.74166572 91.48940726 78.43672267]\n",
      "Done with gradient descent at iteration  852\n",
      "Learned weights =  [ 5.74838586 91.48940726 78.43672267]\n",
      "Done with gradient descent at iteration  852\n",
      "Learned weights =  [ 5.74838586 91.48940636 78.43672267]\n",
      "Done with gradient descent at iteration  852\n",
      "Learned weights =  [ 5.74838586 91.48940636 78.43672177]\n",
      "Done with gradient descent at iteration  853\n",
      "Learned weights =  [ 5.755106   91.48940636 78.43672177]\n",
      "Done with gradient descent at iteration  853\n",
      "Learned weights =  [ 5.755106   91.48940546 78.43672177]\n",
      "Done with gradient descent at iteration  853\n",
      "Learned weights =  [ 5.755106   91.48940546 78.43672086]\n",
      "Done with gradient descent at iteration  854\n",
      "Learned weights =  [ 5.76182615 91.48940546 78.43672086]\n",
      "Done with gradient descent at iteration  854\n",
      "Learned weights =  [ 5.76182615 91.48940457 78.43672086]\n",
      "Done with gradient descent at iteration  854\n",
      "Learned weights =  [ 5.76182615 91.48940457 78.43671995]\n",
      "Done with gradient descent at iteration  855\n",
      "Learned weights =  [ 5.76854629 91.48940457 78.43671995]\n",
      "Done with gradient descent at iteration  855\n",
      "Learned weights =  [ 5.76854629 91.48940367 78.43671995]\n",
      "Done with gradient descent at iteration  855\n",
      "Learned weights =  [ 5.76854629 91.48940367 78.43671905]\n",
      "Done with gradient descent at iteration  856\n",
      "Learned weights =  [ 5.77526643 91.48940367 78.43671905]\n",
      "Done with gradient descent at iteration  856\n",
      "Learned weights =  [ 5.77526643 91.48940277 78.43671905]\n",
      "Done with gradient descent at iteration  856\n",
      "Learned weights =  [ 5.77526643 91.48940277 78.43671814]\n",
      "Done with gradient descent at iteration  857\n",
      "Learned weights =  [ 5.78198657 91.48940277 78.43671814]\n",
      "Done with gradient descent at iteration  857\n",
      "Learned weights =  [ 5.78198657 91.48940187 78.43671814]\n",
      "Done with gradient descent at iteration  857\n",
      "Learned weights =  [ 5.78198657 91.48940187 78.43671724]\n",
      "Done with gradient descent at iteration  858\n",
      "Learned weights =  [ 5.78870671 91.48940187 78.43671724]\n",
      "Done with gradient descent at iteration  858\n",
      "Learned weights =  [ 5.78870671 91.48940098 78.43671724]\n",
      "Done with gradient descent at iteration  858\n",
      "Learned weights =  [ 5.78870671 91.48940098 78.43671633]\n",
      "Done with gradient descent at iteration  859\n",
      "Learned weights =  [ 5.79542686 91.48940098 78.43671633]\n",
      "Done with gradient descent at iteration  859\n",
      "Learned weights =  [ 5.79542686 91.48940008 78.43671633]\n",
      "Done with gradient descent at iteration  859\n",
      "Learned weights =  [ 5.79542686 91.48940008 78.43671542]\n",
      "Done with gradient descent at iteration  860\n",
      "Learned weights =  [ 5.802147   91.48940008 78.43671542]\n",
      "Done with gradient descent at iteration  860\n",
      "Learned weights =  [ 5.802147   91.48939918 78.43671542]\n",
      "Done with gradient descent at iteration  860\n",
      "Learned weights =  [ 5.802147   91.48939918 78.43671452]\n",
      "Done with gradient descent at iteration  861\n",
      "Learned weights =  [ 5.80886714 91.48939918 78.43671452]\n",
      "Done with gradient descent at iteration  861\n",
      "Learned weights =  [ 5.80886714 91.48939829 78.43671452]\n",
      "Done with gradient descent at iteration  861\n",
      "Learned weights =  [ 5.80886714 91.48939829 78.43671361]\n",
      "Done with gradient descent at iteration  862\n",
      "Learned weights =  [ 5.81558728 91.48939829 78.43671361]\n",
      "Done with gradient descent at iteration  862\n",
      "Learned weights =  [ 5.81558728 91.48939739 78.43671361]\n",
      "Done with gradient descent at iteration  862\n",
      "Learned weights =  [ 5.81558728 91.48939739 78.43671271]\n",
      "Done with gradient descent at iteration  863\n",
      "Learned weights =  [ 5.82230742 91.48939739 78.43671271]\n",
      "Done with gradient descent at iteration  863\n",
      "Learned weights =  [ 5.82230742 91.48939649 78.43671271]\n",
      "Done with gradient descent at iteration  863\n",
      "Learned weights =  [ 5.82230742 91.48939649 78.4367118 ]\n",
      "Done with gradient descent at iteration  864\n",
      "Learned weights =  [ 5.82902756 91.48939649 78.4367118 ]\n",
      "Done with gradient descent at iteration  864\n",
      "Learned weights =  [ 5.82902756 91.4893956  78.4367118 ]\n",
      "Done with gradient descent at iteration  864\n",
      "Learned weights =  [ 5.82902756 91.4893956  78.43671089]\n",
      "Done with gradient descent at iteration  865\n",
      "Learned weights =  [ 5.8357477  91.4893956  78.43671089]\n",
      "Done with gradient descent at iteration  865\n",
      "Learned weights =  [ 5.8357477  91.4893947  78.43671089]\n",
      "Done with gradient descent at iteration  865\n",
      "Learned weights =  [ 5.8357477  91.4893947  78.43670999]\n",
      "Done with gradient descent at iteration  866\n",
      "Learned weights =  [ 5.84246785 91.4893947  78.43670999]\n",
      "Done with gradient descent at iteration  866\n",
      "Learned weights =  [ 5.84246785 91.4893938  78.43670999]\n",
      "Done with gradient descent at iteration  866\n",
      "Learned weights =  [ 5.84246785 91.4893938  78.43670908]\n",
      "Done with gradient descent at iteration  867\n",
      "Learned weights =  [ 5.84918799 91.4893938  78.43670908]\n",
      "Done with gradient descent at iteration  867\n",
      "Learned weights =  [ 5.84918799 91.48939291 78.43670908]\n",
      "Done with gradient descent at iteration  867\n",
      "Learned weights =  [ 5.84918799 91.48939291 78.43670818]\n",
      "Done with gradient descent at iteration  868\n",
      "Learned weights =  [ 5.85590813 91.48939291 78.43670818]\n",
      "Done with gradient descent at iteration  868\n",
      "Learned weights =  [ 5.85590813 91.48939201 78.43670818]\n",
      "Done with gradient descent at iteration  868\n",
      "Learned weights =  [ 5.85590813 91.48939201 78.43670727]\n",
      "Done with gradient descent at iteration  869\n",
      "Learned weights =  [ 5.86262827 91.48939201 78.43670727]\n",
      "Done with gradient descent at iteration  869\n",
      "Learned weights =  [ 5.86262827 91.48939111 78.43670727]\n",
      "Done with gradient descent at iteration  869\n",
      "Learned weights =  [ 5.86262827 91.48939111 78.43670636]\n",
      "Done with gradient descent at iteration  870\n",
      "Learned weights =  [ 5.86934841 91.48939111 78.43670636]\n",
      "Done with gradient descent at iteration  870\n",
      "Learned weights =  [ 5.86934841 91.48939021 78.43670636]\n",
      "Done with gradient descent at iteration  870\n",
      "Learned weights =  [ 5.86934841 91.48939021 78.43670546]\n",
      "Done with gradient descent at iteration  871\n",
      "Learned weights =  [ 5.87606855 91.48939021 78.43670546]\n",
      "Done with gradient descent at iteration  871\n",
      "Learned weights =  [ 5.87606855 91.48938932 78.43670546]\n",
      "Done with gradient descent at iteration  871\n",
      "Learned weights =  [ 5.87606855 91.48938932 78.43670455]\n",
      "Done with gradient descent at iteration  872\n",
      "Learned weights =  [ 5.88278869 91.48938932 78.43670455]\n",
      "Done with gradient descent at iteration  872\n",
      "Learned weights =  [ 5.88278869 91.48938842 78.43670455]\n",
      "Done with gradient descent at iteration  872\n",
      "Learned weights =  [ 5.88278869 91.48938842 78.43670365]\n",
      "Done with gradient descent at iteration  873\n",
      "Learned weights =  [ 5.88950883 91.48938842 78.43670365]\n",
      "Done with gradient descent at iteration  873\n",
      "Learned weights =  [ 5.88950883 91.48938752 78.43670365]\n",
      "Done with gradient descent at iteration  873\n",
      "Learned weights =  [ 5.88950883 91.48938752 78.43670274]\n",
      "Done with gradient descent at iteration  874\n",
      "Learned weights =  [ 5.89622897 91.48938752 78.43670274]\n",
      "Done with gradient descent at iteration  874\n",
      "Learned weights =  [ 5.89622897 91.48938663 78.43670274]\n",
      "Done with gradient descent at iteration  874\n",
      "Learned weights =  [ 5.89622897 91.48938663 78.43670183]\n",
      "Done with gradient descent at iteration  875\n",
      "Learned weights =  [ 5.90294911 91.48938663 78.43670183]\n",
      "Done with gradient descent at iteration  875\n",
      "Learned weights =  [ 5.90294911 91.48938573 78.43670183]\n",
      "Done with gradient descent at iteration  875\n",
      "Learned weights =  [ 5.90294911 91.48938573 78.43670093]\n",
      "Done with gradient descent at iteration  876\n",
      "Learned weights =  [ 5.90966925 91.48938573 78.43670093]\n",
      "Done with gradient descent at iteration  876\n",
      "Learned weights =  [ 5.90966925 91.48938483 78.43670093]\n",
      "Done with gradient descent at iteration  876\n",
      "Learned weights =  [ 5.90966925 91.48938483 78.43670002]\n",
      "Done with gradient descent at iteration  877\n",
      "Learned weights =  [ 5.91638939 91.48938483 78.43670002]\n",
      "Done with gradient descent at iteration  877\n",
      "Learned weights =  [ 5.91638939 91.48938394 78.43670002]\n",
      "Done with gradient descent at iteration  877\n",
      "Learned weights =  [ 5.91638939 91.48938394 78.43669912]\n",
      "Done with gradient descent at iteration  878\n",
      "Learned weights =  [ 5.92310953 91.48938394 78.43669912]\n",
      "Done with gradient descent at iteration  878\n",
      "Learned weights =  [ 5.92310953 91.48938304 78.43669912]\n",
      "Done with gradient descent at iteration  878\n",
      "Learned weights =  [ 5.92310953 91.48938304 78.43669821]\n",
      "Done with gradient descent at iteration  879\n",
      "Learned weights =  [ 5.92982967 91.48938304 78.43669821]\n",
      "Done with gradient descent at iteration  879\n",
      "Learned weights =  [ 5.92982967 91.48938214 78.43669821]\n",
      "Done with gradient descent at iteration  879\n",
      "Learned weights =  [ 5.92982967 91.48938214 78.4366973 ]\n",
      "Done with gradient descent at iteration  880\n",
      "Learned weights =  [ 5.93654981 91.48938214 78.4366973 ]\n",
      "Done with gradient descent at iteration  880\n",
      "Learned weights =  [ 5.93654981 91.48938124 78.4366973 ]\n",
      "Done with gradient descent at iteration  880\n",
      "Learned weights =  [ 5.93654981 91.48938124 78.4366964 ]\n",
      "Done with gradient descent at iteration  881\n",
      "Learned weights =  [ 5.94326995 91.48938124 78.4366964 ]\n",
      "Done with gradient descent at iteration  881\n",
      "Learned weights =  [ 5.94326995 91.48938035 78.4366964 ]\n",
      "Done with gradient descent at iteration  881\n",
      "Learned weights =  [ 5.94326995 91.48938035 78.43669549]\n",
      "Done with gradient descent at iteration  882\n",
      "Learned weights =  [ 5.94999009 91.48938035 78.43669549]\n",
      "Done with gradient descent at iteration  882\n",
      "Learned weights =  [ 5.94999009 91.48937945 78.43669549]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  882\n",
      "Learned weights =  [ 5.94999009 91.48937945 78.43669459]\n",
      "Done with gradient descent at iteration  883\n",
      "Learned weights =  [ 5.95671023 91.48937945 78.43669459]\n",
      "Done with gradient descent at iteration  883\n",
      "Learned weights =  [ 5.95671023 91.48937855 78.43669459]\n",
      "Done with gradient descent at iteration  883\n",
      "Learned weights =  [ 5.95671023 91.48937855 78.43669368]\n",
      "Done with gradient descent at iteration  884\n",
      "Learned weights =  [ 5.96343037 91.48937855 78.43669368]\n",
      "Done with gradient descent at iteration  884\n",
      "Learned weights =  [ 5.96343037 91.48937766 78.43669368]\n",
      "Done with gradient descent at iteration  884\n",
      "Learned weights =  [ 5.96343037 91.48937766 78.43669277]\n",
      "Done with gradient descent at iteration  885\n",
      "Learned weights =  [ 5.97015051 91.48937766 78.43669277]\n",
      "Done with gradient descent at iteration  885\n",
      "Learned weights =  [ 5.97015051 91.48937676 78.43669277]\n",
      "Done with gradient descent at iteration  885\n",
      "Learned weights =  [ 5.97015051 91.48937676 78.43669187]\n",
      "Done with gradient descent at iteration  886\n",
      "Learned weights =  [ 5.97687065 91.48937676 78.43669187]\n",
      "Done with gradient descent at iteration  886\n",
      "Learned weights =  [ 5.97687065 91.48937586 78.43669187]\n",
      "Done with gradient descent at iteration  886\n",
      "Learned weights =  [ 5.97687065 91.48937586 78.43669096]\n",
      "Done with gradient descent at iteration  887\n",
      "Learned weights =  [ 5.98359079 91.48937586 78.43669096]\n",
      "Done with gradient descent at iteration  887\n",
      "Learned weights =  [ 5.98359079 91.48937497 78.43669096]\n",
      "Done with gradient descent at iteration  887\n",
      "Learned weights =  [ 5.98359079 91.48937497 78.43669006]\n",
      "Done with gradient descent at iteration  888\n",
      "Learned weights =  [ 5.99031092 91.48937497 78.43669006]\n",
      "Done with gradient descent at iteration  888\n",
      "Learned weights =  [ 5.99031092 91.48937407 78.43669006]\n",
      "Done with gradient descent at iteration  888\n",
      "Learned weights =  [ 5.99031092 91.48937407 78.43668915]\n",
      "Done with gradient descent at iteration  889\n",
      "Learned weights =  [ 5.99703106 91.48937407 78.43668915]\n",
      "Done with gradient descent at iteration  889\n",
      "Learned weights =  [ 5.99703106 91.48937317 78.43668915]\n",
      "Done with gradient descent at iteration  889\n",
      "Learned weights =  [ 5.99703106 91.48937317 78.43668825]\n",
      "Done with gradient descent at iteration  890\n",
      "Learned weights =  [ 6.0037512  91.48937317 78.43668825]\n",
      "Done with gradient descent at iteration  890\n",
      "Learned weights =  [ 6.0037512  91.48937228 78.43668825]\n",
      "Done with gradient descent at iteration  890\n",
      "Learned weights =  [ 6.0037512  91.48937228 78.43668734]\n",
      "Done with gradient descent at iteration  891\n",
      "Learned weights =  [ 6.01047134 91.48937228 78.43668734]\n",
      "Done with gradient descent at iteration  891\n",
      "Learned weights =  [ 6.01047134 91.48937138 78.43668734]\n",
      "Done with gradient descent at iteration  891\n",
      "Learned weights =  [ 6.01047134 91.48937138 78.43668643]\n",
      "Done with gradient descent at iteration  892\n",
      "Learned weights =  [ 6.01719148 91.48937138 78.43668643]\n",
      "Done with gradient descent at iteration  892\n",
      "Learned weights =  [ 6.01719148 91.48937048 78.43668643]\n",
      "Done with gradient descent at iteration  892\n",
      "Learned weights =  [ 6.01719148 91.48937048 78.43668553]\n",
      "Done with gradient descent at iteration  893\n",
      "Learned weights =  [ 6.02391162 91.48937048 78.43668553]\n",
      "Done with gradient descent at iteration  893\n",
      "Learned weights =  [ 6.02391162 91.48936958 78.43668553]\n",
      "Done with gradient descent at iteration  893\n",
      "Learned weights =  [ 6.02391162 91.48936958 78.43668462]\n",
      "Done with gradient descent at iteration  894\n",
      "Learned weights =  [ 6.03063176 91.48936958 78.43668462]\n",
      "Done with gradient descent at iteration  894\n",
      "Learned weights =  [ 6.03063176 91.48936869 78.43668462]\n",
      "Done with gradient descent at iteration  894\n",
      "Learned weights =  [ 6.03063176 91.48936869 78.43668372]\n",
      "Done with gradient descent at iteration  895\n",
      "Learned weights =  [ 6.03735189 91.48936869 78.43668372]\n",
      "Done with gradient descent at iteration  895\n",
      "Learned weights =  [ 6.03735189 91.48936779 78.43668372]\n",
      "Done with gradient descent at iteration  895\n",
      "Learned weights =  [ 6.03735189 91.48936779 78.43668281]\n",
      "Done with gradient descent at iteration  896\n",
      "Learned weights =  [ 6.04407203 91.48936779 78.43668281]\n",
      "Done with gradient descent at iteration  896\n",
      "Learned weights =  [ 6.04407203 91.48936689 78.43668281]\n",
      "Done with gradient descent at iteration  896\n",
      "Learned weights =  [ 6.04407203 91.48936689 78.4366819 ]\n",
      "Done with gradient descent at iteration  897\n",
      "Learned weights =  [ 6.05079217 91.48936689 78.4366819 ]\n",
      "Done with gradient descent at iteration  897\n",
      "Learned weights =  [ 6.05079217 91.489366   78.4366819 ]\n",
      "Done with gradient descent at iteration  897\n",
      "Learned weights =  [ 6.05079217 91.489366   78.436681  ]\n",
      "Done with gradient descent at iteration  898\n",
      "Learned weights =  [ 6.05751231 91.489366   78.436681  ]\n",
      "Done with gradient descent at iteration  898\n",
      "Learned weights =  [ 6.05751231 91.4893651  78.436681  ]\n",
      "Done with gradient descent at iteration  898\n",
      "Learned weights =  [ 6.05751231 91.4893651  78.43668009]\n",
      "Done with gradient descent at iteration  899\n",
      "Learned weights =  [ 6.06423244 91.4893651  78.43668009]\n",
      "Done with gradient descent at iteration  899\n",
      "Learned weights =  [ 6.06423244 91.4893642  78.43668009]\n",
      "Done with gradient descent at iteration  899\n",
      "Learned weights =  [ 6.06423244 91.4893642  78.43667919]\n",
      "Iteration = 900\n",
      "Cost function =  3605254244377257.0\n",
      "Done with gradient descent at iteration  900\n",
      "Learned weights =  [ 6.07095258 91.4893642  78.43667919]\n",
      "Done with gradient descent at iteration  900\n",
      "Learned weights =  [ 6.07095258 91.48936331 78.43667919]\n",
      "Done with gradient descent at iteration  900\n",
      "Learned weights =  [ 6.07095258 91.48936331 78.43667828]\n",
      "Done with gradient descent at iteration  901\n",
      "Learned weights =  [ 6.07767272 91.48936331 78.43667828]\n",
      "Done with gradient descent at iteration  901\n",
      "Learned weights =  [ 6.07767272 91.48936241 78.43667828]\n",
      "Done with gradient descent at iteration  901\n",
      "Learned weights =  [ 6.07767272 91.48936241 78.43667737]\n",
      "Done with gradient descent at iteration  902\n",
      "Learned weights =  [ 6.08439286 91.48936241 78.43667737]\n",
      "Done with gradient descent at iteration  902\n",
      "Learned weights =  [ 6.08439286 91.48936151 78.43667737]\n",
      "Done with gradient descent at iteration  902\n",
      "Learned weights =  [ 6.08439286 91.48936151 78.43667647]\n",
      "Done with gradient descent at iteration  903\n",
      "Learned weights =  [ 6.09111299 91.48936151 78.43667647]\n",
      "Done with gradient descent at iteration  903\n",
      "Learned weights =  [ 6.09111299 91.48936062 78.43667647]\n",
      "Done with gradient descent at iteration  903\n",
      "Learned weights =  [ 6.09111299 91.48936062 78.43667556]\n",
      "Done with gradient descent at iteration  904\n",
      "Learned weights =  [ 6.09783313 91.48936062 78.43667556]\n",
      "Done with gradient descent at iteration  904\n",
      "Learned weights =  [ 6.09783313 91.48935972 78.43667556]\n",
      "Done with gradient descent at iteration  904\n",
      "Learned weights =  [ 6.09783313 91.48935972 78.43667466]\n",
      "Done with gradient descent at iteration  905\n",
      "Learned weights =  [ 6.10455327 91.48935972 78.43667466]\n",
      "Done with gradient descent at iteration  905\n",
      "Learned weights =  [ 6.10455327 91.48935882 78.43667466]\n",
      "Done with gradient descent at iteration  905\n",
      "Learned weights =  [ 6.10455327 91.48935882 78.43667375]\n",
      "Done with gradient descent at iteration  906\n",
      "Learned weights =  [ 6.11127341 91.48935882 78.43667375]\n",
      "Done with gradient descent at iteration  906\n",
      "Learned weights =  [ 6.11127341 91.48935792 78.43667375]\n",
      "Done with gradient descent at iteration  906\n",
      "Learned weights =  [ 6.11127341 91.48935792 78.43667284]\n",
      "Done with gradient descent at iteration  907\n",
      "Learned weights =  [ 6.11799354 91.48935792 78.43667284]\n",
      "Done with gradient descent at iteration  907\n",
      "Learned weights =  [ 6.11799354 91.48935703 78.43667284]\n",
      "Done with gradient descent at iteration  907\n",
      "Learned weights =  [ 6.11799354 91.48935703 78.43667194]\n",
      "Done with gradient descent at iteration  908\n",
      "Learned weights =  [ 6.12471368 91.48935703 78.43667194]\n",
      "Done with gradient descent at iteration  908\n",
      "Learned weights =  [ 6.12471368 91.48935613 78.43667194]\n",
      "Done with gradient descent at iteration  908\n",
      "Learned weights =  [ 6.12471368 91.48935613 78.43667103]\n",
      "Done with gradient descent at iteration  909\n",
      "Learned weights =  [ 6.13143382 91.48935613 78.43667103]\n",
      "Done with gradient descent at iteration  909\n",
      "Learned weights =  [ 6.13143382 91.48935523 78.43667103]\n",
      "Done with gradient descent at iteration  909\n",
      "Learned weights =  [ 6.13143382 91.48935523 78.43667013]\n",
      "Done with gradient descent at iteration  910\n",
      "Learned weights =  [ 6.13815395 91.48935523 78.43667013]\n",
      "Done with gradient descent at iteration  910\n",
      "Learned weights =  [ 6.13815395 91.48935434 78.43667013]\n",
      "Done with gradient descent at iteration  910\n",
      "Learned weights =  [ 6.13815395 91.48935434 78.43666922]\n",
      "Done with gradient descent at iteration  911\n",
      "Learned weights =  [ 6.14487409 91.48935434 78.43666922]\n",
      "Done with gradient descent at iteration  911\n",
      "Learned weights =  [ 6.14487409 91.48935344 78.43666922]\n",
      "Done with gradient descent at iteration  911\n",
      "Learned weights =  [ 6.14487409 91.48935344 78.43666831]\n",
      "Done with gradient descent at iteration  912\n",
      "Learned weights =  [ 6.15159422 91.48935344 78.43666831]\n",
      "Done with gradient descent at iteration  912\n",
      "Learned weights =  [ 6.15159422 91.48935254 78.43666831]\n",
      "Done with gradient descent at iteration  912\n",
      "Learned weights =  [ 6.15159422 91.48935254 78.43666741]\n",
      "Done with gradient descent at iteration  913\n",
      "Learned weights =  [ 6.15831436 91.48935254 78.43666741]\n",
      "Done with gradient descent at iteration  913\n",
      "Learned weights =  [ 6.15831436 91.48935165 78.43666741]\n",
      "Done with gradient descent at iteration  913\n",
      "Learned weights =  [ 6.15831436 91.48935165 78.4366665 ]\n",
      "Done with gradient descent at iteration  914\n",
      "Learned weights =  [ 6.1650345  91.48935165 78.4366665 ]\n",
      "Done with gradient descent at iteration  914\n",
      "Learned weights =  [ 6.1650345  91.48935075 78.4366665 ]\n",
      "Done with gradient descent at iteration  914\n",
      "Learned weights =  [ 6.1650345  91.48935075 78.4366656 ]\n",
      "Done with gradient descent at iteration  915\n",
      "Learned weights =  [ 6.17175463 91.48935075 78.4366656 ]\n",
      "Done with gradient descent at iteration  915\n",
      "Learned weights =  [ 6.17175463 91.48934985 78.4366656 ]\n",
      "Done with gradient descent at iteration  915\n",
      "Learned weights =  [ 6.17175463 91.48934985 78.43666469]\n",
      "Done with gradient descent at iteration  916\n",
      "Learned weights =  [ 6.17847477 91.48934985 78.43666469]\n",
      "Done with gradient descent at iteration  916\n",
      "Learned weights =  [ 6.17847477 91.48934895 78.43666469]\n",
      "Done with gradient descent at iteration  916\n",
      "Learned weights =  [ 6.17847477 91.48934895 78.43666378]\n",
      "Done with gradient descent at iteration  917\n",
      "Learned weights =  [ 6.1851949  91.48934895 78.43666378]\n",
      "Done with gradient descent at iteration  917\n",
      "Learned weights =  [ 6.1851949  91.48934806 78.43666378]\n",
      "Done with gradient descent at iteration  917\n",
      "Learned weights =  [ 6.1851949  91.48934806 78.43666288]\n",
      "Done with gradient descent at iteration  918\n",
      "Learned weights =  [ 6.19191504 91.48934806 78.43666288]\n",
      "Done with gradient descent at iteration  918\n",
      "Learned weights =  [ 6.19191504 91.48934716 78.43666288]\n",
      "Done with gradient descent at iteration  918\n",
      "Learned weights =  [ 6.19191504 91.48934716 78.43666197]\n",
      "Done with gradient descent at iteration  919\n",
      "Learned weights =  [ 6.19863518 91.48934716 78.43666197]\n",
      "Done with gradient descent at iteration  919\n",
      "Learned weights =  [ 6.19863518 91.48934626 78.43666197]\n",
      "Done with gradient descent at iteration  919\n",
      "Learned weights =  [ 6.19863518 91.48934626 78.43666107]\n",
      "Done with gradient descent at iteration  920\n",
      "Learned weights =  [ 6.20535531 91.48934626 78.43666107]\n",
      "Done with gradient descent at iteration  920\n",
      "Learned weights =  [ 6.20535531 91.48934537 78.43666107]\n",
      "Done with gradient descent at iteration  920\n",
      "Learned weights =  [ 6.20535531 91.48934537 78.43666016]\n",
      "Done with gradient descent at iteration  921\n",
      "Learned weights =  [ 6.21207545 91.48934537 78.43666016]\n",
      "Done with gradient descent at iteration  921\n",
      "Learned weights =  [ 6.21207545 91.48934447 78.43666016]\n",
      "Done with gradient descent at iteration  921\n",
      "Learned weights =  [ 6.21207545 91.48934447 78.43665925]\n",
      "Done with gradient descent at iteration  922\n",
      "Learned weights =  [ 6.21879558 91.48934447 78.43665925]\n",
      "Done with gradient descent at iteration  922\n",
      "Learned weights =  [ 6.21879558 91.48934357 78.43665925]\n",
      "Done with gradient descent at iteration  922\n",
      "Learned weights =  [ 6.21879558 91.48934357 78.43665835]\n",
      "Done with gradient descent at iteration  923\n",
      "Learned weights =  [ 6.22551572 91.48934357 78.43665835]\n",
      "Done with gradient descent at iteration  923\n",
      "Learned weights =  [ 6.22551572 91.48934268 78.43665835]\n",
      "Done with gradient descent at iteration  923\n",
      "Learned weights =  [ 6.22551572 91.48934268 78.43665744]\n",
      "Done with gradient descent at iteration  924\n",
      "Learned weights =  [ 6.23223585 91.48934268 78.43665744]\n",
      "Done with gradient descent at iteration  924\n",
      "Learned weights =  [ 6.23223585 91.48934178 78.43665744]\n",
      "Done with gradient descent at iteration  924\n",
      "Learned weights =  [ 6.23223585 91.48934178 78.43665654]\n",
      "Done with gradient descent at iteration  925\n",
      "Learned weights =  [ 6.23895599 91.48934178 78.43665654]\n",
      "Done with gradient descent at iteration  925\n",
      "Learned weights =  [ 6.23895599 91.48934088 78.43665654]\n",
      "Done with gradient descent at iteration  925\n",
      "Learned weights =  [ 6.23895599 91.48934088 78.43665563]\n",
      "Done with gradient descent at iteration  926\n",
      "Learned weights =  [ 6.24567612 91.48934088 78.43665563]\n",
      "Done with gradient descent at iteration  926\n",
      "Learned weights =  [ 6.24567612 91.48933999 78.43665563]\n",
      "Done with gradient descent at iteration  926\n",
      "Learned weights =  [ 6.24567612 91.48933999 78.43665472]\n",
      "Done with gradient descent at iteration  927\n",
      "Learned weights =  [ 6.25239626 91.48933999 78.43665472]\n",
      "Done with gradient descent at iteration  927\n",
      "Learned weights =  [ 6.25239626 91.48933909 78.43665472]\n",
      "Done with gradient descent at iteration  927\n",
      "Learned weights =  [ 6.25239626 91.48933909 78.43665382]\n",
      "Done with gradient descent at iteration  928\n",
      "Learned weights =  [ 6.25911639 91.48933909 78.43665382]\n",
      "Done with gradient descent at iteration  928\n",
      "Learned weights =  [ 6.25911639 91.48933819 78.43665382]\n",
      "Done with gradient descent at iteration  928\n",
      "Learned weights =  [ 6.25911639 91.48933819 78.43665291]\n",
      "Done with gradient descent at iteration  929\n",
      "Learned weights =  [ 6.26583652 91.48933819 78.43665291]\n",
      "Done with gradient descent at iteration  929\n",
      "Learned weights =  [ 6.26583652 91.48933729 78.43665291]\n",
      "Done with gradient descent at iteration  929\n",
      "Learned weights =  [ 6.26583652 91.48933729 78.43665201]\n",
      "Done with gradient descent at iteration  930\n",
      "Learned weights =  [ 6.27255666 91.48933729 78.43665201]\n",
      "Done with gradient descent at iteration  930\n",
      "Learned weights =  [ 6.27255666 91.4893364  78.43665201]\n",
      "Done with gradient descent at iteration  930\n",
      "Learned weights =  [ 6.27255666 91.4893364  78.4366511 ]\n",
      "Done with gradient descent at iteration  931\n",
      "Learned weights =  [ 6.27927679 91.4893364  78.4366511 ]\n",
      "Done with gradient descent at iteration  931\n",
      "Learned weights =  [ 6.27927679 91.4893355  78.4366511 ]\n",
      "Done with gradient descent at iteration  931\n",
      "Learned weights =  [ 6.27927679 91.4893355  78.43665019]\n",
      "Done with gradient descent at iteration  932\n",
      "Learned weights =  [ 6.28599693 91.4893355  78.43665019]\n",
      "Done with gradient descent at iteration  932\n",
      "Learned weights =  [ 6.28599693 91.4893346  78.43665019]\n",
      "Done with gradient descent at iteration  932\n",
      "Learned weights =  [ 6.28599693 91.4893346  78.43664929]\n",
      "Done with gradient descent at iteration  933\n",
      "Learned weights =  [ 6.29271706 91.4893346  78.43664929]\n",
      "Done with gradient descent at iteration  933\n",
      "Learned weights =  [ 6.29271706 91.48933371 78.43664929]\n",
      "Done with gradient descent at iteration  933\n",
      "Learned weights =  [ 6.29271706 91.48933371 78.43664838]\n",
      "Done with gradient descent at iteration  934\n",
      "Learned weights =  [ 6.2994372  91.48933371 78.43664838]\n",
      "Done with gradient descent at iteration  934\n",
      "Learned weights =  [ 6.2994372  91.48933281 78.43664838]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  934\n",
      "Learned weights =  [ 6.2994372  91.48933281 78.43664748]\n",
      "Done with gradient descent at iteration  935\n",
      "Learned weights =  [ 6.30615733 91.48933281 78.43664748]\n",
      "Done with gradient descent at iteration  935\n",
      "Learned weights =  [ 6.30615733 91.48933191 78.43664748]\n",
      "Done with gradient descent at iteration  935\n",
      "Learned weights =  [ 6.30615733 91.48933191 78.43664657]\n",
      "Done with gradient descent at iteration  936\n",
      "Learned weights =  [ 6.31287746 91.48933191 78.43664657]\n",
      "Done with gradient descent at iteration  936\n",
      "Learned weights =  [ 6.31287746 91.48933102 78.43664657]\n",
      "Done with gradient descent at iteration  936\n",
      "Learned weights =  [ 6.31287746 91.48933102 78.43664566]\n",
      "Done with gradient descent at iteration  937\n",
      "Learned weights =  [ 6.3195976  91.48933102 78.43664566]\n",
      "Done with gradient descent at iteration  937\n",
      "Learned weights =  [ 6.3195976  91.48933012 78.43664566]\n",
      "Done with gradient descent at iteration  937\n",
      "Learned weights =  [ 6.3195976  91.48933012 78.43664476]\n",
      "Done with gradient descent at iteration  938\n",
      "Learned weights =  [ 6.32631773 91.48933012 78.43664476]\n",
      "Done with gradient descent at iteration  938\n",
      "Learned weights =  [ 6.32631773 91.48932922 78.43664476]\n",
      "Done with gradient descent at iteration  938\n",
      "Learned weights =  [ 6.32631773 91.48932922 78.43664385]\n",
      "Done with gradient descent at iteration  939\n",
      "Learned weights =  [ 6.33303786 91.48932922 78.43664385]\n",
      "Done with gradient descent at iteration  939\n",
      "Learned weights =  [ 6.33303786 91.48932832 78.43664385]\n",
      "Done with gradient descent at iteration  939\n",
      "Learned weights =  [ 6.33303786 91.48932832 78.43664295]\n",
      "Done with gradient descent at iteration  940\n",
      "Learned weights =  [ 6.339758   91.48932832 78.43664295]\n",
      "Done with gradient descent at iteration  940\n",
      "Learned weights =  [ 6.339758   91.48932743 78.43664295]\n",
      "Done with gradient descent at iteration  940\n",
      "Learned weights =  [ 6.339758   91.48932743 78.43664204]\n",
      "Done with gradient descent at iteration  941\n",
      "Learned weights =  [ 6.34647813 91.48932743 78.43664204]\n",
      "Done with gradient descent at iteration  941\n",
      "Learned weights =  [ 6.34647813 91.48932653 78.43664204]\n",
      "Done with gradient descent at iteration  941\n",
      "Learned weights =  [ 6.34647813 91.48932653 78.43664113]\n",
      "Done with gradient descent at iteration  942\n",
      "Learned weights =  [ 6.35319826 91.48932653 78.43664113]\n",
      "Done with gradient descent at iteration  942\n",
      "Learned weights =  [ 6.35319826 91.48932563 78.43664113]\n",
      "Done with gradient descent at iteration  942\n",
      "Learned weights =  [ 6.35319826 91.48932563 78.43664023]\n",
      "Done with gradient descent at iteration  943\n",
      "Learned weights =  [ 6.3599184  91.48932563 78.43664023]\n",
      "Done with gradient descent at iteration  943\n",
      "Learned weights =  [ 6.3599184  91.48932474 78.43664023]\n",
      "Done with gradient descent at iteration  943\n",
      "Learned weights =  [ 6.3599184  91.48932474 78.43663932]\n",
      "Done with gradient descent at iteration  944\n",
      "Learned weights =  [ 6.36663853 91.48932474 78.43663932]\n",
      "Done with gradient descent at iteration  944\n",
      "Learned weights =  [ 6.36663853 91.48932384 78.43663932]\n",
      "Done with gradient descent at iteration  944\n",
      "Learned weights =  [ 6.36663853 91.48932384 78.43663842]\n",
      "Done with gradient descent at iteration  945\n",
      "Learned weights =  [ 6.37335866 91.48932384 78.43663842]\n",
      "Done with gradient descent at iteration  945\n",
      "Learned weights =  [ 6.37335866 91.48932294 78.43663842]\n",
      "Done with gradient descent at iteration  945\n",
      "Learned weights =  [ 6.37335866 91.48932294 78.43663751]\n",
      "Done with gradient descent at iteration  946\n",
      "Learned weights =  [ 6.38007879 91.48932294 78.43663751]\n",
      "Done with gradient descent at iteration  946\n",
      "Learned weights =  [ 6.38007879 91.48932205 78.43663751]\n",
      "Done with gradient descent at iteration  946\n",
      "Learned weights =  [ 6.38007879 91.48932205 78.4366366 ]\n",
      "Done with gradient descent at iteration  947\n",
      "Learned weights =  [ 6.38679893 91.48932205 78.4366366 ]\n",
      "Done with gradient descent at iteration  947\n",
      "Learned weights =  [ 6.38679893 91.48932115 78.4366366 ]\n",
      "Done with gradient descent at iteration  947\n",
      "Learned weights =  [ 6.38679893 91.48932115 78.4366357 ]\n",
      "Done with gradient descent at iteration  948\n",
      "Learned weights =  [ 6.39351906 91.48932115 78.4366357 ]\n",
      "Done with gradient descent at iteration  948\n",
      "Learned weights =  [ 6.39351906 91.48932025 78.4366357 ]\n",
      "Done with gradient descent at iteration  948\n",
      "Learned weights =  [ 6.39351906 91.48932025 78.43663479]\n",
      "Done with gradient descent at iteration  949\n",
      "Learned weights =  [ 6.40023919 91.48932025 78.43663479]\n",
      "Done with gradient descent at iteration  949\n",
      "Learned weights =  [ 6.40023919 91.48931936 78.43663479]\n",
      "Done with gradient descent at iteration  949\n",
      "Learned weights =  [ 6.40023919 91.48931936 78.43663389]\n",
      "Done with gradient descent at iteration  950\n",
      "Learned weights =  [ 6.40695932 91.48931936 78.43663389]\n",
      "Done with gradient descent at iteration  950\n",
      "Learned weights =  [ 6.40695932 91.48931846 78.43663389]\n",
      "Done with gradient descent at iteration  950\n",
      "Learned weights =  [ 6.40695932 91.48931846 78.43663298]\n",
      "Done with gradient descent at iteration  951\n",
      "Learned weights =  [ 6.41367946 91.48931846 78.43663298]\n",
      "Done with gradient descent at iteration  951\n",
      "Learned weights =  [ 6.41367946 91.48931756 78.43663298]\n",
      "Done with gradient descent at iteration  951\n",
      "Learned weights =  [ 6.41367946 91.48931756 78.43663208]\n",
      "Done with gradient descent at iteration  952\n",
      "Learned weights =  [ 6.42039959 91.48931756 78.43663208]\n",
      "Done with gradient descent at iteration  952\n",
      "Learned weights =  [ 6.42039959 91.48931666 78.43663208]\n",
      "Done with gradient descent at iteration  952\n",
      "Learned weights =  [ 6.42039959 91.48931666 78.43663117]\n",
      "Done with gradient descent at iteration  953\n",
      "Learned weights =  [ 6.42711972 91.48931666 78.43663117]\n",
      "Done with gradient descent at iteration  953\n",
      "Learned weights =  [ 6.42711972 91.48931577 78.43663117]\n",
      "Done with gradient descent at iteration  953\n",
      "Learned weights =  [ 6.42711972 91.48931577 78.43663026]\n",
      "Done with gradient descent at iteration  954\n",
      "Learned weights =  [ 6.43383985 91.48931577 78.43663026]\n",
      "Done with gradient descent at iteration  954\n",
      "Learned weights =  [ 6.43383985 91.48931487 78.43663026]\n",
      "Done with gradient descent at iteration  954\n",
      "Learned weights =  [ 6.43383985 91.48931487 78.43662936]\n",
      "Done with gradient descent at iteration  955\n",
      "Learned weights =  [ 6.44055998 91.48931487 78.43662936]\n",
      "Done with gradient descent at iteration  955\n",
      "Learned weights =  [ 6.44055998 91.48931397 78.43662936]\n",
      "Done with gradient descent at iteration  955\n",
      "Learned weights =  [ 6.44055998 91.48931397 78.43662845]\n",
      "Done with gradient descent at iteration  956\n",
      "Learned weights =  [ 6.44728012 91.48931397 78.43662845]\n",
      "Done with gradient descent at iteration  956\n",
      "Learned weights =  [ 6.44728012 91.48931308 78.43662845]\n",
      "Done with gradient descent at iteration  956\n",
      "Learned weights =  [ 6.44728012 91.48931308 78.43662755]\n",
      "Done with gradient descent at iteration  957\n",
      "Learned weights =  [ 6.45400025 91.48931308 78.43662755]\n",
      "Done with gradient descent at iteration  957\n",
      "Learned weights =  [ 6.45400025 91.48931218 78.43662755]\n",
      "Done with gradient descent at iteration  957\n",
      "Learned weights =  [ 6.45400025 91.48931218 78.43662664]\n",
      "Done with gradient descent at iteration  958\n",
      "Learned weights =  [ 6.46072038 91.48931218 78.43662664]\n",
      "Done with gradient descent at iteration  958\n",
      "Learned weights =  [ 6.46072038 91.48931128 78.43662664]\n",
      "Done with gradient descent at iteration  958\n",
      "Learned weights =  [ 6.46072038 91.48931128 78.43662573]\n",
      "Done with gradient descent at iteration  959\n",
      "Learned weights =  [ 6.46744051 91.48931128 78.43662573]\n",
      "Done with gradient descent at iteration  959\n",
      "Learned weights =  [ 6.46744051 91.48931039 78.43662573]\n",
      "Done with gradient descent at iteration  959\n",
      "Learned weights =  [ 6.46744051 91.48931039 78.43662483]\n",
      "Done with gradient descent at iteration  960\n",
      "Learned weights =  [ 6.47416064 91.48931039 78.43662483]\n",
      "Done with gradient descent at iteration  960\n",
      "Learned weights =  [ 6.47416064 91.48930949 78.43662483]\n",
      "Done with gradient descent at iteration  960\n",
      "Learned weights =  [ 6.47416064 91.48930949 78.43662392]\n",
      "Done with gradient descent at iteration  961\n",
      "Learned weights =  [ 6.48088077 91.48930949 78.43662392]\n",
      "Done with gradient descent at iteration  961\n",
      "Learned weights =  [ 6.48088077 91.48930859 78.43662392]\n",
      "Done with gradient descent at iteration  961\n",
      "Learned weights =  [ 6.48088077 91.48930859 78.43662302]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  962\n",
      "Learned weights =  [ 6.4876009  91.48930859 78.43662302]\n",
      "Done with gradient descent at iteration  962\n",
      "Learned weights =  [ 6.4876009  91.4893077  78.43662302]\n",
      "Done with gradient descent at iteration  962\n",
      "Learned weights =  [ 6.4876009  91.4893077  78.43662211]\n",
      "Done with gradient descent at iteration  963\n",
      "Learned weights =  [ 6.49432103 91.4893077  78.43662211]\n",
      "Done with gradient descent at iteration  963\n",
      "Learned weights =  [ 6.49432103 91.4893068  78.43662211]\n",
      "Done with gradient descent at iteration  963\n",
      "Learned weights =  [ 6.49432103 91.4893068  78.4366212 ]\n",
      "Done with gradient descent at iteration  964\n",
      "Learned weights =  [ 6.50104116 91.4893068  78.4366212 ]\n",
      "Done with gradient descent at iteration  964\n",
      "Learned weights =  [ 6.50104116 91.4893059  78.4366212 ]\n",
      "Done with gradient descent at iteration  964\n",
      "Learned weights =  [ 6.50104116 91.4893059  78.4366203 ]\n",
      "Done with gradient descent at iteration  965\n",
      "Learned weights =  [ 6.5077613 91.4893059 78.4366203]\n",
      "Done with gradient descent at iteration  965\n",
      "Learned weights =  [ 6.5077613 91.489305  78.4366203]\n",
      "Done with gradient descent at iteration  965\n",
      "Learned weights =  [ 6.5077613  91.489305   78.43661939]\n",
      "Done with gradient descent at iteration  966\n",
      "Learned weights =  [ 6.51448143 91.489305   78.43661939]\n",
      "Done with gradient descent at iteration  966\n",
      "Learned weights =  [ 6.51448143 91.48930411 78.43661939]\n",
      "Done with gradient descent at iteration  966\n",
      "Learned weights =  [ 6.51448143 91.48930411 78.43661849]\n",
      "Done with gradient descent at iteration  967\n",
      "Learned weights =  [ 6.52120156 91.48930411 78.43661849]\n",
      "Done with gradient descent at iteration  967\n",
      "Learned weights =  [ 6.52120156 91.48930321 78.43661849]\n",
      "Done with gradient descent at iteration  967\n",
      "Learned weights =  [ 6.52120156 91.48930321 78.43661758]\n",
      "Done with gradient descent at iteration  968\n",
      "Learned weights =  [ 6.52792169 91.48930321 78.43661758]\n",
      "Done with gradient descent at iteration  968\n",
      "Learned weights =  [ 6.52792169 91.48930231 78.43661758]\n",
      "Done with gradient descent at iteration  968\n",
      "Learned weights =  [ 6.52792169 91.48930231 78.43661667]\n",
      "Done with gradient descent at iteration  969\n",
      "Learned weights =  [ 6.53464182 91.48930231 78.43661667]\n",
      "Done with gradient descent at iteration  969\n",
      "Learned weights =  [ 6.53464182 91.48930142 78.43661667]\n",
      "Done with gradient descent at iteration  969\n",
      "Learned weights =  [ 6.53464182 91.48930142 78.43661577]\n",
      "Done with gradient descent at iteration  970\n",
      "Learned weights =  [ 6.54136195 91.48930142 78.43661577]\n",
      "Done with gradient descent at iteration  970\n",
      "Learned weights =  [ 6.54136195 91.48930052 78.43661577]\n",
      "Done with gradient descent at iteration  970\n",
      "Learned weights =  [ 6.54136195 91.48930052 78.43661486]\n",
      "Done with gradient descent at iteration  971\n",
      "Learned weights =  [ 6.54808208 91.48930052 78.43661486]\n",
      "Done with gradient descent at iteration  971\n",
      "Learned weights =  [ 6.54808208 91.48929962 78.43661486]\n",
      "Done with gradient descent at iteration  971\n",
      "Learned weights =  [ 6.54808208 91.48929962 78.43661396]\n",
      "Done with gradient descent at iteration  972\n",
      "Learned weights =  [ 6.55480221 91.48929962 78.43661396]\n",
      "Done with gradient descent at iteration  972\n",
      "Learned weights =  [ 6.55480221 91.48929873 78.43661396]\n",
      "Done with gradient descent at iteration  972\n",
      "Learned weights =  [ 6.55480221 91.48929873 78.43661305]\n",
      "Done with gradient descent at iteration  973\n",
      "Learned weights =  [ 6.56152234 91.48929873 78.43661305]\n",
      "Done with gradient descent at iteration  973\n",
      "Learned weights =  [ 6.56152234 91.48929783 78.43661305]\n",
      "Done with gradient descent at iteration  973\n",
      "Learned weights =  [ 6.56152234 91.48929783 78.43661214]\n",
      "Done with gradient descent at iteration  974\n",
      "Learned weights =  [ 6.56824247 91.48929783 78.43661214]\n",
      "Done with gradient descent at iteration  974\n",
      "Learned weights =  [ 6.56824247 91.48929693 78.43661214]\n",
      "Done with gradient descent at iteration  974\n",
      "Learned weights =  [ 6.56824247 91.48929693 78.43661124]\n",
      "Done with gradient descent at iteration  975\n",
      "Learned weights =  [ 6.5749626  91.48929693 78.43661124]\n",
      "Done with gradient descent at iteration  975\n",
      "Learned weights =  [ 6.5749626  91.48929603 78.43661124]\n",
      "Done with gradient descent at iteration  975\n",
      "Learned weights =  [ 6.5749626  91.48929603 78.43661033]\n",
      "Done with gradient descent at iteration  976\n",
      "Learned weights =  [ 6.58168273 91.48929603 78.43661033]\n",
      "Done with gradient descent at iteration  976\n",
      "Learned weights =  [ 6.58168273 91.48929514 78.43661033]\n",
      "Done with gradient descent at iteration  976\n",
      "Learned weights =  [ 6.58168273 91.48929514 78.43660943]\n",
      "Done with gradient descent at iteration  977\n",
      "Learned weights =  [ 6.58840285 91.48929514 78.43660943]\n",
      "Done with gradient descent at iteration  977\n",
      "Learned weights =  [ 6.58840285 91.48929424 78.43660943]\n",
      "Done with gradient descent at iteration  977\n",
      "Learned weights =  [ 6.58840285 91.48929424 78.43660852]\n",
      "Done with gradient descent at iteration  978\n",
      "Learned weights =  [ 6.59512298 91.48929424 78.43660852]\n",
      "Done with gradient descent at iteration  978\n",
      "Learned weights =  [ 6.59512298 91.48929334 78.43660852]\n",
      "Done with gradient descent at iteration  978\n",
      "Learned weights =  [ 6.59512298 91.48929334 78.43660761]\n",
      "Done with gradient descent at iteration  979\n",
      "Learned weights =  [ 6.60184311 91.48929334 78.43660761]\n",
      "Done with gradient descent at iteration  979\n",
      "Learned weights =  [ 6.60184311 91.48929245 78.43660761]\n",
      "Done with gradient descent at iteration  979\n",
      "Learned weights =  [ 6.60184311 91.48929245 78.43660671]\n",
      "Done with gradient descent at iteration  980\n",
      "Learned weights =  [ 6.60856324 91.48929245 78.43660671]\n",
      "Done with gradient descent at iteration  980\n",
      "Learned weights =  [ 6.60856324 91.48929155 78.43660671]\n",
      "Done with gradient descent at iteration  980\n",
      "Learned weights =  [ 6.60856324 91.48929155 78.4366058 ]\n",
      "Done with gradient descent at iteration  981\n",
      "Learned weights =  [ 6.61528337 91.48929155 78.4366058 ]\n",
      "Done with gradient descent at iteration  981\n",
      "Learned weights =  [ 6.61528337 91.48929065 78.4366058 ]\n",
      "Done with gradient descent at iteration  981\n",
      "Learned weights =  [ 6.61528337 91.48929065 78.4366049 ]\n",
      "Done with gradient descent at iteration  982\n",
      "Learned weights =  [ 6.6220035  91.48929065 78.4366049 ]\n",
      "Done with gradient descent at iteration  982\n",
      "Learned weights =  [ 6.6220035  91.48928976 78.4366049 ]\n",
      "Done with gradient descent at iteration  982\n",
      "Learned weights =  [ 6.6220035  91.48928976 78.43660399]\n",
      "Done with gradient descent at iteration  983\n",
      "Learned weights =  [ 6.62872363 91.48928976 78.43660399]\n",
      "Done with gradient descent at iteration  983\n",
      "Learned weights =  [ 6.62872363 91.48928886 78.43660399]\n",
      "Done with gradient descent at iteration  983\n",
      "Learned weights =  [ 6.62872363 91.48928886 78.43660308]\n",
      "Done with gradient descent at iteration  984\n",
      "Learned weights =  [ 6.63544376 91.48928886 78.43660308]\n",
      "Done with gradient descent at iteration  984\n",
      "Learned weights =  [ 6.63544376 91.48928796 78.43660308]\n",
      "Done with gradient descent at iteration  984\n",
      "Learned weights =  [ 6.63544376 91.48928796 78.43660218]\n",
      "Done with gradient descent at iteration  985\n",
      "Learned weights =  [ 6.64216389 91.48928796 78.43660218]\n",
      "Done with gradient descent at iteration  985\n",
      "Learned weights =  [ 6.64216389 91.48928707 78.43660218]\n",
      "Done with gradient descent at iteration  985\n",
      "Learned weights =  [ 6.64216389 91.48928707 78.43660127]\n",
      "Done with gradient descent at iteration  986\n",
      "Learned weights =  [ 6.64888401 91.48928707 78.43660127]\n",
      "Done with gradient descent at iteration  986\n",
      "Learned weights =  [ 6.64888401 91.48928617 78.43660127]\n",
      "Done with gradient descent at iteration  986\n",
      "Learned weights =  [ 6.64888401 91.48928617 78.43660037]\n",
      "Done with gradient descent at iteration  987\n",
      "Learned weights =  [ 6.65560414 91.48928617 78.43660037]\n",
      "Done with gradient descent at iteration  987\n",
      "Learned weights =  [ 6.65560414 91.48928527 78.43660037]\n",
      "Done with gradient descent at iteration  987\n",
      "Learned weights =  [ 6.65560414 91.48928527 78.43659946]\n",
      "Done with gradient descent at iteration  988\n",
      "Learned weights =  [ 6.66232427 91.48928527 78.43659946]\n",
      "Done with gradient descent at iteration  988\n",
      "Learned weights =  [ 6.66232427 91.48928437 78.43659946]\n",
      "Done with gradient descent at iteration  988\n",
      "Learned weights =  [ 6.66232427 91.48928437 78.43659855]\n",
      "Done with gradient descent at iteration  989\n",
      "Learned weights =  [ 6.6690444  91.48928437 78.43659855]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with gradient descent at iteration  989\n",
      "Learned weights =  [ 6.6690444  91.48928348 78.43659855]\n",
      "Done with gradient descent at iteration  989\n",
      "Learned weights =  [ 6.6690444  91.48928348 78.43659765]\n",
      "Done with gradient descent at iteration  990\n",
      "Learned weights =  [ 6.67576453 91.48928348 78.43659765]\n",
      "Done with gradient descent at iteration  990\n",
      "Learned weights =  [ 6.67576453 91.48928258 78.43659765]\n",
      "Done with gradient descent at iteration  990\n",
      "Learned weights =  [ 6.67576453 91.48928258 78.43659674]\n",
      "Done with gradient descent at iteration  991\n",
      "Learned weights =  [ 6.68248465 91.48928258 78.43659674]\n",
      "Done with gradient descent at iteration  991\n",
      "Learned weights =  [ 6.68248465 91.48928168 78.43659674]\n",
      "Done with gradient descent at iteration  991\n",
      "Learned weights =  [ 6.68248465 91.48928168 78.43659584]\n",
      "Done with gradient descent at iteration  992\n",
      "Learned weights =  [ 6.68920478 91.48928168 78.43659584]\n",
      "Done with gradient descent at iteration  992\n",
      "Learned weights =  [ 6.68920478 91.48928079 78.43659584]\n",
      "Done with gradient descent at iteration  992\n",
      "Learned weights =  [ 6.68920478 91.48928079 78.43659493]\n",
      "Done with gradient descent at iteration  993\n",
      "Learned weights =  [ 6.69592491 91.48928079 78.43659493]\n",
      "Done with gradient descent at iteration  993\n",
      "Learned weights =  [ 6.69592491 91.48927989 78.43659493]\n",
      "Done with gradient descent at iteration  993\n",
      "Learned weights =  [ 6.69592491 91.48927989 78.43659402]\n",
      "Done with gradient descent at iteration  994\n",
      "Learned weights =  [ 6.70264504 91.48927989 78.43659402]\n",
      "Done with gradient descent at iteration  994\n",
      "Learned weights =  [ 6.70264504 91.48927899 78.43659402]\n",
      "Done with gradient descent at iteration  994\n",
      "Learned weights =  [ 6.70264504 91.48927899 78.43659312]\n",
      "Done with gradient descent at iteration  995\n",
      "Learned weights =  [ 6.70936516 91.48927899 78.43659312]\n",
      "Done with gradient descent at iteration  995\n",
      "Learned weights =  [ 6.70936516 91.4892781  78.43659312]\n",
      "Done with gradient descent at iteration  995\n",
      "Learned weights =  [ 6.70936516 91.4892781  78.43659221]\n",
      "Done with gradient descent at iteration  996\n",
      "Learned weights =  [ 6.71608529 91.4892781  78.43659221]\n",
      "Done with gradient descent at iteration  996\n",
      "Learned weights =  [ 6.71608529 91.4892772  78.43659221]\n",
      "Done with gradient descent at iteration  996\n",
      "Learned weights =  [ 6.71608529 91.4892772  78.43659131]\n",
      "Done with gradient descent at iteration  997\n",
      "Learned weights =  [ 6.72280542 91.4892772  78.43659131]\n",
      "Done with gradient descent at iteration  997\n",
      "Learned weights =  [ 6.72280542 91.4892763  78.43659131]\n",
      "Done with gradient descent at iteration  997\n",
      "Learned weights =  [ 6.72280542 91.4892763  78.4365904 ]\n",
      "Done with gradient descent at iteration  998\n",
      "Learned weights =  [ 6.72952555 91.4892763  78.4365904 ]\n",
      "Done with gradient descent at iteration  998\n",
      "Learned weights =  [ 6.72952555 91.48927541 78.4365904 ]\n",
      "Done with gradient descent at iteration  998\n",
      "Learned weights =  [ 6.72952555 91.48927541 78.43658949]\n",
      "Done with gradient descent at iteration  999\n",
      "Learned weights =  [ 6.73624567 91.48927541 78.43658949]\n",
      "Done with gradient descent at iteration  999\n",
      "Learned weights =  [ 6.73624567 91.48927451 78.43658949]\n",
      "Done with gradient descent at iteration  999\n",
      "Learned weights =  [ 6.73624567 91.48927451 78.43658859]\n",
      "Iteration = 1000\n",
      "Cost function =  3605249728359328.5\n",
      "Done with gradient descent at iteration  1000\n",
      "Learned weights =  [ 6.7429658  91.48927451 78.43658859]\n",
      "Done with gradient descent at iteration  1000\n",
      "Learned weights =  [ 6.7429658  91.48927361 78.43658859]\n",
      "Done with gradient descent at iteration  1000\n",
      "Learned weights =  [ 6.7429658  91.48927361 78.43658768]\n"
     ]
    }
   ],
   "source": [
    "multiple_weights_high_penalty = ridge_regression_gradient_descent(feature_matrix, output, initial_weights, step_size, 1e11, max_iterations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the RSS on the TEST data for the following three sets of weights:\n",
    "1. The initial weights (all zeros)\n",
    "2. The weights learned with no regularization\n",
    "3. The weights learned with high regularization\n",
    "\n",
    "Which weights perform best?\n",
    ": multiple weights 0 penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.433051851026171"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((predict_output(feature_matrix, [0,0,0])-output)**2)/1e15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2056078531950627"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((predict_output(feature_matrix, multiple_weights_0_penalty)-output)**2)/1e15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.15299113587998"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((predict_output(feature_matrix, multiple_weights_high_penalty)-output)**2)/1e15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict the house price for the 1st house in the test set using the no regularization and high regularization models. (Remember that python starts indexing from 0.) How far is the prediction from the actual price?  Which weights perform best for the 1st house?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "316839.41522628954"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predict_output(feature_matrix, multiple_weights_0_penalty))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213069.1133217041"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predict_output(feature_matrix, multiple_weights_high_penalty))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221900.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***QUIZ QUESTIONS***\n",
    "1. What is the value of the coefficient for `sqft_living` that you learned with no regularization, rounded to 1 decimal place?  What about the one with high regularization?\n",
    "2. What are the RSS on the test data for each of the set of weights above (initial, no regularization, high regularization)? \n",
    "3. We make prediction for the first house in the test set using two sets of weights (no regularization vs high regularization). Which weights make better prediction <u>for that particular house</u>?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -0.35743482, 243.0541689 ,  22.41481594])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_weights_0_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.7429658 , 91.48927361, 78.43658768])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiple_weights_high_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.784273282524564"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((predict_output(test_feature_matrix, [0,0,0])-test_output)**2)/1e15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.27406761828724496"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((predict_output(test_feature_matrix, multiple_weights_0_penalty)-test_output)**2)/1e15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0040480057955525"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum((predict_output(test_feature_matrix, multiple_weights_high_penalty)-test_output)**2)/1e14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-39546.46969514119"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predict_output(test_feature_matrix, multiple_weights_high_penalty))[0]-test_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77465.47646474396"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predict_output(test_feature_matrix, multiple_weights_0_penalty))[0]-test_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "310000.0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
